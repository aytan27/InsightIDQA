{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alison/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/alison/insight/Insight Fellows_Slack/general/2017-03-25.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import JSON\n",
    "#- Slack data cleaned and ready in an easy to analyze format\n",
    "#Algorithm that identifies questions and answers\n",
    "#Human-readable output list of all questions and answers identified by your algorithm\n",
    "#~90k messages, the format is JSON broken down to one file per month but also subsectioned across channels\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import regex\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#finds json file\n",
    "path_to_json = '/Users/alison/insight/Insight Fellows_Slack/general/'\n",
    "json_filenames = [path_to_json+pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print(json_filenames[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for file_path in json_filenames:\n",
    "   temp_df = pd.read_json(file_path)\n",
    "   df = df.append(temp_df[['type', 'user', 'text', 'ts']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>message</td>\n",
       "      <td>U0AB0AVEF</td>\n",
       "      <td>Not sure if this is a joke or for real… &lt;http:...</td>\n",
       "      <td>1.469199e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>message</td>\n",
       "      <td>U0J794ZUK</td>\n",
       "      <td>That’s a funny joke, but certainly not a viabl...</td>\n",
       "      <td>1.469200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>message</td>\n",
       "      <td>U0J794ZUK</td>\n",
       "      <td>On the subject of fake apps, this was a good o...</td>\n",
       "      <td>1.469200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>message</td>\n",
       "      <td>U03BX0Y4J</td>\n",
       "      <td>&lt;@U03BX0Y4J&gt; uploaded a file: &lt;https://insight...</td>\n",
       "      <td>1.469206e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>message</td>\n",
       "      <td>U03BX0Y4J</td>\n",
       "      <td>(source: _The Ultimate Book of Vehicles: From ...</td>\n",
       "      <td>1.469206e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>message</td>\n",
       "      <td>U0J7S8W0H</td>\n",
       "      <td>Haven’t listened to this yet but it might be i...</td>\n",
       "      <td>1.469220e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>message</td>\n",
       "      <td>U0J7S8W0H</td>\n",
       "      <td>&lt;http://www.npr.org/sections/money/2016/07/22/...</td>\n",
       "      <td>1.469220e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>message</td>\n",
       "      <td>U0ACH145D</td>\n",
       "      <td>Eigenvectors and Eigenvalues: An introduction ...</td>\n",
       "      <td>1.469230e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>message</td>\n",
       "      <td>U3STGMYR1</td>\n",
       "      <td>&lt;@U3STGMYR1&gt; has joined the channel</td>\n",
       "      <td>1.484729e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>message</td>\n",
       "      <td>U3T074LKC</td>\n",
       "      <td>&lt;@U3T074LKC&gt; has joined the channel</td>\n",
       "      <td>1.484730e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type       user                                               text  \\\n",
       "0  message  U0AB0AVEF  Not sure if this is a joke or for real… <http:...   \n",
       "1  message  U0J794ZUK  That’s a funny joke, but certainly not a viabl...   \n",
       "2  message  U0J794ZUK  On the subject of fake apps, this was a good o...   \n",
       "3  message  U03BX0Y4J  <@U03BX0Y4J> uploaded a file: <https://insight...   \n",
       "4  message  U03BX0Y4J  (source: _The Ultimate Book of Vehicles: From ...   \n",
       "5  message  U0J7S8W0H  Haven’t listened to this yet but it might be i...   \n",
       "6  message  U0J7S8W0H  <http://www.npr.org/sections/money/2016/07/22/...   \n",
       "7  message  U0ACH145D  Eigenvectors and Eigenvalues: An introduction ...   \n",
       "0  message  U3STGMYR1                <@U3STGMYR1> has joined the channel   \n",
       "1  message  U3T074LKC                <@U3T074LKC> has joined the channel   \n",
       "\n",
       "             ts  \n",
       "0  1.469199e+09  \n",
       "1  1.469200e+09  \n",
       "2  1.469200e+09  \n",
       "3  1.469206e+09  \n",
       "4  1.469206e+09  \n",
       "5  1.469220e+09  \n",
       "6  1.469220e+09  \n",
       "7  1.469230e+09  \n",
       "0  1.484729e+09  \n",
       "1  1.484730e+09  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.loc[:,\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11936"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "\n",
    "#cleaning throw out \"has joined the channel data\"\n",
    "corpus_clean=[x for x in corpus if \"has joined the channel\" not in x]\n",
    "#while \"has joined the channel\" in corpus: corpus.remove('has joined the channel')\n",
    "\n",
    "#[w for w in corpus if re.search('has joined the channel',w)]\n",
    "\n",
    "#[w for w in corpus if re.search('@[A-Za-z0-9_]+',w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "big_string = ''\n",
    "\n",
    "for combine_list in corpus_clean:\n",
    "    \n",
    "    big_string += combine_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove user tags and URLS\n",
    "tag_re1 = re.sub('<.*?>', '', big_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove \\\\n\n",
    "tag_re3 = re.sub('\\\\n', '', tag_re1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_re4 = re.sub(r\"\\\\\", \"\", tag_re3) #remove \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove :: emojis \n",
    "tag_re5 = re.sub(':[^0-9]+:', '', tag_re3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "re_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "gen_text_tokens = re_tokenizer.tokenize(tag_re5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'sure', 'if', 'this', 'is', 'a', 'joke', 'or', 'for', 'real', 'that', 's', 'a', 'funny', 'joke', 'but', 'certainly', 'not', 'a', 'viable', 'business', 'rates', 'are', 'too', 'low', 'the', 'cheapest', 'plan', 'is', '15', 'a', 'month', 'for', '2', 'scoops', 'a', 'day', 'which', 'comes', 'out', 'to', 'less', 'than', 'a', 'quarter', 'per', 'scoop', 'that', 's', 'not', 'going', 'to', 'be', 'worth', 'anyone', 's', 'trip', 'nor', 'is', 'there', 'any', 'feasible', 'mechanism', 'accountability', 'unless', 'the', 'poopers', 'wait', 'around', 'for', 'the', 'scoopers', 'which', 'the', 'video', 'says', 'they', 'don', 't', 'need', 'to', 'do', 'anything', 'they', 'might', 'do', 'to', 'implement', 'accountability', 'e', 'g', 'require', 'scoopers', 'to', 'stream', 'live', 'videos', 'of', 'the', 'scooping']\n"
     ]
    }
   ],
   "source": [
    "print(gen_text_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "\n",
    "cleaned_tokens = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for token in gen_text_tokens:\n",
    "    if token not in stop_words:\n",
    "        cleaned_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before removing stop words: 128656\n",
      "Number of tokens after removing stop words: 68213\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens before removing stop words: %d' % len(gen_text_tokens))\n",
    "print('Number of tokens after removing stop words: %d' % len(cleaned_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'joke', 'real', 'funny', 'joke', 'certainly', 'viable', 'business', 'rates', 'low', 'cheapest', 'plan', '15', 'month', '2', 'scoops', 'day', 'comes', 'less', 'quarter', 'per', 'scoop', 'going', 'worth', 'anyone', 'trip', 'feasible', 'mechanism', 'accountability', 'unless', 'poopers', 'wait', 'around', 'scoopers', 'video', 'says', 'need', 'anything', 'might', 'implement', 'accountability', 'e', 'g', 'require', 'scoopers', 'stream', 'live', 'videos', 'scooping', 'would']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alison/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "#lemmatize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = []\n",
    "\n",
    "for token in cleaned_tokens:\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'joke', 'real', 'funny', 'joke', 'certainly', 'viable', 'business', 'rate', 'low', 'cheapest', 'plan', '15', 'month', '2', 'scoop', 'day', 'come', 'le', 'quarter', 'per', 'scoop', 'going', 'worth', 'anyone', 'trip', 'feasible', 'mechanism', 'accountability', 'unless', 'poopers', 'wait', 'around', 'scoopers', 'video', 'say', 'need', 'anything', 'might', 'implement', 'accountability', 'e', 'g', 'require', 'scoopers', 'stream', 'live', 'video', 'scooping', 'would']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "lemm_text = ' '.join(wd for wd in lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9545c410df28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfidf_article_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemm_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/insight/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/insight/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             raise ValueError(\n\u001b[0;32m--> 860\u001b[0;31m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m                 \"string object received.\")\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "tfidf_article_matrix = tf_vectorizer.fit_transform(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
