{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alison/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/alison/insight/Insight Fellows_Slack/general/2017-03-25.json\n"
     ]
    }
   ],
   "source": [
    "#import JSON\n",
    "#- Slack data cleaned and ready in an easy to analyze format\n",
    "#Algorithm that identifies questions and answers\n",
    "#Human-readable output list of all questions and answers identified by your algorithm\n",
    "#~90k messages, the format is JSON broken down to one file per month but also subsectioned across channels\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import regex\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#finds json file\n",
    "path_to_json = '/Users/alison/insight/Insight Fellows_Slack/general/'\n",
    "json_filenames = [path_to_json+pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print(json_filenames[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for file_path in json_filenames:\n",
    "   temp_df = pd.read_json(file_path)\n",
    "   df = df.append(temp_df[['type', 'user', 'text', 'ts']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>message</td>\n",
       "      <td>U0AB0AVEF</td>\n",
       "      <td>Not sure if this is a joke or for real… &lt;http:...</td>\n",
       "      <td>1.469199e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>message</td>\n",
       "      <td>U0J794ZUK</td>\n",
       "      <td>That’s a funny joke, but certainly not a viabl...</td>\n",
       "      <td>1.469200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>message</td>\n",
       "      <td>U0J794ZUK</td>\n",
       "      <td>On the subject of fake apps, this was a good o...</td>\n",
       "      <td>1.469200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>message</td>\n",
       "      <td>U03BX0Y4J</td>\n",
       "      <td>&lt;@U03BX0Y4J&gt; uploaded a file: &lt;https://insight...</td>\n",
       "      <td>1.469206e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>message</td>\n",
       "      <td>U03BX0Y4J</td>\n",
       "      <td>(source: _The Ultimate Book of Vehicles: From ...</td>\n",
       "      <td>1.469206e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>message</td>\n",
       "      <td>U0J7S8W0H</td>\n",
       "      <td>Haven’t listened to this yet but it might be i...</td>\n",
       "      <td>1.469220e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>message</td>\n",
       "      <td>U0J7S8W0H</td>\n",
       "      <td>&lt;http://www.npr.org/sections/money/2016/07/22/...</td>\n",
       "      <td>1.469220e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>message</td>\n",
       "      <td>U0ACH145D</td>\n",
       "      <td>Eigenvectors and Eigenvalues: An introduction ...</td>\n",
       "      <td>1.469230e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>message</td>\n",
       "      <td>U3STGMYR1</td>\n",
       "      <td>&lt;@U3STGMYR1&gt; has joined the channel</td>\n",
       "      <td>1.484729e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>message</td>\n",
       "      <td>U3T074LKC</td>\n",
       "      <td>&lt;@U3T074LKC&gt; has joined the channel</td>\n",
       "      <td>1.484730e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type       user                                               text  \\\n",
       "0  message  U0AB0AVEF  Not sure if this is a joke or for real… <http:...   \n",
       "1  message  U0J794ZUK  That’s a funny joke, but certainly not a viabl...   \n",
       "2  message  U0J794ZUK  On the subject of fake apps, this was a good o...   \n",
       "3  message  U03BX0Y4J  <@U03BX0Y4J> uploaded a file: <https://insight...   \n",
       "4  message  U03BX0Y4J  (source: _The Ultimate Book of Vehicles: From ...   \n",
       "5  message  U0J7S8W0H  Haven’t listened to this yet but it might be i...   \n",
       "6  message  U0J7S8W0H  <http://www.npr.org/sections/money/2016/07/22/...   \n",
       "7  message  U0ACH145D  Eigenvectors and Eigenvalues: An introduction ...   \n",
       "0  message  U3STGMYR1                <@U3STGMYR1> has joined the channel   \n",
       "1  message  U3T074LKC                <@U3T074LKC> has joined the channel   \n",
       "\n",
       "             ts  \n",
       "0  1.469199e+09  \n",
       "1  1.469200e+09  \n",
       "2  1.469200e+09  \n",
       "3  1.469206e+09  \n",
       "4  1.469206e+09  \n",
       "5  1.469220e+09  \n",
       "6  1.469220e+09  \n",
       "7  1.469230e+09  \n",
       "0  1.484729e+09  \n",
       "1  1.484730e+09  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.loc[:,\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11936"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "\n",
    "#cleaning throw out \"has joined the channel data\"\n",
    "corpus_clean=[x for x in corpus if \"has joined the channel\" not in x]\n",
    "#while \"has joined the channel\" in corpus: corpus.remove('has joined the channel')\n",
    "\n",
    "#[w for w in corpus if re.search('has joined the channel',w)]\n",
    "\n",
    "#[w for w in corpus if re.search('@[A-Za-z0-9_]+',w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not sure if this is a joke or for real… <http://pooperapp.com> :slightly_smiling_face:',\n",
       " 'that’s a funny joke, but certainly not a viable business. rates are too low. the cheapest plan is $15 a month for 2 scoops a day, which comes out to less than a quarter per scoop. that’s not going to be worth anyone’s trip. nor is there any feasible mechanism accountability, unless the poopers wait around for the scoopers (which the video says they don’t need to do). anything they might do to implement accountability (e.g. require scoopers to stream live videos of the scooping) would only make scooping even less financially attractive.',\n",
       " 'on the subject of fake apps, this was a good one: <http://welldeserved.me/>',\n",
       " '<@u03bx0y4j> uploaded a file: <https://insightfellows.slack.com/files/u03bx0y4j/f1u9ys3pe/screen_shot_2016-07-22_at_9.50.33_am.png|viable business with one of these?>',\n",
       " '(source: _the ultimate book of vehicles: from around the world_)',\n",
       " 'haven’t listened to this yet but it might be interesting',\n",
       " '<http://www.npr.org/sections/money/2016/07/22/487069271/episode-576-when-women-stopped-coding?utm_medium=rss&amp;utm_campaign=planetmoney>',\n",
       " 'eigenvectors and eigenvalues: an introduction for some but beautiful #dataviz for everyone: <http://setosa.io/ev/eigenvectors-and-eigenvalues/>',\n",
       " \"hello past and present fellows!  it's great to be part of the community.  in exploring project ideas this week, i've become interested in uber's movement product as a way to answer some questions.  i requested access last week, but have not heard back.  have any of you alumni worked with the product or know how to expedite access?  thanks! <https://movement.uber.com/cities>\",\n",
       " 'dear past and present fellows. does anyone know of a dataset that they know for sure contains information on people who have experienced falls?',\n",
       " '<@u3th8nehm> here’s something. it’s a paper, but hopefully it has a citation for a dataset that’s publicly available. <https://www.ncbi.nlm.nih.gov/pmc/articles/pmc5012977/>',\n",
       " \"i haven't worked with uber, <@u3t3u8gaf>, but i'm pleased to see that the number of willetts among insight fellows is growing at a rapid rate.\",\n",
       " 'thanks pwinslow!',\n",
       " \"<@u3th8nehm>  here's resources from the cdc <https://www.cdc.gov/homeandrecreationalsafety/falls/pubs.html>\",\n",
       " \"<@u1drb7nd7> see ^^ re falls; don't we know someone that wrote a dissertation on that?\",\n",
       " 'any stock junkies want to join <#c4pny4ysg|finance> with me?',\n",
       " \"you can also kill machines that are performing worse than others because it's been shown that they will be worse in the long run with high probability. but you have to collect the performance of all your machines together.\",\n",
       " 'anyone have an open room they can rent to an insight fellow coming in for the jan 2018 sv session? let me know in this thread if so. super awesome friend coming in. preferable for south bay.',\n",
       " 'good morning, all! can anyone recommend any resources for getting up to speed on kubernetes? ideally, i’d be seeking a tutorial that has a toy example, but other resources would be very helpful too. thanks!',\n",
       " \"i stumbled into this a few months ago:\\n<https://medium.com/intuitionmachine/kubernetes-gpus-tensorflow-8696232862ca>\\nhaven't tried to actually go through the examples, but it seemed a nice introduction.\",\n",
       " 'the oreilly book kubernetes up and running is decent',\n",
       " '<@u0j7gejhk>: some people have found it useful to publish both on medium and linkedin. they reach different audiences.\\n<https://www.linkedin.com/pulse/why-every-blog-post-should-crossposted-linkedin-medium-simon-owens>',\n",
       " '<@u0j7gejhk>: i second <@u055a7xa7>\\'s recommendations. i\\'ve personally had great success with linkedin posts as they act as a sort-of \"portfolio\" project for your ability to communicate complex topics. ',\n",
       " '<https://metrouk2.files.wordpress.com/2015/09/serena.gif>',\n",
       " \"if anyone is interesting in attending this event about unsupervised machine learning  (6.30pm @ sunnyvale) , please let me know.  i can't attend so you could use my tickets.  the event is sold out.  <https://www.eventbrite.com/e/unsupervised-machine-learning-application-to-data-fusion-tickets-30641586811?utm_source=eb_email&amp;utm_medium=email&amp;utm_campaign=reminder_attendees_48hour_email&amp;utm_term=eventname>\",\n",
       " 'correction 6pm-8:45 pm.',\n",
       " \"if you are interested and you don't find me @ slack, shoot me an email at martin.gascon  (gmail)\",\n",
       " \"for your industry-curious friends who did not get to do insight (yet), we're hiring for three (paid) internship positions at my company fast forward labs. we are looking for curious team players, creative builders, code- and wordsmiths. science * and * humanities phds welcome! more info in the job descriptions that are linked in this blog post: <http://blog.fastforwardlabs.com/2017/02/08/summer-internships.html> please refer your friends! thank you!\",\n",
       " \"if you are going to use age categories (i assume 0~10, 10~20, so on) then you will create like 8 orthogonal features after one-hot encoding. it will greatly increase the dimensionality of your dataset and hence not suitable for clustering. why don't you just encode them into (0~10 --&gt; 0, 10~20 --&gt; 1, and so on) and preserve the ordering?\",\n",
       " 'by scaling, i mean for example, if you define male=0 and female=1, then how will you mix it with your continuous features such as lat-long? you can use min-max scaling, or standard scaling, etc. you can tweak the scaling to increase/decrease the effect of gender category on your clustering. this really depends on your use-case, and you have to have some justification of why you use certain scaling method. thats\\' why i said scale features in a way it \"makes sense\".',\n",
       " 'a nice post about identifying trends at instagram: <http://instagram-engineering.tumblr.com/post/122961624217/trending-at-instagram>',\n",
       " '<@u5l7x4lbc>  sure why not ?',\n",
       " '<@u3thca2g7> i cn understand that, the feeling in ny insight was somewhat different.',\n",
       " 'it all depends on the company, and how hierarchical it is, but i\\'d imagine most of us would just be \"data scientist\". typically a senior data scientist role would require some time working in an actual data scientist role. then, there are companies that just don\\'t use the title of senior data scientist at all (you still get promotions, just without title change).  as for the other options, most of us that went through insight came straight from academia, so to land a role as a principal would strike me as odd.',\n",
       " '*principal. :wink:',\n",
       " \"<@u3sqf4e67> i think that's a data scientist with strong ethics\",\n",
       " 'got it! thank you!',\n",
       " 'likely this will depend a lot on the company as well -- i\\'m sure the same person could conceivably be cast as a senior/lead at a small startup, but be a \"data scientist\" at a larger one',\n",
       " 'some people would also argue that titles are only meaningful to make yourself satisfied, and to make it easier for recruiters to find you for your next role :wink:',\n",
       " 'also some larger companies may have pay scales that cap out for certain roles and to attract talent they may give you a title to distinguish you as a phd-haver from data scientists with other degrees and to allow them to pay you a higher salary.',\n",
       " 'ah got it',\n",
       " 'my aws account was recently hacked. approximately $3,500 of ec2 usage charges had accrued by the time i found out about it. for anyone who still has their aws account active, i highly recommend setting up a billing alarm to avoid this happening to you. from the aws console, click on *services*. then under *management tools*, select *cloudwatch.* in the  menu on the left, select *alarms*. i set mine up to email me if the estimated charges exceed $1.',\n",
       " 'i disabled my insight account after reading an article about someone getting their aws account hacked :disappointed: sorry it happened to you!',\n",
       " 'you can also go to *instances*, select your instance then go to *actions*, *cloudwatch monitoring*, *add/edit alarms* :alarm_clock:',\n",
       " '_vps dime_',\n",
       " '<http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html> :cool:',\n",
       " 'closed my account permanently, the end of an era ….',\n",
       " '<@u2964gma5> not sure if that would set up alarms only for that particular instance. in my case the usage was accrued on a bunch of new instances that the hacker launched',\n",
       " 'jesus. they let you out of the bill, right?',\n",
       " 'still waiting to hear back about that',\n",
       " 'from what i’ve read online about past cases like this, they generally waive the bill',\n",
       " 'keeping fingers crossed, <@u3t082v8c> ! <@u28ss4aqg> me too...',\n",
       " 'how do people create hive table from csv files that contains json data in one of its column, where the data of interest for building the hive table is in the json data',\n",
       " '?',\n",
       " 'hey there, do you guys know of implementation of text pattern matching package called autoslog? i am trying to run some name entity recognition task leveraging this package developed from university of utah, but couldn’t find an implementation of it yet.  it basically extracts pattern like:\\n&lt;thing&gt; happened in &lt;location&gt;\\n&lt;name&gt; lives in &lt;town&gt;\\nor a package that does similar thing would be great too.',\n",
       " 'spacy (python package) can do ner',\n",
       " 'i’ve had good luck with stanford’s corenlp as well',\n",
       " \"i'd contact prof. riloff directly about autoslog. \",\n",
       " 'google also has some built-in ner through their nlp api on google cloud, but i haven’t explored it yet',\n",
       " 'thank you guys! i am checking out spacy, corenlp and google nlp, and have sent an email to prof riloff too',\n",
       " '<@u0ad60hnu> i’ve played with spacy for named entity recognition. it does a decent job out of the box. not to hard to figure out. btw, how you doing buddy?',\n",
       " 'to everyone, what are good external method for validating clustering results.',\n",
       " 'i’ve been doing text clustering using word2vec—&gt; affinity propagation (after proper tokenization and all that jaz). the results work pretty well, but i want some advice on how to validate my clusters without to much intensive manual curation.',\n",
       " '<http://www.r2d3.us/visual-intro-to-machine-learning-part-1/>',\n",
       " 'nice visualization in case someone missed it',\n",
       " '<@u1d2rq5fw>: you might want to look into <http://prediction.io|prediction.io> for remote hosting of a classifier too',\n",
       " 'thanks for the input! <@u0ab8tn84> <@u1d5rj46l> ',\n",
       " 'anyone else science marching in nyc?',\n",
       " '<@u3sv9sk1t> you might want to try the <#c06dr6p6h|nyc-alumni> channel!',\n",
       " 'hello, do you guys have any quick thoughts around database management best practices?  that\\'s a bit vague, but it can be very general stuff like \"utilized cloud based services such as aws\", things like that',\n",
       " 'know the difference between olap and oltp.',\n",
       " 'did anyone submit to icml 2017?',\n",
       " 'looking to see if you were able to update the paper submission, or only the response to reviewers',\n",
       " 'i was curious if anyone had any interest in participating in a survey which would collect data from insight fellows anonymously to provide feedback on how compensation, promotion, etc, progresses as time passes after insight. \\n\\nmy feeling is that this aggregate anonymous information can help fellows coming up on their one-year anniversary navigate the whole getting raises thing, with regards to what to expect. if you’d like to contribute answers, please check out the following link:\\n\\n<https://goo.gl/forms/pjczvarwzkimnr553>',\n",
       " '<@u0j791ujc> this looks really useful; it would be great to add location as well, since salaries are heavily geographically dependent',\n",
       " 'i considered location, but in the interest of keeping it anonymous, i opted to not include it',\n",
       " 'ah, fair enough :slightly_smiling_face:',\n",
       " 'can we make it geographically broad...ie midwest, east coast etc.?',\n",
       " 'sure - what geographic regions would you suggest?',\n",
       " 'san francisco bay area, east coast, midwest, west coast',\n",
       " 'anything besides that?',\n",
       " 'maybe just optional write-in?',\n",
       " 'i think i’ll add just an optional write-in',\n",
       " 'i can merge later',\n",
       " 'fyi, link to survey is on my user-profile',\n",
       " 'us geography is weird. which bucket would you put e.g. austin and denver in? those buckets probably cover most cases though',\n",
       " 'i think with the write in now, i can program in some geographic clustering logic later',\n",
       " 'thanks for the responses so far, folks!',\n",
       " 'you could anonymize after and state that in the consent, using wolfram to convert all salaries to sf ',\n",
       " \"without geography i don't think the results are generalizable: <http://m.wolframalpha.com/input/?i=%24150000+salary+austin++to+san+francisco+&amp;x=0&amp;y=0>\",\n",
       " 'cool',\n",
       " 'well, there’s a location field now, but i think people are updating as comfortable',\n",
       " 'live link to summary spreadsheet: <https://docs.google.com/spreadsheets/d/15ak9160s9zbqpguyub1gerzdbzbe7mlbt9ylkjsfwxq/edit?usp=sharing>',\n",
       " '<https://docs.google.com/spreadsheets/d/15ak9160s9zbqpguyub1gerzdbzbe7mlbt9ylkjsfwxq/edit?usp=sharing>',\n",
       " 'submited! what are you trying to build?',\n",
       " \"hi all. former de making transition to ds here. what are your thoughts of using p-values for hypothesis testing? i'm reading islr and it is mentioned all across the first chapters, but when checking out the implementation of the regressors/classifiers in sklearn i see that this feature is not present, which makes me wonder how much relevance do p-values have for nowadays machine learning.\",\n",
       " 'i used collaborative filtering with explicit feedback for my project and ethan’s blog has some good resources for that as well. ',\n",
       " \"in everything i've read and at my company wayfair, p values aren't important for evaluating models these days, only performance stats like r^2, mean absolute error, f1 etc relative to other models and train-test, but we do use p values to evaluate a/b experiment results, along with rigorous power analysis etc.\",\n",
       " 'agree with all of that^ and especially \" but we do use p values to evaluate a/b experiment results, along with rigorous power analysis etc.\"',\n",
       " '<@u055a5e53> thanks much! i may just come your way with questions :) ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "big_string = ''\n",
    "\n",
    "for combine_list in corpus_clean:\n",
    "    \n",
    "    big_string += combine_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove user tags and URLS\n",
    "tag_re1 = re.sub('<.*?>', '', big_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not sure if this is a joke or for real…  :slightly_smiling_face:that’s a funny joke, but certainly not a viable business. rates are too low. the cheapest plan is $15 a month for 2 scoops a day, which comes out to less than a quarter per scoop. that’s not going to be worth anyone’s trip. nor is there any feasible mechanism accountability, unless the poopers wait around for the scoopers (which the video says they don’t need to do). anything they might do to implement accountability (e.g. require scoopers to stream live videos of the scooping) would only make scooping even less financially attractive.on the subject of fake apps, this was a good one:  uploaded a file: (source: _the ultimate book of vehicles: from around the world_)haven’t listened to this yet but it might be interestingeigenvectors and eigenvalues: an introduction for some but beautiful #dataviz for everyone: hello past and present fellows!  it\\'s great to be part of the community.  in exploring project ideas this week, i\\'ve become interested in uber\\'s movement product as a way to answer some questions.  i requested access last week, but have not heard back.  have any of you alumni worked with the product or know how to expedite access?  thanks! dear past and present fellows. does anyone know of a dataset that they know for sure contains information on people who have experienced falls? here’s something. it’s a paper, but hopefully it has a citation for a dataset that’s publicly available. i haven\\'t worked with uber, , but i\\'m pleased to see that the number of willetts among insight fellows is growing at a rapid rate.thanks pwinslow!  here\\'s resources from the cdc  see ^^ re falls; don\\'t we know someone that wrote a dissertation on that?any stock junkies want to join  with me?you can also kill machines that are performing worse than others because it\\'s been shown that they will be worse in the long run with high probability. but you have to collect the performance of all your machines together.anyone have an open room they can rent to an insight fellow coming in for the jan 2018 sv session? let me know in this thread if so. super awesome friend coming in. preferable for south bay.good morning, all! can anyone recommend any resources for getting up to speed on kubernetes? ideally, i’d be seeking a tutorial that has a toy example, but other resources would be very helpful too. thanks!i stumbled into this a few months ago:\\n\\nhaven\\'t tried to actually go through the examples, but it seemed a nice introduction.the oreilly book kubernetes up and running is decent: some people have found it useful to publish both on medium and linkedin. they reach different audiences.\\n: i second \\'s recommendations. i\\'ve personally had great success with linkedin posts as they act as a sort-of \"portfolio\" project for your ability to communicate complex topics. if anyone is interesting in attending this event about unsupervised machine learning  (6.30pm @ sunnyvale) , please let me know.  i can\\'t attend so you could use my tickets.  the event is sold out.  correction 6pm-8:45 pm.if you are interested and you don\\'t find me @ slack, shoot me an email at martin.gascon  (gmail)for your industry-curious friends who did not get to do insight (yet), we\\'re hiring for three (paid) internship positions at my company fast forward labs. we are looking for curious team players, creative builders, code- and wordsmiths. science * and * humanities phds welcome! more info in the job descriptions that are linked in this blog post:  please refer your friends! thank you!if you are going to use age categories (i assume 0~10, 10~20, so on) then you will create like 8 orthogonal features after one-hot encoding. it will greatly increase the dimensionality of your dataset and hence not suitable for clustering. why don\\'t you just encode them into (0~10 --&gt; 0, 10~20 --&gt; 1, and so on) and preserve the ordering?by scaling, i mean for example, if you define male=0 and female=1, then how will you mix it with your continuous features such as lat-long? you can use min-max scaling, or standard scaling, etc. you can tweak the scaling to increase/decrease the effect of gender category on your clustering. this really depends on your use-case, and you have to have some justification of why you use certain scaling method. thats\\' why i said scale features in a way it \"makes sense\".a nice post about identifying trends at instagram:   sure why not ? i cn understand that, the feeling in ny insight was somewhat different.it all depends on the company, and how hierarchical it is, but i\\'d imagine most of us would just be \"data scientist\". typically a senior data scientist role would require some time working in an actual data scientist role. then, there are companies that just don\\'t use the title of senior data scientist at all (you still get promotions, just without title change).  as for the other options, most of us that went through insight came straight from academia, so to land a role as a principal would strike me as odd.*principal. :wink: i think that\\'s a data scientist with strong ethicsgot it! thank you!likely this will depend a lot on the company as well -- i\\'m sure the same person could conceivably be cast as a senior/lead at a small startup, but be a \"data scientist\" at a larger onesome people would also argue that titles are only meaningful to make yourself satisfied, and to make it easier for recruiters to find you for your next role :wink:also some larger companies may have pay scales that cap out for certain roles and to attract talent they may give you a title to distinguish you as a phd-haver from data scientists with other degrees and to allow them to pay you a higher salary.ah got itmy aws account was recently hacked. approximately $3,500 of ec2 usage charges had accrued by the time i found out about it. for anyone who still has their aws account active, i highly recommend setting up a billing alarm to avoid this happening to you. from the aws console, click on *services*. then under *management tools*, select *cloudwatch.* in the  menu on the left, select *alarms*. i set mine up to email me if the estimated charges exceed $1.i disabled my insight account after reading an article about someone getting their aws account hacked :disappointed: sorry it happened to you!you can also go to *instances*, select your instance then go to *actions*, *cloudwatch monitoring*, *add/edit alarms* :alarm_clock:_vps dime_ :cool:closed my account permanently, the end of an era …. not sure if that would set up alarms only for that particular instance. in my case the usage was accrued on a bunch of new instances that the hacker launchedjesus. they let you out of the bill, right?still waiting to hear back about thatfrom what i’ve read online about past cases like this, they generally waive the billkeeping fingers crossed,  !  me too...how do people create hive table from csv files that contains json data in one of its column, where the data of interest for building the hive table is in the json data?hey there, do you guys know of implementation of text pattern matching package called autoslog? i am trying to run some name entity recognition task leveraging this package developed from university of utah, but couldn’t find an implementation of it yet.  it basically extracts pattern like:\\n&lt;thing&gt; happened in &lt;location&gt;\\n&lt;name&gt; lives in &lt;town&gt;\\nor a package that does similar thing would be great too.spacy (python package) can do neri’ve had good luck with stanford’s corenlp as welli\\'d contact prof. riloff directly about autoslog. google also has some built-in ner through their nlp api on google cloud, but i haven’t explored it yetthank you guys! i am checking out spacy, corenlp and google nlp, and have sent an email to prof riloff too i’ve played with spacy for named entity recognition. it does a decent job out of the box. not to hard to figure out. btw, how you doing buddy?to everyone, what are good external method for validating clustering results.i’ve been doing text clustering using word2vec—&gt; affinity propagation (after proper tokenization and all that jaz). the results work pretty well, but i want some advice on how to validate my clusters without to much intensive manual curation.nice visualization in case someone missed it: you might want to look into  for remote hosting of a classifier toothanks for the input!   anyone else science marching in nyc? you might want to try the  channel!hello, do you guys have any quick thoughts around database management best practices?  that\\'s a bit vague, but it can be very general stuff like \"utilized cloud based services such as aws\", things like thatknow the difference between olap and oltp.did anyone submit to icml 2017?looking to see if you were able to update the paper submission, or only the response to reviewersi was curious if anyone had any interest in participating in a survey which would collect data from insight fellows anonymously to provide feedback on how compensation, promotion, etc, progresses as time passes after insight. \\n\\nmy feeling is that this aggregate anonymous information can help fellows coming up on their one-year anniversary navigate the whole getting raises thing, with regards to what to expect. if you’d like to contribute answers, please check out the following link:\\n\\n this looks really useful; it would be great to add location as well, since salaries are heavily geographically dependenti considered location, but in the interest of keeping it anonymous, i opted to not include itah, fair enough :slightly_smiling_face:can we make it geographically broad...ie midwest, east coast etc.?sure - what geographic regions would you suggest?san francisco bay area, east coast, midwest, west coastanything besides that?maybe just optional write-in?i think i’ll add just an optional write-ini can merge laterfyi, link to survey is on my user-profileus geography is weird. which bucket would you put e.g. austin and denver in? those buckets probably cover most cases thoughi think with the write in now, i can program in some geographic clustering logic laterthanks for the responses so far, folks!you could anonymize after and state that in the consent, using wolfram to convert all salaries to sf without geography i don\\'t think the results are generalizable: coolwell, there’s a location field now, but i think people are updating as comfortablelive link to summary spreadsheet: submited! what are you trying to build?hi all. former de making transition to ds here. what are your thoughts of using p-values for hypothesis testing? i\\'m reading islr and it is mentioned all across the first chapters, but when checking out the implementation of the regressors/classifiers in sklearn i see that this feature is not present, which makes me wonder how much relevance do p-values have for nowadays machine learning.i used collaborative filtering with explicit feedback for my project and ethan’s blog has some good resources for that as well. in everything i\\'ve read and at my company wayfair, p values aren\\'t important for evaluating models these days, only performance stats like r^2, mean absolute error, f1 etc relative to other models and train-test, but we do use p values to evaluate a/b experiment results, along with rigorous power analysis etc.agree with all of that^ and especially \" but we do use p values to evaluate a/b experiment results, along with rigorous power analysis etc.\" thanks much! i may just come your way with questions :) p-values are just one piece of evidence to use if you\\'re working with experimental data, smaller sample sizes, or if you\\'re interested in inference. if you want to wade into the p-value debate and replication crisis going on in psychology (and spreading elsewhere), andy gelman\\'s blog is one place to start. sklearn is focused on prediction / ml and not stats.  you might want to use something else if you\\'re asking if model coefficients are significant (or write your own p value thing for sklearn models), but if you\\'re just asking about predictive ability there\\'s other measures.from a more practical perspective, i\\'ve been asked to explain p-values at almost every ds interview i\\'ve ever had. although, i think it\\'s as much about communication skills as about the statistics.who is at data science elevate today?i wish :disappointed:  my whole office is empty hahahi\\'m like teh only one not therei was just traveling last week and again next week, and i actually have to get stuff done at work so couldn\\'t go this timeassuming it is a regression problem, if you are interested in the contribution and strength of each feature (basically the coefficient), then p-value can be informative./poll “what python environment tooling do you use?” “venv” “virtualenv” “virtualenvwrapper” “pipenv” “docker” “i’m a special snowflake” “what’s python?”i don’t see an option where i do \"sudo pip install\" how about conda?+1 for condathere’s also pyenv — i use pyenv, virtualenv, virtualenvwrapper with autoenvmake sure you consider practical as well as statistical significance…with enough data you can make any inconsequential difference hit a small p-value.no anaconda ?maybe that’s the special snowflake optioni don’t use conda but my understanding was people still use virtualenv or similar to handle environments? or does conda have a built in separate way of doing it? :slightly_smiling_face:ah nice, didn’t know about that, there isn’t a way to edit polls so i guess conda folk are special snowflakes for this exercise :smile:¯\\\\_(ツ)_/¯from what i saw pyenv was deprecated in 3.6, but i haven’t used it so i don’t know all the detailsi use docker for real things, conda for super quick thingssame - conda runs my local dev environment, docker for deployingmy understanding is that the `pyvenv` script was deprecated () but `pyenv` () is a stand-alone project. it’s not confusing at all.lol, well good to knowthank you all for these pointers!conda++one can use conda inside docker too :wink:yes ;)watson machine learning is finally generally availablewhat does it do? can we hang our coats on the hook and chill all day by a cup of coffee before we get sacked? :coffee: :coffin:your an exception little pixi you could do it just on your looksanother thing is to add multi-factor authorization. use authy in your phone or something.is that why their retrieve&amp;rank is down? pydata nyc is coming!! \\n\\nif you submit a talk/workshop that gets accepted, then the registration is free!\\n\\nalso, talks &amp; workshops are recorded and posted to youtube too!erm i hope we\\'re all past that phase in our lives when _free registration_ is a perk...i still look for free food quite often. especially pizza.don’t do this modeling myself, but i think a graphical model would be a good approach here. create some graph where nodes are people, and edges are weighted by feature value, then do something like djikstra’s shortest path between any two people.after insight my pizza-capacity has been reduced to a mere slice per day.hey fellows. does anyone have experience with elasticsearch api on python? i need small advice on quering data. please pm me. meet  = es genius! thanks for introduction.i have no idea if this is the same elasticsearch but @ronak has a series of blogposts on elasticsearch hey max, i’ve used the python elasticsearch client a while back. what did you need to know? if your question hasn’t already been answered as i am sure many insight fellows have experience with it.just saw that you got intro’ed to ryan. he knows way more than me. :slightly_smiling_face:hi alumni, we at pixability (boston office) are looking for a sr. data scientist to join and be a leader of our data science team: \\n\\nif you have practical experience leading progress on data science projects, know how to add business value to an organization using your ds and organizational skills, have experience working in and preferably leading a team, and you are willing be the face of _our_ team to the rest of the company, this role may be right for you. direct message me if interested!have any alums made a decision about hiring their own manager to spend more time as a senior ic vs becoming the manager as the team grows? i\\'m around 50% ic and 50% managing/strategy with 2 other ds on the team, but with more coming it\\'s going to be unsustainable at some pointthat\\'s a great idea! thank you, i\\'m trying that now!hey kevin! not in the exact same situation, but i’ve also fallen into the “now i’m the manager…” role and need to decide if i want to go back to ic and get my manager to replace me, or just embrace it.hi everyone, i wrote a general practical nlp guide. it can be a good resource for anyone trying to stand something up quickly that works well in the realms of classification and clustering.it’s just made it to the top of hn, which is exciting :0it\\'s great!#brusheswithfamei guess you’re looking for advice, so i can only really say this: i lead a team of three in my last job, and the biggest mistake i made was using tech work to procrastinate doing management. i would say “man, i need to respond to that customer email, but i’d rather work on this model…” if you find yourself doing that, it’s definitely time to get a manager, or make the jump. :confused:yeh i had to make a similar decision around 4 months ago. the team was growing and it was becoming impossible to wear two hats. ended up staying as an ic on the condition that i could shift between verticals and focus more on long  term strategy for data, projects etc...but no people management responsibilities at all. i\\'d say it\\'s important to make that distinction very clear when bringing in a new manager though (whether internal or external)as for ic vs manager… i wish i had great advice there! still trying to figure it out for myself.in a perfect world, companies would nail the ic / management parallel tracks :(and i just saved that article in feedly...well done! wow tom is alive!?hey all, i have a friend who’s looking to get into sports analytics but not really sure where to start.  she’s a career physicist so she has a good technical base to start from.  anybody know of any resources for training up and/or breaking into that specific subfield? thanks!but even in a perfect world, it’s not a single company issue. you have questions like “am i more marketable as a manager or an ic?” which depends on everyone in the ecosystem, and yourself! :confused: it’s not easy may be a good person to ask! can reach out to fellow megan schroeder (dodgers) or i have friends at hudlyeh i agree with that! funnily enough though, my final decision was facilitated after i interviewed someone for the role of staff r&amp;d engineer (it was one of those interviews  where you end up being the one getting interviewed after 15mins). he was incredibly good, and it did convince/reassure me that if you\\'re really good, then you\\'re basically marketable anywhere regardless of ic/managerjust got to work on getting really good now hahathe hard part!nice, thanks! reached out to megan, and i wonder if we have the same friends at hudl… ex-hep people?thanks april, i hit her up for some info!william spearman, austin basyeyep :slightly_smiling_face: i went to undergrad with will and met austin at cern #smallworldi reached out to both of them, thanks!hey guys! current insight fellow, working on analyzing arterial pulse waveforms. i was wondering if folks have some suggestions on good possible features, particularly in the frequency domain? (i’ve been looking at features related to individual pulses, but have yet to explore frequency-related features. any suggestions would e greatly appreciated!) uploaded a file:  commented on ’s file :  can’t think of any off the top of my head but have you looked at tsfresh? if you haven’t, it might be helpful for feature engineering of your time series data: frequency, number of harmonics…amplitude, area under the fourier transform, min/max frequency at threshold amplitudepeak to peak interval, distribution of peak to peak intervalthis message was deleted.project or product?projecthas anyone used both chart.js and plotly.js (or any other js charting tool) and can give me a recommendation? it will be used in production so need to be fast, work well with data updates and css-customizable preferably. my charts are very simple. thanks.best python library for plotting latitude/longitude data?you might check out `folium`: if by plotting you mean geometric-like pretty plots in 3d, or grid surfaces mapped onto 3d surfaces, maybe think about the vtk library.  it\\'s used by the computer graphics community.to take a binary classifier to a multilabel classifier you could use the onevsrest wrapper in sklearn. it treats each label classification as independent. folium looks like exactly what i need. thanks!speaking of lat/long data, anyone have experience doing choropleth maps in folium? i\\'m having trouble getting it to play nice.we’re hiring! if you are interested in iot (or know someone who is) please dm me. hi all, i was recently lamenting on twitter that even though i’ve been a ds in sf for ~14 months now, i still feel pretty out of the loop as far as conferences go (e.g., i hear about conferences 2 blocks from my office only 3 days before they start!). in astrophysics we had this great, yet fairly simple website () that listed all the astro conferences around the world and would regularly get updated. so my questions to the hive mind are:\\n1) does such a website and/or mailing list exist for ds-related conferences or meetings?\\n2) if the above answer is ‘no’, is there anyone on here who’s interested in working on such a website and/or mailing list? i already have interest from one non-insight ds in sf and i think with a few others we could throw together something that will hopefully be useful!if you are living in the bay area please help my friend with her research by filling out this quick survey - she’s having problems finding a large group of respondents with advanced degrees and i know y’all have that on lock :wink:\\n\\nlink here: this is such a huge oversight in my opinion, thanks for making noise about it!  my boss is telling me he\\'s got money for me to go to conferences and i got nothing...  count me in if you end up trying to build somethingawesome, will do!what big lessons did you learn after you deployed your first machine learning predictive model as a data scientist that helped you improve for your second model?make sure your evaluation matches what the production environment is like. on search, we handle position bias currently as a feature within our model, but in production we assign ranking scores to documents assuming they were in first position. our evaluation, however, didn\\'t adjust the position features for a few weeks until i looked closer at the explanations for the scores. anyway she could make this a bit anonymous? you gotta provide name, dob, and location(almost). thats a 2.5/5 hit on the pii data list. mabbe just take the age?following ’s point, try as much as possible to use canned tools to control such things — the earlier in your process you start having things in self-contained conda envs/docker containers/whatever, the easier it’ll beif you find yourself building a lot of pipelining from scratch/one off coding, good chance you’re reinventing the wheel+1 for ensuring production environment is as close as possible to test environment. \\nwhen this is not possible it becomes important to remove barriers for gaining visibility to production environment, and ability to quickly retrain model and push to product. it’s not birthdate it’s the date you take the survey, i.e. for consent; name will be removed for analysis  thanks :smile: fitm moment!oh haha you’re not the only one, i totally put my birthday when i did the survey, without even thinking of pii. that’s why i’m not in cyber security i guess :wink:hmm, yeah that can get tricky as you could potentially run into correlation-causation issues. honestly i would start with feature engineering and logistic regression, and think about causal analysis (i\\'m no expert) if causation isn\\'t clear.i have some friends at work that are having fun competing to automate a tank maze game. it is 10x16 bits. i thought it would be fun to make a neural network with q learning to play, but the darn thing either goes trigger happy if i weight getting kills too high or spins in a circle if i weight it blowing itself up too highly. i was wondering if i could borrow somebody\\'s expertise for advice.definitely happy to help, as we were just talking about this at insight!this is pretty cool … centralize your access to datageneral question: anyone out there either in a lead/senior ds or hiring role willing to [briefly] act as a sounding board? i\\'m pondering different roles falling within the greater data realm and the trajectory they set one upon. (recently finished the inaugural boston session.) thanks.dropping this here: in terms of aws athena vs spectrum, is the concurrency more auto scaled with athena?  i’m basing this on what i’m reading from  vs hi all, my team at digitalocean is currently growing and we are looking for both a manager and two senior data engineers! if anyone here is interested, or if you know someone that may be, please free to get in touch and i can provide more details!if you\\'ve never heard of digitalocean, we are a cloud infrastructure company, and its a pretty awesome place to work at. the data that we are currently in the process of consuming/analyzing is also super interesting (both customer and machine/hypervisor level data)how does people create emr clusters on spot instances? :question: you can do it directly from the aws emr console.  as you walk through the steps, you\\'ll have to choose what instance types to run.  there is an option on that menu to select spot instances and set your price.  the usual spot instance caveats apply.ah cool, thanks ryan, i was looking at the cli tool, and couldn’t figure that out, will try the console. thanks!and the source for the article: although i\\'m having a bit of trouble distinguishing ```meaning of work: are we working on something that is personally important for each of us?```\\nand \\n```impact of work: do we fundamentally believe that the work we’re doing matters?```\\ncan you believe that the work you\\'re doing fundamentally matters, but not have it be personally important to you? i guess.i think that\\'s called being an intern :smile:eh, i think lots of folks can be in professions that they think are important but not for them. i can\\'t help myself but share this article which says this language is due to economic theories we started to implement in the 1990s. it even highlights the strategic use of \"personal passion\" by recruiters to nudge people to overlook many other positive aspects of jobs to convince them to quit their jobs (and take on new ones, which serves the recruiters).i can find my day to day work (writing good code) satisfying without finding its ultimate purpose (selling ads) importantgood points :point_up:to my brain changing \"meaning of work\" to be something like \"satisfaction with my work\" clicks a bit better for me.of course, also, this is descriptive of successful teams. if there\\'s a team put together to decide the color to paint a bike shed, there\\'s probably little one could do to help the endeavor.red, obviously.i think we\\'ll just go ahead and agree to go with green this time.or the opposite. maybe there\\'s work you really want to do or concepts you really want to learn but it\\'s not necessary for the work you\\'re doing.can i pm you to add to this? this is awesome.*insight\\'s nyc office is moving this week!* if you never returned *your keycard*, we are declaring an amnesty and will be taking them back, no questions asked, until the end of the month. drop yours off in person, or mail it c/o tracy clark, 45 west 25th street, 9th floor, ny ny 10010.looks like you were right! when would this be?this is likely going to be towards the end of the week of the 21st of august.so end of next week.interested in coming?probably, as it sounds very promising. will have to see if i\\'m available then, but will definitely try. just let me know when you hear of a more definite time.\\nthanks for posting this here! and , you should contact terrell, it\\'s possible the file they sent was corrupt: yes i did yesterday and he sent the file, and all works fine now, thanks!: try pyqtgraph or oxyplot if using c#insight alum chris moody just published a paper on his hybrid algo. lda2vec on arxiv: this may be a naive question, but does anyone ever play with different random states when fitting random forests?  i\\'ve noticed nearly a 5% difference in fit based on which random state i use. are hyperparamaters generally considered to make these differences inconsequential or is this something that may be useful in considering?this is on a 33% holdout testing set, not the training set. i suppose k-folds may be more useful for random forests than a simple holdout set though, given the \"randomness\" involved i\\'m operating under the assumption that the more data you have for validation the better. check out i am at the conf. also herelots of great info on the web, i set my own up a few weeks ago for similar reasons. disclaimer: this is by no means official legal/account advice. key notes:\\n1) if you\\'re considering out of state, the \\'best\\' choices are delware, nevada, and wyoming for reasons of taxes, fees, and anonymity (balances between each, i went w/ wyoming).\\n2) filing fees and requires vary a lot.\\n3) depending on how/where/when you do business, you might need to register as state foreign entity (which may tip cost of registering llc + f.agency to more than just registering in the state you live/will work in). best bet is to talk to accountant.\\n4) required if out of state, but either way you\\'ll almost definitely want/need an agent to be official recipient of legal paperwork. services and fees vary wildly, choose wisely.poet rnns\\nthe article that spawned that is also amazing: anybody know how to include \" \" as part of string in hive sql (escape chracter for \" \" ). eg. i want to print               1 \"john\" and i tried select 1, \\\\\"john\\\\\"  and it wouldn\\'t work... any suggestion?better example: hive -e \"select id, \\ncase\\nwhen age&lt;4 then \\\\\"0-4\\\\\"\\nelse \\\\\"5-10\\\\\"\\nend\\nfrom patient_2\"the aim is to print id =1 \"0-4\" but the above just gives 1 0-4 and the no quotesi think strings in hive are actually single quotes and not double quotesso if you run hive -e “select id, case when age&lt;4 then ‘0-4’ else ‘5-10’ end as dim2 from patient_2” it should workreplace dim2 with actual column namethanks  i figured it out and its ==&gt; \\'\\\\\"0-4\\\\\"\\': that returns literally 0-4 in quotes as a stringi’m not sure if you want that.you want 0-4 as a string, not “0-4” as a stringam i correct?i tried it and it gave me \"0-4\"right because it gave you the quotes as wellthe string contains the quoteswhich i don’t think you wantor needyeah thats what i wanted ... i want the string to be there literallyi meant the quotes*alright, if that’s what you wantshiny equivalent for python, fresh off the stitch fix blog: fun article: have you checked out the jsonlite package in r,  ? i have had to do this a few times recentlywhen you also say grouping var, do you mean a categorical variable?: check out tidyjson does anyone know of a good customer event tracking analytics platform? we looked into mixpanel and heap, but are looking for other alternatives.  i like amplitude is the other event analytics platform that comes to mind.  what did/didn\\'t you like about mixpanel and heap?  are you looking to just track and collect the data, or do you want a visualization layer on top of it as well?addepar is looking to hire a data scientist in mountain view. if you are at all interested, let me know and i can make an intro.\\nas many of you probably heard from the insight team, today is the deadline for the next insight session. some of our best applicants come from recommendations from all of you. if you know someone who would be a good fellow in the future, you can dm me or fill out this form:\\nnyt article from yesterday about the desire to hire a.i. research talent: sweetm. who do i go ask about my 500k raise?+250k for scikit-flow uploaded a file: oh right, i need to rsvpdoneanyone planning to go? hi, i\\'m elizabeth from the current new york de session. i am interested in working on a de project that involves biochemistry and/or bioinformatics. if it is possible, i would like to be able to talk to someone one with background about whether or not my project idea is feasible. please pm me. i would really appreciate any and all help i can get! thank you!hi elizabeth.  i\\'d suggest you try summarizing your project idea in a few sentences, and post it here.  then anyone who has the appropriate experience can pick up the discussion.  i think bioinformatics is a pretty broad field, and people have varying expertise in the many topics it encompasses.  for example, i could probably comment on genome-wide association studies, but not on proteomics.seriously would love a copy as well if anyone follows through on this.i can probably make an appointment and get it. i’m just trying to figure out how right nowhas anyone seen any more details than the times article offers?this mashable article makes it sound like it will take a special kind of black magic to get access to that data:  uploaded a file:  and commented: looks like you can download it!hmm slack is acting up on me and sometimes not showing all the links posted. sorry about that.: i think that’s a different data sourcebut super interestingi think that insideairbnb may have scraped public data^ what  saidsweetgotcha! good to know!hi.  curious how people have approached version control + code development on emr + s3 systems when they would like to also have jupyter access to data on s3?i have. but the solution for us was to fork jupyter and create a new format for notebooks that\\'s more git friendly instead of mixing data and code in a json blobbasically, we\\'d only push code to github and the outputs of the notebook would not be part of the code that get\\'s pushed to github... mixing code and output is a bad idea imo since (1) the repo can become huge, (2) i care about doing code diffs mostly and not about data being different from one run to next one cause data gets updatedi have had terrific experience with plotly dashoh, interesting, that\\'s very cool.  so, do you spin up an emr cluster w/ jupyter running and ssh to the master node to run git there?  do you only do code development in jupyter itself?  or, if you have code outside of jupyter, do you edit your non-jupyter code while ssh-ed into the emr master node?let\\'s start with non jupyter first cause that will clarify the problems with jupyter and why we had to create our own jupyter basically...if writing regular code one can use github like one would normally use github... you have your repo on your local computer push to git and then can update the code also on the master node and run the codethat code should run if it was developed with your emr configuration in mindi.e., yarn cluster or any configuration you may haveanyone has a favorite tool to generate an erd from an existing database (redshift in my case)?now with jupyter is more complicated because users are using a webui (aka jupyter) to write code and that code get\\'s saved somewhere in the emr cluster.  jupyter doesn\\'t only save the code but also the output of the notebook in a json format so effectively you are not only saving code but also output.  we didn\\'t like this so we created our own jupyter.  \\nnow if this is not a problem for you then in theory you can have your repo in emr, save the notebooks and push github changes like you\\'d normally do and i think it should workjetbrains datagripgotcha, that\\'s a huge help, thank you so much!thanks!i got this message this morning: ```your file was uploaded — it\\'s safe and sound in slack. unfortunately your workspace doesn\\'t have any storage space left. ``` . wondering if others have gotten the same…maybe insight hit the space limit for storage on the free account?luis -- you\\'re correct :slightly_smiling_face:i think it works like a queue though, so it\\'s the older items that get moved out and we can still share with each otheryeah, message could be worded betterhey all, i got put in contact with a rice u alum who’s finishing her masters in stats in december and is looking to get into a data analyst role. she is in houston currently, but willing to move. if anyone thinks they might have an opening in a couple months or is interested in chatting with her or checking out her linkedin page, let me know!depends what level she\\'s looking for, but we have student opportunities here:and other opps for full-time (not intern or fellowship)well she’ll have a masters in december and no longer be a student after thatlots of other relevant postings in that case :slightly_smiling_face: she could have a look and ping menyc folks: apartment sublet \\na coworker is going to spend a month in our la office so would like to sublet her apartment during that time.\\nthe month in question is flexible: it will be sometime during october to december and is flexible based on finding a good sub-letter!\\nthe apartment is a sunny 1 bed in bay ridge near to 59th st subway (n)\\nif you\\'re interested, or know anyone who might be dm me and i\\'ll connect you!  do me proud with that space time project!space time project?you also have my attention...:rocket:agreed still a mystery to me... unless you don\\'t have enough gpu ram to store all the data and there\\'s loading/unloading going on during the trainingin my mind, ds more general than ml which in turn is a more general concept than dl (which is one of the possible models to fit) - i still think dl is more general than logistic regression/rf/gradient boosting in the sense that dl encompasses multiple types of architectures as well as very different models such as ltsfm versus a regular deep net or just the old and forgotten regular neural net... in my mind a job should say what the objective is and what problem one will be trying to solve... if the problem totally requires dl knowledge then one should be explicit about it but so far i am very skeptical that this would be true about most ds/ml work... as far as ai, well, that in my mind is more related to the problem of creating generally intelligent machines which is still very much in its nascient stage (this only in my opinion) and we still don\\'t know if deep nets or something else will be the right answer to this problem... this is just my mental model on how i think of the ds/ml/ai space... which could be all wrong too jajajaon an unrelated topic, this is why i am more concern about hs than ai, and the misinterpretations and hype around ai too...\\n from what i’ve noticed, bs and *some* ms often do not have the same breadth and depth of knowledge in statistics and machine learning as phds. just like what marco mentioned — “knowing what you don’t know”. (of course, they are much younger:slightly_smiling_face:). and they are mostly hired as analysts or engineers in my company. what i’ve also noticed is that they often follow protocols and prescriptions learned in classes, and do not apply the same rigorous analysis as more experienced researchers would do. for instance, when reporting monthly traffic and unique visitors, no one cared to include uncertainties in their reports, and just announces what looks like noise fluctuations as ups and downs. but on the other hand, perhaps most of the work do not need that much rigor anyway. companies want results not papers. and there’s usually tons of work to be done.i think it’s just too much hype. most of the work we do don’t really need dl and are not suitable for dl. but there seems to be a trend that eveyone should write some keras code to do image classification. what i saw at nips was that a lot of the demoed dl projects have either minimal value (putting some masks on your face, change your photo to some whacky style) or are still in academic settings (generating images). there are applications where dl would shine, but i think the hype is just way too much.hey all, there’s now a  for all european based alumni and usa based alumni who are looking to get outso, what\\'s our responsibility to push back on all the hype?anybody else not have admin permissions on their work machine?  just started at a health care company where i\\'ll be working with patient-level info and can\\'t install any 3rd party software.  i kinda feel like i\\'m trying to work with one hand tied behind my back without all the extra tools i\\'m used to.  curious if others have similar situationsum, i think your responsibility is to learn dl (sorry, i mean ai) and ride that hype train also windows is terrible, amiright?can you dual-boot into a linux env, or do most work remote vis ssh?good thinking, i\\'m gonna ask about that.  i heard windows 10 can install a linux \\'subsystem\\' so you don\\'t even have to dual-bootwhat\\'s the status at spotify? mix of oses? did you get any choice?it\\'s pretty laid back, geared toward whatever makes developers develop faster :wink:ha, \\nsounds nice...also at a healthcare company, windows is standard here. i had to ask for a mac, which they procured easily. i think i had to justify it, the windows pc\\'s we had used windows 7, so no linux sub systemoh man, that sounds amazing... what did you use for \"justification\"? just that you wanted to get actual work done?i\\'m in a small-ish company so i think they may not have anyone who\\'s qualified to support non-standard computing situationsi\\'m on a mac too and can install whatever i want...it almost feels too free sometimes #butilikeit :grinning:#jealousfinance here.  we used to run a heavily sanitized version of cygwin.  my business unit lobbied heavily for macs, so that\\'s what we use now.  i feel your pain.nice.  i\\'ll have to start building some political capital around here to achieve a similar take-oversubsystem does work pretty well for a lot of things if you don’t need access to the gpualso, i highly recommend \\nooohwhat about ubuntu linux on windows?feel ur pain...if u code python/r i\\'d get jetbrains and then just work as if i were on a macyes! just found out about that, gonna see if it will go for itlol almighty it - how do i exercise power? i\\'ll take long on trivial requests, dissaprove rational approvals, and whatnothahalinux subsystem works great if you don\\'t need any gui applications. however, you still need admin permissions to install it.\\na vm would probably work best since you should be able to control what to install on it, and if you configure the mount points right, you should have access to all files in the windows system.good to know, thanks!also healthcare - we work on windows vms to work with patient level data. usually installing 3rd party software requires a call with the system admin and a conversation about how we can’t do our jobs without r/pywindows sucks, vms suckhaha, glad to know i\\'m not the only one! what about things like slack? i\\'m using the web client but maybe you found a way to trick the sys admin into thinking it was a new python tool or something?it\\'s definitely useful to help me do my job!typically just ask for r/rstudio/git. recall i’m on a vm so i can always just switch back to my actual computer for slack and emailaaah, right, makes sense, thanksthough i will never. never. never learn when to control+c vs. command+chaha, i\\'ve already accidentally hit the windows key like a thousand times...so you use your personally-owned computer and work via vm from that?no, i have a work computer that i connect to 10ish different vms (1 per hospital) that can access data for that hospitalooh, i see, so the phi stays on the hospitals\\' end.  for me, my company stores all the data internallyi wasn\\'t able install any ide or for that matter any packages needed. also there\\'s no predefined set of packages that you can ask admin to install for you, as i didn\\'t know what i might try out next. so couldn\\'t really do my job without admin rights, that was the justification, to ask for admin rights on windows until i got the machi y\\'alldoes anyone remember the names of the engineers who gave those quick talks?these talks were about a month agothere was yashashree kokje from yelp and catherine alverado from qventus. can\\'t remember if there was another... judit would know!i was looking for catherine! thank you!hey  !hey folks!\\nwould anyone be up for speaking to a molecular biologist (dna sequencing/quantitative genetics, 5 yrs of industry experience) interested in discussing ways to broaden her skillset/pivot careers? perhaps someone with a genetics, bioinformatics, or molecular bio background? please reach out if so - thanks all!while we\\'re on the subject, there is now a  channel for any/all computational biology/bioinformatics discussion!welcome new fellows!! :wave:: never worked on topic myself but this link seems to be dealing with sim prob: folks i am having issue with  hive over hbase,ubuntu@ip-172-31-1-133:~$ sudo apt-get install hive-hbase\\nreading package lists... done\\nbuilding dependency tree\\nreading state information... done\\ne: unable to locate package hive-hbase\\nubuntu@ip-172-31-1-133:~$ sudo apt-get install hadoop-hive-hbase\\nreading package lists... done\\nbuilding dependency tree\\nreading state information... done\\ne: unable to locate package hadoop-hive-hbasei am trying to install package hive-hbase on  cloudera cdh 5.5cant locate it, if any one has info. pl. let me knowthx: \\nfyi we have 2 channels you might appreciate:   hi all, we’ve been having more conversations on professional growth lately and one topic that stood out is what are some resources to read on for both data engineers and data scientists, any suggestions?usually for me keeping tabs on this slack channel has been useful, but not everyone is an insight alumni :stuck_out_tongue:kenny_kwan: twitter/kaggle/department seminars. mostly just be engaged with the community you have around you should cover it.hi everyone, we’re looking for a few more practicing data scientists (who code analyses and models in languages such as r and python) to collect some feedback in a 20 minute survey, the results of which will be used to directly inform the roadmap for our product.  \\n\\nas i said before, we’re also offering a $25 amazon gift card to everyone who participates.  if you’re interested please dm me, and sorry to ask again in such a short time, but we’ve had some unforeseen openings.  :slightly_smiling_face:  thanks!thanks again for the interest everyone :slightly_smiling_face:  we’ve once again filled out our slots!some people have just too much time on their hands ..  that makes sense i do some combination of that, but it feels like it requires too much time to go through :stuck_out_tongue:it\\'s not reading but i find talking machines and o\\'reilly data show podcasts good for keeping up.   thank you!  i feel much better now. :slightly_smiling_face:  ?about powerpoint thanks those look great i remember subscribind to talking machines at some point. podcasts are things that i definitely need to keep reminding myself of :stuck_out_tongue:if you\\'re into privacy or really into jem and the holograms:  that is truly truly truly outrageousdoes anyone have recommendations for a/b testing analytics platforms? i\\'m in a position to decide what software my company will use for a/b testing, but it\\'s not something i have much experience in. what are the pros and cons? i\\'m aware of issues related to bad statistical practice (e.g. stopping bias) and bookeeping (e.g. keep the bucket assignments forever). what else should i be looking for?lots of deep learning in the (geek) news this week: top 10 rising and falling buzzwords in tech job postings: \"big data\" down, \"real-time data\" and \"artificial intelligence\" up.\\ni, for one, am glad to see \"big data\" die off; it\\'s such a horrid buzzword! :wink:the only time the use of the term \"big data\" is acceptable:\\n: that connection between deep learning and renormalization is incredible, i hadn\\'t seen that before. thanks for sharing!\\n\\ninteresting article on using ml to create photo albums\\n\\nalso, i had no idea there was a disney researcha whole new meaning to mickey mouse research projectshow us gov might prepare for ai-induced economic shifts:\\nsomething to do over the holidays:\\ntil where the oreilly animal pictures come from```many of the animal engravings that appear on o’reilly animal books\\nare from illustrated 18th and 19th century books on natural history,\\nincluding richard lydekker’s royal natural history (1893);\\nreverend j. g. wood’s animate creation (1898), insects abroad (1883),\\nand illustrated natural history (1862); riverside natural history (1884);\\nmeyers kleines lexicon (1894), and the brockhaus lexicon (1882).```(for anyone who\\'s written anything in latex)a non-negligible portion of my grad career was spent wrestling with tables and figure bounding boxes in latex...yes.they didn’t even use $\\\\latex$ in the title of that bookanyone know of openings in the san jose - campbell - santa clara area?  i\\'m in the market for a shorter commute.  thanks.my latex challenge was getting margins to match for my thesisgood morning! does anyone have experience installing qgis via conda in ubunut/lm? i am running into various issues doing so, and believe it may have to do with the pythonpathi\\'ve written a few scope docs. happy to chat! let me know if you\\'re still looking for answers!hello everyone, i want to ask what your experience has been using sensitive data from enterprise clients.\\n - on one side, there is the \"everything on-premise\" deployment: preprocessing, training, testing all happen behind client\\'s firewall.\\n - on the opposite side, there is \"sign nda, get data dump\" policy.\\nwhat has been your experience, and do you have any tips/best practices to share? thanks!prefer to connect directly to their systems with their security policies so that i’m not in any way responsible if somehow security is compromised.hi everyone, de fellow akshay just posted an excellent blog:\\n\\n\\n\\ncomputing shortest distances incrementally with spark\\n\\nit’s also on the front page of hackernews\\n\\n: i just started a new channel for all nyc alumni, i.e., all former fellows living and working in/around nyc. please join  if you match that filter :simple_smile::thumbsup:just saw the discussion above, but i’ve found  to be a pretty decent intro into the why/how of cnns.it doesn’t go too deep, but also isn’t all that shallow either.i see what you did there : \\n“it doesn’t go too deep, but also isn’t all that shallow either.\":slightly_smiling_face:: curious: is that the entirety of the course?ie 3 assignments?: i believe they also get a final project?but i haven’t taken the course so i’m not entirely sure.well meant it in the sense would i get a good experience goign through thoseor if they just stopped updating it halfway through and i\\'d be missing stuff hehstay positive markwangbut it definitely seems usefuloh, i see. no i don’t think it’s incomplete.new folks welcome? :smile: prepare for the onslaught :slightly_smiling_face:we’ll be closer to 800 members by the end of the day!i wonder what is the upper limit for slack.  mabbe we should stress it, just like hadoop.i know i have one channel with around 2000 membersoh boy is it time to haze the new fellows? the 1st rule of fight club is.../giphy fight club uploaded a file: insight only supports shade-grown data scientists how do you propose stress testing slack?! hard to bring down their nodes :wink:preferably grassfed...has anyone here found or bought data on credit scores aggregated by regions in the us? for example, average credit scores by census zone, zip code, etc. i\\'ve found sources, but i\\'d like to hear anyone\\'s thoughts on accuracy, usability, price, and so on.neat use of ml:   just releasedhappy to talk to her/himfinally, stopping gan violence - generative unadversarial networks: \\n&gt;&gt; \"while the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (non) violence popularised by generative adversarial networks have yet to be addressed. [...] experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. our work builds on a rich history of carefully argued position-papers, published as anonymous youtube comments, which prove that the optimal solution to non violence is more guns.\"enjoy: &gt;  unlike previous generators which were content with simply reaching this goal, our generator is more motivated and gives it ‘110%’ moving it a further 10% past the data distribution. while this isn’t terribly helpful from a modelling perspective, we think it shows the right kind of attitude.it\\'s a gem.\"in this work, we have shown that network-on-network violence is not only unethical, it is also\\nunnecessary.\"\\nthis is the trolliest article i\\'ve ever read. lol.lol, i think the ai fellows are full of hate right now….also, why didnt they wait for april 1?sigbovik is always held on april 1st but i think they released the paper earlyhere\\'s a fun one that people liked a couple years ago:  when i cannot scrape natively with scrapy, i use phantom jsthat is call phantomjs inside the scrapy scriptmy team is hiring a senior data scientist: \\nmessage me for a referral or apply directly.anyone who has experience doing social network analysis on large graphs (paths, communities, cliques) ? currently, i\\'m using graph-tool and igraph in python and want to move to a distributed system because the graph is growing very rapidly. i\\'m exploring different technologies and would love to hear your experience with any technology that you have used.i use igraphit\\'s both in r and pythoni think in python there\\'s networkxfor community detection there are quite a few algoslike edge_betweeness, modularity greedy maximization, infomap which uses a random walk surferi\\'m using igraph for now but the graph has grown too big and i\\'m facing scalability issues. so looking for distributed way of doing it now. spark graphx or some graph databasedon\\'t have experience with very big graphs like the one you\\'re mentioning. sorrythanks :slightly_smiling_face:i wonder if there\\'s a way to factorize the adjacency matrix, like cholezky decomposition, or svd, or similarmy 2 centsi\\'m reading papers on distributing adjacency matrix. let\\'s see if that solves my problemmy experience with networkx in python has been great```when it played against the version of alphago that first beat mr lee, it won by 100 games to zero.```some exciting stuff going on at deepmindif net neutrality is a policy you feel strongly about, here\\'s a friendly reminder to contact your representatives. the fcc is likely to announce a vote to strip net neutrality in the coming month.  has more information to help you contact your reps. if you don\\'t have a formed opinion on it, totalbiscuit explains its importance (). thanks to  for bringing it up via fb post.if anyone is interested in tech in africa, come check out the event we\\'re putting on at fenix in january!  i think that’s a good idea. is it something that some companies already have deployed, do you know ? i see such work for hotel, doctors, and restaurants reviews in 2014 ().:smile: that\\'s cuteany vim users out there who have successfully gotten their clipboards to sync over ssh? i.e. can yank from a vim terminal in ssh and paste locally, and vice-a-versa.if this e-commerce place is more of a marketplace, you can use nlp to suggest improvements to sellers\\' item descriptions to make them sell quicker/higher price etc. (ebay does a bit of this, also see  for a related idea)i don’t know a straightforward way of doing this but instead you can scp the file to your local machine or mouse select, command+c, command +v to a new file and then scp the file to your local machine.anybody else going to this conference in chicago next week?   ?would this help? as far as the hmmlearn package goes, i played with it and it didn\\'t work very well. the documentation was poor and some things were broken. i\\'m not sure the creators are still maintaining it.thanks adamazzam, that’s a great idea too, i may mention about that in the talk. i decided to go with the harold’s suggestion for this time because i checked with the research team that they are interested in the topic. thank you guys !i hadn\\'t heard about it till now but it looks interesting!it\\'s on the pricey side, but if nielsen will front the cash you should totally come!:point_up: probably your best beti haven’t seen that before, i’ll have to give that a trytmux works as well, especially if you change the copy/paste bindings to be vim-esquei’ll need to explore tmux more then, i’ve tried a few time and haven’t been able to get the copy paste to work on there, even locallyhey insight! i mostly do model building in the p &gt;&gt; n space, where feature selection is important, but really i work in the p &gt;&gt;&gt; n space, where i have very few samples (think less than 10 with hundreds of features, in some cases). one of the biggest issues i have with the pipelines i construct is validation— if i choose a sane and simple model, i am virtually guaranteed to get 100% training accuracy (which is likely overfitting). cross validation is not really an option with such few samples. \\n\\ni am curious if any yinz have come up with other ways to validate such models in this corner of ml space. the ones i have used so far: first and foremost, domain-specific sanity checks (in microbial ecology, in my case), and also randomly initializing the models a few times and looking at the variance in the feature importances across those initializations (the hope being that if the model is truly doing well, there shouldn’t be high variance in the feature importance of the most important features). any other tips anyone has got?get more data email austen - this is what he does as well i‘m having a really hard time understanding how one can do any validation on less than 10 data points total (if it really is your grand total). i don’t think it should be possible. if you get any lead please do share here because i’m very interested to know!! that’s kind of my point— you can’t. what i am trying to do is find proxies for that validation that are useful would if i couldsounds hard.np-hardmake up a model, assume a flare prior, strap your seat belt and go bayes beast mode. your samples seem like they may be even smaller than this, but it might give you a few ideas. the only thing that comes to mind is leave one out cross validation with model averaging. leverage is also something that may be of interest for small data sets.\\n\\nalso: it seems kind of like a phylogenetic problem - so tree building, where you could assess support using bootstrapping. thanks i had actually read that medium previously; i’ll have a look at the article you linked too i will give that a thought as well, thanks: thanks for the clarification! i know that the roc auc is also a measure of how correctly predictions are \\'ordered\\'. so if i transform this regression problem into a classification one (by normalizing the regressor to turn it into a probability), then the problem is akin to finding an algorithm which has the highest roc auc. do you agree? \\n that is a very helpful article with more useful metrics, thank you!: i guess i agree in some fuzzy philosophical sense that there’s a similarity with the roc score. i’m not sure how that transformation would go exactly, but it seems like a roundabout way of doing things. i think the better approach would be to write a gini coefficient metric for scikit learn. but first, i\\'d check the kaggle forum to see if anybody has posted that code already. (some discussion here, but i haven’t checked anyone’s code yet: )hi  yes, there is a code for it.but there are some wrong codes, gotta be careful :neutral_face:yeah i\\'m just saying i don\\'t know of any are right : here’s a python implementation that looks good to me: . i haven’t tested it, but it looks right (famous last words!) and comments seem favorable.wonder if anyone has tried to access redis database from multiple worker nodes in a spark cluster. the problem is that every rdd that\\'s processed needs to access the database in some way and it needs to connect afresh. the ideal solution is to have a static connection per worker node or better yet, use a connection pool feature that redis provides. redis lets you define a pool of connections and lets you pick one when you need one. the problem is that spark does not let you create a static variable in a worker node. it lets you define a broadcast or accumulator variable in the driver node that can be shared by workers. what i need is a wroker-node-private variable.: one way to do that is to refactor your spark job slightly.  instead of using map/foreach, use mappartition.  this allows you to operate over an entire partition and you can then keep state for the whole partition.  so when you get a new partition in spark, you start a connection pool, then you can call map on the partition to do operations on the rows within the partition and write the results using your redis pool.anyone an expert in nonlinear non-convex optimization? i have a polynomial non-convex cost function and have done some lit search to find possible solutions. i was wondering if someone could share their experiences on best practices.one of our ai fellows who came from a pm background shared some of his experiences being a product manager for ml-driven products and how it differed from regular product management. we\\'d love to hear more from the insight alumni community on other differences you\\'ve seen. hi all. i was a de fellow last fall in nyc. i’ve always been kinda in the middle between ds and de. i finally finished my phd and i just got scheduled interviews with google and apple for ds positions so i wanted to ask for any tips/prep material/suggested readings/sample questions that any of you may know or think are worth revising, since i did a lot of de interview prep during insight but not for ds. everything is appreciated. thanksmost people really like andrew ng’s machine learning course on coursera and udacity’s a/b testing course. both are free.+1 to bothhey insight amigos+amigas. i’m hiring for a data sci position and wanted to post the job description here before posting on our company website. dm me with any questions or interest! also, if you know anyone junior who is looking for a ds summer internship let me know.\\n\\n-----\\n*about us*\\n\\nat stealth security we protect more than 100 million consumer accounts from automated bot attacks and give fortune 100 companies broad real-time security. join our small (growing fast), highly collaborative team who enjoys working on large-scale, challenging and interesting problems. come help us fight malicious bots and defend enterprise’s web, mobile &amp; api endpoints!\\n\\n*responsibilities*\\n\\n* work across teams to frame problems, with an emphasis on deliverables for our products.\\n* perform exploratory data analysis to gain deep understanding of the problems\\n* design and construct statistical, machine learning, and optimisation models\\n* write production code; collaborate with software engineers to implement algorithms and models in production\\n* design and run both simulated and live traffic experiments\\n* analyse experimental and observational data; communicate findings\\n\\n*experience &amp; skills*\\n\\n* m.s. or ph.d. in statistics, mathematics, computer science, physics, or other quantitative fields\\n* 2+ years professional or research experience\\n* passion for solving unstructured and non-standard mathematical problems\\n* end-to-end experience with data, including querying, aggregation, analysis, and visualisation\\n* proficiency with python (preferred), or another interpreted programming language like r.\\n* knowledge of spark desirable.\\n* willingness to collaborate and communicate with others to solve a problemi did coursera’s machine learning a year ago (very good intro level course). i’ll do now the other one. thanksany ds/de peeps looking for remote work?  hmu!, ,  ?anyone at pycon this weekend?at nuna health we’re looking to hire a data science manager in san francisco. we’re trying to lower health care costs and improve quality of care. if you or anyone you know would be interested please let me know. the ideal candidate has been an individual contributor for a couple of years and is ready to transition to the manager track. the job posting is here good luck !hi all, a good friend and former astrophysics collaborator of mine has a son who\\'s a junior math major and cs minor at occidental college in los angeles. he\\'s very interested in the tech industry and is trying to get a summer internship in the bay area. do any of y\\'all have any advice of resources he should look at? or, even better, do any of y\\'all have direct connections with folks (or you yourselves) looking for summer interns? thanks in advance!: where are you scanning the systems from?insight svhey  and previous fellows - anyone have experience enrolling in medi-cal / coveredca as someone who just got here from another state?  were you able to enroll right away? did you have to prove some residency in ca (which i’m not sure i could do at the moment)?  did you just enroll in your previous state’s exchange and pretend you still lived there?i just enrolled in medi-calpretty painless.  ~2 week turnaround?don’t think i had to prove residency.  just provide address.i have a question. can an ordinal feature be used in a  random forest model as an ordinal variable?yes: and  what\\'s the medi-cal website?: when i lived in california last year, i signed up for medi-cal somewhat accidentally (i was just trying to check eligibility), and i’m pretty sure i didn’t provide any proof of residency. i couldn’t figure out how to cancel it until somebody from the office called me; it was actually kind of annoying.in ny, the medicaid stuff was part of the same exchnage processi imagine medi-cal can be done through covered caif you tell them you have no income it probably steers you straight there(former ca resident, doing de in ny for assorted reasons)github is down:grimacing::scream:my favorite part was how github\\'s status page was up, and claiming the site was working for about 10 minutes. :wink:oh godhas anyone came up with a good name for the angry unicorn?twitter had the fail whaledoes anything rhyme with unicorn?forlorn?orangealmost time for ladies in tech nightgithub is back! current fellows who are about to interview: this is a great blog post on engineering principles that you will encounter : very cool (and a really nice walk through/blog post, too). cool stuff! has anyone used bayesian inference (w or w/o deep learning component) in an industry application w. real data? most applications i have found are academic (or fabricated data). anyone of the stichfix folks, perhaps?: i know in the defense industry, they use bayesian inference to detect whether a some object is an actual missile vs a decoy.  not sure if you’ll find public blog posts about it :slightly_smiling_face:i generally try some form of bayesian methods for many of my modeling problems. one recent example that comes to mind is classifying medical abstracts that describe a problem in a patient as being due to an adverse drug event or not. i compared a few standard methods like bag-of-words, tf-idf, etc. to using word2vec to produce a representation map that can be used for document classification. the method is described here:\\nthere isn’t any fancy mcmc, just a simple application of bayes rule. it worked pretty well.: most of my exposure is in the academic setting, but i know foursquare has used bayesian inference in the past. : neat!   thanks for sharing! i’ll look into it.hi guys, i am looking for some good resources on hidden markov models, ideally some lectures or notes along the lines of andrew ng\\'s (i.e. something that can be understood without a phd in maths). suggestions?hi guys, does anyone know of a way to export the results of a google search into a list of urls? i have already looked at their custom search api and it seems that there is a limit on the number of urls you can extract. any help will be greatly appreciated!google is pretty notoriously hard to scrape because they dynamically generate class tags and the like to purposefully keep you from trying to do what you want.i’ve noticed that bing, on the other hand, is quite welcoming to scrapers.probably because they’ll take what they can get in terms of traffic. :slightly_smiling_face:: i second  in that google is extremely hard to scrape, especially since a few months ago. dynamically generated class tags and suchlike can be dealt with given enough work, but google will also block your ip after 100-150 requests. getting around this problem requires a good proxy service that can be very expensive.\\nif all you need is 1-10 search requests per hour and ip blocks are not a problem for you, then let me know. i can share some of my code to get around the dynamic tags problem.: can you share your solution to the dynamic tags anyway? i’d be interested in seeing what you did.:  thanks for your inputs. currently the number of queries is not high and i’m looking for a quick dirty solution before we get our content into an internal indexi’ve been looking into this  but it only gives the top 10 results or so, not all the resulting urls of a query:  let me find that script i wrote, and i will dm you some of itthanks!thank you!first beta release of matplotlib v2.0.0\\nwill i be able to compress and store 2.0 in my head? i certainly wasn\\'t able to with 1.0. :disappointed: you could always tell when i was making a plot by my google searches: \"change axis spacing\" \"change label size\" \"move tick labels\" \"remove legend border\" etc.i’ve committed so many of those commands to memory, , i don’t know if i can take drastic changes to the apinot without an aneurysm at least!nothing nuggetbot?? :stuck_out_tongue:dear alumni, i am a health insight fellow at boston. for the insight project, i am trying to create a predictive model from a dataset that consists of ~2000 features and 10k+ data points. i hope to end up with 10-20 most significant features. does anyone know a robust way of doing feature selection? any suggestion is greatly appreciated. :thinking_face:stay positive higginsc: principal component analysis ? : you can do forward stepwise selection for one, or throw in a l1 regularizer with a large tuning parameterpca will help you reduce your feature space as a place to start...: to add to what others have said you should find the number of dimensions using pca that explain a large fraction (say 95-98%) of your total variance. sklearn has a fraction of variance attribute in its pca class that returns an array of length n-components that you can do a cumulative sum over and find the right cutoff. if you want retain interpretability you should do a model fit with an l1 prior as eddie said and possibly look at the largest feature coefficients. that part is somewhat model specific. however you should be able to determine the most significant feature in any model by dropping a feature and looking at how your fit changes with your feature space projected down to n-1 dimensions. btw, the power for recommendations using this procedure is by doing vector embeddings in a space which naturally preserves heirarchical relationships is really cool: thank you everyone. i will give a try.dear alumni, i am a current health data science fellow at boston. i am trying to find datasets about  running records with multiple users. any suggestions? /giphy find datasets about  running records with multiple users. can you message me your email? i\\'ll send details todaycolaboratory notebooks have a “search stack overflow” button whenever you run into an error… it’s really streamlined my development process will also be here at least one dayquestion for airflow users: what’s your preferred message broker?make sure you don’t compete with the salesforce thrasher on wednesday evening, celeryah, sorry, i asked that wrong. what do you use as the broker?rabbitmqbut i haven\\'t used the other options so i\\'d also be curious what others thinkthose who are going to salesforce, we can figure something out. i thought it was only 1 or 2cool, that’s what i’m leaning toward. thanks!btw if you\\'re planning on running airflow in docker containers this repo has it all: thanks - found that one. was just trying to decide if i should use redis or rabbitmqhi, has anyone used any recommender system in python? which one is a good one to start? the matrix is about 9000 * 18000xi -- can you tell us a bit more about your problem? what are you trying to recommend? what are the features like? how sparse is the data?rabbitmq i\\'m helping put together training material for people to use our software (i\\'m at tamr btw), which involves some machine learning understanding and sql-esque transformations. has anyone used lstm for time series anomaly detection?*herthe  for ai. helpful to navigate the ocean of sensationalist, misleading news articles as well as fearmongers and soothsayers who watch too many sci-fi movies hi ds fellows, i have an idea of analyzing twitter and youtube data specifically the impact of a tweet referring to a video on the popularity of this video on youtube and the number of subscribers. anyone has experience on analyzing twitter/youtube? thanks!for either ds or de fellows… thx!yes, i’m doing something similar for my project, and have quite a bit of infrastructure set up. let’s talk tomorrow. uploaded a file:  and commented: for my immigrant friends on this channel, i am posting this card that explains what to do if ice shows up at your door.  moral:  you have constitutional rights even if you are not a citizen.  the phone numbers are for santa cruz; be sure to have an immigration attorney on speed dial.so anyone know how to delete older files?  i just got a slackbot message saying we are out of space when i tried to upload a small pdf file.your pdf worked -- i think it\\'s just warning you that it\\'s bumping older files out of the cache.in other words, i don\\'t think there\\'s anything different to do. i used postico for some time, but the lack of tab-completion made me switch. i now use pgcli, it\\'s a cli rather than a gui, but do give it a try: i’m setting my project up on aws. does anyone know the easiest way to transfer a postgresql database from my computer to the server? should i use pg_dump? how do i include the security key? thanks!`pg_dump` is probably the way to go. what do you mean by security key?`pg_restore` on the aws serverthat worked perfectly, thanks! at first i thought pg_dump needed aws login info to transfer the db, but i see it just outputs a file that i can transfer. thanks for the help :slightly_smiling_face:insight alum  and i wrote up a little post on our collaborative work together using webcams to measure clothing at stitch fix, with description of the algo included. no deep learning nonsense this time, just straight computer vision: does anyone have any tips for regularizing names in a database. for example, in this they\\'re listed multiple ways \"john smith\", \"smith, john\", \"smith, john a\", \"smith, john a.\".  i\\'m not sure if there\\'s a standard approach for this.: i would spilt the name into first, middle, last then recombine in a standard format. it would probably be good enough for most but not all names. does the db have location info? some countries list surname before given name even without a comma.: in addition to what  suggested, you may want to look at the openrefine library, which performs clustering and better normalization of names, including international character conversion to ascii (e.g. ö -&gt; o)google open sources syntaxnet.  nn for analyzing english textaka parsey mcparsefaceour data team stands “alone” in a weird way. our engineering team is completely independent of the engineering team, but our analysts are mostly embedded to product teams/business lines. in my opinion for a stand alone team i think that it starts with who is leading that team, the person needs to be someone who is pretty much always taking the initiative to make sure that the product and engineering and additional stakeholders are aware of the roadmap and of the data team’s capability. i agree with kevin that depending on whether you get slotted to the engineering or product team then you end up on those buckets. but then that starts limiting what each individual person in the team does, it becomes more “siloed”.thanks kevin, soumya, and kenny.for anyone getting options this is something to keep an eye oni know!!i almost posted the fred wilson article yesterday, but i think this one presents more perspectives how did you try that method in the paper? i mean code-wise..yeah, i liked that it at least mentioned possible prosmy toy model had y be a gaussian distribution whose width depended on the input featuresi can\\'t figure out how to attach a file to this thread, i\\'ll dm it to youok, technical question for you guys. i have an extremely unbalanced dataset (99:1)  for which i undersampled to balance. however, how do you go about normalizing the data? i am testing on my unbalanced dataset so using the same scale feels incorrect. also, if you guys know of any models that are better suited to this problem, let me know!an old colleague of mine put together this blog post on the topic. i recommend giving it a read anyone have experience with ranking/recommender systems in python? i do - what\\'re you working on? awesome!  so i have a set of user attributes, and a ranked list of products tied to each user.  want to be able to reproduce some of the ranking (realistically, only care about the top 5 — there’s 144 products in the db, and any given user has ~10-20 ranked) given the user inputs.  ideally i’m thinking of something like a user-user recommender system, so i don’t have to have the full attribute payload for a useralternately a modeling approach like a bayesian network could workbasically just haven’t worked with that type of outcome beforemy usual go-to is a collaborative filtering approach. if you know which products a user likes, then you can build a user-product matrix and run, say, item-based or user-based collaborative filtering or matrix factorization. this would ignore all of the user attributesfor a more complicated solution, the python package  is pretty awesome and will handle both ranking and the user attributes in a matrix factorization approachcool, i’ll check it out — i think it needs to be attribute-based, cause the reproduced ranking would be for a new user coming in without ranked products — can’t base it on pre-existing rankings, so user-product matrix wouldn’t work if i’m understanding ithave you worked with the surprise package in python at all?ah, gotcha. yeah, lightfm should be able to incorporate attributes and work for new usersspeaking of hiring and recommenders...vevo is hiring for a director of personalization and recommendation. let me know if you\\'re interested in the role.i\\'ve never heard of surprise - looks interesting!likewise for lightfm — thanks!starting my first data scientist job soon and they\\'ve given me the freedom to purchase my own work laptop (with reimbursement). any suggestions on specs? also, i\\'m most comfortable with windows, but started using ubuntu for insight. however, there seems to be a strong mac preference in the data science world. should i start familiarizing myself with mac or stick to ubuntu? thanks! it honestly depends on your role. if often you’ll find yourself having to do some excel work in addition to the more fun stuff, i’d recommend a mac over dual booting ubuntu/windowsi tend to just get the ram-maxed out macbookpro of the month, but if most of your computational/io intensive work will be done on servers / clusters, the air tends to have lots of fans (though maybe that’s changed with the newest macbook pros? i’m out of the loop)there is no inherent benefit to using a mac these days - it’s easy to do both local and remote work on windows os.  i have both windows and mac os for work and the only reason i use mac is because our it people have set things up with that in mind.  might want to find out if that’s true for your work place alsoi haven’t tried the windows bash shell thing, not sure if it’s out yet actually i shoudl update my windows :slightly_smiling_face:  potentially that could close the gap even moreuse a mac so you’ll fit in at data science conferences &gt;_&gt;i have both a macbook and native ubuntu dell for work. for bash/docker/aws there’s little difference, but i still can’t find a good sourcetree equivalent in ubuntu. i feel like your life will be easier with a mac (coming from a guy who prefers ubuntu)thanks for the feedback! looking at some of the on-boarding information, it definitely seems like mac is used by most of the employees. macbook pro of the month it is!i use a macbook pro (and have used many other platforms, though not in the last year or so) and it is great *except* i wish you could get more than 16gb of ram; i’m pretty sure you can’t even now. ram is huge limitation in my experience. just download it :troll:you need to get your boss to fax it to you get a mac  if you are a person who would like to tweak everything, and i mean everything - thinning down kernels, guis, window managers, modules, etc - all of which are possible on linux.  and the reason why i say this is because mac has limited ability to be a playground - so you get lot more *work* done. if not, windows, mac, and linux do not matter. almost all of them have amazing cross compatibility these days.  windows has great support for a bash shell, its tough to find limitations. if you go the windows and linux route consider getting a lenovo x1, you could run win/nix on a intel mac too but its just more expensive. does the lenovo x1 have the same planned obsolescence crap that mac has (soldered ram, etc)? leonvo has the same problem, especially in the gen 4. i have heard people say that you have to choose the socket option instead of the solder option, and then you would be alright ( that costs a bit more moolah, apparently. no data to back the socket version, btw)has anyone used spark-sklearn, and would you recommend it vs. other implementations of spark and sklearn?just to follow up: windows *does not* have awesome support for bash shell. their version has broken symlink support and has since release... there is something to be said for slow patch roll outs, but at some point it\\'s horriblealso, from experience, the right choice us *not* macbook with ubuntu on it... :disappointed: i\\'m running that now and they fight each other pretty badly.yeah i haven’t tried it, i mostly use mobaxterm and remote terminals, hopefully they’ll fix it so local command line stuff is bearablelong time lurker, first time poster here- more on the windows vs mac debate. i noticed that the ctrl key on the mac is in a sucky location. this usually doesn’t matter except i just started using jupyter notebooks, and a lot of the shortcut keys involve using the ctrl key instead of command key. also, it’s really bad when you have to use the ctrl key to crouch when playing counter strike go. after this mac, i intend to go for windows in the future, because they now have bash, and they just have more options, like the yoga/surface tablet convertibles that can also stylus pens for note-takingthis is a super under rated point! :+1: also... i have the keys swapped to the correct position in software... :nerd_face:haha, i swapped it with caps-lock! you can bind `ctrl` to `caps` no?there ya goonly if you don\\'t use :vim: and already have `esc` as `caps` :disappointed:that’s what i have (why would you use `ctrl` in any case?)this has been mentioned on many a forum but the new escape “key” on the mac touchbar isn’t consistently there and can be frustrating. or you could try the new windows linux subsystem, it was developed together with canonical and it might help depending on your needs .  also, you can always get say an i3 or i7 and run linux mint xfce. currently, i\\'m working with an i3 with only 4gb of ram and works very nicely with linux mint 18.1 xfce.the advantage of working with a laptop is that you can upgrade memory, hardrives, etc very easily.fwiw, you can run windows on a mac using the bootcamp partitioning utility.  i set up my old macbook air this way and am quite happy with it.  installation is easy, switching oses is easy, and you get the full windows or mac experience in your active boot session.my laptop running the bash subsystem in windows, is an i7 with 8gm ram, 4gb dedicated video memory , mind you it also does well with games :grinning:wow thanks for all the information everyone! \\n\\nnext up: xbox or playstation? just kidding :laughing: ... was a long time linux user and recently changed to mac when my workplace offered to set up my workstation ... got to say that i\\'m enjoying the experience! the big drawback of the new bash shell on windows is that it has no gui functionality. otherwise, if you would only want some terminal action, including ssh, it works great.you could also have a system set up as a virtual machine, without the need of dual boot. for example, installing ubuntu in a virtual machine inside the windows os.nonetheless, i\\'m not sure how long it would take to learn how to use a mac effectively, if you\\'re coming from windows and linux. anyone with experience on that?does anyone know of a package for logistic regression that will allow you to enter total count and percentage of positive responses? the summary data is about 1100 rows, i could create the full data but it\\'s something like 500,000 rows. i came across someone mentioning that this was possible in sklearn but it didn\\'t explain how and i\\'ve been unable to find any documentation regarding this function.i\\'m trying to predict accuracy, and linear regression is doing pretty well. it has an intercept of 0.9 and all variables reduce this value and none are going out of range. however given that it\\'s unbounded and knowing it\\'s a suboptimal approach i\\'m trying to implement a model using logistic regression.whoa \\n\\nthat made my day.`explain the difference between an inner join and an outer join.\\n  \\nlet\\'s say you have two tables: one with &lt;strong&gt;n&lt;/strong&gt; rows and one with &lt;strong&gt;m&lt;/strong&gt; rows. what are the minimum and maximum numbers of rows that could be generated as results by joining the two of them (one join of any type)?\\n    \\nwhat\\'s the difference between `union` and `union all`? which one executes faster and why? (not super important)\\n      \\nif your query runs slowly, what will you check to try to make it run faster?\\n        \\nwhat is a subquery and when might you use one?\\n\\ncalculate a mediansql questions from derek steer and i are going to be part of a data science career panel, just in case anyone is interested in attending:\\non  naming big data projects: : what language are the pokemon in? it is trying to tell me that avalugg is called crebase...submit a pull request with the json file fixedit\\'s just javascriptloading a json filefinally a way to contribute to the big data community through my love of pokemon :pikachu:also i meant natural language lolpokemon have different names in different languages...oh... hahai think the author is french or something so it\\'s in romanjii was like \"i\\'ve caught all 720 pokemon wtf is a crebase\"hahthat\\'s true, steph has a shiny charmwell technically i don\\'t have some event legendariesthose are required for the shiny charm in black &amp; white 2 thoughbut i am intimately aware of those i do not have and none of them are called \\'crebase\\'... at least in english :stuck_out_tongue_winking_eye:in x &amp; y/or &amp; as event legendaries are not required for the shiny charmwell you\\'re getting sometrue :pikachu:with your help :smile::smile: what are friends forcatching pokemon, obviously :smile:clearlythat quiz is really hard for someone who only played red...quiz?the pkmn or big data onedoes anyone have a macbook pro? how do you like it? does yours have the touch bar?it\\'s ok. touch bar is a pain - but you can always map the keys somewhere else. i like the usb-c power supply though. it takes me a while to get used to the keyboard. still not a big fan of touch bar.the touch bar is my main concern. would you buy it again?i hate it. usb-c is a downgrade, touch bar is a serious downgrade, keyboard is a downgrade, and the larger trackpad is annoying and feels like it has worse palm rejection. on the positive side, it is 10% thinner (barf)  i haven’t found much use for the touch bar yet, and it can be annoying at first (accidentally calling siri happened a lot before i removed it from the bar). i think the keyboard is great, but it did take a bit of an adjustment. i really like the large trackpad, and haven’t had any problems with it.  how much ram does your machine have?8 gbwould you rather have the macbook pro or the macbook air?8 gb. macbook pro. i\\'m happy with my decision.i just got one (3 weeks ago) and i am happy overall.  i am pretty ambivalent on the touch bar - i used it to escape and control the volume mostly, but occasionally there are fun features (like navigate among weeks in google calendar).  i like the big track pad, but was kinda bummed about the usb-c.  i bought a converter ($20) for my thumb drive and other things.same here. i have to get a usb-c converter for my mouse.the keyboard is really loud, the touchbar pretty gimmicky. i like the big trackpad actually, but i’ve dealt with daily crashes (possibly a bad video card, which is common enough … just started the process of getting it looked at). i’d trade it in for a comparable non-apple linux machine in a heartbeat, but i don’t know which brand stands behind its hardware the best anymore.: being that it is a cloud warehouse the advantage is that any data coming into domo can be automatically subject to their magic etl workflows, sliced and diced and thrown into a preconfigured view. the views i.e graphs themselves are mature , based on d3 and have some specialized functionalities like the ability to drill down into a part of the dataset . once the listening jdbc/odbc connectors are setup they permanently listen to that data source. the differential advantage comes from being able to leverage the warehousing capabilities rather than just the dashboards. also you can push and pull data and custom viz in python and r to domo.thanks  for the informative answer. i think i\\'ll sign up for the free account and try it out. thanks stephanieany one with ear phones that he/she is not using? i want to borrow for today: this is the chat for _all_ fellows and alumni  — in sv, nyc, and boston. you may have more luck asking under your specific channel (de-ny-2016a / ds-ny-2016a) :simple_smile:thankssv people! you should check out this event from pintrest about machine learning on tuesday the 9th: i was going to sign up until i saw that it isn\\'t in glorious magnificent new york city but instead is in ... uh ... well... san francisco: could you share some code snippet of how you save grayscale image with transparency? specifically, what dimension of np array do you store your grayscale image with alpha channel as? opencv\\'s `cv2.imencode` balked at saving a mxnx2 array...hi, does anyone know how to password-protect a site hosted on aws? i’d like to host my project on an instance, but it’s a consulting project so there’s proprietary data involved. do i do this at the flask level or is there some other way?: i\\'ve done that before. come chatis anyone at aws reinvent and interested in meeting up for dinner today?hi everyone! our team at viacom is desperately in need for a good candidate for a vp level position. here is the job description: please let me know if you are interested or have anyone in mind. any help would be greatly appreciated! :slightly_smiling_face: if you ever need a vp of de, i’d be happy to consider :smile:haha i wish we were that advanced  :wink:anyone out there using neural networks to take heterogeneous data about users and embed a user into some vector space?not yet, but i am actually thinking about doing this exact thing.so... i\\'m interested in it too.we meet again, dr. ainsleydr. mercurio sounds way cooler. like a nerdy super heroand i suspect we have the same types of user data (site visitation, purchases, random demographic data)yes, so say my spanish friendsyes, and i have some of that information for some users and it\\'s missing for others. so, it\\'s a bit challengingi am also thinking about doing the same thingi think we’ve just created a working group.it\\'s been in my queue too.can i get in on that working group too? that application would be super useful on some of my projects.me too :stuck_out_tongue:+1 :wink:i just thought i would check again if anyone was at aws reinvent and wanted to meet for dinner?    excellent idea. that\\'s something we\\'re starting to work on. hey dr. mercuriothis is dr. rubinme too!hi everyone! has anyone here tried their hand at spell checking book/movie titles?\\nlike input examples :  \\'the theory of everything\\', \\'theory of everything\\', \\'theory of the everything\\', \\'theory the everything\\'. \\'theori of the evrythng\\' etc.it does not have to correct but atleast identify them all as the same thing i’ve used `fuzzywuzzy` in python for word similarity, the interface is pretty simpleor go with the core functionalities in `nltk` to get edit distance / levenshtein distance etcany nyc alumns going to the office tonight? want to meet up briefly beforehand?thanks! so here\\'s my understanding of a (basic) recc system. i have a dataframe where each row is a review, and the columns are user_id, item_id, and review. \\nin total there are 700 users, and 9000 items. i convert the data matrix into a n_users (row)  x n_items (col) matrix, where each element is the review score that user gave that item. my first question is: is there an elegant way to do this? right now i have a for loop that iterates over all the rows in the dataframe and adds them to the matrix one by one. i feel there is probably a better way to do this using pandas.\\n\\ni want to find items that are similar to other items. so i treat each column (1xn_users) as a vector, and for each vector pair i compute their dot product to give me the cosine similarity between  each pair of items. next, to find the most similar item i, i find it\\'s row in the similarity matrix, and find the max value in that row. the location of that value tells me the most similar item. \\n\\ni\\'m getting reasonable results with this, but i have a few questions. is there a more efficient way to compute the cosine similarity besides just iterating over each item? i know this is the most basic type of recc system out there. what else do you recommend? i want to add more features (like item description) into the system. thanks again!does anyone have install scripts readily available for hadoop multi-node cluster? have my hands on a respectable amount of bare metal to build out an infrastructure ... do you know how to choose which function to use in fuzzywuzzy? there seems to be a bunch of those.i’ve only used it a little, so there’s some trial and errorfor ’s example (dealing with misspellings, rather than semantic differences like “braves vs. yankees” and “ny yankees vs. atlanta braves” meaning the same thing), i think the simple `ratio()` method is best`partial_ratio()` would handle things like missing the leading “the” you can take a look at . if you don\\'t want to use the cli, the bash scripts are under the install folder and hadoop thanks austin!!! really appreciate it!!!!! for configuring the nodes you\\'ll want to go to the `config` folder and for service start stop under the `service` folder ... thanks for the directions! i\\'m going to start the process, hopefully, tomorrow when the servers are activated ... will definitely keep u updated!noice yeah feel free to ping me whenevshi all! i\\'m a fellow in the remote data science program. anyone out there have experience figuring out who\\'s speaking in an audio recording? i have zero experience with audio analysis, but my project involves finding the % time a particular person is speaking in a recording with multiple people. the voices sometimes overlap. i would really appreciate ideas.wendy.carande: hi wendy, i\\'m currently doing the data science program in nyc, and i\\'m dealing with some similar challenges\\nto get off the ground i\\'m using the \\'pyaudioanalysis\\' package that will extract some basic audio features like mfccs and chroma vectors. there are also functions to take these features and use them for speaker diarization. have a look in here:\\n\\n\\ni\\'m still trying to get the outines of my project all ironed out, and i\\'m super happy to chat with you about this. i can\\'t promise any deep insights though! thanks for that suggestion. we should definitely trade ideas.for sure. feel free to send me an email at  if you want to move the conversation off of slack. we can also set up a skype or phone call if you want. im free after 3pm est tomorrowsome 3rd party apis may do this as well-- google ml or something in microsoft azure, for example  worked on a similar sounding project in our session - i think using mfccs and hierarchical clusteringthis isn’t my project by any means, i’m not entirely sure who is responsible for it..hi all, is anyone aware of existing work related to education (school etc)? tax related? thanks!could you be more specific? data science applied to education?: yes exactly sorry it was a weird way to phrase ithi everyone, has anybody used tornado web server for python before?yeah. there’s a ton.\\n\\n\\n\\n\\ni’ve done contract data science work for pearson.hey kenny, what are you looking to do?does anyone know a good package for multi-document summarization? i\\'m looking for something that would take a few hundred reviews and return an average/aggregate review.might be useful for you thanks josh! i looked into it but it doesn\\'t seem to do summarization.for any subsequent happy hour with pizza at insighti was wondering if someone could decode an acronym for me.  a recruiter just asked me in an e-mail \"have you thought about user insights or cro?\"  ... i\\'m not sure what she means by cro though. any ideas? thanks!conversion rate optimizationi found this weird thingah, of course. thanks  !no problem!stay positive katie.amrineneil degrasse tyson has issued a great call to action: “make america smart again”.  i believe the power of the insight hive mind can make a unique and valuable contribution to this mission.  i created a new channel -  - for sharing positive, action-oriented ideas.  the first step i will take is to contact a friend at the southern poverty law center.  they do a fantastic job of tracking hate groups in the united states ().  i’ll inquire about their needs and about opportunities for the larger tech community to volunteer services.  what ideas do you have?  please join .i think this is a fantastic idea. \"make america smart again\" sounds like a worthy call to action, and the splc does seriously good work. i would love to help coordinate with them.while i in no way consider a vote for trump to be the \"smart\" choice, to me \"make america smart again\" isn\\'t a slogan that\\'s going to make any friends. it\\'s only going to serve to create more divisiveness by perpetuating the assumption that you can\\'t be a smart person and vote trump.i understand the sentiment, but perhaps the catch phrase is a little too exclusionary. the majority of people who voted for trump are not terrible racists or bigots. they\\'re confused and scared people hoping to regain what they perceive to be their rightful place in the world. most have been pandered to and incited by demagogues hoping to capitalize on the situation for their own ends. some vocal fraction are bigots and racists, yes, but understanding how to connect with the others is not only laudable, it\\'s essential. \"smart\" doesn\\'t come into it.stay positive higginsc how do you know the majority are not “terrible racists and bigots”?stay positive varunrajansource: grew up in iowaworth reading jamelle bouie on this: i left the midwest for silicon valley precisely because i was responding to the same economic influences that are driving moderate conservatives toward trump. i saw no future there. i wanted to be a part of the optimistic technological future here, and i was privileged enough to have the education and connections to do so. most people still living in rural areas don\\'t have that opportunity. you can blame them and say it\\'s their fault, that they could always move, but really they can\\'t. people have children or other family, many never went to college. if we\\'re so \"smart\", why can\\'t we figure out how to manipulate the \"stupid\" people into seeing things our way?i don’t disagree with any of that, and i think you make good points. (and i didn’t mean to get into a heated argument on friday morning.)\\n\\ni simply think that, as “data scientists”, we should use data before making quantitative statements like “the majority of people xxx”. i think if you look at the political science literature, especially on “racial resentment”, you’ll find that race appears to be a much bigger factor that any economic tendencies. here\\'s an article you might find interesting: \\n\\nanyway, happy to take this offline if you want to talk further.i\\'ll check that out. i also recommend  which despite being a listicle on cracked, is quite goodalso, if this is what passes for heated, i\\'ll take it. i haven\\'t even gotten out of bed :)i only called it heated because i got the “disappointed” emoji from stephanie. :grinning: i was not disappointed in your statement! i’m saddened and frightened by the reality it represents. as someone from north carolina, and as someone who has just legally formed a family that many vocal trump / pence supporters would like to legislate out of existence… i see and personally feel the normalization of bigotry. this isn’t just about obsolete careers and struggling to make ends meet. some people voted for donald trump because he shouts the things that they can only whisper and dog whistle about. maybe we’ll be ok, who knows. but i have real things to fear right now.stay positive stephaniemarilol i’m working on it, nuggetbotbefore we get too lost in the existential despair of the white working class (which is a very important economic and sociological topic for discussion and action--let\\'s throw de-industrialization and the opioid epidemic in for good measure), it\\'s important to point how how jokes work. \"make america smart again\" is funny and effective because it hijacks an existing meme and turns it on its head. if you want something like this to work, it has to have a hook that stays in people\\'s brains.\\n\\none last, and even more important point is that if anything i think i know about ndt is remotely correct, this will be an effort in enhancing the public understanding of science, not something to dump on conservative voters. i am guessing that there will be pushback against the denial of evolution and climate change, the fact that undocumented immigrants commit crimes in less often than most other demographics (which could be seen as having a \"liberal\" agenda), as well as pushback on the anti-vaccine and anti-gmo movement (which will be more likely to antagonize some of those same liberals cheering the first few points in this list). it\\'s possible that tyson will drop the ball on this, but i doubt it. \\n\\ni apologize if this message is in any way aggressive or incoherent. it\\'s still pretty early in the morning, and i am trying to drink decaffeinated coffee to see if it improves my health at all.good point. ndt is a baller.anyone else at mlconfsf!? i already saw  and  ! want to grab coffee or lunch?we’ve got openings for data scientists and a data engineer on our machine learning team at crowdstrike. (a remote option is available, but it would be really awesome if you were in the los angeles area — you would still able to work from home :slightly_smiling_face: )\\n\\n\\nhi folks! international network question: i am possibly moving to paris relatively soon, and am starting to look for data science jobs there.  does anyone happen to have any connections out there, or suggestions of particularly interesting companies to check out?  (i swear this was my plan even before tuesday....)  thank you! with marine la pen you might need to move again soon  i have some french people on my team, i’ll ask around that would be great, thank you!fwiw, ndgt loves soundbites and definitely used that phrase/slogan to sound good on tv w/ colbert. i have no idea if he\\'ll actually use it more often (but if others have seen him use it in other contexts please let me know cuz i\\'m curious). it also isn\\'t clear to me that he\\'s going to be doing anything different than what he already does. he hosts star talk, tweets, gives interviews, and will almost certainly be doing a book tour for his 2 recent books, but i find it very unlikely that he\\'ll be putting together some new big public science education program.\\nthose kinds of programs already exist in many places and i personally hope that they expand and grow now more than ever. if people are interested in tutoring kids (in any subject), the boys &amp; girls clubs of america are all over the place and do great work and if you want to do more you can get matched with a kid through them. some cities have education and/or scientific organizations that match scientists with teachers/schools and you get to talk to kids about science directly. lots of places also have public science programs for adults, either free lectures at universities, or in pubs (i\\'ve been involved with these kinds!), or at libraries, etc. many of these programs are volunteer-based and free to attend and try to target underrepresented groups and lower-income groups, both in urban and rural areashe also likes talking about other sciences as if he was an expert, and makes spectacular mistakes.to be fair, if any of our voices sounded as good as his, people would instinctively believe pretty much anything we said too. he\\'s barely an expert on astrophysics these days... he still knows his stuff.  your comment could be interpreted as an attack on science educators by implying that they’re not “real scientists.”  all that does is discourage other scientists from doing their own public outreach.as a nationally funded scientist and science educator until this past summer, and currently an unpaid and volunteer science educator, i am well aware of the views held by some scientists of science educators. neil does great work getting the public excited about science. he has also done harm by passing off incorrect information as fact, about astrophysics and many other fields. as a poc in science, as a \"famous face\" of science, and as a communicator, he is doing good work for society and creating a scientific literate public that dwarfs my humble contributions. that being said, he is not perfect, as a human or a science communicator. i, and many other astrophysicists and scientists, find it infuriating that he rarely, if ever, apologizes for misinformation or corrects himself, which is the exact antithesis of the scientific method.to all my insight fellows, please join me in this petition to block myron ebell, a climate change denier, from being appointed to the head of the epa. trump\\'s major victories came from oil/fracking states and he has already started trying to lift emission/mpg limits from car companies. we need 100k signatures by the end of this month. man. did anyone know about this? stay positive markwangstay positive americaanyone have any advice or experience with event coding within a mobile app? preferably something that\\'s interpretable but also can be easily analyzed. primarily for tracking user movement throughout the app, identifying where users may be getting confused, etc.hi all! my company (unifyid, security startup in sf) is greatly expanding and hiring mostly engineers in the following specialties:\\n\\ndevops engineer\\nfull-stack engineer\\nfront-end developer\\nios developer\\nandroid engineer\\n\\nwe use advanced ml &amp; physics applied to phone sensor data and similar, to identify and authenticate users into apps or websites and eventually replace passwords.\\n\\nplease let me know if you are interested or know anyone who is. i am happy to split any referral bonus :slightly_smiling_face: i’am at your service for any further question.\\n\\nhi all! a colleague of mine is a founder of an early-stage ai company in the business travel industry called , and is looking for nlp, ml, ai and data engineers for the founding team. they are based in the bay area. if that sounds interesting, please let me know and i’d be happy to introduce you you should reach out to  to get connected with some of our recent ai fellows! :partyparrot: i know that  would be excited to talk to you!and jumping on this bandwagon! my company (tempalert, boston) is looking for a data engineer/data architect to build out our analytics platform and integrate it with our product. let me know if you are interested!  talk to ! :smile_cat:sending her a message as we speak :slightly_smiling_face:for all you visual learners out there...\\nsince  is too modest to promote his own work, he wrote a fun post exploring glove and different styles of english as used on twitter. preview: \"workin - working + going = goin\" :wink:that’s neatway to go nice  . this is the first time i’ve ever thought about a negative letter :confused:thanks !showing solidarity with the victims of the pulse tragedy in orlando, fl, latinotech (), lesbians who tech (), queer tech nyc (), out in tech (), ny tech alliance (), &amp; outnexus () are donating all of the proceeds of their happy hour tomorrow to “the center”, the glbt community center of central florida. \\n\\nthe event is in nyc (), but anyone can make a direct donation here:  :rainbow:hi bay area peeps, anybody need a roommate or know of housing around menlo park / anywhere south of sf? (my male cousin is moving down from portland)can anyone suggest what is the best practice to do bulk load from file to cassandara? - i think  might have some experience.anyone currently watching lcs?league of legends?yes thanks for the tip.  i will reach out!a new blog post by  compares a/b testing and multiarmed bandits for adaptive movie recommendations take a look! if any fellow/alum wants to write a post on our blog, hit me up and let\\'s chat.i think all you have to do is emphasize the minimum requirements for the job. title is less important. also, keep in mind that anyone that might join in a junior position may want the flexibility to grow into a senior position. you could also just create a tiered system like other companies (l1, l2, l3, etc) or “junior data scientist, data scientist, senior data scientist, principal data scientist, senior principal data scientist”.many companies (like daimler) try to match seniority to other company ladders. in our case, we match roughly to google’s, but change the designation/titlei would just make the requirements (and comp range) obvious so folks know if its worth applying with their skill set.hi all!  my company, proteus digital health, is hiring a data scientist.  the role is on the platform team and uses data to optimize our core product, a pill that transmits a signal when swallowed.  please contact me if you\\'re interested.\\nnot yet - still building the main scraper, might try by the end of the weekwe really should. the survey will help us with that!anyone has experience with jupyterhub as a data science collaboration tool within their team?you might try this.  kindle version is $7.  done! looking forward to seeing the resultsyes. i like go with something involving an imbalanced data set (fraud, clicks, etc) and ask the candidate to walk me through how they’d detect the minority class. it’s a standard question, but i like it because it gives me plenty of insight into how the candidate thinks, what tools they’re comfortable with, and what types of creative solutions they can come up with if i throw a wrench in their approach (what if you discover that 90% of the clicks were bots?). i also like to go into the details of the tools they’d use, any etl (let’s say the data is in two different tables organized like this….), how models could be stored, etc, just to get a better gauge on the scope of their understanding of a tech stack.how flexible it is in order to pick up a teammate’s notebook and edit/add codes?  can you also compare it with any other platform?any recommendations for visualizing a very large amount (~300k+ records) of time series data in redshift? interested in features like panning and zooming. lots of bi tools out there and not sure where to start.i\\'d love to hear if caravel can work for this: linkedin is open-sourcing a scalable ml library for spark called \"photon\". it\\'s used widely here internally:\\ncarlos, what does “open-sourcing” mean? is it not publicly available on github already?open sourcing also implies a particular type of license.i wouldn\\'t read anything into \\'open source\\' besides \\'being able to see the source\\'but that\\'s just mesamei believe this is why richard stallman hates the term “open source”. _free software_ implies a certain kind of license.: i miswrote. you\\'re correct indeed.: if you need power you can try vispy which leverages gpus (there\\'s a pycon talk from last year about it on youtube). i\\'ve also been playing with plotly lately, it has interactivity built in.‘;n’mmn cxcvbn\\\\hey all, i’m trying to find a style guide to write quick-running, yet fairly human-readable sql. any suggestions?\\npostgresql, in particularif anyone is interested in attending this spark streaming workshop, here is a 25% discount for insight:\\nhi illustrious alumni, if you guys have a chance, could you try out  and let me know if it survives? (it takes a long time to load). extra appreciate authing your reddit account to it. extra extra appreciate using it like it’s a real thing for a bit :slightly_smiling_face: thanks!\"’s server dns address could not be found.\"i already broke it i don’t have a reddit account, but i ran it through my twitter account and it worked! it recommended subreddits about music, outer space and onion-esque news stories. i accept.works fine for megot a lot of space recommendations, which is somewhat but not  wholly indicative of my tweeting habitsif you need personal validation, it recommend ksp as a subreddit, which i was already subscribed tooh it’s just blocked at work…guess i shouldn’t be procrastinatingr/dataisbeautiful, r/data_warehousing, r/data_warehouse... i guess it thinks i\\'m some kind of data scientistseeing as how i follow statfacts and hadley wickham and drew conway and such on twitter, seems like it\\'s workin```\\ninternal server error\\n\\nthe server encountered an internal error and was unable to complete your request. either the server is overloaded or there is an error in the application.\\n```10,000 data subreddits... which is what i\\'d expect from my twitter(but not ones i\\'d actually follow on reddit)lots of ml stuff tooi mean it nailed my interests expressed on twitter!wow, so many data subreddits, good job!thanks for the help everybody! :slightly_smiling_face:50 free data science books\\nanyone has experience using flask+swagger  (flask_restplus)?any companies out there looking to hire an entry-level ux designer? one of our badass ux interns here at app annie is finishing up soon and looking for full-time work. pm me if interestedwho\\'s at odsc west?i’ll be there giving a workshop tomorrow !clean yo desks!help i\\'m locked out the office would be great to have a dockerfile in the github repothis (not @)channel just keeps on delivering :popcorn:i feel like taking glee in these interruptions is all thanks to you ! clean desks foreverpug bomb 50arrgghh how does it worknuggetbot pug meit’s onlinenuggetbot pug bomb 50:trollface:this is a good dayi thought these pugs were wrapped in towels or something. no those rolls are their bodies.:pug-bomb:renaming this channel to #adorablepugsruuuuuuuttttthhhhhh daaaaaaaaaaaaaavid!:bunching::bunching::bunching::bunching:\\n:bunching::bunching::bunching::bunching:\\n:bunching::bunching::bunching::bunching:\\n:bunching::bunching::bunching::bunching: uploaded a file: hhahahaha...sorry i know we are not suppose to laugh at this meme but i cant help it :stuck_out_tongue:   i think it was me :smiley:. i didn’t formulate my problem correctly.\\nfor me it is often safe to assume that  data is likely missing completely at random. it’s just my feature matrix is a triangle at best: number of predictors is growing.\\nthe correct way to formulate would be: i wonder if somebody get to work with the growing set of predictors, rather then growing set of samples while doing online learning.if anyone is interested in a freelance web developer job, see the message below from a friend of mine at upenn.\\n\\n\"my research involves virtual navigation behavior and i use a custom built website to administer the studies and collect the data online. i didn\\'t build the website and don\\'t have the knowledge and/or time to change it. the guy who built it is awol. the website runs fine, but i have no one to maintain it, update it, or make changes. we\\'re looking to continue hosting it online, but it uses a plugin for unity 3d, which, i understand, browsers are pulling support for. this poses a pretty serious issue for future online data collection.\\n \\ni\\'m looking for a web developer who could provide a time/cost estimate for some of this. the project uses html, ruby on rails, and is built on a mysql database. the server is a version of red hat, and is currently hosted for free on a university server. \\n \\nif you would be interested in consulting on this project, or know someone who would, please contact steven weisberg by email: .\"cc:  :wink:a bio on mr. lecuna good friend of mine who’s a total badass data scientist (non-insight) and who cares deeply about equity and inclusion issues (in tech and beyond) and who’s been in the field for quite a few years is looking to switch companies in the sf bay area. so, if anyone has or knows of an open lead/director-level data scientist position, please dm me!heard uber is really big on diversity. \\njust kidding.i must share this with the world. thanks! !anyone seeing any performance issues with tensorflow on aws today? we’re seeing a &gt;15% slowdown in training and speculating that it’s from the intel bug but honestly have no ideaindeed amazon has already applied the patch for meltdown which is expected to slowdown cpu performance by 20%unsupervised/semi-supervised learning question - i have a binary classification problem where i know if something is true positive, but not if something is a false positive (basically, some of my data is labeled with a 1, but everything else could be 0 or 1 but i don\\'t know the answer).  is there some weaker analogy to an roc curve that exists for a situation like this? i can show \"what % of true positives does the model correctly classify\", but i\\'m wondering if there\\'s something more sophisticated i can do (because of course i can tweak the thresholds to catch all of the true positives, but i\\'m probably also incorrectly classifying a bunch of other points as 1).i do have a sense that probably not more than 10% of the data set is a 1 (so 1\\'s are relatively uncommon - this is basically an anomaly detection problem)hey, in medicine this comes up a lot, where it is often easy to get a subset of the positive population via a query, but getting the complete positive dataset is very hard (manual review). i found the following 2 papers helpful:\\n\\nit sounds like the usual caveats apply here, i.e. wanting to make sure you minimize false negatives, and that you try to preprocess your training set to balance the data. depends on what a true positive means though, and how bad it is when you miss one.this is great, thanks! (and good guess - this is indeed a healthcare problem) has a cool blog with some others which discusses this issue. the relevant post is: hi, i am looking for *basic or simple github or tutorials* for  *user recommendations by cosine similarity*. any recommendations ? thanksall mayors should take public transportation at least once a week.i have seen rahm emanuel on the cta in chicago so some do :slightly_smiling_face:linear regression question:\\nif i have both categorical and continuous features (&gt;0) what is the best way to preprocess the data? my continuous features have wide ranging scales - some are in [0,10], others are in [0,100000]. i can rescale the continuous ones and one-hot encode the categorical ones, but is that the right thing to do? seems this scheme would favor the categorical variables. i am just curious why “this scheme would favor the categorical variables”my approach, based on this article by gelman is to to center at 0 and then scale continuous variables by two sd, and center binary ones at the mean. here\\'s the reference to that this pretty much makes parameter estimates comparable in regression models and helps with convergence in bayesian models there is no right way of preprocessing your data. usually it’s an iterative process and you should try a few different approaches. here is a nice guide to normalization, scaling, mean removal and other preprocessing steps from sklearn: one-hot-encoding is most common way of encoding the categorical variables but there are better approaches. i really like this deck from an experienced kaggler, where he describes different approaches on how to handle categoricals: because when i rescale (and in this case i really mean normalize, not standardize) very few of my continuous variables will be close to 1. however my one-hot encoded variables will be 1 or 0. one of my categories is coming out on top in feature importance. i am just not sure if this is a \"true\" signal or a consequence of this choice of preprocessing. are you going to apply l1 or l2?right now playing with l2in that case it may be marginally important. you can always map your categorical feature to 0 &amp; 0.5 for example and see if there is any difference. it would be interesting, if there is :slightly_smiling_face:thanks for the suggestion. i\\'ll let you know if i find anything interesting :slightly_smiling_face:hi, i\\'m trying to use multinomial logistic regression in spark (to predict ~300 different outcomes) but keep seeing some strange behavior (eg. the number of jobs triggered vary even if i keep both the data and the model parameters the same, as well as when i use the trained model it just predicts the majority class for each record so my assumption is that the model is  quite messed up). do you have any suggestions how to debug this issue? thanks!thanks  and  i\\'m leaning towards luigi.  it seems easier to digest.   anyone at gtc this week? pat and i are all four days.does anyone still publish now that they\\'re out of academia?  i\\'m looking for what journals are considered good for data science/analytics articles.  thanks all!: i have a few in the pipeline, but like anything else it\\'s likely to be specific to your analytics (i.e. distributed clustering, nlp approaches, etc.)probability by peter norvig i\\'d just like to say that i love almost all of the notebooks that peter norvig writesupdate: he ended up wearing a navy suit jacket, white shirt with no tie, khakis, and dress shoes. he was interviewing with cfo and coo so this felt appropriate. for interviews with non-exec, he would have kept the top but gone with more casual pants-shoes combo. thanks for the tips, y’all!^ this is cool but it also freaks me out a little?they should make one for ratsstay out of my dreams, ! last night i had a dream that the nyc office was infested by rats! the alpha rat was named darryl! thank you for your reply. it is a great idea to visualize the data first with some domain knowledge. the  predict value `open_rate` is heavily related to human behavior - people tend to open their emails during the middle of the day and the weekdays, so there are periodicity involved in the time feature. besides that, it’s worth to look at the correlation between domains - `` vs ``. also between ips - presumably we assign emails with different qualities to different ip. but for our customer `portal_id`, i will assume they are independent of each other.thanks!i am making a list of common human cognition pitfalls when using stats to analyze data... more ideas?\\n\\nhuman cognition pitfalls in stats\\n\\non basic statistics\\n\\n1. null rejection. to think that because one proved that x does not cause y implies that x does cause y. i.e., to think that not having evidence to reject the null implies that the null is true (one cannot disprove that medicine a is effective in treatment t does not imply that actually, medicine a is effective in treatment t) \\ncorrect: keep trying to find evidence to reject the null and accept it as not disproven but never as true\\n2. correlation and causation. to think that correlation implies causation and that therefore the pattern will repeat in the future. \\ncorrect: correlation does not imply causation and to infer causation one can use causal inference or experimental design.\\n3. correlation of non-stationary time series (i.e., prices). they will always appear correlated even if they are not due to correlating trends but not the underlying stochasticity of the series.\\ncorrect: first make the time series nonstationary. i.e., correlation of returns (stationary time series)\\n\\non fitting models\\n\\n4. future leakage. to use historical data that was construed using future information, i.e., using a fundamental indicator that appears built at time t but actually used information from time t+x.\\ncorrect: always use point-in-time data with correct dates.\\n5. backtest overfitting. to fit n+ models using the same dataset until getting good results (even when doing this \"kosher\" and using the out-of-train times to test the model performance). \\ncorrect: take into account the number of models that were fitted and use proper multiple testing corrections.maybe it\\'s better to start a doc and let people to contribute there?that\\'d work but we can leave it on the thread in the meantime... do we have a way for all slack members to contribute in a common doc?... sounds like we would have to create a google doc (or quip) and therefore invite people into it which creates some friction into getting everyone\\'s input...you could also make the doc accessible to anyone with the link (public)looking for significance in posthoc subgroups when the overall result is null i think i\\'d be ok with that if the next step was to run a dedicated experiment in a sub-group, rather than to make a  decision to roll-out a feature to that subgroup based on that first testi am having a problem with my reverse proxy on ubuntu (aws) for my website. i have a shiny app that runs on the apache web server, but i am getting a 502 proxy error when i try to load the site without the port number. some of it loads, but some files do not. here\\'s what i have in the default config file for apache2:stay positive alexisyelton&lt;virtualhost *:*&gt;\\n        # the servername directive sets the request scheme, hostname and port that                                                 \\n        # the server uses to identify itself. this is used when creating                                                           \\n        # redirection urls. in the context of virtual hosts, the servername                                                        \\n        # specifies what hostname must appear in the request\\'s host: header to                                                     \\n        # match this virtual host. for the default virtual host (this file) this                                                   \\n        # value is not decisive as it is used as a last resort host regardless.                                                    \\n        # however, you must set it for any further virtual host explicitly.                                                        \\n        servername localhost\\n\\n        serveradmin webmaster@localhost\\n#       documentroot /var/www/html                                                                                                 \\n        documentroot /var/www/html\\n        proxypreservehost on\\n        proxypass / \\n        proxypassreverse / \\n\\n        # available loglevels: trace8, ..., trace1, debug, info, notice, warn,                                                     \\n        # error, crit, alert, emerg.                                                                                               \\n        # it is also possible to configure the loglevel for particular                                                             \\n        # modules, e.g.                                                                                                            \\n        #loglevel info ssl:warn                                                                                                    \\n\\n        errorlog ${apache_log_dir}/error.log\\n        customlog ${apache_log_dir}/access.log combined\\n\\n        # for most configuration files from conf-available/, which are                                                             \\n        # enabled or disabled at a global level, it is possible to                                                                 \\n        # include a line for only one particular virtual host. for example the                                                     \\n        # following line enables the cgi configuration for this host only                                                          \\n        # after it has been globally disabled with \"a2disconf\".                                                                    \\n        #include conf-available/serve-cgi-bin.conf                                                                                 \\n\\n\\n&lt;/virtualhost&gt;any thoughts?if its not in sk learn or statsmodels, that probably means it doesn\\'t exist :confused:\\npython tends to be lacking in more advanced linear modeling.yeah i figured.  i guess the only option is to either build it in pymc or use r hahasomeone needs to give statsmodels some more dev love i’ve had to switch to r for things that aren’t implemented in python - it’s kind of a pain but doable. if you’re happy using a notebook, then it’s pretty easy to load up an r kernel. alternatively you can call r just within a single cell.does anyone have good examples of (technical) topic summarization formats?  either interactive or not.possibly using d3 or other tools.geolocation question: has anyone figured out how to return all lat and lon coordinates within a circle of a particular radius? thanks!how do you define a circle?just wondering since a circle has infinite points and there are infinite lat lon coordinates in a circle.do you mean get all lat lon from a select list?ah, yes from a list! like you want to find all taxis near some point at a timeand i would define near as a particular radiusin python, you can do all that with a the shapely packageoh! some databases, like mongodb, have built in functionality to do that too.cool thanks! i\\'ll check out shapely:plus_one: for shapely. i use it all the time.hi all! we are looking for project reviewers at udacity for our machine learning projects. you get paid per review and capstone projects are paid at like $70 each review. let me know if you are interested.good video on how google makes their teams: ha! i had just this idea a little bit ago: haha sweet. feel like there\\'s a few title missing, got any idea to complete the set?hmmmmm i’ll think on it!facebook has open-sourced a pretty neat tool for time-series forecasting! \\nroger.w.shaw: nice find! was looking for something just like this.i think there are lots of converts,   - vim was the fourth-ranked option in the 2016 stackoverflow survey. and the top two (notepad++ and visual studio) are windows-only products, so i\\'d guess vim\\'s use in open source systems is higher. hi all, for those of you in the la area or planning a summer visit, the date for `big data day la 2017`, a free to attend conference with over 1500 attendees and featuring many great speakers from the industry, has been set and finalized!  the conference will take place `08/05/2017` at the usc campus (yay!  we upgraded our venue from previous years as we continue to grow).  so save the date and i hope to see some of you there :slightly_smiling_face:\\n\\nfor those who are local and want to volunteer, please reach out to me directly and i can loop you in to our volunteer meetings and opportunities.  it’s a great way to ensure that quality data and data science conferences like big data day la remain free to attendees.looks like ai is ready for insight! :+1:link to github repo: it’s worth noting jupyterlab (the evolution of jupyter notebooks) will also have collaborative notebooks. link: . i think these also use google drive on the backend. the beta milestone is due out before the new year (fingers crossed), so colaboratory is probably a better experience at the moment.colab is also really focused on using google compute backend. juypterlab seems more flexible beautifulsoup question. is it possible to use soup.find_all( ) to find two separate tags and classes? i\\'m looking for both (\"p\", class_=\"program\") and (\"div\", class_=\"center-hrs\") ideally in one final list in order. thanks!!!solved it by passing in two lists!so, dropbox got pwned. check if your accounts are compromised.   and whats your score? mine is 3 :slightly_smiling_face:1 (damn you 000webhosting!)stay positive carlosfahammy old gmail has 8 breaches and 4 pastes :confused:waoh!just one, frikkin dropbox^ dropbox for me tooshitbitching ass-bastardsstay positive johnrwalkyou’re not my supervisorno breaches ^_^no dropbox for me….but skeptical.  i had a random sign in event a few weeks ago and changed my pwstay positive rociosngdropbox, but thankfully i have a unique password and actually don\\'t use italso adobe!haha adobe yesman, using plus addressing makes searching so hardwtb regexand ..ugh myspace. yes i had one like 14 yrs agoflash player is the dirty thing for the internet ever.adobe needs people to signup to get flash player! how stupid is that? its like signing up for getting a jvm from oracle.oh wait.. you have to do that too:smile:woah, i had a myspace account. it must have really sucked because i don’t even remember having it.  i believe the system admins said, “here you go kind sir, myspace, is down and dead, take this server and do what you want with it”. it turns out it had passwords.but the most valuable thing on the server was really an index of all of our tastes in music during middle schoolhaha..i am sure half of those passwords were threedoorsdown, sum41+blink182, allstarz, etc.apparently my adobe password is somewhere to be found on the internet, but i still have to reset my password every single time i login there. well, apparently someone has access to one of the 5 backups of my thesis research. it probably would have been easier to just look on github than to hack dropbox...for all of you word2vec users: what is the method that you use to create a sentence vector from a word 2 vec model?paging , you\\'re nlp magic is needed!@alex_guide: hey alex.for short things (3-5 words) we\\'ve found that summing works finewe tried some weighted sums, but i don\\'t remember their performanceand we\\'ve used some different models like paragraph2vec and skipthoughtpat though is more expert than i, hopefully i can poke him into showing up: for the weighted sums: are you just taking the average, or do you assign different weights to the different words?: paragraph to vec is doctovec?yeah, although i think we\\'ve done with and without averaging; i don\\'t know if those experiments ever went anywhere.: i was assuming that we would use just the simple sum. example for simple bow: ‘marry had a little lamb’. assuming dropping the ‘had a’. would just use model(marry) + model(little) +model(lamb) to get the vectorthat\\'s worked well for small numbers of words for usi\\'d certainly try it as the first thingthat seems to be the way i was thinking.what data set have you had luck on training with?i know it matters for the actual problem.you could also try tf-idf weights on the word vectors before summing^--that\\'s what we did for weightingbut again, don\\'t recall the performance :confused: you would have to do the tf-idf prior to training the model as well corect?for training we generally do one of two things:\\n\\n1) take a pretrained model, and maybe retrain it a bit\\n2) train from scratch on the text we\\'re interested inyeah, you\\'d have to calculate it on the full sample, you can\\'t do it on the vectors afterthanks  . it’s been a while since i’ve tried itwhat do you find has better performance. training on the data to be used. the ‘docs’ that i am trying to cluster are short. maybe ~ 2-3 words on average.train tf-idf on your dataset, train word2vec on a larger dataset that’s domain-specific if necessary (e.g. law)  thanks!!!!just to add to the noise, i\\'ve used skip thought vectors a bit... they are very interesting but the author\\'s implementation tends to kill your python process with floating point overflow errors so if just for that reason it\\'s worth pursuing slightly simpler solutions like tfidf and sum as   is suggestingstatistics without borders:   sign up for the september barug  meetup here\\'s an interesting read on earnest!   : in my company it isn’t based on paternity/maternity but on the primary care provider.  (although, looking at that sheet above, maybe that is not uncommon.)who else will be at pycon?perhaps of interest, github just released a new 3tb dataset: nuggetbot animate me happy gilmore nuggetbot animate me grateful dead: hey smart data scientists! does someone want to explain rectified linear unit and the hidden layer to me?over the chat ?  i suggest you read this : : thanks chris. let me take a look at that.okay, so ```an example neural network would instead compute s=w2max(0,w1x)s=w2max(0,w1x). here, w1w1 could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. the function max(0,−)max(0,−) is a non-linearity that is applied elementwise. there are several choices we could make for the non-linearity (which we’ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. finally, the matrix w2w2 would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores.```i get that you could do that. but why would you do that?why the hidden layer ?sure, what advantage does this have over not having ita series of non-linear transformations (each hidden layer does this) can be a very powerful non-linear predictoryou can think of the original features undergoing a series of non-linear transformations to a space where classifying them becomes easierwhat\\'s your definition of linear in this case?the rectified linear unit is piecewise linear, but not a linear function, other activation function are also non-linear like the sigmoid or tanhso the features space is mapped to a different one that cannot be described by a linear functionor better said a linear transformationwhy is non linear better than linear?so in this example, say we do w2max(0,w1x)swhy is that better than w2(w1)s?don\\'t we lose information by taking away the negative values of w1s?same reason you might want to use a svm with a non-linear kernelhi i’m a remote session 2016 b fellow with a world of problems on her project - but right now, i’m looking for a bootstrap expert. i had working flask/bootstrap code and a beautiful webpage on my mac, but once i tried to host it on centos, i lost all the images and the bootstrap nav bar. loading text files and other images outside the static folder works fine. i am very confident i have the right address - i had two other fellows check it too.anyone have any idea why mac working bootstrap code wouldn’t be working in centos? :confused:: are you serving your site with gunicorn? gunicorn doesn’t serve static files.no… haven’t even made it to gunicorn yeti’m just typing in the ip addresson my local machine - everything else loads just fine. just nothing in static or templatesand i checked permissions on those folders, it’s not thati strongly suspect this is a centos/bootstrap problemstay positive loisthanks nuggetbot but i feel like you’re taunting mecould it be a path issue? i had to change the paths in my html.i changed all the paths and have double/triple/quadruple checked themi did relative and hard pathstried with and without the leading ‘/\\'tried with .. and witouti get errors that look like this:it might be useful to know what http status codes are being returned for the bootstrap requests. you can get that info by loading the page with your browser’s developer tools open to the network panel uploaded a file: or in the log which you just postedgreat minds think alike :wink:so they’re 404si tried googling this set up too and working through all the stackoverflow pages with suggestionsme and 2 other fellows have been working at this for about 3 hours… we’ve tried a lot of different things :disappointed:this is happening with the other fellows’ apps, too? or just yours?just mineyou could use a cdn for the bootstrap files, but that wouldn’t help with your imagescdn?this is a really good suggestionbecause i can try to put the images where my output files are to load them inand see if i can get the cdn to workthanks  !!i am going to go for a run and then give it a try :slightly_smiling_face:you might also consider just moving on to gunicorn because you’re going to have to do that eventually, and there’s not much to be gained by getting it working on the flask server in the mean time.good luck, !thanks :slightly_smiling_face:: that worked perfectly!!! you’re my hero of the day!!!yay! i’m glad you’re up and running!….  !!!!  thanks for your replies the other day on the multi-label issue! i wanted to ask for some clarification... so multilabelbinarizer can be used with any sklearn classifier, and this is just a way to feed in the target…  but i believe we have to use either the onevsrestclassifier or onevsoneclasssifier meta-estimators? whereas the multi output classifier is both multi-label and multi-output, so it doesn’t seem to apply here… ? thanks!link for the lazythis might justify an @ mention for the whole channel. i’ll let you incur the wrath of the @ channel police. :stuck_out_tongue:has anyone been to datascience: elevate? what was your experience like? \\nhello everyone. we are trying to deploy cassandra in production with very less experience with how the deployment should look like. could someone suggest few thingsdo you really need cassandra?  managed nosql databases like aws’s dynamodb or gcp’s datastore/bigtable have a much lower bar for developers to use and maintain.thanks so much for the reply aws is out of question in first release. we are deploying cassandra now some of the  questions are do we need physical machines or vm. what kind of disk. what is the max disk we can addwill your team be operating the cluster? yes we will be managing the cluster too i work at datascience so yes :smile:  this is the second elevate event we’re doing so the n is small, but the first one had some great, very high quality talks from la companies and ones that are on the national scale ones.  there were a lot of people from great companies big and small in attendance also.  \\n\\nthe talks included i think a really nice balance of covering companies’ specific data science uses cases and technical solutions (types of models and technologies), and tying them very well to the business.  i was actually surprised that everyone brought such consistently high-quality talks, since we don’t control the specifics of any given talk and only the general themes, and i’ve seen a lot more fluff at other events i’ve attended before.  whereas the elevate talks were packed with substance.  but i think our event coordinators spend a lot of time vetting and filtering the right people.\\n\\nwe have some great people coming again for this one, so i expect it to be very good.  let me know if you have any questions.anyone know if it\\'s possible to use the rangepartitioner for pyspark? i only see a way to access it in the native scala api....is there anything like “schema registry” for kinesis?i never found one. i wish there was!both those issues are consistent with their core offering though..they’re basically just trying to be a simple gui front end to whatever your data warehouse is for users that are not interested in editing lookml or writing sql. they’re not making any claims of being able to speed up queries beyond what your backend is capable of and i don’t know of any other companies/products (accurately) making such claimsmy issues were more on the limitations of that gui…plotting options were often limiting (though quickly growing and customizable if you’re not running a hosted solution) and i was curious if others found domo or other products less limitingour main performance issue is essentially that you can\\'t force aggregation to happen before joining. this effectively increases the run time complexity of many of our reports.  you might not mind it in red shift,  but it\\'s still poor design, and wanting better is not asking them to \"speed up queries beyond what your backend is capable of\". politics + linguistics + data science! “semantics — what does data science reveal about clinton and trump ?” by maixent chenebaux hey guys, thought this was a good cheat sheet for big o complexity of different cs and machine learning algorithms^ very good! much useful.has anyone had luck logging into facebook on python? if so what did you do? thanks!daniel lakens (of the ’no more statistically unsound science!’ revolution) has a brand new coursera course: : thanks a ton, that is super helpful!datacamp is hiring for multiple roles related to data science:\\ncurriculum director: lead our curriculum team and help to define what it means to be a data scientist in the 21st century. you will shape the future of our curriculum and expand our network of world-class instructors.\\n\\ndata science curriculum lead: help us engage with leaders in the data science community who could become instructors, and work closely with them to design new courses.\\n\\ndata science content developer:  work closely with instructors to build new data science courses. you will help create exercises, slides, and scripts, and most importantly be an advocate for our students.\\n\\ndata scientist (growth): help us make sense of what our users are doing so that we can better serve their needs. you will be the first technical member of this team, so you will have lots of opportunity to make a big impact.\\n\\nyou can see more info here:\\ni used/battled with the instagram api for my project. happy to help!generally speaking, if i\\'m scraping multiple sources for a set of values, is it worth the computation time to use fuzzy string matching to keep it resilient to minor changes in formatting?does anyone have information on best ways to identify and store metadata on faces in videos ?the requirement is to identify human faces in the videos and then store, the related metadata - for eg., vectors of only those that contain the faces. i am wondering if anyone has experience with this and can provide some guidance.does anyone use any internal tools or services that patrol aws/gcp costs and look for efficiencies in some sort of automated way?for example identifying scheduled jobs or queries that all scan the same data every day, meaning a new rollup or aggregation might be a good idea you write this for fun with your coffee breaks, i assume :wink:lol :joy:on a train right now but can dm you later tonight \\n\\nalso does slack mobile not let you start threads? :disappointed:yeah it’s awful about thatif you single press the message where you want to start the thread a new screen will appear with a box to start a thread! thank you you\\'re right, it is pretty much zero. but the quantity being calculated is a ratio of 2 differentials, i.e., how much the prediction changes when you change that delta. just as the slope of a curve, you can\\'t computationally have deltas, but you can extrapolate the derivative as delta approaches 0.\\ni\\'m not privy to which algorithm they use for this extrapolation though. uploaded a file:  and commented: did a seed vs. seed analysis going back the last 30 years, enjoy! commented on ’s file : i hate you all.: . i still need to fill out mine though jejejejdoes anyone know if there is an easy way to change the distance metrics in sklearn classifiers?: which classifier(s) were you thinking of?, the easiest way is to put in a distance matrix.  it will take sparse matrices as well.that question doesn\\'t necessarily make sense, depending on the classifierdistance metrics... euclidean or bust!: no manhattan :trollface:actually better yet: chebyshev distance :troll:hmmhi, guys, i am wondering if any one knows a good tutorial or source site of ml in c++ ? the only machine learning stuff i\\'ve done in c++ was with open cv (computer vision). there is a ml part and a separate deep nn part.   thank you so much,  . i will take a look and would be coming back more question. i guess. please let me know if you have any source of this.  :smile:heads up: meetup is considering hiring another data scientist. nothing posted yet, but thought i’d put it out here first. dm me if you’d like to chat about it.anyone have any tips or suggestions for mobile app a/b testing across multiple devices (android and ios primarily). bayesian methods like multi armed bandit are something we\\'d like the solution to include. a third party vendor is fine, but we\\'d likely need to self host due to phi/pii. oh, i\\'ve also seen that you can deploy changes to an app without deploying a new build. does anyone have experience with that? hbo talked about doing this with their neural net for the hotdog app. thanks ! would love to get your input/help whenever you get the chance :slightly_smiling_face:hello everyone - i\\'m trying to get a large dataset from an external ftp server to s3. i\\'m currently downloading the files by sshing into my ec2 instance, and running wget to download the files from the ftp server to my ec2 instance - i\\'ll then upload the files from my ec2 instance to s3. the download from the ftp server is slow, and is taking a while - i\\'m not sure if this is due to the bandwidth of the server, or if this is due to the fact that i\\'m downloading a large number of small files. anyone have any ideas on how i can increase the throughput? (here\\'s the ftp server: )what happens if you try to download several files from the ftp server simultaneously?the server seems to throttle me to 3 parallel connections per hostthis might help excellent! thanks, that looks like exactly what i needthanks again guys - this saved me hoursdoes anyone know how to compute the efficiency with true positives, false positives but also unknown detections that you cannot validate?: if you are only concerned with speed, you might want to consider using cython () to compile parts/all of your python code. last time i checked cython was just a tad slower than pure c. also, if you are familiar with java, take a look at  scala -- a very concise language that offers the speed of the jvm and relatively easy parallelization thanks to tons of nice java libraries that it can use natively. i want to use the dirichlet process gaussian mixture model.  \\n\\n do you have a good reference/doc page that you can give me?  i have been looking all over.: there\\'s also waffles, which is written in c++ :  thank you very much guys. i will study them and may come back with lots of questions :sweat_smile: thank you so much guys , keep posting me any other references!!!: i have found vw pretty good at times, its pretty fast and written in c++, unfortunately docs suck : i was thinking of clustering algs. it is very straightforward to have a custom distance matrix there.  for the model you referenced, it sounds like you want a custom covariance matrix? i\\'ve only taken a brief glance at the source, but i think you could subclass sklearn\\'s dpgmm or gmm and override _get_covars and _set_covars with whatever you want.: ok! thnx.  was hoping not to go that direction but looks like that might be the only way...: thanks for the source. at least, this try to be detail in its contents. worth to look at. so you have tried this?: yep!  code runs well once you figure out how to tell it what model/parms to use.  good luck!: thank you, i will play with it and come back to you :smile: thanks ~~does anyone have any experience with non-negative matrix factorization packages in python? i\\'m thinking about scikit-learn\\'s nmf and biolab\\'s nimfa, but perhaps i\\'m missing some others.stay positive alex7non-negative is the best i can do  :wink: i\\'m using nimfa.  if you find something better let me know.  there are a couple packages in matlab as well.  r also has an nmf package f y ithanks guys!i\\'m writing python code, so i guess i\\'ll stick with nimfa or sklearn.: any reason in particular you\\'re using nimfa over sklearn\\'s nmf?nimfa\\'s api gives finer control of the l1 regularization out of the box.  basically i want to increase sparsity in one factor but don\\'t care about the othermakes senseone last question: do you know how it deals with missing values in the utility matrix? does it impute them, or does the cost function just skip over them?i don\\'t know offhand but i assume you could control for that by preprocessing the utility matrix.  thanks  ! i\\'ll investigate furtherif anyone is interested in running 5k/10k/half sometime in the spring, a few of us are starting to look for races in  - maybe a fellows reunion run where we get insight jerseys. join in if you are interested!to cover letter or not cover letter, that is the question.i don’t find prose format particularly useful at the resume stage of applicants.  portfolios + key skills sections are a much better alternativeagreed with , i will read a cover letter out of respect if it is attached, but it will more often sway me to reject than to accepti’m also put off by cover letter requirements on teh applicant side of things.  when i was applying for positions i always skipped that part even if it was required, opting to miss out on the opportunitytoo much fluff needed in that format for my liking :slightly_smiling_face:hi all, i\\'m a current fellow that also  has a 1.5 year old son. i just started the channel  to commiserate about how our kids are ruining our careers (joking). more seriously, i\\'m really curious as to how everyone has navigated their careers with a family.i’m taking a look at this hortonworks schema registry, supposedly it doesn’t require kafka wow, cool. i’ll give that a look.welcome fellow fellows!can you invite others? i\\'ve invited everyone from our class, but it\\'d be great to have all fellows on this same slack, don\\'t you think?hello world!tomv: how\\'s dow jones / news corp. treating you so far?hi all!hello!: fabulous thank youi\\'ve only deeply offended 4 people so farso it\\'s a major winalthough i\\'ve spent the last week doing nothing but flask -- the irony has not escaped me....was one of them rupert? :simple_smile:unfortunately not, but my next target will be one of the fox news talk show host/giphy fox/giphy hannitylamehahagreenhouse all good?very good. lots of interesting data and good nice co-workers. i don\\'t feel like i\\'ve been doing too too much yet, but i can see some meaty data science on the one-month horizonshould i start an #impostersyndrome channel? :wink:we are hiring a senior data scientist on my team at square. a lot of happy insight alums here! happy to answer any questions. question about evaluating offers that i’ve had a hard time answering via internet searches: what is the value of stock options in the case where the company doesn’t ipo or get acquired?: have you taken a look at this? hadn’t, will now. thankstl;dr: if it’s private, you can only estimate to the best of your abilitiesshort answer:  absolutely nothing.(to the question of what is the value if no ipo or acquisition). this is on top of the usual estimating you can do about the potential value in an ipo and all the the other caveats such as vesting, dilution, etc.statistic i got from a vc back in 2008:  1 in 10 of funded startups result in an ipo, 2 of 10 in an acquisition.and absent those exits, shares (or unexercised options) are essentially a worthless, illiquid asset, right?yes. though there can be exceptions such as the recent twilio buy-back: …but they may be preparing for an ipo so this may not be much of an exception. a colleague who has exercised some twilio options said the repurchase offer came with a very strict nda - it wasn’t officially supposed to be public.preferred stock is usually investor stock.  employees have common stock.  although sometimes some founders elect to participate in a funding round and purchase some preferred stock too.hey co-fellows! does anyone know of a ready-to-use method to get feature importances from a multioutput randomforest classifier in scikit-learn? thanks!, hadn’t seen it, thanks!: i started a new channel  as a place to discuss tools that improve the quality of life of data scientists in the wild. there is a rapidly expanding ecosystem of ds technology out there, and hopefully we can crowd-source insights about the best solutions.tcmoran: excellent idea!hi all. i\\'m working in a poc for a recommender system based on cf. i recently came across variational autoencoders and though it might be a good idea to use them to generate new samples in the input domain in order to reduce sparsity. any thoughts on this?anybody going to google i/o?\"you\\'re going to overfit!\"i have hopefully one last question - the webpage works great when i run using ./run.py from the command line, however the output page doesn’t work when i use gunicorn/supervisor. it gives a syntax error: \\n\\n    cur.execute(query2)\\nprogrammingerror: syntax error at or near \"(\"\\nline 1:  create table  (date timestamp, steps real, distance real, c…\\n\\nand i’m not sure why this happens with gunicorn/supervisord but not with running the python 2.7 code directly (./run.py). any thoughts?we figured it out :slightly_smiling_face: it was an issue with time in gunicornsays it runs on vm. i wonder if that vm has access to gpu.  linkedin is open-sourcing a very useful tool called \"dr. elephant\": seems like a very useful site especially for people getting their bearings in python:\\n\\nthoughts on the best unit testing package to use in python?  specific use case is to unit test on functions heavily dependent on luigi.we use unittest, it works, i like it, i don\\'t have enough experience with the other frameworks to give a pros/cons list though. :confused:anyone did kaggle competetion: allstate purchase prediction chanllage?hi all, does anyone have experience working with large scale geo-location data? in particular, i am looking to find a way around using the haversine function.using the mgrs grid system is a good way to organize geospatial datai\\'ve used mongodb geospatial indexing in the past. easy to configure and fasti understand elastic search has similar capabilitieswhy do you want to find a way around haversine? is that a bottleneck for you?analyzing the data for feature-selection before making the train/test split. i was guilty of this during some data challenges i did at insight :stuck_out_tongue:. there\\'s a good discussion about it in elements of stat. learning, 7.10.2.i do, long story short is kinda of a pain in the butt so far for me but i am managing to do it... my tech stack currently looks more like a babushka than a clean barbie dollfor fast processing i use mapd but it\\'s geospatial capabilites are still nascient and it\\'s scale still limited... so i had to combine this with postgis (not scalable)... i still haven\\'t found a full scale solution... i am looking into geomesa now (spark-based geospatial processing engine)as of haversine, what are u specifically doing there? thanks.  thank you. the data is already in a sql db and cannot be moved to another db. i am trying to find locations that are in a certain radius of another list of locations. the large scale of data makes it very expensive to compute haversine for all of the data.i am looking into mgrs right now.+1 to  if you organize points on a grid that could help and then you\\'d approximate the distance from point a to point b as distance from grid centroid that point a belongs to, to grid centroid that point b belongs to... you can easily convert points to x miles by x miles grid using any db, i did that in aws redshift using python udfs... and then distance from grid a to grid b is known since you built the grid.. i personally built my own grid since mgrs cells are too large.. look here: as of the data being in sql it sounds terrible to have large scale data in a sql db... but well, it shenanigans are always hard to fight...look into vowpal wabbit — lightning fast linear learner seems like your data is in dense format, even though it’s sparse, if that’s true, i would recommend converting it into sparse format, which will drastically reduce the size of the data and allow you to try wide range of models. \\none more suggestion, try other encoding schemas other than one-hot, this will help with size of the data. some that come to mind are weight of evidence, label, frequency and mean encodings. in my experience they work better for categorical features with higher cardinality,additionally, some algos allow checkpointing. this will allow you to train in batches, but personally not a fan.engineer humor uploaded a file: watch out! if you flat map too much, the beer is going to get dumped in the cooler! :wink:i’ve found postgis very useful. it handles a lot of these grid computations very wellgreat idea! just learned pipeline in the odsc!these are all great ideas! thank you! another thing i am thinking is cat2vec with keras.facebook median salary at 240k... that seems highanybody find a clean way to insert github, email, linkedin symbols in word for resumes?: fontawesome font, install it on your computer and then save the resume as pdf. you can find the cheat sheet here and just copy and paste the icons into the word doc. thank you this is currently my favorite data science application:   so, so usefuldoes anyone know of any interesting papers/tools for conducting audience segmentation in news media -- e.g. identifying engaged vs. experimenting vs. unengaged readers? i\\'m currently reading this paper  but am looking for something for recent.for the jupyter fans out there: has anyone played with declarativewidgets and jupyter-dashboards?for those of you interested in attended pydata in sf on aug 12-14, use the code insight for 15% off early bird admission and 20% off regular admission. hi all! i posted a couple months ago about an open position at coach, inc. for a data scientist. right after this, we acquired kate spade and hiring was put on hold. the position is open again, and now we\\'re a bigger company with even more data. data science at coach is a green field right now and we\\'re only limited by having enough people to get the work done. our work touches all levels of the company from understanding the customer, optimizing marketing and pricing, dealing with fraud, forecasting sales, product design and advertising, hr analytics, etc. here\\'s a link to the job description, and please let me know if you are interested:\\nhey guys, got wind of this awesome plugin for jupyter notebooks on atom\\nhello fellow insighters (hah, see what i did there....i\\'m here all week):\\n\\nhas any worked with twitter data where they are trying to find the number of hashtags mentioned in a certain time period? how easy/difficult it is to do this?\\n\\ni work for boston, and the mayor\\'s office wanted to get some data on how many tweets are made using the #mbta hashtag for all you boston alumni. the mayor mentioned to the press that the mtba was generally reliable, so we\\'re trying to get him the more correct information so he\\'s aware of mtba\\'s performance. :wink:\"once again, the train is on time. #mbta\"i guess all of you could do some mad social engineering to the mayor if everyone hashtagged positive comments. \\n\\nthe idea is to have a dashboard in his office so he can look up at a screen and see how the mbta is doingwho is running mbta, is this information accurate and can we get them to come fix the ny mta?haha i\\'m not sure you want the folks running the mbta to fix up the mta...boston\\'s transit is notoriously terrible as welli mean, for a system that\\'s as flammable as it is, the mta works pretty ok, until it catches fire.maybe we could do a mayor swapsomeone created a website that tells you if the dc metro is on fire  . i\\'m sure they could use some help tooas someone who has lived in both boston and nyc, i\\'ll take the mta over the mbta anyday. except i\\'ll trade the card taps for the card swipes.are they trying to measure the sentiment of all tweets with the hashtag #mbta? or just the number of them? because if it\\'s the former, it\\'s not going to look pretty. i\\'ve met a lot of people that only joined twitter to complain about the t to the t.no sentiment. just raw number.the point is actually to get the mayor to know that people complain about the mbta, alot, and to get him to noticesomeone from the policy office was frustrated at his comments that the mbta \"generally runs pretty smoothly\"yeah, right. i especially liked on the t scheduled daily interruptions of the d line during morning rush hour (from 8:30 til 3pm) for scheduled tree removals. couldn\\'t they have done this not at rush hour???sometimes marty says some pretty weird sh*ttoday i learned that the mayor of boston does not have a charlie card.he gets escorted by a security detail from his dorchester home so...yeahditto to what kevin said. it is great if you may have multiple different clusters of outliers. but if you are looking for a \"normal\" cluster and an \"outlier\" cluster perhaps something as simple as k means may work. i would personally run dbscan and set a frequency threshold like if the cluster didn\\'t have at least  1% ( for example) of the samples it would be an outlier. fix that threshold based on the parameters of your study. thanks to all the replies! you guys rock. ```in 2016 google\\'s ai had an iq of 47.28. it came out ahead of chinese search engine baidu (32.92) and microsoft\\'s bing (31.98) and had almost double the iq of siri (23.94)... in 2014 google\\'s iq score was 26.5,```\\npretty impressive, almost 2x improvement in 2 years. wonder if it plateus now    thanks so much for all the input!  i may ping you guys individually.  i’m seriously exploring this because i may potentially need to get this set up!alecalthat’s what i do. i communicate clearly to both sides “person x is looking for a job at your company”; the point of the chat (with resume) is for person x to learn what the company is really like and the position actually is, and for the person at the company to meet person x so they can make a more informed decision if they want to recommend them. you can often hear about job openings before they get posted online if you ask around. also a personal reference carries far more weight than a random online application!oh that sounds perfect then! :smiley_cat: uploaded a file:  uploaded a file: to fellows in the nyc office. i lost a pair of blue bose headphones. please let me know if you see them around.  thanksluigi vs airflow wondering if anyone has experience or opinion.  ps relatively unsophisticated data team.   we use luigi pretty extensively at work. so far, so good. relatively quick learning curve to get people writing tasks and we haven’t had many issue. happy to dm if you’ve got more directed q’s about it. can’t say anything on airflow. i’d recommend talking to  about airflow. he’s used it quite extensively. we looked at both a few months ago and decided on airflow.  luigi is really elegant and simple but ultimately we needed the full functionality of airflow to accommodate a pretty complex workflow.  i\\'ve found airflow to be frustrating at times but it meets our needs.  if your workflow is standard and relatively simple, i\\'d pick luigi.  it will be easier for your data team.thanks  and  maybe i will try bothran across this recently:  (look for links to more extensive reviews in row 3)we use luigi on my team. the choice was made before i arrived, but i’m told that luigi won over airflow because it’s really easy to install.: this might be a good place to start: \\n\\nwhat do you use/recommend for scraping sites, beautifulsoup?!old school regular expressions!but sure beautiful soup is okay.. if you want it the easy way. heh.yes beautifulsoup is the way to go.scrapythanks  and !if you need to digest pages and interact with javascript on pages in an automated way, you might try selenium. i haven\\'t tried it myself, but i have heard rave reviews.\\n\\n: damien had suggested splinter instead of selenium.  from my experience, selenium isn’t the fastest.good to know, thanks!splinter is also very easy to use used selenium for his project i believe.hi channel, does anybody have experience on hyperparmeter optimization on a large dataset?  can i optimize hyperparameters on the subsets of the dataset, and use the average of these to train my model on the whole dataset? do the hyperparameters of the machine learning models depend on the size of data?: what type of model are you using? in an lda (say), one hyper-parameter controls the number of topics per doc, which can definitely depend on the size of the corpus. another controls the words per topic, which wouldn’t depend on corpus size provided your sample of the data is truly random.: i am using supervised models such as random forest, gradient boosting. so i am trying to optimize number of trees, tree depth, etc.yes, it’s totally ok to optimize on a subset (esp if dataset is large) as long as it’s representative. nice thing abt rf is that it doesn’t need much tuning except ntree. gbm, gd luck with tuning =pthanks  isn\\'t tensor flow using the gpu instead of intel? which instance are you using?^^exactly! that’s why this is so confusing. i’m using a p2 instance, which uses an nvidia gpu and an intel cpu.  the time per step does seem to show slow downs at intervals equal to my batch size, which suggests that the data loading is even more of a bottleneck. but shouldn’t the training be largely unaffected once it’s on the gpu? because if that assumption is correct, a 15-20% overall slowdown would suggest that data loading of the intel cpu is slowed far more than just 15-20%... hi all, there\\'s something i\\'ve been wondering about recently and hope there are people here who can share some perspectives. i\\'ve noticed a few of my younger acquaintances have recently been hired as data scientists straight out of college to work at established companies like facebook, apple, etc. in my mind i would like to think that data scientists with a phd background offer value to companies that ba\\'s and bs\\'s cannot offer, but i\\'ve never had the opportunity to find out for sure. so my question is, for those who work with both phd and non-phd data scientists, do you see a noticeable difference in quality?i think there\\'s a lot of ds work to be done all around. some work is more about designing metrics, coming up with hypotheses, designing experiments, visualizing data, testing, etcsome is more like de, some is designing new algorithms in deep learningphds may be more biased to enjoy certain areas and non-phds may gravitate to other areas tooi think there\\'s some truth to the stereotype phds typically do spend longer on projects and have a tendency to go for 100% perfect, while non-phds get shit doneso if you\\'re a huge company and have 100 roles to fill, you might be better off taking a super-smart person from a bs and training them to be a great ds rather than taking a great academic and training them to focus on short-term deliverablesthere\\'s a need for both, to be surei\\'m curious to hear more of others\\' experiences. here at salesforce, people hired at the bachelor level are \"data analysts\". i think they can level up to data scientists through work experience, additional classes/training, and achievement. phd and masters level are treated as work experience and specialized training and start at a higher salary/grade.i think our company is similar to what people described above. we have a range of degrees, and more of the bs/ms people are in bi positions, while the phds are all in r&amp;d. one of the big distinguishing factors for phds in my experience is the ability to work on larger, unwieldy projects. that’s certainly learnable, but i wouldn’t expect someone right out of college to be able to take on as large of projects until they were more experienced.i\\'ve worked with both types of data scientists (as well as sat on hiring panels for both), and i think there are advantages as well as disadvantages to hiring either. phds do have the added skill set of thinking scientifically, but we tend to have a tougher time transitioning to the speed of industry than someone out of college. companies typically will hire college grads into lower level positions, whereas oftentimes hr and other factors dictate that phds must have higher salaries and roles. i\\'ve also noticed that some companies prefer to train up their own data scientists from scratch (for better or for worse), which again lends itself to college grads at lower levels. ultimately it comes down to the preferences of the company and hiring manager. i agree with \\'s point about it all being conditional on the company/culture/needs. i would add that there is also a lot of value in a phd knowing what you don\\'t know, which is typically not the case with a bs. that might not matter in certain company/culture/needs, but it definitely matters in some environments&gt; value in a phd knowing what you don\\'t know\\n\\n:100: this is clearly from what i have seen across my industry, and i mainly work in the area of cybersecurity, fraud and malicious behaviour - state/antistate/criminal actors. there are two kinds of phds ones who have graduated with work/thesis where their pi is running a project and these students have contributed a large body of work that has earned them a phd. the second type are students who work with pis where there is just a pile of data (call it pile of junk) and their pis wants them to explore and come up with insights/hypothesis/proofs/algorithms.   the second type is purely exploratory. you fight with the data, do years of analysis, come up with ideas about projects/thesis then identify insights, create hypotheses, and eventually publish them and earn a degree. when  phds transfer to the industry, the later kinds are the ones who are the ones who get into positions of driving projects from start to finish *faster* as they have done the same thing multiple times, that does not mean that the former kinds cannot, it just means that they need to be bootstrapped a bit - work with a seasoned analyst/scientist etc, and they get the hang of it. the hang of it being owning a project, the success and the grave failures that come with any research challenges. this approach is evident across all people i have worked with. people with with bs/ms degrees are not much different and that is the reason i really do not focus on the degree one has earned while hiring them. i primarily look for ownership of the project. can you own something? can you look ahead and think of pitfalls, validation mechanisms, can you imagine pitfalls, what are your recovery plans, contingency plans etc.  few folks with bs/ms degrees too have gone thru similar training and that makes them not much different than us and they can operate autonomously - that is important. but this number is small, because bs and ms does not give one the luxury of time to explore data for extended periods of time. so, quite a few folks with bs/ms degrees require some amount of boot strapping.  *my experience comes from a field where there are no prior algorithms and you almost always have to start from exploring data.*  that might not be the case in other fields.i was going to reply with something similar from the finance perspective but i think that about sums up what i\\'ve seen.  only thing i\\'ll add is how bad a new hire is if there is a mismatch between the manager\\'s expectation and the new hire\\'s actual ability to own their own projects.  in the past, we\\'ve made the mistake of hiring people of the latter type under managers who expected them to do their data science manual labor for them.  those hires end up leaving once they realize the opportunity for career growth doesn\\'t match their expectations.we\\'ve also made bad calls in the other direction: hired people who need a lot of hand-holding when we expected them to work independently.  those people end up finding different managers.purely by experience, i have figured that you could have the mismatch in hiring the one talked about by  . in that case, i it would be great if the manager takes up the onus and is flexible to operate in both the styles, makes it harmonious.thanks for all the thoughtful responses. it was helpful to realize that there are many different types of data scientists out there, and at the end of the day it\\'s about finding a mutual fit.i do find that phd data scientists on average offer advantages.  but it\\'s also true that i\\'ve worked with some bachelor data scientists who were extremely talented in all sorts of things, from modeling to experiments.  so it\\'s not surprising to me at all to see companies hiring these people (assuming this is who they are hiring) straight out of college.  \\n\\nas good scientists, we should keep in mind that just because the average is some amount, doesn\\'t mean there aren\\'t a handful of exceptional outliers :slightly_smiling_face:and just as an anecdote, someone i worked with in the past started with just a bachelors and behind the phd  and masters data scientists at his organization, but within the course of a year went on to basically surpass all of them in application of modeling techniques at his organization (though not quite as strong on the experimental side as some of them).  \\n\\nwoudl definitely hire this person out of college easily, if i could somehow identify them :slightly_smiling_face:i would be happy toanyone have experience with building a google chrome extension for the final deliverable of their project? currently, i\\'ve implemented a flask-app that takes in a url, scrapes data, generates features, and outputs a probability using an embedded model. i\\'m thinking an extension would reduce the friction of physically visiting my webapp and inputting a correctly formatted url--it\\'d ultimately still report a probability. i\\'m just not sure if this is easy to do in a few days, or if i should just stick with snazzing up the web app. thanks! ^^that article looks cool, but wow, that photo is majesticwe have microservices (maintained by de outside of juypyter) that provide access to what\\'s in s3. then jupyter notebooks can make api calls to those serviceshi, sparkcognition (my company) is desperately seeking for data scientists. this is an ai startup (founded in 2013) and grows bigger every day. it currently has 130-150 employees and 1/3 are data scientists. there are lots of cool data science projects (nlp, vision, autonomous driving, finance, security etc.). feel free to connect with me if you need a recommendation. if anyone is going to the grace hopper conference this year and wants to come to the ghc square events, shoot me a dm!thanks for posting!\\n\\n\\n\\nis referenced in the post and seems super interesting, but i’m wondering how a distributed protocol like this is not susceptible to malicious behavior. : deep learning and nlp, via the useful @pythonweekly on twitter ^^ tufte is good here too: thanks! that is very helpful, do you mean programatically within sql(redshift) or using python to query and find counts?my dev-sql skills is not up to par, but i know it’s fairly easy through python/pandas to get column names programatically and do sums and counts.the prob with that is that it requires me to load the whole data to a pandas df, which is not good (i guess i could use chunksize and it would be a generator)you don’t need to load the whole data, just do a count(*) or somesuch on your table.the alternate client is tempting, but i fear we have a network effect that doesn\\'t transfer :disappointed: my favorite:  really good for communication in generalwish you were a few months earlier! i love boulder!  i\\'m going  : moving into my new place tonight :s but would love to hear your impression of the meetup after sure.  maybe lunch next week?: with pleasure. i\\'m working somewhat near insight. calav a place where you\\'d like to have lunch?opinion piece on toxic data: sounds great.  monday at noon, you pick the place on cal ave: how tempting to get good old chinesei can taste general cho’s chicken already :simple_smile:hello channel: do you guys know of a way to retrieve query status when you connect to a database server using sqlalchemy?  waiting and hoping for the best is not the best solution, and would be great if there is a way to retrieve query status while waiting!are you using a cursor to query using sql or are you using the orm?i am using a cursor to pass sql query to the databasei mean, she wasn’t wrong thoughthis mess is a place^ a universal statementnice to know twitch keeps you slack game up to parmy company, pear therapeutics, is hiring for a sr. data scientist with potential to lead an experimentation team. you should excel at experimental design and advanced statistical modeling. the position will include the exciting opportunity to lead the development of a scalable system to test the efficacy of pear products. chance to work at a rapidly growing sf startup that’s doing a lot of good. feel free to message me if interested  question for data scientists from canada working in the us under nafta tn statuswhat tn profession did you indicate? computer systems analyst or mathematician / statistician?started by same author as  and : i\\'ve been using the xlsxwriter package with pandas, and it is pretty intuitive and has url functionality. : a somewhat random request for those that are active on this slack (alumni and current fellows). would folks mind updating their profiles with their insight program-location-session, current role &amp; company? i find it useful to know more about people than just their username, especially if i haven’t met them in persongreat suggestion aren\\'t you on a boat in greece ?alright. i no longer do \"sweet, sweet data science\". i have a boring job title in my profile. unfortunately the boat has ended :wink: in seattle today you should have negotiated for a less-boring job title like `sr. director of data science and chief slayer of dragons`if only i was a good enough data scientist to predict how bummed i would be about the situation.it\\'s a great idea! i saw someone doing that a few months ago and changed mine to matchdone!: anybody here has been working on churn prediction? i would like some pointers regarding where i can start learning it. thanks! and  i thought we were c? did it for his insight projectno more @ here ing uploaded a file: looks like someone woke up the ruth monster i had the same q earlier and had a chance to talk to some of the fellowsi can give you a transcript of the conversations if you’d likeabout not using @ here, duly noted :slightly_smiling_face:, would like to see the transcript.there’s a channel we setup last session i believe for fellows to chat about churnno reason that alums can’t use it too :slightly_smiling_face:thanks! looks pretty empty, not sure if slack has deleted old stuff or fellows didn’t end up using itprobably got deleted by the output of i didn\\'t even know about the channel stop blaming the foreigners  stay positive till_bethere is a ltv channel, is case anyone is interested. i created it couple of weeks ago.: i just used survival analysis and bullshitted my ways through the demos  don’t share the secret sauce!i think one important thing is that your definition of churn will heavily depend on your business case i’ve seen others do survival analysis (non-insight), is there an issue with not doing cv? i.e overfitting on data?i’m not too too familiar with survival analysisme neither. i think i ran it on different samples to prevent overfitting but i can’t quite remembermight be easier to approach it as a classification problemyea, i was wondering what the pro/cons wereherelet’s move this to lolnuggetbot animate me pikachucrap wrong channel sorrystay positive scstarkdoes anyone know if there is a common statistical analysis that allows you to evaluate two outcome variables at once? for example, fitting a slope and intercept to each group in a study, and being interested in whether other variables are correlated with changes in those slopes and intercepts. it seems like evaluating one in the absence of the other may not be appropriate but i\\'m also unaware of a common technique that allows you to take both into account at once. in the case of this study, the slope and the intercept both reflect two different specific ways in which the other variables could relate to a change in these outcome variables.i\\'m not sure if i\\'m explaining this clearly enough.: they\\'re bread and butter macro-econometrics, i\\'ve never seen them in ds. so they probably suck at prediction.stay positive patrickdmy first nuggetbot :heart_eyes:: in the simplest case you could try a mancova, or you could look into structural equation modeling. if you want something more focused on prediction rather than inference, you could try multivariate tree boosting () long live net neutrality!: if both variables are dichotomous, you might want to try something like a bivariate probit (), or else some sort of seemingly unrelated regression (sur) setting if you can comfortably assume that the relation comes through the disturbances (). clearly both methods would be inferential...thanks for the suggestions everyone, i\\'ll take a look at these.there’s a mention of a y combinator experiment in this article. does anyone know anything about that?\\nubi (universal basic income) is all over the news recently, partially thanks to the swiss’s (rejected) referendum on it. y combinator is supposed to be running an experiment unconditionally giving out ~$1000/month to some hundreds of families in oakland (just remembering these numbers from my head - don’t quote me on that) to see how it affect people’s behaviorsthere was a couple of announcements on hacker news as well (in case you didn’t see these yet) - \\ndoes anyone have experience passing variables in flask between html pages/ app.routes? thanks!figured it out using session!!hi can a de person help me out by confirming / correcting my plan to take data from our current internal oracle db, convert it to parquet and store in aws s3?(private message if interested)seems like a good plan if you’ll be analyzing it in spark in the futurewe currently have an internal spark cluster but we’re migrating over to aws. i was just curious how/when the best way to do this is.we currently use sqoopanyone used airbnb’s superset (née caravel (née panoramix)) with nosql databases? perhaps mongodb?the git issues are unpromising/”left as an exercise to the reader”-esqueyeahi\\'ve done some with ithi! i am looking for volunteers to represent insight and tell the story of your transition to data science. the catch - it’s through the lbnl postdoc association and they would like specifically people who did their phd or postdoc at lbnl. it would be in the evening of 3/7 or 3/9, please let me knowbay area peeps: my old company (palantir) is hosting this event in downtown palo alto on thursday 2/23 that looks pretty cool — i have school that night or i’d be there! check it out and rsvp at the bottom if you’re interested: “it starts with girls is a year long, five part series that investigates each part of the pipeline in engaging girls and young women in stem.” the speaker for this one is debbie sterling, \"the founder and ceo of goldieblox, an award winning company on a mission to ‘disrupt the pink aisle’ with toys, games and media for girls.” anne: i can helplooking for someone with extensive *marketing ds* experience for a few hours of *paid consulting*. dm for details or with recommendations, thanks!hi all, we’re a small startup focused on helping companies make better hiring decisions though data analysis and predictive modeling. we are looking for a *(data) engineer contractor* to support us on a project in a few ways:\\n• access production data systems and build etl pipelines\\n• identify, assess, and execute pdf parsing services in terms of cost, feasibility, and accuracy\\n• work with us to make infrastructure recommendations to support data analysis and modeling\\n• time estimate: 60-80 hours\\nplease dm me if you or someone you know might be interested.how do folks distinguish between data analysts and data scientists, in terms of job descriptions/recruitment/hiring? at my company we are having a tough time emphasizing that we don’t necessarily need the full qualifications for a data scientist, but instead want someone more junior that still has statistical chops. any input is appreciated!when you say you\\'re having a tough time emphasizing it, how does that manifest itself? do you have too few applicants from people of the desired skillset / salary expectations you\\'re looking for? or is it you(r team) emphasizing to higher-ups that they’re overshooting on what they’re looking for? i.e., the classic “5 years experience in a 2-year-old tech” job posting request or they’re just gung-ho for a phd/senior-level when it isn’t necessary? an excuse to skip your siesta.hi everyone! i’m posting this for a friend.  vl49, a new company from the same flagship venturelabs that started my company, is looking to hire for a principle / sr. computational scientist.  the role is focused on using ml on multiple ’omics and high-throughput datasets to drive therapeutics development.  it’s a great opportunity to lead the computational arm of a well-funded early stage startup.  dm me if you’re interested.\\n[unrelated] i know a friend looking for a da positions(from engg backgrounds). if you’re looking, let me know. can share their resumes :slightly_smiling_face:has anyone tried out the packages for slack integration with python and r? they\\'re really easy to use and pretty useful. i have automated scripts sending daily metrics messages into some of my company\\'s internal channels.i\\'ve used the python package for slack to do some visualizations of channel activity at work.this guyhey everybody!  i just created a new channel  - a place for people to chat about founding and starting companies.  we have seen fellows and alumni go off to start their own company, or have a strong desire to do so even if they dont have a product idea of there own.  let’s plan to use  to connect!ps - insight also gets early stage companies that are looking for ctos and founding heads of data/eng.  currently have a couple interesting opportunities kicking down our door.  pop over to  to find out more.has anyone used neo4j in a production environment, and in particular within docker? we’re running into some snags getting it off the ground and i’d love to pick someone’s braini\\'d be interested in this as well! now we just need someone who knows it hahit’s a delicate dance to get right. depends on the workflow you want to have. for ex, if you have a shared directory accessible from different team members’ jupyterhub then by default notebooks created by others are executable but read only to you. how you structure your version control becomes an interesting complication. i don’t know how to compare it to other platforms. thanks jonathanwould anyone that has experience with facebook\\'s graph api be able to answer a few questions to help me get an app up and running?any recommendations for movers from sacramento to san jose? i got a quote today that estimated we had over 4 tons of stuff… that’s roughly 4 ford focuses :car: :car: :car: :car:beltmann relocation groupwould be one i can think of, thank you! i’ll ask for a quote from them.i had a good experience with  nuggetbot: animate me settinghmmm.... i know nothing about thatgood settingi am doing a price comparison between aws ec2 and another service, would people here mind saying what they spent on a monthly basis hosting their insight project on aws?~$50 /month but can\\'t remember if it was for a small or medium instancethanks chloe: : thanks. i am aware of the pricing model, but i find its still hard to accurately predict what the monthly cost would bethat is probably by design :simple_smile:eh?what is the study for?i\\'ve done some cost comparisons in the pastec2 is rather opaque because of the reserved instance pricing schemegce is rather nice because your discounts automatically kick in based on utilizationthis is a new feature thoughi am evaluating another service for my own personal use that uses a flat rate pricing model once you\\'ve reached 1 month and i am trying to figure out how much i\\'d save or how much more i\\'d spend vs using awsit really depends on your use casethere are many many many vendors that provide a vps service(virtual private service)at the lowest end that can beat the net cost of a t1/t2 micro ec2 instancebecause the lowest tier instances require that you use ebsexample: digitaloceanlinodeetcec2 provides the largest variety of instance configurations or \\'flavors\\'what is your use case?many different casesif you expect to have sustained use, the appropriate ec2 strategy would be a reserved instancein one situation, i want to experiment with building a cluster of elasticsearch nodesin another situation, i want to build a small web application and host it, perhaps with a database such as mysqland finally, i may also be interested in hosting several static or dynamic websites (not sure yet)which of the cases, if any (or all), would be running in perpetuityand -- do you want to be a sysadmin for all of the websites?that depends, as of now, i am not surefor standard web hosting, a resellers account at a webhost probably makes the most senseunless you want to also be a systems administrator and want to build everything from scratchfor your web applicationswhich i doa vps or \\'droplet\\' from linode / digital ocean probably makes a lot of senseand for a cluster/lab --- ec2 / gce / azure probably makes the most senseyou can also build your own stuff from metal :simple_smile:yeah, i already have several servers at home which i am using currently, but due to network issues in my area, they are no longer reliablehow about a colo ?haven\\'t looked into that option, but thanks for giving me another ideacolo is classicdon\\'t forget to check out rackspace too for a vendorif you wanted toyou could provision yourself a virtual private cloudand use that as a labhomelab!it typically turns out to be more expensive in the long run if your utilization is greater than 33%which is why homelab ftwthanks, i will look into ithomelab + colo would be a really cost effective lab setup if you find someone friendly -- keep in mind you\\'ll need something that fits in a smaller footprint than most homelab gear since they\\'re charging you based on rack space, power, hvac, and the network connectionif you’re already using jupyter notebooks, extensions + jupyter widgets is probably the fastest and easiest solutionyou can just spin up your notebook dashboard w/widgets, using your extensions to hide input cells/lock editinghas anyone used loftey in nyc for finding an apartment?*what separates _great_ data scientists from the _good_ ones?* if you have opinions, ideas, and/or anecdotes on this topic, i\\'d be excited to hear them! please add a comment to this thread or send me a direct message.short q, is there a way to get list of all actors, directors and movies from imdb?  how big is that? by that i mean title id, title name, title rating, title genre, title type, country, director, actors (4 main) contains info about the data they make publicly availablei don’t know if it helps but might have an starting point via the bokeh gallery, upper left. has anyone had to set up a vpn on their aws instance because a site they were (in this case scraping) blocks the ip range from aws? uploaded a file:  and commented: hi alumni interested in health!\\n\\nmy husband (who’s a high school math teacher), wanted me to pass along this community service opportunity that’s taking place in oakland on february 17:\\n\\ni wanted to invite you to help support the new 9th grade interactive assessment for our integrated project about cancer. this february, 9th graders will be looking at the many faces of cancer through a deep study in english, ethnic studies, biology, bioethnics, and math classes. they have been given the task of testing the genetic material of a family that has a history of colon cancer for the hnpcc gene. they will then be given the opportunity to be either a patient receiving these results who needs to ask the right questions, or the genetic counselor who will be delivering these results to various family members. \\n\\nthis is where you come in!\\n\\nwe are looking for community members to assess our students’ simulations of this conversation live in the patient rooms of our on campus health clinic. following the conversations they will be writing a formal lab result of their results. \\n\\nwhen: february 12th \\nwhere: life academy health clinic (best entrance is on 34th street).\\ntimes: students will be engaging in 15 minute role plays from 8:30-12:25. we will be scheduling students and would be happy to have you for any portion of those times slots. you can come for one 15 minute session, or stay for 4! \\n\\nplease see the attached flyer and we are looking forward to having you support our students in this real world learning.\\n\\nthank you very much.\\n\\nsincerely,\\n\\ndavid gardner\\n*please rsvp to david gardner at . i think those are sample datasetslike this would be all the data which is not muchmovie 467,523i am wondering where\\'s all that availabel u may be right these may be all the titles - it seems to be missing country of origin for director, actors and titles, but the rest seems to be all present imdb sells the full data set... same with rotten tomatoes, etc... you can ping limited amount of cases using their apis, but not get their full dumpwould anyone have experience saving a checkpoint model with just the relevant trained weights, ie. the minimal needed to restore the model.  i’m working on trying to package my model while trying to save space.  it’s currently 800megs.  that’s the output of `tf.train.saver()`.  i’m looking for a way that’s more efficient on model size.  any input is appreciated. thanks! hi i am using flink complex event processing and i heard that you had issues with out of order sensors?nlp solves the voynich manuscript language: was it a d&amp;d manual all along as suspected? ;)ancient hebrew. the original sourcebook language.whoa!hello! does anyone have any experience playing videos from aws using flask (with nginx and gunicorn)? i\\'m trying to play a .mp4 video that is saved in my static directory. i tried two methods: (1) a redirect to the video filepath in flask, and (2) rendering a html template with the video filepath (\"/static/video_filename.mp4\") as the source. any help would be appreciated... thanks!the government uses evidence? :thinking_face::grimacing:in all fairness , the fed is not really \"government\" as much as a hybrid bank-regulator in the financial sector... :wink:they have the best and biggest evidence!hello wise former-fellows — current fellow here: what do you think about job titles? as far as career growth, how critical/useful is having the “data scientist” title?  some of the companies are offering positions with other titles (research analyst, quantitative analyst, etc.), and i am wondering whether its something that should be given strong consideration when examining offers. thanks!i would also love to know what the wise former-fellows think about this. and also, do the different titles correspond to different wage brackets (assuming the roles are ~equivalent)?although it should depend mostly on the actual skills shown in your resume, i would rethink taking a \"data analyst\" position, as it has a different foundation and career trajectory than a \"data scientist\".piggy backing on what  said, i will suggest talking to the team about the role and what the day to day job entails to get more clarity. lot of established organizations has their own nomenclature for data-scientists, so don\\'t be disinterested just coz job-title doesn\\'t look that fancyi have been presenting in countless meetings where i had to explain performance metrics to non-technical people. i wrote a medium blog about this topic. please let me know if you have any comments or edits. i think some of the fellows in our cohort negotiated for a title changeyes, title change is totally on the table when talking about offers.i negotiated for an external-facing title change. outside of large tech areas i found that many organizations have their own terms. sometimes places would be happy to have a data analyst turn out to be a data scientist, but don\\'t know the skill set exists until you sell your skill set to them during the interview process.on the hiring side, a job title of data scientist generally gets more applicants than jobs where the work is the same, but the title is different. we have our own internal titles, but when i hire, i never use that title for the job ad. if i want more applicants, i put the job title as data scientist. i also don\\'t care much about previous job titles that applicant have held as long as their experience is strong. so, it could be that applying to jobs that aren\\'t titled data scientist could mean less competition. this is only my anecdotal experience though.i’ll agree with josh, but reiterate joao’s point: if they say “analyst”, they might really mean it is a different job than “data science”. see what they actually do day-to-day!when i’m browsing resumes i find the titles don’t make a lot of difference but they do makes some difference. i do glance over what they say they did in those roles and i do know typical alternative titles and titling quirks of some of the large employers. in fact i recently screened a resume that was pretty good but it had all “data science” titles going back to 2004. no one was using that title back then. when you look at his linkedin profile they were all non-ds titles. i felt it was too sketchy and you definitely don’t ever want to give a hiring manager a chance to question your honesty. though a colleague disagreed and thought it was ok given the work described was fairly data sciencey.\\n\\nyou definitely should demand that your title reflect your work. if you are using substantial statistics on large projects or doing engineering you probably should not have analyst in your title.and of course the point about getting a lot of clarification on what the current role is going to entail is the most important thing. plenty of places will happily give you a data science title and then hand you a locked-down laptop with only excel.what **data** pipelines do people use for working on small amounts of data? something under 10gb where you can\\'t justify a map reduce library.if it fits into memory on a single machine, i\\'ve used the sklearn pipeline pretty successfully. if it\\'s larger than that, python/pandas/dask is a solid option.for sql lovers:  \\nthe query:  uploaded a file: following on to  sklearn-pandas () is good for stitching transforms together - i’ve found it more frequently useful than sklearn’s in-built pipelinesif you need to encode more complex etl tasks, i like luigiminimal boilerplate to build the task pipeline, and it actually scales decently well (mainly due to good integration with pyspark)i was more curious about what does the bulk of the transforms. spark\\'s nice, but the map reduce paradigm works well for larger datasets, it\\'s not really needed for datasets under 10gb.i\\'ve looked into luigi though, and it seems cool! i\\'m using an aws stack though so i\\'m looking at aws data pipelines, but they seem...unpopular in the search results i\\'ve gotten because they\\'re configured via json.pandas is cool! i didn\\'t know much about it on the de side, but it\\'s the best answer i\\'ve gotten that close to what i want. it kind of stinks that it\\'s only single core though.some of pandas’ functionality is vectorized under the hood via numpy (and through that some compiled c libs) that should run more efficientlyand then there’s dask for running pandas(ish) at the cluster leveli like luigi (another similar option is airflow) since it can run p much arbitrary python for its tasksi\\'ve used a containerized airflow deployment on a single box running with the localexecutor for stuff of this scale. i used this repo: can\\'t wait until this is fully up and running... anyone want to come have wine and hear a q/a from cofounder of stripe on friday at 4:30? at 405 howard street in sf.this is matar’s talk at strata this winter: starting with a spectrogram (looking at the signal in fourier space over time) and clustering based on derived features is a good startinsight alum  just published a killer paper on a new approach: hey, i am trying to get hold of a data set of marriage information. the data should include spouse names, the spouse birthday (day/month/year) and the marriage date (iff possible)i did a bit of related work in grad school, and spectral features are definitely quite good for speaker identificationmarriage records are held by the state, often by a department of vital records. finding a dataset that includes names doesn\\'t seem very likely to me, because you usually have to request records on a one-by-one basis. however, you can probably find aggregate, anonymous marriage/divorce datasets (but, again, on a state level). thanks. i was thinking that might be a limitation as wellyeah once you are down to names and birth dates that\\'s highly identity theftable pii if you\\'re okay with older records, census data i believe gets you name, birth date, and marriage status, so you can get within a ten year period of the marriage date by comparing censuses (censi?).  but the most recent records publicly available are from 1940 to address privacy concerns.,  thanks for the info about matar\\'s project! that is super useful to have as a reference pointold timey big datavintage data@limventa  thanks! yea, i was aware of her project based on a recommendation from someone who was in her cohort. :smiley: pretty cool methodology. this site has marriage license application data as well as divorce reports. it is limited to texas. it might be helpful:  \\n thanks!i wish i had been wrong.   by no means an expert, but have some experience. feel free to dm.: thanks. i am wondering if domo has any advantages over de novo dashboards created in r or python.for silicon valley and san francisco members: a good data science conference in the area this year?anyone here done hackerx before? i got an invite and can\\'t tell if it\\'s a solid program. i\\'m happy where i\\'m at but always enjoy the opportunity to network.i see rwd groups at pharma moving this direction ^any airflow experts in here?   i am trying to to understand why when a task times-out  and fails it  does not call the on_failure _callbacksto piggyback on ^^, does anyone have guidance on best practices for writing tests for airflow pipelines? we’re having an internal debate about how much to use airflow-specific features like plugins, sensors, operators vs. keeping that code in separate libraries that are testable outside airflow context and just calling them with the python operator currently we add “check” operations before and after every task to test assumptions of the data before we land it in production tables. we’re starting work to basically write a testing library outside of airflow, and even starting to think of building an entire etl framework on top of airflow. airflow is really just a scheduler, not an etl framework, so it’s not optimized for these things.also, you might want to check out this: it doesn’t quite work with our infra right now, but could be a fit for youha. funny you should mention that. we had abe in yesterday to present to our team and had a good discussion with himi guess he’s on a roadshow! he’s coming here on monday :slightly_smiling_face:i use af, but haven’t encountered that, but try af mailing list, contributors there are quite responsive.not sure if this is relevant, in my team, my teammates prefer to do everything w/ operators, like we may have stuff like mysql to redshift operator, etc.  but i don’t feel personally strong in having dags only use operators, because a lot of the dag tasks are very one off sources or sinks that aren’t reused elsewhere.hey all, ai fellow  just published a cool post at the intersection of video games and machine learning, give him some love! hi all, if you\\'re interested in grace hopper 2018, the deadline to submit talks and workshops is next week! you can read more and apply here:  a few insight team members are working on submissions, so let us know if you\\'d like to collaborate, or if you want anyone to proofread your submission. we\\'re hoping to have an even bigger insight presence this year :slightly_smiling_face: you can also join the new channel  to share conference-related updates and eventsi’d like an emoji for “i enjoyed reading this discussion”this is great !  during insight i read an article on a/b testing that had a great plot on the dangers of early stopping, which showed the p value of an a/a test over time. anyone know what i\\'m talking about and know where to find it?maybe this one? this? i don\\'t think the air bnb one is the one i remembered but is perfect for my current needs. thanks to you both!*where may i buy a gpu desktop* (for deep learning , not gaming... of course) ? it has been 10 years since i did diy myself... i do not wish to spend my time on learning diy now. :sweat_smile: thanks maybe: figure out what it costs..then, see how many hours it will buy from paperspace cloud or gcloud or aws - spot instanceshey folks, thanks so much for the responses to the alumni compensation survey. we got about 50 responses so far, which allows for some splitting of the data among region, seniority, etc. the data is available here (with some prelim descriptive statistics on one of the worksheets): \\n\\n\\n\\nif you’re looking for a reason to procrastinate a bit on a monday morning, and haven’t filled out a survey, feel free to do so here:\\n\\n\\n\\nthe survey is anonymous, and may help fellows figure out based on region and experience what kinds of compensation they should expect from employers, especially helpful during performance review/compensation adjustment periods.\\n\\nadditionally, once enough data is collected, one can compare to other online compensation aggregators to measure the collective effect of “insight alum” on compensation.\\n\\ncheers!\\nmike in future iterations can you please track info like gender/race (provided you can suitably anonymize it)?that\\'s a great idea, i\\'d be happy to.gwen.gillingham: for gender and ethnicity, what do you suggest for an anonymization strategy? i think this is important to include in the data set, but i think maybe the best way to include it is to make it optional?making it optional will make it less useful. maybe the simplest option is just to use a randomized response paradigm? there\\'s other options that have been used for generating differentially private datasets in medical fields...(but they mostly involve generating data after collection, rather than mitigating identifiability risks during collection)hi all! can anyone talk about “productionalizing” their ml model? more specific i have a sklearn random forest model that i would like to put into production. tips, tricks, tutorials anything that has helped. thanks!we pickle our models .. save it to s3 .. and save associated metadata in dynamodb .. and the production pipeline loads the models from s3 if the models are labeled ‘for prod’ in their dynamodb metadataand make sure your models pass a sorta smoke test before it is labeled as `for prod` (i.e. expected scores given a static set of data)i’ll have to see if google forms supports randomized responses. i understand the basic principal, thats a good idea that still gives the survey statistical relevance. i’d be interested in making the survey even better, so if you (or anyone else) have any additional feedback, i’ll look into ways to implement, or just add y’all as collaborators on the form itself. uploaded a file:  and commented: hello, i am looking for a machine learning/software engineer for a vc backed early stage startup in the bay area to help implement image processing algorithms for personalized virtual reality experiences. if you know of some one who will be interested in this space, please connect with me at . thanks!hi everyone, i’m doing some research and need a handful of volunteers.  this will take about 45 minutes of your time to video conference with me where i’ll ask you some of your opinions on some data science topics, then send you a followup short survey that takes about 5 minutes to complete.  then i will get you a `$100 amazon gift card` for your time :slightly_smiling_face:   only need a handful of people working as data scientists at any level, but who are actually in an industry position, so first come first serve.  please dm me!  thanks!hello ny: with cmj music festival coming up next week, i\\'d love to hear your suggestions about what bands are a good pick. i personally want to go see the henry millers as they are friends of mine and i like their music.we’re trying to sound more statistically sound...the non-science parts of funding agencies like p-values?that\\'s my guess. grant or journal reviewers started asking for them thinking they were useful.any other folks in media talking about facebook\\'s metric mishaps these days? and to a broader degree, how much reporting accuracy falls on data scientists? given that a majority of us come from academics where job number one of the scientist is accuracy. i think someone before was saying they wanted to retrieve old messages off of slack. i figured out that on the iphone app, you can retrieve the old messages without a premium account. it just doesn\\'t work off of desktopgiven that a majority of us come from academics where job number one of the scientist is ~accuracy~ getting results for funding.fixed that for you: someone posted this earlier:  i have not yet listened to any, but would like to. if anyone has some hand curated suggestions my gym time could use some data science podcasts!: the insight project i just finished is actually a podcast recommendation app. i can’t link to it at the moment since i need the site to survive demos, but here’s some recommendations (note i haven’t personally listened to these): friday lunchtime lectures, software engineering daily, uie brain sparks, ctrl+click:   thanks all! and good luck with your demos, lindsaycan anyone shed some light on why hadoop streaming would map and reduce my input perfectly when the input is in a single hdfs file, but the mappers output nothing when the input is split across multiple files?are the multiple files partial zips? raw text.  if i run my job on the output of a prior mr job it works finei wonder if it\\'s because your splitsize is set too small, so the raw texts got chopped up and not being mapped properly, but if i use more than one reducer and then run on that, no mapper outpuyin case anyone is or knows a software engineer who wants to make the next generation of tools for genetic analyses, we\\'re looking for someone at the broad (cambridge, ma): pretty recent post about netflix\\' data stack: : anyone else at spark summit east?no, but you should share cool things you see on here. :simple_smile:hey guys! anyone at scala days today?another analysis tool: .  seems interesting because of the potential for flexibility.  potentially not quite as useful for people who don\\'t code, but i haven\\'t looked into it much yet.: i have a friend who is finishing his phd and is looking to go into data science. but he is mainly interested in working in the public sector. would anyone happen to know of any opportunities, companies, or government agencies that are hiring data scientists? is awesome, and dc was looking for a civic hacker for a law fellowship- the free law innovation fellowship. bayes impact is a 12 month ds fellowship for social good: have also seen some good things about the data science for social good summer fellowship: maybe?  another maybe?  lots of opportunity to work in private sector (consultant), yet support the public sectorthanks for the suggestions, very helpfuli really don\\'t like the keyboard. otherwise, i\\'m happy with mineit\\'s thinner and lighter than the older macbook pros,  that\\'s mostly the pros. neutral - the new keyboard has much less travel, takes getting used to but once you get used to it, it\\'s fine. no complaints on the size of the touchpad. i have very mixed feelings about the touchbar, i think it\\'s mostly a gimmick, but sometimes, when there\\'s a button to click like save or install, it offers a quick alternative to using the mouse. cons- 4 usb-c ports are very limiting. the dongle apple sells isn\\'t great, e.g. the usb-c ethernet adapter we got only works at the port on the mac body, not through the dongle. each dongle supports only 1 monitor. 2 monitors, 1 port for charging and 1 for ethernet and all my ports are occupied. hopefully i don\\'t need to connect anything else :confused:. battery life isn\\'t as good as my older 4 yr old mbp. limited to 16gb ram, expensive as hell, especially at the top end. hey nyc folks, i\\'m sourcing projects from companies for a class i am teaching at columbia in the fall.  the class is project based where mbas and engineers will work together on an analytics project over a semester.  this is an awesome opportunity to get some work done (reasonably scoped of course), get connected with analytics and ds folks at columbia for recruiting, and possible get quality content for a blog.  message me if interested and i can mail you more details, thanks!!channel notifications for hundreds of people, the majority of whom are probably not in nyc? gutsy move, !h2o is awesome. sometimes, not really sure why people use anything else. i feel like it’s pretty much won this spacesorry for being so mia wendy et al. i\\'m in the process of moving and will try to get the strata slides online soon. there is a good github repo for mfccs. i can\\'t remember what it\\'s called, but i think i forked it (so you can see it on my github account). sorry for not being more helpful - i am not near a computer. any people interested in security/exploits seen this? insanely simple exploit for root access on many inexpensive arm devices.reminder that ethics and anonymizing data = a really important part of data science:(re the ok cupid data dump today)working on a federal grant with human subjects will teach you how important anonymization is. :slightly_smiling_face:they have some pretty strict rules about data storage.none of my datasets have any personal information, it\\'s possible to link them eventually through subject numbers and experimental logs, but then you need access to our database of subjects.we\\'re expanding the data engineering program to boston. really excited to see the insight community in boston grow. we\\'re already at over 100 insight alums in boston and now we\\'ll add a  data engineering contingent to that - applications open for april now woo! :party_parrot_shuffle:hi, has anybody used h2o with python wrapper on mac? i can’t get it to work. it keeps telling me i have to install java but i already did it.just a guess but is it asking for a specific version of java?i think  has some experience with this.thanks. somehow i solved the problem. i switched the default browser of jupyter notebook to safari and then manually activated java for safari. it might have helped.i’m not sure if it was asking for the exact version but i ended up installing the latest version.nice advice when updating your cv!   this person likes 10 page cvs apparently:smile:\"another good example is where the candidate explains why they chose to use one algorithm over another for a specific project.\" i guess he is assuming a several page cover letter?there is no way i can fit: \"built a recommendation engine using collaborative filtering in spark. we considered using content based filtering but we needed to cover a very wide variety of item types so there was no single good way to parameterize them all. we also considered writing our own big data processing code, but you know what that\\'s crazy. during this project i mentored two engineers who were new to recommender systems....\" on a cv for the 5 major proejects i\\'ve worked on. :wink:i am always skeptical when a single person extends their personal anecdotal experience in their title to the plurality of people like them.  want to make a claim about what hiring managerssss look for?  i’d love to see your data that goes beyond what you want :slightly_smiling_face:  also i think this is generally some horrible advice `but you should never not apply for a job if you think you don’t fit all of the criteria. `listed job criteria usually not representative of actual job requirements :smile:oh woops that’s a double negative in there, uh, so it’s good advice after all in that line :smile:yup :wink:also he does specifically say he loves 5 page cvs\"and candidates should not be afraid of having a cv that is longer than a page or two. i would be happy to read even up to 5 pages of useful and relevant details, especially if it allows me to move faster in the interview process\"does the uk not do phone screens?a lot of what he\\'s advising is stuff that you would want to cover in a chat, not typed out on a cv of death\" from experience i have found that combining the hypothesis driven approach with the kanban agile framework supports the lean startup framework really well” … :skull:look, if candidates write a 10 page cv that gets them rejected _everywhere else_, then he always gets first pick. it’s really a brilliant ploy....if your filtering criteria is \"does not understand the hiring market\"zero attrition!that company they use as an example of a current gen crm on page 202 must be awesome and everyone who works there must be really handsomeanyone using tableau? would love to discuss usage with python :slightly_smiling_face: thanks!hello everyone! has anybody had experience with tensorflow. i want to figure out if use it or keras for my problem (my mentor softly isists on tesorflow)you can use both: you can use keras with a tensorflow backend. in my experience, it’s much easier to get going with keras. but i think keras may not support all of tensorflow’s functionality.an, nice! so, i can you tensorflow without knowing much about itthe question is whether your mentor is insisting on tensorflow because it’s the best tool for the job or just because it’s a buzzwordi am not sure. he said that it is a lot going there. for example google put out some new algorithm or something like that.as a matter of fact i am not sure if neural networks are the best. mentor also wants to have insights from the dataif _he_ actually uses tensorflow, that would be a good sign. if it’s just something he’s heard about, i’d be worried.i see. thank you for the advice. i\\'ll ask him what is they use iton the subject of tensorflow: it is what everyone in deep learning is really excited about now. at gtc this year they had to move the tf tutorial to the ballroom so they could fit the o(1k) people that showed up for it. i think this excitement comes from two things: \\n\\n1) google is really good at deep learning, so people assume tf must be awesome\\n2) google has some of the biggest scale problems imaginable, hence people think (and i think there is evidence that) tf can scale to whatever size problem you have.\\n\\ni haven\\'t used it enough to say; we don\\'t have the scale issues that others have, and i don\\'t have enough experience with tf to answer the \"is awesome\" question. but our current deep learning project (that we just started and i\\'m working on) will be mostly tf.that said, if this is for your insight project, pick the thing you can develop fastest in!! :wink:there is integration with scikit learn that is supposed to make the interface simpler.  it was mentioned briefly at a workshop i attended but i have not try to use it myself.from my limited experience, you probably fine using keras with tf backend. the tutorial linked above is a good place to start.a note about the top 10 example that niharika, mudit and i discussed this morning. if you look at the program am listing around page 62 an 63, you will notice that the mapper class declarer a map, that maps a \"reputation\" (which is derived from the input record) to the entire record. as it keeps reading more input, the map keeps growing. whenever the size gets bigger than \\'k\\', the \"first_key\" is dropper. after all records are procssed by the mapper, the map contains \\'k\\' elements. now the \"cleanup() is the critical part of the process. it\\'s this \\'cleanup() that takes the map and emits a (key, record) so the reducer can combine all these inputs and pick out the top \\'k\\' elements. if you look at the program, the mapper class has two mthods, map() and cleanup(). presumably map() gets called every time for each input, but cleanup(0 is called after call input has been processed. the cleanup is doing what a local combner does. but it is not clear where cleanup() is getting called. it\\'s clearly not called explicitly from the mapper anywhere. it must be happening behind the scenes my feeling is cleanup() is being registered as the \\'combiner\\' method. but this needs clarification.i am looking for fun and interesting applications of probabilistic programming and bayesian inference like anomaly detection, inference of missing data, or making decisions that involve risk from limited or sparse data. at my company (fast forward labs - an r&amp;d company), we\\'re working on a report on probabilistic programming. if you use these approaches, i’d love to chat. :upside_down_face::slightly_smiling_face:: these posts might be relevant for you : \\nnuggetbot animate me parsey mcparsefacethanks  the bayesian deep learning stuff is really cool, you can imagine all the applications one could build. :slightly_smiling_face: for now, i am curious about what people have built! anyone?i built it something for a toy problem, doubt it will be useful for your work :  i haven\\'t but was looking at this paper recently... ... keep us posted on this, i am def interested hi kevin, thank you for your response. i am trying to recommend if a given domain is a spam trap domain or not, by using the known spam trap domains. the feature is the name serve of the mx records of the spam trap domains. (we find out that spam traps domains tend to have the same name server.) i am in the process of collecting data now. we have ~9000 spam trap domains. from what i know so far, each spam trap domain has at leas two name serves.so you have the raw text of the name server and the raw text of the domain, as well as a list of spam trap / not spam trap labels for other domains?i\\'m just not familiar with the domain  at all unfortunately -- if anybody couldn’t make it to nips, we wrote up our thoughts about day 1: benjamin recht just laid down the hammer in his “test of time award” talk! yes, you are correct. i am trying to lean “how the internet works (dns records)” what about the data made the recommendation system seem like a good approach?this is a long read, but it’s very relevant to all scientists turned data scientists. includes a great quote from fernando pérez, creator of the jupyter notebook: “i have seen many talented colleagues leave academia in frustration over the last decade,” he wrote, “and i can’t think of a single one who wasn’t happier years later.” there\\'s...a lot that went a bit too underexplored in the article. the title\\'s def sensational. i think a more honest conclusion after reading this would be\\n\"scientific papers need to be published in a medium that allows interactive integrations. the pdf is too static for the types of complex ideas that can be easier interacted with than explained. the pdf also can\\'t embed data necessary for scientific analysis.\"\\n\\nthere\\'s a couple of problems. like...evident problems that you see right after reading the article. the notebook for gravitational waves is getting the \"hug of death\" right now from all the traffic. and if there\\'s a lot of data, as you\\'d need sometimes for modeling complex systems and for training in ai, then there might be problems just storing all of it. \\n\\nthen...there\\'s just cultural, financial, and incentive problems in academia that might need to be overhauled before a system like this can thrive.the title is total clickbait--obviously we’re not going to ignore scientific papers--but i think the article demonstrated how research workflow in academia is archaic and could be so much more innovative. of course, an alternative approach will almost certainly not catch on any time soon in academic research--yet another reason why i’m glad i’m no longer part of it.maybe i should preface anything i post with “i don’t agree with 100% of this…but…”workflow is definitely outdated but i’m not sure how well thought out this article is.  \\n\\nfor example, i think sharing raw data in a paper is a very tricky idea.  the naive idea to give it out doesn’t hold in many situations.  a lot of what matters is how data was collected for example - and a lot of what you should be doing as a scientist is interpreting raw data responsibly.  without literally writing a book, it would be impossible to describe how data that is difficult to get is collected so as not to confuse what a user playing with it may encounter.  what i think would happen is that it would flood many fields with false objections to quality work, and bog people down with addressing them, by essentially writing ‘this is precisely how the data was collected’ book to accompany each paper.  \\n\\nthere’s something to be said for interactive figures and such in papers, but i often find the best figures are simple and striking - and don’t require interactivity or manipulation to understand the concept - it’s sort of an art to do this well.  and in that sense, interactivity could also be a distraction.  \\n\\nand of course, many people still do print stuff :slightly_smiling_face:  a very important thing in academia that isn’t as useful in industry is to surround yourself with pieces of your findings that you can consume simultaneously (e.g. a giant wall of printouts ) as you try to put together the various pieces.  like those detective shows with all those pin-up figures in the wall and pieces of string connecting them :smile:\\n\\nall this said, should definitely integrate new tools to improve old workflows - and people are slowly beginning to do that.crawling through \"a giant wall of printouts\" should have a software equivalent though. \\n\\ni think \"data gathering\" and documentation could be a separate issue, but there are other places like data.world and aws that can expose data like that. i think in psych data sets aren\\'t as useful, but for things like twitter it doesn\\'t matter as much. \\n\\nwhatever comes of this should probably integrate data with a separate data hosts so that researchers could \"see\" data in the same way that a python programmer could import libraries instead of trying to package the data with the notebook.the “software” equivalent in industry i sa wall of monitorsso far anywayit’s pretty great, but also less portable and more expensive :smile:as far as documenting data, there’s cases where it’s easy and should be done and shared - and there’s others, where the data collection, because it’s done with some newly developed thing heled together by pieces of string, or is pushing old tools and methods to the limit of their capacity, can just be endless.so it can be done but probably at a big cost to productivity, and would not add muchin terms fo valuenah it\\'s necessary. papers just show their results. having data, even badly gathered undocumented data is at least an indicator that something could be wrong. having nothing but the results means someone has to replicate the experiment without documentation which is just as bad because sometimes descriptions and methods are too vague to re-implement.that’s definitely a problem.  but having data without the right details in the wild will bog down productivity to more of a crawl than it already is.  the reason is that people will make bad assumptions about how stuff was collected, and be mislead down wrong pathsthe best thing to do would be to consult the scientist and work with him / her in getting the data, telling them what you hope to accomplish, so they can tell you any relevant things about the data as they hand it to youas far as trusting the ‘results’ that’s more of an argument for improving the review processthat’s how i recycled the papers i read during bbq.hi insight peeps.  anyone know a good beginners reference for sizing a kafka cluster?  i am looking for something that works through an example problem.  thanks in advance.  has anyone switched from jupyter to zeppelin and been happy with it? i’m trying to figure out what the advantages of zeppelin are, but i’m coming up short. it seems to be missing a huge number of features compared to even a standard jupyter installation.no, we tried them out and went back to jupyter.this was from many months ago, but we found it to be very buggysame as above, checked it out, but for python it wasn\\'t an obvious win. i\\'ve seen people use it for monitoring etl jobs or other things more successfully.i\\'ve used it and liked it, but i haven\\'t used jupyter. i think one is more collab friendly than the other.i setup zeppelin on our hadoop cluster a few years ago and gave it a solid try to run adhoc spark code.  it worked alright, not sure if jupyter could be integrated with spark back then.  i like jupyter over zeppelin though.the sv-insight office is really cold after the party last night. anyone know where the thermostat is? (so far, 15 phds can\\'t find it)hvac is under central control. hope you can find some blankets!:disappointed: thanks!apologies for the off-topic question, but my family is traveling to boston next weekend and i was wondering if any boston-based folks can recommend a good babysitter? if so please pm me. try : thanks, will do!datakind is hosting an hackathon on nov’13/14th in sf.  check it out if you are interested - does anybody know of a good book/resource that covers a/b/n + multivariate testing?i like slater stich’s series (it’s biased towards bayesian methods, admittedly) has anyone set up ds pipelines in aws? specifically a pipeline with a daily retrain/score task and then a persistent api for retrieving predictions. bonus points if you had a probabilistic model in there somewhere.i\\'ve worked with lambdas to do data collection, storage data in s3/rds, daily retraining of models in ec2/sagemaker, and serving of predictions with rds&amp;api gateway/sagemaker endpoints, happy to help with any one of those points if needed :slightly_smiling_face:thanks  will check it out!if anyone\\'s looking to sublet a room, a friend is renting in fidi:  - $525 worth books for $15!i have an onsite interview coming up on monday next week and i could really use some advice for interview preparation. it\\'s a data scientist position at travelers insurance. i\\'ll be speaking with seven people, six of who work in predictive analytics and/or dev. the other is hr. i\\'ll speak with each person for 30 mins. i really want this job! so i have to ace it. any advice on types of questions i should prepare for and/or resources would be great. thanks!:  my biggest advice is to not put so much pressure on yourself: before having your on-site, there\\'s really no way to know that you really want to work there. i know you\\'ve heard all about \"cultural fit\" from the program directors (and that it seems like a lot of b.s.), but it\\'s actually pretty true.  to be honest, you\\'ll never know if the company is right for you before your on-site.  at this point you\\'re probably ready for any question they might have, but that might not be so when you bring a lot of self-generated pressure to the interview.  so, as hard as this \"advice\" might seem, the best one i can give you is to \"relax\", and go a little easier on yourself.  :wink:`chill`stay positive  good luck  !my friends are now hard launching their funding campaign for their educational non-profit startup. if you are interested in the cause, you can definitely donate. but another way you can help is upvote online and spread the word. thanks! twdoes anyone have a good reference for the starting salary of a data scientist at a very small start up?  all the numbers that i see online are cumulative over all company sizes and i would assume that a small start up would offer less than this; but how low is unreasonable?: did you look at glassdoor? this is close to the numbers i have heard: there’s also the o’reilly data science salary survey: with glassdoor, can you sort by company size?  the o\\'reilly survey does have salary by company size but is again cumulative over location.  i know that salary for data science is quite a bit lower in other parts of the country....insight it self may have some of the best information on this  i would also keep in mind the cost of living, etc. absolute numbers mean little on their own. 100k in nyc wont take you as far as 50k in san juan, pr. just saying...:  glassdoor, gives some minimal sorting options, by popularity and more recent updates. also, glass door gives you the median pay by the nation and by city. : you might look at angellist. you can filter by location. although, there are some reasons the data might be biasedangellist is the first thing that came to mind. and i work at glassdoor!company size is one thing that glassdoor doesn’t have great data on.oh, correction: apparently mr. salary estimator at gd tells me he *is* using company size now. i can tell you this: it is the weakest indicator of base pay out of everything we look at. maybe a little surprising. that is interesting.  do you mean if you are looking at all data or is it more predictive if you are looking at specific data (e.g. data scientist in sf)?not specific to data scientists unfortunately.we do have numbers specific to data scientists… that i’m not allowed to share. but you won’t find anything revolutionary.insight should have summary statistics on this, reallyyeah, insight should have some very good data on this.it\\'d probably be to their benefit to share aggregate data with the fellowsi found it hard to negotiate coming out of academiasome random thoughts: you do have to think pretty strongly about totally pay. there are a lot of complicated issues around it. i’m sure insight gives every session some instruction in this regard.\\n\\non one hand signing bonus and options are often very negotiable. people compare salaries much more often than bonus structures (in part because it can be complicated) and in fact the comp expert in hr (if the company is big enough) is doing this very carefully once or twice a year. in a reasonably fair company salaries will tend to normalize but the hiring bonus stuff is a one-time thing.\\n\\non the other hand, compensation in the form of options is a huge gamble without some very solid understanding of the company, its industry, and all the annoying financial quirks of it (amt, 90 day limit on vesting after separation, dilution risks, etc).true: are there any resources for us (current fellows) to learn about options and how to assess those opportunities during the negotiation process?we had one session where a mentor talked a bit about how that stuff works, but i’d love to dig into some more information if you can point toward any good sources useful tools in the field of bigdata: quora can be have some decent info: the tldr: “… this is why i value pre-ipo stock options at $0. …” (maybe a little extreme, not much)very interesting, thanks for the reference has anyone used the python difflib module?? is there anything similar?: similar to difflib in what way? i’ve been using fuzzywuzzy which has difflib as a dependency.having issues opening ports on aws ec2.  security group is set to open ports 80, 22, 5000 but port scanning shows only 22 is open for ssh.  does anyone know how to go about fixing this?all ports are opened to anywhere: have you checked both the inbound and outbound ports?: so the security group for the instance has the desired inbound ports opened.  outbound is open to all trafficodd, are you sure that you’ve put that instance with the correct security group?me tooi was planning to start down this path myself with ch. 13 of bishop:  they are very popular in comp bio, speech recognition, etc.these papers are very good overviewnow for almost all about them this book is good but it\\'s completely focused on math/algo\\'s but no implementation examples... hey everybody!  does anyone know a good place to start to learn coding for someone that would like to eventually be a software engineer, but has zero experience coding?there an `hmmlearn` python package, but i haven\\'t played with it yet.the wikipedia page is actually really good.the examples are easily understood and there is even example python code for the verterbi algorithm thanks to  for the pointers!there\\'s an outstanding mooc on probabilistic graphical models (of which hmms are an example) by daphne koller on coursera. i think hmms are discussed in week 2.+1 for daphne koller\\'s coursera classthe textbook that was used in my stochastic processes class in college was *“introduction to probability models, tenth edition” by ross*. it discusses markov chains, hmms, the poisson process, queueing theory, etc. i would recommend it as reference material even if you don’t do well self-studying out of textbooks. the bdt (xgboost) -&gt; logistic regression approach is used quite a bit here at li (an internal library does this for us automatically), and it works really well since you get the best of both worldshere is a useful reference: thank you for the paper carlos!i\\'ve been trying out the method you mentioned since you suggested it.i\\'ve also been exploring using something called shap values for individual feature importance and that for interactions. have any of you used these?hi everyone, i created the  channel to connect with other fellows who are either interested or are already working in sports analyticspandas docs has some interesting results as well i usually suggest that people brand new to coding check out , which is cute and well scaffolded.that looks awesome , thanks!i use it with hourly granularity over a few years of data for a production task. it isn’t fast, might require 10-20 minutes of running even if you don’t want seasonal uncertainties, but it is very solid. it is really just a wrapper around  which preps the data and compiles an appropriate stan model.is anyone familiar with nlp and willing to discuss with me about good nlp related projects in e-commerce industry ? i’m going an onsite interview for a nlp research scientist position and asked to give a 20 minutes presentation on a research proposal. i’m a complete beginner of nlp and need some suggestions from experts !thanks.i have not used shap. looking through the docs, it seems interesting for interpretability, but not sure it\\'s useful for large-scale feature selectionas a massive disney fan i like this one: i\\'ve done both ubuntu and windows 10. ds work was easier in ubuntu. communication with rest of company varies as others have saidif the e-commerce has a review system, you could use nlp to detect fraudulent reviewshahaha... hehehe.... haha.... no\\n:arrow_up: (for anyone lucky enough to have used root)nuggetbot: weather antarcticaweather for antarctica\\ncurrently: overcast -28.5°c/-19°f\\ntoday: light snow (under 1 cm.) tomorrow night.\\ncoming week: snow (7–15 cm.) throughout the week, with temperatures peaking at -10°c/14°f on saturday.a free webinar titled keys to data strategy: online seminar this thursday at 11am pdt hosted by svds \\n\\nit\\'s aimed at business leaders, product managers, marketing leaders and senior architects -- so if you know people who would be interested please pass this info along. \\n\\n  that’s surprising to me, but interesting!super interestingyeah, i imagine many of us had our start theredoes anyone have experience using this:  ?  or built their own data and model management system? well, not a rumor anymore! for bay area folks: a team of graduate students at berkeley are starting to organize the 5th annual cdips data science workshop (), which runs july 15-august 5 and they are looking for mentors. from one of the organizers:\\n\\n\"the workshop is designed for berkeley grad students and postdocs to get a taste of what it\\'s like to work on data science projects. the core of the program is the mentorship aspect - 3-4 students/postdocs are paired with an industry mentor who devises a project. if you have a project that could use a few eyes on it, or are just interested in exploring a particular topic, you might find this useful. the only fixed commitment as a mentor is joining us for a few hours for the opening and closing ceremonies; we do ask that you check in with your team twice a week, but the timing is flexible. if you\\'d like any further details, please let me (diya das &lt;diyadas@berkeley.edu&gt;) know - i\\'d love to chat more. you can also reach all of the organizers at .\": i changed my project to predict what songs/music genres are best suited for health recovery (mood or emotions change). would you know any psychological site that connects or labels emotiond with health. thxpublic service announcement to people writing their resumes: it\\'s ipython not ipython (it\\'s not an apple product :simple_smile: ) source: you make one of my favorite editing points on resumes,  !  \\nto current fellows in both sv + nyc -- if in doubt, please look at the language/tool website to see how the name is (or _is not_) capitalized. \\ne.g. ipython, scikit-learn, pandas, git, python, numpy, latex, etc.  \\n\\nfeel free to direct message me if you have any questions.i once got a complement on my resume because i don’t flagrantly assume c and c++ are basically the same, a less subtle distinction than ipython vs ipython but i think common.  the facts of a resume are easy, the really tough task is instilling confidence that you are trustworthy and have sophistication in your field.can someone help me understand what a typical ai stack should look like? or any literature that can help me understand on a high level? haha, so true...welp, boss approved the cost, but now registration is closed :cry: looks like it\\'ll be back in the fall though? i\\'ll try to watch for that registration to open! pinned a message to this channel.this is too realdarn! yeah, i think it\\'s bi-annualohh common! that’s completely self-inflicted by using anaconda and easy_install :smile:i’d like to add the eleven virtual environments i’ve set up in various places with uninformative names and conflicting dependenciesjust created  for attendees later this week. we can use it to coordinate meeting upi learned about pipenv during one of my interviews. it’s become my go-to for environment stuff: i’ve been totally happy with the python built in virtual environment `python3 -m venv my_venv` …works great. i think it’s way easier to keep track of than all the anaconda path stuffam i the only one that is happy with exclusively using conda environments? i have a dozen or so, but they’re all named well, no conflicts. just...stick with one method?just out of curiosity - has anyone tried any of these? nuggetbot: animate me welcome newbiesthanks  ! what you forgot to mention is that it\\'s free!hey everyone.  i have a 2bd 2ba in mountain view and i\\'m looking for a roommate.  gimme a msg if you\\'re interested!hi fellow fellows: if you know anyone looking for a room for either short term (feb) or longer term, my room is up for rent.  place is really nice and 5 min walk from f/g: good morning all!  does anyone know of a clean way to incorporate a python model into a sql database?  basically, all our data is stored in sql databases, and we want to streamline a process that appends the model output as a new column in the database. any reason you wouldn\\'t create a new table with the predictions and be able to link by id? might avoid conflicts if you re-run the predictions.  but you could use alter table to add a column, and use update to change the values within the row. great suggestion to improve the structure, thanks!  the challenge here is really that we want an automated process to run new data through the model as it comes in - so how to incorporate the model component.  i can see it working using a cron job to run the python script outside of sql, but i was hoping there was a nicer way, especially since i\\'m the only python programmer at my company, but we have a few people who know sql. so do you basically want to generate a model prediction as each new data row comes in in real time?  i think you may be able to implement that at the layer that generates the data row, e.g. if it’s on a website, call the model from there (e.g. if you put it behind some type of api you can call).  but i think if there’s a lot of data rows coming in and its not critical to do in real time, using a cron job to do it in batches and during less busy periods is probably ideal.  we use rundeck for scheduling these types of jobs, hi everyone. does anyone have experience analyzing bed-sensors and/or acti-watch (acti-wear) sensors for sleep staging? it doesn\\'t have to be in real-time.  really the problem is the data warehouse guys do everything in sql, so they want the model in sql as well, and i\\'ve gotta convince them that hard coding all the rules for a random forest into sql is a bad idea (note: doing the math for him on how long that would take and how inflexible that would be did not work).  so if the general consensus is to run it in python with a cron job, as there\\'s no easy way to run the model in sql, i can use that to support an argument for leaving it in python.: i\\'ve done some work with sleep analysis from wearables.  feel free to pm me.ah i see.  that’s interesting, i wonder how hard it would be to store the trained rf as some sort of sql structure.  however, yes, i think it’s a bad idea in practicei’m quite surprised someone is asking you to do that hahathat’s why i didn’t even realize what you were askingyeah, it\\'s much more a business company than an analysis-heavy company at this pointyou can see how it sounds easy from an outside perspective, just switch from python to sql.  i figured i\\'d see if anyone in the community knew of something i was missingyeah, makes sense, but i think if you give in to this type of ask, it will be more difficult in the future to deal wiht your model , e.g. refitting it, and things like that, i bet even if you come up with some sort of sql solution in  a sort of ‘minecraft emulator’ sort of way it would not be easy to manage some db (redshift for sure) support python-based udf (user defined function). have you looked into those? no, that\\'s a great idea, thanks, i\\'ll check them outthat’s neat, i hadn’t seen that beforehi everyone!  i\\'d like to do some crowdsourcing. can somebody please recommend a simple tool? something besides amazon mturk (i\\'d like to avoid php if possible)does anyone have any experience with chunking in natural language processing?i don’t have personal experience, but i think `spacy` has an inbuilt chunkersee:  thanks!this might also be helpful: another nlp question from a newbie: i\\'m trying to build a corpora from reddit posts (using praw), and ultimately apply lda or lsa for classification. \\n\\nfor example, i\\'m focusing on extracting titles from posts on reddit. to build a corpus, would it be useful to write each title out into a text file, or append each title into a single string, then write it out, or something else?\\n\\nhere\\'s some sample code i have `# create a for loop that iterates over a subreddit, excludes posts made by the automoderator,\\n# and appends titles to a string\\nfor submission in reddit.subreddit(\\'anxiety\\').hot(limit=100):\\n    if submission.author != \\'automoderator\\':\\n        corpus_titles += submission.title\\n        print corpus_titles` uploaded a file:  and commented: better code snippet.really depends what you want to do with it after, but in general the more general your data storage is the betteri.e. write it out in a way that you can further process itmy understanding of praw is that each title comes in 1 by 1 so saving it out individually makes sense in case of failure is a much better way to get reddit data out -- the reddit api is capped at 1 request per second, and can only return out to 1000 results due to how their data storage is structuredso scraping a significant amount of data would be very very slowpushshift has archives of praw outputs as monthly dumpsyou can also download a reddit dump from bigquery directly, i forgot about that. that’s what i did for my project.iirc to build the initial archive up to like fall 2015 required his scraper to be running continuously for like 9 monthsthanks for the advice  and ! you can pass python data structures around via pickle over a network - in which case this is just a bytestream. i bet its possible to store this bytestream in a database, but i have no idea how that would scale, and it introduces the complexity of having to track what these collections of bytes are when they are unpickled, in addition to ensuring that all necessary libraries to access this byte-stream’s attributes are installed everywhere the pickled object needs to be used.does anyone know of any semi-supervised clustering algorithms for network data that are usable through either python or r? i have only found some packages in java for weighted-vote relational network models or unsupervised community detection algorithms. or am i stuck with importing java through some java-to-python package? uploaded a file: what dis, ?an example of how one can copy/paste a plot from a jupyter notebook (the jupyter notebook talk was tonight)by the way here is the link to the repo from tonight: if i could not copy paste plots from jupyter into slack i think my company would fall apartat razorfish we often took the sign nda, get data dump route. for us that meant we’d spin up a cloud environment for each client engagement based on need, proposing aws/gcp/azure based on the specific case, and we’d have a somewhat cookie cutter approach to each use case. part of the contract was that those providers guaranteed some security standard and the client would transfer the data to that cloud env, which was separate for each client.  they paid the cloud costs as part of the engagement, and therefore they still “owned” that storage etc, so it never really moved to something they didn’t own. \\n\\npros were that we could iterate faster because we could customize our env and resources, which sometimes you can’t do entirely on premise because they’ll give you some small vm that’s insanely slow and you have to keep asking for changes. also you can develop some expertise in one env so you aren’t learning something new for each client. \\n\\ncons include that set up takes time, and they’ll also usually give you prototype datasets initially, and sometimes you run into walls looking for the most new/correct data or just build models on data that you later find is incomplete. probably that happens anyway though and you just need good communication w the client all the timeanyone going to be in seattle for data day?hey all,  , i have 2 tickets for red bulls vs nycfc @nj this sunday that i’m trying to offload. anyone interested?does anyone have any suggestions or tools that you like for  automating data analysis on data from  google analytics? so far, i\\'ve been using the rgoogleanalytics package from r. hi asli! sounds like it might a really useful question — what sort of analysis are you trying to automate?for those interested: : pandas has google analytics api integration:  have not used it though, but thought i\\'d give you more options!hi . i  am trying to automate user segmentation reports. thanks . i wasn\\'t aware that world bank data was supported for remote  access. hey folks, do you have any recommendations for data science/data engineering meetup groups in the silicon valley area? too many to choose from :thinking_face:that\\'s a lot of people...any one know when the east river swim initiation is for ds-nyc-2016c ? the trick is where the source is… sourcing from.  for example, lending club produces a free dump of issued loan data, including fico scores and localization data (at least to the state level) but it’s biased (naturally) to people who are applying for personal loans, which is emphatically not representative of the overall population.  most other groups will have similar issues, as they can only report on their own users.  the best source for representative populations would be the credit reporting agencies themselves, but they’ve been quite protective of that data in my experience.ibuder: integration with whatever platforms you’ll be testing on (web, native apps, email, …)? how attribution for various success measures is done (how close can you tie a success to a testing scenario)? are you only testing ‘users’ or could you potentially split on other entities? does it tie together logged-in and logged-out sessions?does anyone have a hacker-rank type tool to send out data science screens?we\\'re looking for something that\\'s more like a tool to screen for data frames + plotting rather than cs fundamentals let me see which of the internal stuff i have i can dish out. in the meanwhile do you want more technical stuff or just a general audience overview thanks! technical reading would be super appreciated!we used swrve at various points at my last job, and it was okay, but definitely built for non-experts. to be honest, it might be better to build a platform in-househi all, a good friend of mine/colleague has to move to the west coast for personal reasons, are any of your teams looking for a data engineer in the south bay (california?) thanks!this is a good point, thanks for reminding me of these privacy concerns. we only have mobile device  ip adresses, which i think is fine in the us.anyone have experience with building ml models on full-homomorphically encrypted data?  we used monetate, optimizely, and an in-house platform at various points during my time at bonobos.optimizely seemed the best of those options from my perspective, allowing the raw allocations to be written back to our tracking backend () so we could do our own analyses when their tools fell short and the team having enough statistical sophistication to not be doing things obviously wrongthanks! this helpssomething to keep in mind is that in our experience nearly half the experiments end up requiring some sort of ad hoc analysis. either to figure out what was broken or get some sort of explanation of the result. so you really need the allocations easily available alongside all your other data.  our analysis platform was mostly created in-house, the allocation system is a java port of facebook’s planout. this is for search algorithm and somewhat complex ux changes. for simpler, try different creatives in a marketing funnel type stuff, we have also used off the shelf stuff.does anyone know what kind of (emergency) us healthcare insurance options exist for a canadian who is considered resident of us for tax purposes (on j1 visa)?i know a little. j1 visa holder (excluding full time students) can buy the emergency (bare minimum coverage) of obamacare or whatever providers. but they are not required to get health insurance until they are considered resident of the us for tax purposebut, genrally, the us won’t issue you the j1 visa from the first place if you didn’t show that you’d have health care coverage at the time you apply for the visahowever, the visa allows you to enter. so it does happen that someone had health insurance, say, though employer/school when entering the us. but then lose coverage for whatever reason. at that point, you are not breaking immigration law. and if you’ve been in the us for less than…i think2 years…you’re also not breaking the law which mandates everyone to have some types of coverage or you’ll get a penalty in your taxthanks  !!hey guys, check out this kaggle competition that was put together in my team at mskcc work on cancer and win $$ : if you want to know more about the connection between deep learning and renormalisation, there is a much more technical (but also more precise) description here (with some nice references within):  \\ninsight is continuing to expand, and we\\'re looking for a great program coordinator to join the new york city office.\\n\\nif you know someone that would make a great fit with the insight team, and would work well in the role, have them submit their resume to . also, if there\\'s someone in particular that you\\'d like to recommend, please send me () a quick email. many thanks! :blush:: fantastic, thanks!: you\\'re welcome!does anyone have significant experience doing data science in java? i\\'m a python/r guy but will need to build some analytics functionality in java production code. are there tutorials/books/java libraries that folks would recommend?  i deployed some java models in production and a few months prior to that, i thought learning java is going to take time. it turned out moving from python to java isn\\'t that bad. this is a good tutorial: but what really got me started were:\\n1. reading through some production code and make it some minor changes (and of course with pr)\\n2. having someone available for asking question. does not need to be a java expert necessarily\\n3. a good ide. this is as important the other two. i recommend intellij. there is a free community version which does the job for a ds projectthis book ain’t bad to help better appreciate the mindset, constructs and design of java\\ndoes anyone have recommended reading around centralized data science teams vs embedded in products? thanks in advance!i would  be super interested in it as welljust to let you all know, i\\'ve sent an invite (to receive an invitation) to the current insight nyc fellowswe could have channels for different sessions to partition the conversations... yes, i said \"partition\":clap:well done andrew!haha, thank you leifur !(my coworkers were criticizing me for not knowing enough emoji ..)i would like to invite all fellows soonall. the. fellows.let\\'s allow the conversations to build up, then perform nlp to identify popular conversation topics. that sounds like a data sciencey thing to do right?or we can just start with a #beer channel and see where things go from there.dvats: you on that?,  excellent idea!! :simple_smile:we\\'ll do supervised topic modeling for now: i\\'ll be attending these workshops in nyc, may be useful for others:\\n\\nmining the quantified self :  does anyone know of a service that provides financial text data (letters to shareholders, investor presentations, etc)? you might want to ping  - he’s done work in the area. i can provide you with an email by pm if that’s helpful; not sure how frequently he’s on here.hi  yes an email address would be great, thanks! i\\'ll try him here too.yep  is the right person to ask, as far as i knowfrom training a model to predict gender on twitter from tweets: men talk about wrestling, women use :heart:.men love to nerd out on pro wrestling :wink: also,  new avatar is amazing  and bayesian lovers - can anyone advise a good accessible book on bayesian networks? there\\'s some material on it in koller and friedman \"probabilistic graphical models\". daphne koller also has a coursera course on pgms.there\\'s also barber \"bayesian reasoning and machine learning\"didn\\'t someone build an app for finding the right textbook? :wink: thanks   - book in dropbox. i still believe in well-calibrated human advice, this is not like some cooking where you can just find any recipe online :wink:semi-supervised learningmy project used rnns also (for video analysis). i found the keras wrapper for tensorflow to be a lifesaver. also, this site might help: \\n\\nbut message me if there’s any questions i can (try to) answer!what\\'s everyone\\'s opinion on the best python package for live dashboard implementation?i haven\\'t had a chance to explore this, so i\\'m wondering as well. i heard some good things about superset. i\\'m curious how plotly dash stacks upsuperset was a little clunky in my experienceeasy enough to get it set up thoughi tried out metabase as wellsuperset is nice! fairly easy to get up and running.still nascent so lacks a ton of dashboard options, but it\\'s getting therebootstrap has very nice dashboard templates (). after that, plotly is very convenient for the content.i\\'m sure your imagination was better :simple_smile:we\\'ll do our best jboysen0 !has anybody (in nyc) done pyspark training they would recommend?i have permission but policy says i can only install “approved” stuff.  i tried to go through proper channels at my company to install anaconda, r and rstudio.  i was told no one ever asked for that before.  (!!!???). then i talked to other ds folks who just did it anyway.  i followed suit and approved software list eventually caught up with us.  :grinning:before that i worked at a financial company where admins refused to install impyla on server even though it satisfied all their security requirements.  so i had to make system calls to manipulate hive in python.  painful.   not in nyc but i liked spark moocs on edx developed by databricks.  they teach pyspark via jupyter notebooks in vms.nice to hear maybe the ds people have the power to affect the policies over timewe had better change the policies, otherwise we\\'re not doing our job :slightly_smiling_face:who is the best person to send a referral to for insight?an insight employee should comment on whether this is up-to-date, but i’ve used this in the past:   not answers to the question, just tagging folks who might be able to give an answer. to what program would you like to refer someone?probably health data science, and regular insight. both would be good fits(data science)i checked with our admissions manager - the link above is the correct one! thanks for the referral!what do people use to make dashboards in python?\\n\\ni\\'m looking at:\\n\\n\\n\\n\\nbut it\\'d be great to know if there are other good options as well.dash is awesomeplotly dash. also look into superset by airbnb i’ve used metabase and superset, plus bokeh in an in-house ember app by one of our engineerswe ended up going with the in-house for access-control ease because like 30% of my job is fighting our cto over data accessthat said bokeh is pretty easy to get up to speed with, and you can make some cool shit with iti found metabase to be a little easier to work with than supersetlooked into dash for the project as well, it looked cool but i haven’t done much of anything with itplotly dash in my opinion by far the bestthat link is pretty good to get started since it has lots of examplesand exercisesi will say dash seems way easier to work with at the “dashboard” stage rather than just visualizations themselves compared to bokeh as wellthis is what it’s looking like… i think we were more tightly bottlenecked on the i/o due to the bug, which let the gpu have more idle time, and the slowdowns started stacking on top of each other. we’re seeing decent returns on speed by tuning our i/o parameters.now we play the game. who @ channels general first. new fellow or  :smile:do you use free, or commercial version? how do you share dashboards internally where you need to limit access? yeah, you can have grayscale pngs with alpha as well. i used to use them a lot before coming to insight. i\\'m really digging this silly webapp: \\nwould\\'ve been a fun insight projecthello! i\\'m looking to overlay a kernel density estimate output, plus some basic lat, lon scatter points onto a street map. anyone with experience in this? i\\'m handling my data and generating the kde in python.: i did something similar on googlemap, but i have to use heatmap to do it, which is not very accurate representation of 2d distribution. i am not sure what you can do with openstreetmap. most of our interns at viacom tend to be undergrads and we are always looking. we just finalized our spring interns so the summer program will start recruitment events soon. the company page should have all the info , let me know if you need something specific and i would be glad to help our unit at mskcc accepts grad and undergrad interns - in the two years ive been here ive seen math, pre-med, econ and or folks come through and do some really cool stuff. its a great way to see how tech and data science can impact healthcare.let me know if you want to be connected to the person who manages the programany insight fellows playing overwatch on ps4? i just got it and am looking for friends - my psn name is avicura :slightly_smiling_face:i am also looking for friendsin general:thinking_face:stay positive alex_rheebut i am stephaniebut i am:sakamoto:: generally i think its best to start with the simplest and most interpretable model that will solve the problem.  be sure to understand how to implement and interpret any results you will get in addition to assumptions and limitations to the model.  if the interviewer wants you to provide more complex models they should make that clear in their feedback in the interview.  be ready to explain how you would choose the best model to solve the questions..as  points out there may be tradeoffs between accuracy and speed/interpretabilityhey data scientists! how does one determine the threshold for winsorization? any papers you can point me to? :smile:does anyone know how to write files while in a flask app? i am trying to create a png image from a third party api based on user input, and then displaying said png image.any body using d3.jsuber news: : ton of useful stuff on d3.js: .  the modules c3 and nvd3 listed therein are particularly nice, easier to use for simple graphs.fantastic collection of data science ipython notebooks: jure leskovec\\'s team has been advocating large memory machines over distributing over a cluster.  the snap project ought to have some benchmarks for this.hi all, does anyone have code or suggestions on how to convert a json with a grouping variable to a data frame in r? if anyone is curious, you do this by extracting the json data and using \"unnest\" from the tidyr package here is a good example of solving expectation value questions with recursion: has anyone worked with api consumers in python? i am looking for something like  which lets me define schema in sqlalchemy? any suggestions?@newbies: welcome to the world of data science!welcome, [ds,de]-[ny,sv]-2015c!hey sv guys, can\\'t wait to meet you at the party this friday!the shit list: stay positive sebastien_dery: i\\'m all good, but thankspanama papers go live tomorrow: thought i would share:\\nso, i have an acquaintance (fellow phd in physics) who is looking for ds jobs anywhere in the us and asked me to let him know \"if i hear of anyone hiring\".  if i don\\'t know of anyone close to me actively hiring, any tips on how to respond?  it\\'s such a general request i don\\'t even know where to start...  i guess tell him about insight :slightly_smiling_face:we\\'re hiring at samba tv in sf!but, i guess, does it somehow help him if i send him your way?  i just feel like he could find plenty of postings online (i imagine you have an ad out?)the way i’ve done this in the past, and it’s been moderately successful, is:\\n\\n1. use the insight network to find people i know at companies.\\n2. ask them if i can make an intro to my friend.\\n3. use that to set up an informational interview with the goal of finding out about the position and job in detail, and getting a warm intro to it (either through the employee referral tool, manager intro, whatever)very cool, i really like that workflowand maybe i can talk to him first to narrow down what he\\'s looking for? i mean, i don\\'t want to spam all 1,549 of us here...that’s always a good start! i normally start with step 0 of “hey, i know people at x, y, and z. what companies are you most interested in and what sort of ds track are you looking for?”i’m partial to the “introduce them to insight” route… -- i get that a lot too, and i recommend reading our 6 white papers: ds / de / ai / hd / devops / pm to try to think more about what they want to do in their day to day and where to do it. doing a job search in all locations for all kinds of roles isn\\'t usually very effective.awesome, thanks kathy!there’s an ai backlash underway for sure. even henry kissinger is getting in on it in this month’s atlantic. thought this piece actually cut through the hype the best. quick question: for someone moving to the bay area, who is interested in staying in touch with other techies/broadening skills/hackathons, can anyone advise how to find those and where the best ones typically are? are they more in sf or sj? any advice about getting the most out of the area is welcome! i very much appreciate direct refs from insight alums!anyone working in city infrastructure/smart cities? this is from a friend:\\n```i’m on the steering committee for a leadership training program called the water leadership institute. we’re planning a panel (webcast) on smart cities and topics related to how data impacts the water industry in cities. i wonder if you have anyone in your network that might be in thinking about data as it relates to city infrastructure - specifically water and wastewater but not necessarily so. a lot of engineers are really new to this topic and could really benefit from learning about the potential of big data in our industry. let me know if anyone or anything comes to mind!```?ooh, i’d hesitate at that use of informational interview…would consider informational interview more as ‘interested in learning more about this field/your career/general insights/etc.’ and would much rather be told straight up ‘friend of mine is interested in job opening at your company, do you mind talking to him/her and, if good fit, referring’.also, would def want a resume sent to me with the request!i would say it\\'s evenly split, with probably more hackathons in sf. plus sf has that density thing going on, everything is blocks away. i would hop on , type in all my specific interests, and just start attending as many meetups as possible... that would definitely open the door to hackathons and other networking opportunities.for those of you who live in the bay area, and climb and/or drink alcohol: i propose thursdays at dogpatch boulders followed by drinking nearby.  the bouldering gym is very close to the 22nd st. caltrain station and several half-decent bars.  i\\'m going to attempt to get to the gym between 6:30-7:30 typically, and to a bar by 9:30 or 10pm.  for a bar, this week, magnolia brewery (pretty sure that\\'s the name), about a hundred feet up from the bouldering gym.  perhaps i\\'ll send out an email too, given the low density of people on slack at the moment...fellow fellows! i\\'m trying to get my jupyter notebooks to render on a github enterprise account and having no luck.  the internet is giving me mixed feedback on whether gh enterprise supports notebooks.  do any of you know if/how to get notebooks to render? i’ve encountered this problem before - i believe that this is an add-on feature in gh enterprise and needs to be configured by an administrator. a quick workaround is to save your notebooks as markdown files, which will render in gh enterprise.thanks !i am wondering if anyone has thoughts on the best way to build a model in the following situation. its a regression problem but the predictors are all 0/1 categorical and the matrix is very sparse, but we want interpretable coefficients (e.g. this variable is contributing this much when others are held constant, so no deep learning fun... etc) and preferably doing some predictor selection (e.g. these predictors are contributing the most). linear model with l1 penalty is not performing well --- its not doing any predictor selection in this case --- we suspect the matrix is so sparse that the coefficients are super unstable.sounds like an interesting problem — what’s the context? site browsing data or ad exposure data?you can try some type of leave-one-out method and see how removing predictors affects the accuracy of the model.  it’s sort of different and aimed at prediction so may not be applicable.you might try an elastic net regression if your solution is unstable. when you combine l1 and l2 penalties, you gain the benefits of a more stable solution. yes, it does support it. how big is your notebook?anyone going to odsc-west this week? sadly, can\\'t say more. :disappointed:   i\\'ll look at that.  it came up in our discussions, but then the added complexity was cited as a reason to not necessarily go that way...stay positive amelia are you also using interaction terms between the features? with many binary input categorical features, i assume there must be some correlations there that may be interestingi would think so also, i\\'ll have to ask the person who is primarily working on the model about that one.  i\\'m just helping on this one on the side...i’ve also heard of people applying pca to discrete variables with some fancy math, but i haven’t ever read up on iti\\'ll be at odsc this weekend! neither a small 2k nor a larger 250k notebook are rendering, so i think its more likely a configuration issue hmm. interesting. i didn\\'t need to play with the configuration. anyway, have you tried this?\\ndoes anyone have experience using play framework for scala to serve spark ml models? if so, dm me :sunglasses:or just post some info about it here :slightly_smiling_face:does anyone know how to open an .xz file on mac?: did you figure it out?? are you doing ml or probability? i am doing probability, and nothing i try is opening the file!it\\'s all part of the test!!!!oh no, i pretty much fail at the beginning :disappointed:that is *many* billionsi agreeso, curious - i\\'ve set up ssh servers in the past, and made some attempts to secure it with key-authentication, no root login, etc. i\\'ve noticed that even with password-authentication only, there is a slight delay in the response - is this a network effect generally, or a security feature of ssh? i imagine that even adding a slight delay to password validation would basically make most brute-force attacks worthless. so, curious, if someone does want to brute-force a password - how are they actually going about overcoming these kinds of delays to make it a viable method to crack an account?generally you need offline access. either a dump of the site, or hardware access to the machine. things like this are useful when $site_x has it\\'s database stolen, but you only have the hashes. then you can brute force hashes, and now you have a pair of usernames and passwords which you can attack other sites with.jollyhrothgar: just speaking of generic ssh servers (openssh) its probably your network delay or some processing (ssh -vvv should tell you). there is a feature in pam to prevent common attacks, but its negligible enough for us common users to not notice. you can check that by spinning up local machine, setting up ssh server and try logging in, it should be much faster (in ms). speaking of attackers attempts, it depends on a myriad of things, there are automated tools (even a distro: kali linux)  that are custom written for different versions of services we use. so if you use a really new pen-test tool against really old service(including ssh, never tried it but just guessing) you have a really high chance to cracking into the system just with brute-force. at this time, its just a race in versions against who finds out vulnerabilities first, attackers or developers :simple_smile:: openssh does not have built-in delay for wrong password to my knowledge. but one can always use fail2ban or google authenticator (two-factor) to make the server more secure. to actually break in ssh servers,  one has a lot better chance through bugs in the implementation of various security protocols as  pointed out . passwords were rarely the weak point.jollyhrothgar: three\\'s also a distributed brute-force attack, with a coordinated effect of thousands of machines hitting an ssh server. assuming don\\'t crash the host, you can try multiple passwords in rapid succession., , , : ah, okay - i guess it makes sense that these kinds of attacks would be used offline, or in a distributed attack (assuming a server can handle a high volume of requests). thanks for the info, folks!hello. i was wondering if anyone has any resources on the impact that social media can have on a business. e.g., from the perspective of a data scientist, how can inclusion of social media data impact the type of analyses (and business insights) of a company? ideally, i’m looking for research/articles and/or use cases, but even just examples that come to mind would be helpful. thanks!apparently slack is losing it\\'s appeal for some: slack: let\\'s kill the inbox so we can be your inbox! :wink:does anyone remember google wave? hehe.this starts off slowly but is very well-done and gets at several of your interests: is anyone familiar with using things like app usage patterns for classification of things like churn, purchases, etc. i\\'m having trouble finding the term for this type of analysis. it\\'s not really time series, as i only have two measurements, prior to app usage and after. but i\\'m curious if different usage patterns correlate to certain outcomes. is there a term for this type of analysis. or is simple feature engineering using app usage times the best approach, things like average time of day, variability in time of day, average days between app use, probability of using the app for each hour of the day, etc. interpretability of the results are extremely important, so black box or ensemble methods may not be the best approach for me.i would call this *cohort analysis*.that\\'s somewhat along the lines of what i\\'m looking for. though in this case there is a single cohort, and use was limited to about one month. think of something like a work tracking app with a one month free trial. i\\'m interested in something similar to whether certain usage patterns were more productive after the 30 days as compared to before using the app. implications could be importance of implementing daily reminders, whether reminding at certain times of day matter, etc. there is no chance for future data collection and i can only rely on the data at hand. things like forecasting obviously aren\\'t appropriate because there isn\\'t a measurement at each time point, but it seems like this is a problem that has likely been tackled before. is there a specific type of cohort analysis you\\'re referring to?this sounds pretty similar to engagement measures in e.g. mobile gamingmeasures like sessions per day and session length might be a good startthanks, sounds like the feature engineering approach and logistic regression may be a good starting place. i just wanted to make sure i didn\\'t have an obvious blindspot with regards to a standard approach.anyone know a good way to deal with changing data schemas with an implemented data pipeline? for example you have a data pipeline configured to save data in a specific schema. business/product decides to change the schema, is the only way to ensure pipeline still works is to change the way data is handled. is there away to allow pipeline to have visability of schema changes with minimal changes?why don\\'t just take the entire fft ? if that\\'s to big, you can take mfcc instead (or an equivalent mesure that does not use the mel scale) confluent has a schema registry. aws also has a strategy it’s been telling businesses, which is to use dynamodb to store metadata on data stored on s3, which potentially could be use in your case. that said, any alumni who have had direct experience with this?i’m getting a shape miss-match error from somewhere deep in a pretty complicated model (keras implementation). i was wondering if anyone had any tips-and-tricks for debugging this kind of thing? unfortunately, printing out the model summery wasn’t helpful for this particular issue.the model compiles, but during training it breaksyou should be able to call each layer, one at a time and look at the shape of the output (are you using functional or sequential api?)i’m using the sequential apiuhm, it\\'d probably be easier with functional (see this example: ), but it can work also with sequential, if you explicitly define the instance for each layer (and append with model.add)interesting, okay yeah i haven’t been exposed to the functional api yet. i will definitely check that out.yeah, it requires a bit of getting used to but it makes it easier to handle complex models, imo/poll \"straw poll - where would you be most excited to work in europe?  i\\'m wondering what city would have the most draw for good talent.\" \"amsterdam\" \"berlin\" \"paris\" \"#nevereurope #godblessamerica\"i retract my previous commit… i am using the functional api…:five:london:six:munich:seven:warsawhey all, i know an experienced executive assistant who is looking to move out of the finance industry and into tech. if anyone thinks their company might be interested, send me a pm!ha, fair point - i only included those three cities because they would make sense for us for other business reasons :grinning:i figured…and we have an office in warsawi have a similar problem, and in the middle of investigating differnet tools.  i have been looking into aws glue, and think that it can be a potential solution  cool - yeah i looked into aws glue as well. any other tools you think are worth reading up? thanks in advance.i also rolled my own tool currently, and deployed as a lambda, but that’s all i can think of for nowfor all des, what do you guys think of managed services like alooma, stichdata, segment, fivetran, xpenty, etc that offer etl and data pipelining solutions. what kind of companies would benefit from these teams better than let\\'s say hiring an entire data engineering/infrastructure resources?so happy i added this extension. :slightly_smiling_face:what techniques/strategies do you use to improve your precision without hurting your recall? this is for an imbalanced classification problem where i sub-sampled the negatives to match the number of positives. i have plenty of positive data to do this. i use some of those services, it’s cheaper than doing it from scratch and putting people to manage that when you want to centralize a bunch of data sources like hubspot, salesforce, etcdid anyone join amazon as their first ds job after insight?i\\'d love to connect with you!, , ? happy to gchat or gmail. won\\'t respond to slack unless directly pinged, appreciate the help michelle :grinning: . amazon is great! i\\'d love to help in any way i can.hi, becky. i am a current ai fellow. for my project i am working with mfccs. if you want to discuss about that, you can talk to me.i can connect you with several people in seattle if you\\'d like :slightly_smiling_face:a good way to start is to set up a chain of different methods.\\nyou can start with a good high recall algorithm that doesn\\'t have a high precision and then systematically reduce the amount of fp with other algorithms and therefore increasing the precision without affecting the recall.you guys hiring this round?yes please!a good chunk of the company isn’t hiring. but, many teams are still hiring this round. we, down in san diego, will start hiring again hopefully in the fall, except for a few teams which are hiring software eng right now. no data scientist hiring in san diego at the moment. thanks! i mightmichelle, i can\\'t get over my initial reading of the event, \"zero data\" . it makes me a very sad panda.hahaha, i just noticed that! unbeknownst to all participants, we are trolling them by continuing to dangle cool data despite not even having any. :upside_down_face:has anyone gone to grace hopper?  love/hate breakdown?based on my experience last year, i believe that every women working in a technical field should at least once go to grace hopper. i was, for the first time ever, surrounded by overwhelmingly deeply technical people who happened to be overwhelmingly women. when i got up during talks, i did not stand out just because i was part of a minority. it gave me the experience of what it feels like to belong to the majority, it allowed me to better distinguish between personality and situation, if that makes sense. also, it is an extremely friendly, very fun conference. there are many undergraduate students in the audience, many of the events are not directly relevant to data science/machine learning/ai, but you learn a lot about your field in ways that have less to do with, say, \"stochastic gradient descent\". also, you will be able to help undergrads figure out what they\\'re excited about, what it means to work in data science/ml/ai, etc.hi all -- a new seattle fellow here. does  anyone have experience with saving/loading tensorflow models? i have managed to re-load a model i pre-trained, but i am having trouble using it to make a new prediction... my goal is to re-use the pre-trained model to make a prediction on a new data example (e.g.: user uploaded data in flask! :fearful:)what kind of model are you using? i retrained the last 2 layers of inception in my project. that is an awesome visualization   thank you for sharing!!hi fellows!! i have a question. i am looking for a way i can implement a register website to download load a file. like, let\\'s say, you go a web site of data set. then you have to type your name, email, institution/company, and so on the web page, then you will receive a link to download the data set.  anyone have any idea or know any website that help me? i think you could set that up pretty easily using flask and some extensions: thanks! would you point me any tutorial site if you know any?i won’t do anything besides google it, so you might as well just do that yourself :slightly_smiling_face:ok i see, flask with extension...thanks!!if anyone is interested in the dataengconf in nyc next week, you can get 20% off by using the code insight\\ni think you should still encourage people to post senior level positions on the board (in addition to emailing them). i know i always look at the descriptions to see what\\'s out there and figure out what skills i should focus on next. also, while i have no intention of embarking on a job search, if something interesting pops up, i will apply. hello, i was curious if anyone had any experience with enterprise-level api design. our current stack is all java but we’d like to setup a python api that will be queried 100,000+ times a day. if anyone has any expertise in the area and can answer a few (newbie) questions, please pm me! that\\'s possible.  100k requests / day can be solved in many ways.  one way it\\'s done in wsgi apps like django or flask is to have many workers behind nginx or a load balancer.  the number of workers depend on how many *concurrent* requests do you want to handle.  that also depends on how long does it take for the request to get processed by the application.  is the response in 500ms vs 1s means that you need half the workers to do the same job.\\n\\ni have used python tornado (which was designed to solve the c10k problem).  tornado is usually architected when you there\\'s lot of wait time (i/o or db) in processing the request on the server.   i have written a sample app that might be useful -template: `` or sample project: ``.  i have given a few talks on this as well at pydata, so ping me if this looks interesting.\\n\\ncreated a public channel called  to talk more about it.for anyone interested in attending next week’s dataengconf in sf, they’ve given us a discount code for 50% off - “insight50”. there seems to be a lot of good presentations and we’ve got several alumni presenting. looking forward to it!\\n and i + a few friends will be starting at 11am at the justin herman plaza. i\\'m borrowing a lab coat. anyone else interested?^i’ll be at civic center helping organize some of the official march volunteers!hi everyone! i am very excited to announce that my first course is now up on datacamp :tada:.  it is geared at building an intuition for how to use clustering for exploratory data analysis. i would greatly appreciate any shoutoutouts on social media to help spread the word! \\nwhen i\\'m doing personal stuff i use conda for envs. at work i\\'m a really special snowflake.hey guys! quick plug for one of my mentors: you might have seen in datascienceweekly that lending club are on the hunt for a weapons-grade data scientist. chaomei chen, chief risk officer at lending club, is widely regarded as the reason lending club achieved the second largest ipo in tech history behind facebook. she is an incredible woman, she was humblingly kind and helpful when i panic-emailled her out of the blue to get my insight project over the line, and many people owe their careers and their success to her. we would be lucky to work for her. let me know if you are looking for a new position and i would be delighted to make an introduction.does anyone have or know of some nice slides covering \"ml for pms\". i am giving an introductory-ish presentation to our product group on monday and it is always great to see how others have organized the topic.i’ve shared around  and  to our groupi think the latter has slides up on slideshare as wellyes, found those slides. thank you! thank you ! one of the things the bothered me the most in insight was the fundamental lack of flexibility in the infrastructure we usedi used digital ocean personallyyeah and i used hyde+google cloud+blumix, and the support was amazingyou can always just say “thanks for the interest, i’m not looking right now, but can i keep your contact information and reach out should that change in the future?” i was actually quite happy with the level of freedom (and everything else, generally) at insight. they provide tools/support for certain ways of doing things (e.g., aws) so it makes sense for them to encourage people to do things those ways, but in my experience they never stopped anyone from doing things different ways.hi all, for your very first job right after insight, is it normal to go in with the title “data scientist” or “senior data scientist” or else?“junior”, “principle”, “associate”, etc.slide-share woes:  anyone have any suggestions on having a nicer looking slideshare?  mine is cutting out edges of my tree on p. 13, even tried to change the color but still a few aren\\'t showing up in slideshare, and do show up on my .pdf export from keynote:\\n\\ndon\\'t use slideshare? [hi dan]yeah, i’m just linking ot my pdfsha, hi matt!  how else would you recommend us present our demos to companies?  slideshare was recommended to us as being safer than relying dropbox.  note:  i do not have a front-end :simple_smile:well, a link to my github repo is my \"front-end\"you can stick a pdf on github, why notor make a website that serves as a slide deckflash drive?internet of thingsthings of internetdoes not show up as a presentation on github:\\nah, well i can click \"raw\" and download the presentationyeahpsa: github renders markdown/rmarkdown/ipynb inline nowi used google slides.yes me too: check out github pages thanks worked like a charm!\\nif you structure your readmes well and use markdown you can turn your repo into a project website using the automatic conversion processnice: i used speaker deck for my demo  (slides are actually still up at ). no issues with the slides getting cut off, and the website automatically updates when newer versions are uploaded.i’m also using speaker deck. slideshare destroyed my vector graphics, even though i converted to pdf.stay positive brad.deutschyour daughter is a good icon designer uploaded a file:  and commented: good luck to the current batch of fellows as you embark on demos.  here\\'s a checklist my daughter (age 7) made for me when i was getting ready to demo last summer.  i think it\\'s pretty good advice, not just for insight but for presenting in general.see now i look prescient and weirdhah, sorry, couldn’t get the file uploaded…should work now.hey all, i’ve a very open ended question for the health people: i’m looking for some starting points/articles to start thinking about merging and using messy emrs. experience/tips/packages/cat gifs welcome. :cat:hey  messy emrs are the bane of my work life. not sure what you want to do  but the fhir project might be a good starting point: what kind of emr\\'s are you talking about? is it still a data integration problem or are you looking to do something more?this is the python client for it. our team (leifur\\'s and mine) is slowly moving towards adopting the fhir protocol and setting up a fhir api that allows patients to be retrieved as fhir objects. its essentially a standardized protocol/format for exchange of emr data across providers. we are using it internally because msk has a really wide variety of emr systems. so integration and lining up data from different emr systems is quite painful. ok really painful!!! :scream_cat:/giphy banecatthis looks super good thanks ! it’s good to know what your data should look like, even if it doesn’t. we (at mt sinai) have a dump of emr data from different hospitals and are looking at extracting any information. at the moment the main objective is integrating and lining up data. :upside_down_face:yeah the fhir project is really pretty awesome.  which team/dept are you with in mt.sinai?  we\\'ve seen some really good stuff come out of there...just started at the arnhold institute of global health. mainly focused on global public health but with some local and internal things too.hey all, does anyone know good sources for learning time series analysis with python?:: also,  thanks the tensorflow dev summit is happening next wed. on-site in mv has sold-out but there are still spots for the live-stream viewing party at google sf office (for now): that\\'s great, thanks for sharing! does one need a work visa for this?anyone know any data scientists in vancouver? i have a smart friend trying to get into the field who just moved there.i know someone who used to work at mobify as a data scientist in vancouver, but he is now a data scientist in ottawa at shopify. if there\\'s no better connections, he may know someone or be useful himself. thanks  - probably that’s removed enough that she should look for more direct avenues. but thanks for the offer!mike aka schnaww works for kablamo in vancouversadly, most likely yes!oh yeah! i remember that now! thanks : when you say you don\\'t want to fit a distribution to the data do you mean you\\'d like that continuous function to respect the weird shape?that\\'s correcti was thinking perhaps some kind of gaussian expansion, if that\\'s a thingmy first thought was edging towards using some spline interpolationfitting the spline to your data will give you continuous measurements over your initial range of datatoo naive perhaps ?i\\'ve used spline interpolation with this data set, and it does work, however, this distribution has to be sampled hundreds of thousands of times. spline interpolation increases the run time significantly, unless there is some way to serialize the spline interpolation?i could probably serialize that, but it would still require lookup of the proper splinebut still, it would probably be faster, since the interpolation would only be computed oncei was considering some kind of fft, using an exponential basis, since i require this distribution to converge to 0 at +/- \\\\inftyhmm guess depending on the precision required by your sampling procedure you could use some form of memoization and only compute those that are absolutely necessary and lookup from the previous computation whatever falls within a threshold distancei\\'ll mull over the fft, don\\'t have a great intuition as to how it could be applied at the moment :slightly_smiling_face:alrighty, i\\'ll mull it over as well. thanks! i\\'m here!just find out this online ai conference next month - seems cool. can\\'t argue with that price also the results are in: thanks gabriella!so, i haven’t read that article yet, but are physicists back in fashion? i thought the \"physicists are great data scientists\" thing was very 2010, and that humanities and social sciences were growing in demand. no?half my team is physicists for what it is worthi’m also a physicist, and i hope i do at least average work :wink:hi all,\\n\\ndoes anyone have experience web scraping using python, in particular scraping for images?  i’m currently trying to scrape google images, and am hitting some road blocks since the api is now deprecated. i’ve created the channel , please let me know if you have any tips/advice!i have used splinter for my scraping in the past ()i’ve used requests + beautifulsoup, works pretty wellhas anyone had experience getting data from hcup, namely the kids\\' inpatient databasetrying to get data on a particular diagnosispediatric asthmathis is more of a pm question, but can anyone refer me to a framework on a best practices approach on how to evaluate projects?i’m aware of irr/npv for larger capital allocation, but i’m wondering how often these are usedif you have projects a,b,c,d.  other than the hippo, how does one decide what to pursuewe’d used a scale in the past where we scored projects across 1) estimated impact of project 2) estimated confidence of that impact 3) ease of implementationand then added projects into a sprint in a way that seemed to balance implementation time with potential impactthis was passed along to meseems very similari think reach is really important to keep in mind as well — we run into this when prioritizing analytics projects for clients, e.g. this model will add value, but only for 10% of users so we will not pursue scaling it yethi past and present fellows. does anyone have advice for downloading medicaid/medicare outpatient claims data? ? on the free end, meps is a good choice (medical expenditure panel survey) -- . hmm, just saw you were interested in outpatient claims. not sure if meps includes those or not. thanks for the suggestion! i\\'ll check out meps and see if anything clicks.does anyone have experience writing scopes of work for contract data science gigs? if so could you please pm me? i\\'d like to pick your brain. thanks!  i\\'ve used scrapy new fellowship here. throwing this out into the ether. is anyone knowledgeable regarding multiple sclerosis? do you know what type of drugs or therapeutics are commonly used for treating it?there you godata science from images obtained by google street view though no longer a neuroscientist by occupation, i can tell you some general stuff. since ms is an autoimmune disease most drugs used to treat it those that block the immune system or mitigate the pain, tremors, and other symptoms of ms.  have you examined ? they have a section on treatments that fairly comprehensive and easy to read (bonus!)., thank you!  how about ny state hospital inpatient discharge data:\\ncongrats  !  great post: awesomethanks ! it was a fun project! data and analysis code is in a github repo (linked on the viz page)i appreciate your implicit use of the isoperimetric inequalityi didn\\'t know that name for it before, but now i do! :simple_smile:“that thing about circles\"hey there, anyone have a clever way of working around distil network\\'s scraper detection? i keep hitting up against captchas while trying to scrape streeteasy using selenium, even when using very polite sleep timers and vpn cycling.from my memory, distil embeds some javascript in the client side code that is used for detecting automated clients and esp browser automation tools like selenium. so sleep timers and vpns won’t help  it also acts as a reverse proxy for the target website’s traffic like cloudflare does. so no real way to get around it i have a couple ideas for dealing with it that are out of the box, so message me if you want to discuss further :) :+1: thanks francesco and eddie! looking through the medium post now, it is quite helpful. :slightly_smiling_face: (and i\\'ll probably take a look at the book once i delve deeper)the bay area bike share has released it\\'s year 2 data:  perhaps of interest for future projects!question for python de types: can you suggest any resources (e.g. articles) about building scalable python production servers? context: we have a ml algorithm written in python to put into production. in the past, we have rewritten algorithms into our existing java production stack. however, this one uses enough scientific python libraries that we wanted to explore the feasibility of a production python stack. info about memory issues, performance comparisons, or server and container options would be especially helpfulhi ,  we actually do a lot of this at datascience as deploying python models is a main part of our data science platform.  what i\\'ve found is the main thing for performance is python itself.  therefore using modules like `numba` or building the models in cython will have by far the greatest impact on performance.  we also find that load balancing the model between multiple instances (e.g. if you have it running on 10 different hosts or docker containers, then use a load balancer to route requests to them) can help scale python models.  as far as python-specific options, i would be weary of default web servers in python projects like django.  these can be a bottleneck, so you should look into a more sophisticated python web serving stack, e.g. :  thanks  !hi guys, question: in my company we are trying to come up with a title system for data scientists that can show different levels of seniority/experience. we span from people with undergrad degrees to people with phds and years of experience. any suggestion? all i can think about is data scientist/senior data scientist but would love something more granular...junior ds (bachelor)/ regular ds (grad school or exp) / senior ds (grad school + exp) ?what purpose do you want it to serve?  junior / regular / senior are all based on a combination of education / experience levels.  what do you want to highlight with these titles?data *analyst* for bachelor folks?we have people with bachelor degrees who are definitely extremely talented data scientists :slightly_smiling_face:  i would not base titles on things like that alone, that’s why it’s important to consider what you want the titles to convey before just adding granularity^i personally agree, i was just suggesting what i’ve seen at companies...data analyst is usually a pretty different rolecan be, at least.yeah it can be a mix, we in the past have had people who do data scientist things but don’t have the experience or education needed for higher level, as data analysts who were later promoted to data scientist, then other analysts who do entirely different things and are on different career pathsi\\'ve been chatting a lot with companies about this recently actually. the distinguishing of analyst vs scientist can vary from company to company. the general trend though is that analyst\\'s job is to surface numbers while the ds\\'s job is to understand them. you could ask an analyst to find the % of users that click on a button for android vs iphone, but you\\'d ask a ds that works on analytics to find why android users are churning out at a higher rate.so if you\\'re an analyst on paper, but actually do ds/ai work, that distinction\\'s easy to communicate to potential employers?hey - i\\'m a new fellow in the health data program in silicon valley. does anyone have experience getting plotly plots(specifically chloropleth) to run on a webpage through flask ...?? ^^ have you done this? in an interview it’s easy to explain, on paper less so. squarespace went through this angst recently hits the right points too — in my company analysts are great with sql and tremendously helpful with feature engineering, but perhaps less sure of why those features are likely to be mathematically relevant. we’re trying to train them in python to move up the stack a bitjunior, ds, senior, principal for individual contributors. above the ds level, the managerial track can add \"manager\" and \"director\", which naturally funnels up to the vp and cdoalternatively, junior has a bit of baggage, so associate works as wellyou can always do the really boring microsoft-ish way: ds1a, ds1b, ds2a, ds2b, senior dsa, ... :wink: but i guess big-ass companies need huge laddersthat’s an interesting problem, how would you find the best so page? compare code block you’re currently working on to the question or answer code? or only look up the actual line of code you’re working on?  is anyone else going to nips?,  from what i’ve read that only works for python 3 though.has anyone ever had a grumpy it department who will not install required ssms (my work chose this for sql) on their computer and instead wants you to use the interface remotely? i am not familiar with this or what it will accomplish since out database will be hosted securely on their server without full administrative rights to us.  but i am also worried how this will affect my interacting with sql from python. anyone done anything like this? or should i stand up for myself and tell it this is bonkers? thanks!i guess the question is what would you want to use ssms for? an it department is probably wise not to install software you don’t needwe are using tsql and i assumed that i would need ssms to create and modify  the databases but i\\'m really not so familiar with it so maybe there is an alternative?i haven’t worked with microsoft sql server, but i doubt you’ll need the administrative tool it to create schemas and tables. in general, once you can connect to a database server via python (using pyodbc or sqlalchemy or one of the many other libraries) if the user you’re connecting as has the appropriate permissions, you can do everything you need.yay!!!! i love this solutiondefinitely read up on it first before taking a stand with it…best not to make enemies:wink:very sound advice! thanks!anyone have suggestions on what kind of “hustle stats” they would like to see in their workplace as part of employee performance metrics? anybody else going to meet up on survival models in stan tonight in nyc  ?  \\n: sorry for the spam but can my mentees this evening please slack me? i will be late.i had a few questions related to an app i am developing. every query to this web app (in flask + spark) is answered by a simple pre-trained machine learning model in h2o which is then served to the user.  now if too many users begin to go to the web app, what are the problems that i would face and need to be solved? how many users it too many? and how would i solve these problems? if someone can shed some light, it would be terribly helpful! p.s. someone mentioned that i should be using spark streaming + kafka to solve this problem.: normally a web server has a queue for incoming requests and one or more workers to handle those requests. in flask, there is an easy way to set the number of workers, and it is usually set to the number of available processors on the computer + 1. that means that if the average rate at which the requests come in is more than the rate at which they can be handled, you will soon run into trouble since the queue will overflow. \\none way to solve this problem is to get a better computer with more processors to have more workers, but that soon becomes impractical. kafka is a messaging system that has its own queue and allows you to scale to more _computers_ rather than more _processors_ on one computer. it is not at all trivial to set up and maintain, but it is a much more robust solution for systems that need to handle millions of requests per minute.thanks alex for the great explanation! millions of requests per minute is certainly not something i\\'m expecting :smile: one last question, what happens if there are more incoming requests than processors on a computer? does flask queue these requests for later computation, or are they dropped?the requests should be queued, but i don’t know the default size of the queue in flask (it’s probably 2048 or something). another point is that http connections tend to time out after 30 sec of waiting time, but you should be able to increase this timeout in flask settings if needed.great episode of data skeptic discussing a ds interview problem from the perspective of the interviewer: stay positive dima… someone really needs to take on a project to make  smarter.... i listened to that last week, it was great!i think i’ve removed that sentiment “analysis” from nuggetbotcan be summarized in a single bullet point:\\n* give everyone access to the data.¯\\\\_(ツ)_/¯it’s a good point, though probably doesn’t need 15 minutes.  the one complication is that sometimes you need to have spent a lot of time sql monkeying around to understand it’s quirks.so  you need at least a couple sql monkeys somewhere that you can quickly find and exchange bananas for insight.also, i’m staring at a empty banana peel on my desk right now. what does this say about me?hahatrue, or, you need to have someone etl the data into a nice denormalized schema that is easy to query for non sql expertsbeen finding some inspiration from this article\\nyes, though such schema can make easy questions easier and harder questions secret traps. also, the only thing i like less than being a sql monkey is working with bi to get an new etl setup.it leaves you to do the hard stuff on a different data store while everyone else can pull their own data with the bi tool + data warehousewe had a client with a horrible, several-hundred table database that only one guy at their company understood. i was asked to pull some analytics out of the database. because their one expert never got back to me to answer my questions, what should have taken me about an hour took over a week. oh well... we bill hourly, so it was good for us.haha, yeah, bad for you and i don’t want to be that “one guy\"hmm, i\\'m a male scientist married to a female teacher... as predicted! data science verified!n=1 and doneit works for my parents :smile:: \"the small sample size is made up for by the sheer consistency of the sample!\" :wink:i think one lesson learned is if i want to find a spouse in the future, i should be a truck driverthey’re basically married to everyone, not simultaneously i believefor us physicists and astronomers uploaded a file: i dunno… a mobile lifestyle might lend itself to having multiple lives and marriages.if you’re into that. i\\'m also married to a teacher, n of 2!qed!new blog post: feeling out the famous mnist handwritten digit set.\\nhey guys! two of my best friends from the uk are coming to sf for the week, and i wondered if anybody could help me with a couple of favours: 1) are there any spare spaces they might crash on monday to thursday evening? and 2) might somebody lend one of them a bike during the day on tuesday? cheers!sorry everyone, wrong slack group, ignore!stay positive mattgibbany alumni currently work with aws tools? please dm, i\\'d love to pick your braini feel like the answer to that is probably almost everyone. any specific tools you have questions about?trying to pipe simulated data through kinesis streams, dynamodb, and lambda. i\\'ve found a few tutorials on the web, but none quite seem to work (not sure if me or them)etl question:  i have an sql database with 3700 people.  each person has hundreds to thousands of associated multi-category columns events, such that when loaded into python the 3700 people become a 10+ million row database.  i want to one-hot-encode it for sklearn.  after one-hot encoding, it would become 10+ million rows and thousands of columns. by using db.groupby(\\'person\\').max() in python i would then have an accurate and useable one-hot-encoded 3700 row dataframe (one row per person) for sklearn.  i don\\'t have the compute for the large one-hot-encode step.  has anyone experienced the similar and have any suggestions that don\\'t require more compute?for big transformations like this, i often use the just-in-time compiler from numbabasically convert your dataframe into a numpy array and then write for loops to do the transformation that you wantmake sure to use the @numba.jit(nopython=true) annotationi often get huge improvements in run-time this waybtamadio: thanks brian!  i\\'ll give that a try!no problem. let me know if you run into any issues. also if it works for you, i’d be interested in hearing about that too!that’s super shitty of nyc. tho, congrats on the baby !i\\'ve worked with geospatial indexing in mongodb and it is fast! if you can get a way to index your points and/or your shapefiles geospatially you might find a good use for it.yes, it is. and thanks  !if you love scala, building data pipelines, creating key metrics, building and enabling actionable visualizations, and helping to measure a new business, please come work with me :wink:\\nis there anyone who has experience in policy or regional collaboration? i have someone at brookings (vp of cities) looking to work with a data scientist to review a proposal. dm me if initerestedprobably depends on the subject area, but maybe try some of these folks?      i\\'m happy to talk if you\\'d like. what are you planning to do with this data?i’m looking for advice on navigating the process of taking over a team, especially for a first time manager. i’m sure a number of people here have experienced this—any willing to share publicly (or privately)? (also, please don’t share outside of insight network—nothing set in stone yet.. just looking down the road.)i\\'ve read several articles over the years, so that\\'s where i\\'d start. i\\'d say probably 80% of it is good advice, subject to the usual disclaimers about personal style and company culture.message me and let’s catch up? prodigy looks really neat! how have you liked it so far? can it handle multiclass or multilabel labelling?yay! genevieve you made it!nuggetbot: animate me welcome genevieveit would be fun to attend and hang out but might be too soon for me to give my insight spiel again :robot_face:good input, nugget!haha, i think it\\'s totally fine! do let me know if you\\'re up for it! why not talk about work then, were all data scientists :stuck_out_tongue:hi all. i\\'m thinking about a move to the bay area, but my husband and i are not sure how affordable it would be. i\\'m trying to figure out salaries for data scientists and i was wondering if i could crowd source some data here. i have looked at glassdoor, but i would like some more specific information. i\\'m wondering what the salary range would be for someone like me, a lead data scientist currently with three years of experience in the field by next year. i\\'m thinking about looking at jobs at established companies or late stage start ups (100 or more employees). i would also like to get a sense of more than base salaries, like what do bonuses and stocks/options plans look like since these can make up a large part of compensation? thanks in advance for your input!check it! alexis there was a recent crowdsourced salary survey run here (so only insight alumni) that might also be useful: sorry that link is from another source, here is the insight one: links look the same, let’s try againwow, that\\'s awesome! thanks!for those who use spark, do you prefer pyspark or scala? my team is moving to spark and considering which route to take, since we’re primarily a python shopi’ve used both and i would suggest going the pyspark route and have a few people dive deeper with scala as the api is complete (pyspark has a few limitations). the future of spark appears to be in python/r (and other higher level languages).i don’t think i’ve encountered a spark use case where i couldn’t get it done in python, and that intergrates with the rest of the workflowwe use pyspark. works well and i only moan about the lack of typing when reading pull requests.scala! :thumbsdown:if you really want to understand and leverage spark, scala is the way to goalso worth noting that we use luigi for task flow management, which plays well with pysparkthings are typically developed in scala first, and extended to pysparkanyone know if there a preferred method to advertise job postings?  we have a bunch of insight alumni at grand rounds and have a bunch of positions open.  pm me for all the juicy details.interesting. thanks for the advice!i have also found pyspark to be able to cover all my needs.  for reference we use it in standalone mode (etls, exploration, some ml) since our data is not at hadoop level scales yet.anyone have recommendations for an introductory spark text or two? preferably r-based but python would also be okay. a focus on ml applications would be a plus.i prefer scala - error messages are easier to parse (just jvm rather than python runtime wrapped in a jvm), the api is more feature-complete since spark is written in scala, and type-safety can really cut down on the rate and variety of errors you can get in the middle of a long-running job. however, scala is a steep learning curve from python so weigh it against how heavily you\\'ll be using spark.hi guys! any idea why the insight websites are down? this may have been announced but i\\'ve been offline!... and we\\'re back up! the internet overindulged last night, perhaps? :wink:internet, you\\'re drunkhey folks! i have a tuition reimbursement program at my job and was wondering what kind of external learning/moocs people have found helpful?i\\'m more on the engineering side, and found an offering for a data engineer certification from google here:\\n\\nudacity is pretty greatcoursera has some helpful specializationsfor all our nlp projects, we use py3 because of better text handling. it wasn’t hard to switch to py3, so ideally, the choice of 2 or 3 depends on the project.hey fellows, i have a quick question about recall optimization for imbalanced data sets. any insights appreciated :slightly_smiling_face:how imbalanced? i can\\'t quite remember the exact cutoff before it becomes worrisome, maybe 15%/85%?hi fellow fellows, does anyone now of any existing, straightforward methods to calculate error bars/confidence intervals for xgbregressor predictions as applied to time series forecasting (where you can’t do standard randomized cross-validation/bootstrapping due to time stream autocorrelation)?  i\\'ve been scouring for one and have only found the following semi-cryptic blog post.   thanks!  i wish :slightly_smiling_face: it\\'s 99.1% and 0.9%. i\\'m using smote and xgboost, which is helping the model recognized the minority class. the business situation is detecting credit card declines.  i have an intuition about the answer but i\\'m curious if there is  a convention for optimizing recall for a minority class when doing so lowers the recall for the majority class.do you know the potential cost of lowering the recall of the majority class vs minority class? if detecting the minority class has higher value for the business, it makes sense to optimize for it and just take into account for future predictions that this will mean an increase in falsely labeling majority class cases as minority class.+1 to what  said. also, xgboost has a `scale_pos_weight` parameter which may help directly balancing your optimizationsince you can’t apply regular cross-validation for time series data you should use something like it:\\nfold 1 : training [1], test [2]\\nfold 2 : training [1 2], test [3]\\nfold 3 : training [1 2 3], test [4]\\nfold 4 : training [1 2 3 4], test [5]\\nfold 5 : training [1 2 3 4 5], test [6]\\n\\nsklearn provides a special class for timeseries splitting sklearn.model_selection.timeseriessplit there\\'s a minimal cost to lowering recall for the majority class and the scale_pos_weight has been super useful.  i wanted to be cautious not to make any amateur missteps for this type of case and my thinking sounds in tune with your suggestions. thanks for your input; it\\'s reassuring.hi all! a bunch of us from insight are going to grace hopper next week, if you\\'ll be there too and want to meet up, join us in . we have a panel on wednesday, event on thursday night, and we\\'ll be doing office hours with applicants if you have friends who would like career advice. :woman-woman-girl-boy:i know what you mean, . if you come up with something else, i\\'d love to hear about it.oh, and as a reminder for folks who want to try upsampling the minority set, make sure to split out a validation subset before you do it, so as not to accidentally pollute it. :smile:hi georgiy, thanks for your response.  i have implemented this walk-forward/expanding window forecasting with xgbregressor, but i\\'m wondering what the error bar/confidence interval is at each prediction step.  there  doesn\\'t appear to be one for the model output (unless i\\'ve somehow missed it).  i\\'m thinking i might bootstrap a confidence interval, but have to be careful there as autocorrelation means leaving some samples out doesn\\'t entirely remove all of their associated information from the model (and leaving \"holes\" in the data may destroy  autoregressive components the model relies on).hey folks, data for democracy is having a hackathon this weekend and i\\'m co-hosting an in-person event saturday in boston.  there\\'s also a nyc one! hi folks, a friend from indiebio is organizing a science hack day that i thought some of you might enjoy: i\\'d love to see updates on this project as it progresses!our start-up company (grapevine ai) is looking for a freelance full stack developer,  javascript, node and angularjs expert, please shoot me a note if you have contact with someone like that.. thanks!@hee you can use the 80gb file by manipulating the rdd (`rdd = sc.textfile(’filename\\')`)most probably, you’d wanna forget about the order of rows in the column, given that the computation is distributed across the cluster.please feel free to pm me.hi everyone! can anyone share best practices for code review processes that work for their teams? we are a small group, working mostly in jupyter notebook environments (python, r) as well as lots of sql queries. our team is just getting started using git/gitlab, documenting code, and generally collaborating (new analytics team within gov\\'t agency). any suggestions would be great!we have two kinds of reviews, a style review and a requirements review.  style review - we have slowly done away with it if the language has a robust set of linters (flake8 for python for example).  requirements review - done by another dev.  there are blocking and non blocking comments as welland if the comments in github review is getting long, best to  have a face to face with the reviewer.and we try to make the code for review as small and atomic as possible so it won’t be a pain to reviewfor big/lengthy code commits, a face to face at the start, usually helps to explain things to the reviewer before she/he spends hours trying to figure the code out :slightly_smiling_face: i saw this the other day: thanks    for your comments! nice! thanks for sharing i think one of the most important ingredients of successful code review process is the individual pull request (pr) themselves. modular, topical pr are much easier to understand than one gigantic pr that contains lots of changes. as you build up the code base, keep pr small (maybe one class per pr, or one set of related functions per pr). when it comes major architectural changes, share and hash out the design in a different platform (like google docs) before putting out code changes. code diffs are really bad ways to see big changes. \\n\\nfor jupyter notebooks, try to refactor most of the code into utils .py files and library, and import those in the notebook instead of putting most of the code in the notebook. it’s fair to have different code standard for those common utils/libraries and the code in the jupyter notebooks themselves.: thanks for the detailed response! i\\'ll follow up on separate channel ...super bowl sunday!has anyone done a pid controller for discrete data?does anybody know of a compilation of common character substitutions made by ocr software (e.g. i-&gt;l, e-&gt;o, 0-&gt;o)?thanks guys!what you expectbay area -&gt; austin for data/tech people!where in south bay? and is month-to-month do-able?mountain view and no, it\\'d be for a 6/12 month lease :disappointed: my friend susan is helping to hire 10-15 data analyst/ds positions at the federal reserve based on these two descriptions if anyone knows someone looking. you can email her for more info: does anyone have experience with azure and some spare time to chat about it? thanks!: if you\\'re really into emoji nlp, check out idibon, especially tyler schnoebelen\\'s blog posts. he\\'s a stanford linguistics phd that wrote his thesis on the grammar of emoji, and cofounded idibon.: thanks! i’ll check it out. so cool.my mil wants to come visit on the ues. anyone have any good experiences with inexpensive hotels in the area?: we got a hotel room friday night for $250 with hotwirei might be missing something but i believe at least for sqlalchemy you want to use \"?\" (without the quotes) apparently the formatting is driver-specific, but will be chosen from one of the 5 param styles detailed in pep-249: pymysql driver at least uses the “pyformat” option (’s answer)thanks for all the help.  i saw that page above and have tried the ? and also found some ideas on . i can get any number of \"correct combinations\" in string and params code to not throw an error, but i still get no data...so weird.  when i figure it out, i\\'ll post here.time to make a point. if you support net-neutrality here is the link that makes it easy for you to voice your opinion to the fcc. ^jon oliver also purchased the domain  (which links right to the page to comment on the fcc site) which driver are you using?  i think sqlalchemy defaults to using `mysql-python`mysql-pythonconfirmed.as so my connection string starts with `mysql`right.  if you have the pymysql package installed you can use that as the driver instead, e.g. `mysql+pymysql://...` for the connection string.  think the pyformat style `%(param_name)s` should work thenleast that worked on a mysql instance i have locally herethis is in a prod system and so adding a package is going to be frowned upon, but i\\'ll bring up the option in a review meeting i have later today and i can test that...i\\'m not sure if anyone is looking for opportunities right now, but if you are, automotivemastermind is looking to hire data engineers and scientists in both san francisco and nyc. the (possibly outdated?) job descriptions can be found here:\\n\\n\\n\\nif you\\'re interested, feel free to reach out to me for more details that\\'s exciting to hear! if anyone is interested in connecting with automotivemastermind _directly_, then the best thing to do is to reach out to me or someone else from the insight team! :smile_cat:whoa, microsoft to buy linkedin yes it\\'s big“linkedin, powered by microsoft” sounds a lot worse than “linkedin” :disappointed:stay positive tigrani guess the question is: why would ms drop that much money for linkedin?same reason they paid so much for minecraft. to make even more money.to be fair, after failing to buy yahoo! for $45b, they have a lot of \"we need to buy someone!\" money burning a hole in their pocket. this is basically free!i meant “how” will they make money from linkedin? linkedin apparently made $3b in revenue in 2015. unless they’re looking at stellar grown, it’ll be a while until ms makes money off this deal.i suppose they’ll leverage linkedin’s network in other ways...it doesn\\'t make sense to me either... my ms spies tell me they got an email saying there will be \\'great synergy with office 360\\'microsoft user data + linkedin data = :hippo:ha!this is going to make it so much easier to recruit via xbox live*xxx skull destroyer xxx* would like to talk to you about working at abc corp!my guess is they are trying to keep a foothold in the office software game to fend off google apps and such. linked in is the biggest professional network and will probably give them a lot besides the revenue streami think msft was supposed to have a call to talk about all of this today. not sure when it was going to happen though.yeah  , that\\'s what my contacts in ms said, it\\'s a 360 play.which does make some sense, office is (to my knowledge) their biggest product, so spending $25b to try to protect it makes sensei\\'ll try real hard to not take offense from these comments :slightly_smiling_face:yeah. if they are going to continue to “give\" away os upgrades for free, they’ll need to be sure to keep the 360 revenue stream going.would love to hear from both our linkedin + microsoft alumni  :slightly_smiling_face:*addendum*: my spy has told me that it is actually *office 365*, not 360... oops. :wink: working on that branding guys!office 360 sounds more funbut 365 &gt; 360how about office 720 or even 900? tony hawk styleoffice graham’s numbersoon we can upload resumes using word!seems a lot of folks have mixed feelings about this.. i believe it\\'s an interesting movebut not as interesting as that gif...there is a lot of potential to utilize cortana intelligence using linkedin data to empower networking. let\\'s seei guess that\\'s all we can do, wait and see right?i look forward to the future of this.i think emmanuel has had too much kool aid. but i agree. as much as i don\\'t use microsoft products, i do think they have some reason smart people working for them and there is a good chance something interesting will come out of it.  maybe, this is the start of microsoft bob 2.0bing?bingedin: the best place to search for a new jobyer welcome msft:stuck_out_tongue:lol, you guys..hahhaahey all! i\\'m helping udacity to build a new nanodegree on data science. we\\'re looking for partnerships to build industry relevant projects for students to work on. let me know if you are interested in.sounds like a familiar plan :wink:yes and no. udacity is offering a paid educational program with no job placement promise.has anyone\\'s company migrated from a vertica warehouse to snowflake and been subject to the very unpleasant surprise that all queries in snowflake return column names in all caps? uploaded a file: i don\\'t think it can be changed but i have found that snowflake is not case sensitive unless the database/schema/table/column was created with double quotes around the name. commented on ’s file : this is an old tutorial but might be useful if you haven\\'t seen it yet\\nif you use docker there are some great containers here: the appropriate containers come pre-built with spark running nicely in jupyter along with kernels in scala and python and r.\\npretty well maintained, when new spark versions come out they usually update the containers within a few weeks.use docker as much as you can  why docker? why not...anacondadocker makes it easy to standardise, and it’s portable. \\nfor example, i have docker images that colleagues and new hires can run with very little setup, then they have access to my entire stack.yep- you can port the environment and code anywhere in a box, no need to ask a colleague to set up their env in the same way, or to set up a server the same way etc. just build the container reduces env issues that come up inevitably just being silly. plenty of good projects to go around, and most people here may be good at coming up with well-planned ones hi question, how many of you are managed by non-technical people (what are their backgrounds) and how has that worked out for you?what task (regression/classification/ranking/etc) are you planning on using it for? i can flood you with ranking information and pitfalls if needed.i\\'d like to hear about ranking :slightly_smiling_face:@ channel, ha ha, just joking. my company is looking for a machine learning engineer, let me know if you\\'re interested:  uploaded a file: anyone at dataengconf in sf today and tomorrow?update: i actually phone screened the guy who’s great at hiring women today. he was not great, especially considering he’s been an analyst since 2004 and a ds since 2012 (including being a sr ds for the last couple years).\\nfrom my interview summary:\\n-candidate did not know quite a few basic ds ideas and was overconfident to the point of condescension, even arguing with me on a couple of responses\\n-what got you into ds: “this is how i’ve always thought about the world”\\n-when describing a logreg model: “throw it into logistic regression then things happen” and “the model will spit out a thing”that last one is amazingwould it be unethical to give him an on-site just for the additional slack gems it would generate?i\\'ve had interesting experience with the first one, where the interviewer was clearly wrong.  instead of arguing i used my standard strategy of immediatly accepting the statements as true, then qualifying them into oblivion (\"yes, but it\\'s only true in this impossible scenario, otherwise it\\'s generally not\")\\n\\ni\\'d definitely prefer someone who  challenged me if they thought i was wrong though :slightly_smiling_face: it seriously crossed my mind…nothing to lose if he\\'s local :smile:it\\'s win win!hahaand i totally agree that i appreciate someone standing up for something if they can make a cogent argument in their favor…this was not such a case howeverright, it doesn\\'t sound like it\"throw it into the model\", \"the model will spit thing sout\" is enough to convince me of that :smile:though... that\\'s where data science is going, this guy is going to be a star soonoh god nuggetbothahaq: how do you build model x?\\na:  buy data robot and throw data at it\\n\\ngreat!  welcome to chief ds positionit\\'s the future!did not expect a follow up on this sagathis guy has been a sr ds manager at a well-known company for a couple years so he’s already a star!did you consider that if you don\\'t hire him, you guys won\\'t be as good at hiring women as you could be?definitely a considerationsure, he may not know logistic regression, but he brings women hiring to the table“sr ds manager at a well-know company” … makes me nervous!well, alex, did you interview recently and forget the details about logistic regression?i mean, what even is a logistic?! i think you throw data into it and it spits out a thing!i can shed some light into thatit does logistic things to the data before spitting out logistified outputspsh, i have so many logistics just laying around the office, we just shovel data into one of them and it spits stuff outlogistic oven is the best for neapolitan pizzai think it has to do with shipping and storing things.  \\n\\nwhatever.there’s a quora for that: i argued with an interviewer when he said that a random forest was an interpretable model, and i can\\'t say i managed to stay polite. they did invite me for an on-site including an hour long talk. i seriously considered going and giving a talk about model interpretability, but in the end i decided it was too much work and declined the interview altogether.^i fully support you in that argument. also, it woulda been hilarious to give an hour talk on model interpretability!could you explain where you draw the line on “interpretability” and why random forest misses your definition?are you asking for the hour long lecture?i think we are :pworking on some content right now on model interpretability with respect to gdpr. a lot more gray area than black &amp; white, so i\\'m also in favor of this lecture.not using it for any specific task.  trying to learn as much as i can about it.  let the flooding begin. :slightly_smiling_face:i’m curious which company will be the first to get fined under gdpr, though it might be more fun to guess which company will accidentally drop table when trying to add the ability to remove all of a user’s data at their request…i have a fun announcement: if you\\'ve seen my jupyter notebook talk during the first week or so of insight, you know that i have a lot to say about the notebook. i\\'ve sat down and brain dumped an extra 4 hours of content into a one-day workshop held at stanford through their continuing studies program. jupyter notebook is only one piece of the workshop. lots of other tools and techniques will be addressed. hope to see you there!\\n\\ndetails here: \\n\\n hey all, i\\'m on the fb people analytics team, and we\\'re considering various options for knowledge sharing. functionally speaking, we want/need: access control, toc, tags/topic partitioning, relatively simple set-up/admin, and a nice ui. could you please share what your company and/or team uses? i\\'ve implemented airbnb\\'s knowledge repo () locally, and i personally am a big fan - but i should consider other options as well. thanks a lot!thank you - i recently joined airbnb and the fact that the knowledge repo exists is amazing. it\\'s been so helpful in getting up to speedooh i\\'ll have to check that out too, already dabbled with superset i agree - i may be in touch if we end up using it here. there are some issues i\\'ve come across e.g. needing to reboot the server in order to see any new postsi’m curious about that tool - is it essentially a nice way to organize notebook-like outputs in a centralized location?  can answer this better perhaps, but yes exactly, \"centralizing\" content is one really nice feature of the knowledge repo. some other nice features are: tagging, toc, being able to view the raw markdown (e.g. so you can borrow other people\\'s code snippets)yeah it sounds pretty neat i’ll look more into itdefinitely, feel free to ping me about itgreat! i am going to use superset for my new project deepplay - i have to have a look at knowledge repo too.i don\\'t know much about the technical aspects, but i could put you in touch with people who do if you need it. - that pretty much sums it up. it makes it easy to find others\\' work, reproduce it, and use itif anyone is in the office can you yell at my scheduled people to pick up the phone. do you have a name ?noneok, i\\'ll just shout in general then.i\\'m on itplease embarrass the shit out of anyone who is scheduled.  :grinning:  hey, we\\'re supposed to meet at 5:30pm. i haven\\'t received any hangouts requests.call me. 7144250620or initiate on  \"google can change company data in one part of this database—running an ad, say, or debiting an advertiser’s account—without contradicting changes made on the other side of the planet. what’s more, it can readily and reliably replicate data across multiple data centers in multiple parts of the world—and seamlessly retrieve these copies if any one data center goes down. for a truly global business like google, such transcontinental consistency is enormously powerful.\" good morning - anyone ran into problems importing scrubadub in python3? \\ngetting the following error: attributeerror: \\'callable_iterator\\' object has no attribute \\'next\\'hello, has anyone ran into  java.lang.stringindexoutofboundsexception: string index out of range: -1 while trying to write to elasticsearch? (not a corrupt dataset problem) thanks :smiley:not sure if anyone else is working with scala but if you are i found a great book for getting your head around functional programming:also, i\\'ll share the writeup i\\'ve been slowly building about how to use spark dataframes in scala uploaded a file: it is probably not completely uselesshi everyone! i am one of the current ds fellows and i was wondering if anyone has aby experience with the amazon api ? specifically i am interested on how to get the reviews without applying for a merchant license.i think there are data dumps for reviews, which might be more helpful unless you need specific reviews. has used amazon data beforethere are!i know there is an old recommender dataset  that would be fantastic as well! you can download a large sample of previously scraped reviews here they come as json … i can also open up my aws db to you is you want to query reviews by asin thanks for the offer !  i think it could be a good experience understanding how to work with json if i hit too many road bumps then i would definitely take you up on your offer!ok, i’ll send you some code to help extracting the data from the json file  thanks! has also used amazon data beforei used this data source: has anyone exported a data frame to excel with hyperlinks?hi all.  i’m trying to use kubernetes to deploy a cluster on google container engine.  does anyone have any experience with that.  i’ve worked through much of the documentation and hello world examples, and am still having a hard time making all the connections and getting things to actually work.\\n\\nthanks!: have you tried  ?president obama was interviewed by google (2007) and he knew about bubble sort :simple_smile: hi fellows, anyone had experience working with flask, pickle, and heroku?  two questions. 1. i have a training model (~40mb)  need to load to make predictions per user request, except host on aws s3, any other way to host this train model file? 2. loading the model take around 6s, any other way i can improve the efficiency? for a production api, 6s is too much for me. you may want to try something like predictionio for automated model hosting. there\\'s some stuff here about using it w/ heroku i haven\\'t used myself but have heard good thingsthank you  use a hosted service or write your own separate model service.  the challenge with using flask to serve models is that requests are meant to be stateless.  that means that unless you do something really tricky you\\'ll have to load the model into memory for every request.  with a dedicated service you have more control about when and how to manage the model in memory. , any example of hosted service?  i am a little bit clueless. fyi, my current api is running, the working flow now is user send request, i load model, using model make prediction, return the prediction. and you are right, now i had to load model per user request. and  with flask you can load any variable as a global one and reuse it - no need to reload for each request.\\nthe view functions will be called per request, but other objects can be shared…\\ni’m not sure why you were hosting it on s3 to begin with, why not host it on the local machine that flask is running from?: i think it\\'s more complicated than that.  see the first answer to this question for a really good explanation if one has more than one instance of the serving process running, mutable shared variables are not a good way to go. but having a static variable (the model doesn’t change, does it?) is fine. db connection objects, for example, are such shared objects for multi-process flask applications., thanks for the info. i will try the global variable way. my model does not change. you may want to look into gevent: the following explains how gevent can be used to import read-only data structures into global scope, which can be shared across all requests:\\ngood point  i think that might make the case for using spark or more of a programmatic approach. i think  has some xml files he wants to load into redshift and the easiest thing is to turn it into csv filesah! it only runs on stata. it is probably too much of an investment in time to get everything setup to run the scripts right now. but i completely agree with you   that with a few tweaks the approach is very exciting for gender prediction by name and place of residence. also, race prediction refers to the typical american ethnic backgrounds, while i am looking to a diverse international community. i am now looking into ethnea hi all, i am trying to find a clustering technique (in python) than can work with very large data set (about one million samples, 10-20 features), small 3-5 number of possibly uneven clusters. any help would be appreciated. \\ni tried dbscan and aghromerative clustering, but i get memory problem, or they are too slow. i don\\'t know if that\\'s a technique problem. we\\'ve used those techniques and others in python on larger data sets just last week. what kind of machine are you running this on?windows pc with 32g ram, intel e5-2620 v4 2.1ghz 2 processors cpuwe\\'re doing it on machines with 64gb ram, but i would think 32gb would be enough with data of that size.is the data super sparse? categorical? you could try pca or downsampling before running the clustering and see if the results are sufficient.i am already using pca to reduce the number of features to half. i\\'ll try downsampling.  thank you alex. my problem is exactly as hoa mentioned. now we find that i can also try to transfer xml to json file and then load josn file to redshift. i am wondering if you know any good python package or spark package for this?is anyone using google cloud dataproc?thanks guys, will check those videos out.i\\'ve done the scatter plot picker thing in a jupyter notebook with plotly and it was very fast and easy!hello everyone - time for the seasonal insight discount to odsc! save 65% on an odsc east pass by registering with code insight. this discount applies to base prices, and is in addition to other promotions on their end. code expires february 23rd. hope to see you there! anyone have adhd or know someone with it working in tech? loved one is struggling with it and trying to find her a mentor/buddydoes anyone have any opinions on  ?or more succinctly the idea that soft skills are  important on teams rather than technical prowessi remember the wapo article.  i’d be curious to know the roles the non-stem background people have.\\n\\ni imagine a mix or technical + humanities is good for software engineers, and that tech companies have a strong need for non technical people that play a super important role in the company’s direction (*cough cough facebook cough cough election*), and can see humanities people being self-taught in code (like 75% of engineers are self-taught in the trade i once read), but the report feels a bit hard to judge w/o knowing what type of roles they were in.people use the word \"humanities\" but i\\'m not exactly clear what that means.  awareness of the situational context.  empathy?or in particular, what does humanities bring to the table? awareness of the cultural context?  and can these skills  be learned without formally studying humanitiesso when those articles were discussed on hackernews (take it with a grain of salt) one point brought up was selection effect: they have already selected people who pass google’s tech screen, which is a high bar, so given that you have that, then it sort of makes sense that also having “soft skills” is useful.that makes sensethat said, i’m biased towards the idea that diverse backgrounds makes sense for teams!i guess they don\\'t screen for soft skills as well then?or just a lower bar so more dynamic rangeprobably harder to measurealso true, consulting companies try to at leasttwitter is trying to, at least when i interviewed theresimilar to technical concepts, yes, but also similar to cs and software engineering, cs grads have a head start, in the same way humanities majors have formal training in this.we try to, but we don’t have a good wayyes, i think it has to do with cultural context. i also think formal training isn\\'t required, but it simplifies the process of gaining exposure to diverse viewpoints.i think it\\'s a little bit backwards, in that places tend to select first for technical skills and second for soft skills. i get why from a \"risk\" perspective, but really, of the two, i think technical skills can be picked up much faster.why do you think this? i taed lots of students who i\\'d never want writing code i\\'d have to maintain, hah.ever ta a complete jerk?you can do code reviews and learn along the way (until your code is accepted), but what is a ‘code review’ for people skills?yeah, much harder to change a personality than your code.i think about technical skills on a \"months\" scale and soft skills on a \"years\" scale.i think it also is an issue of what has been historically valued and therefore what has been measured. we\\'ve come up with metrics for what good code looks like because we think of there being a direct link between that and $; but there hasn\\'t been much incentive to create evaluation metrics for soft skills because the connection between them and $ hasn\\'t been clear. (which in turn might be an issue of where people are looking.......)i personally think that being intentionally kind to others is not really something that is taught in a curriculum in school, but definately a young person can be trained in it via  supportive relationships in their lives.*shrug* my thinking is we\\'re still conditioning on people who already go to top 20 schools and are probably pretty smart. if you\\'re asking how long it takes to teach some random nice person to code well enough to work at google i\\'d say that takes a while.\\n\\nin this case the engineering ability equivalent to \"complete jerk\" would be 60 year old mom who can\\'t use email.that’s well put alan!i also agree with mark somewhat; before teaching i thought “i can teach anyone calculus!“. having tried to do so… i conclude that i can not. :disappointed:\\n\\nyou definitely want to screen for both tech and soft skills, but i would argue we, in general, probably put too much weight on the tech stuff. there needs to be some, but it’s not the only thing.yeah, there\\'s definitely an expectation around technical skill. and because it\\'s quickly learnable, there\\'s also an expectation that an engineer has put in the time to learn that stuff, and it\\'s easily measurable. the thing that amuses me is the idea that there\\'s probably some point of diminishing return on technical skill, where the effort is more effectively spent on improving soft skills. an unsolved problem!bwahahahathat actually looks pretty deliciousour newest blog post is up, this time covering reinforcement learning to play battleship. does anyone have suggestions/advice on a trustworthy moving company (or any to avoid)?  this is for a move from pennsylvania to florida.  did my relo, pretty satisfied especially since i wasn\\'t there to supervise. i think if you do the full-service and let them pack, the items are insured for any damages during transport.thanks so much ! i had a very good experience with moishe’s (). they were very competitive money-wise and did a great job with packing, travel, and keeping me updated along the way. they have a location in philly.i’ve used abf’s pods before. it’s just what it says on the tin. does anyone know if pickle will save the standardscaler as well? right now i\\'m doing pickle.dump(clf,  pkl_file), and then classf = pickle.load(pkl_file), test = classf.predict_proba(newdatafeature), do i need to redo standardization?is the standardscaler in the clf object to begin with? if not, then it\\'s not going to be there, if so then it will.if not, then you can still pickle it, but make the object a tuple and do something like:  \\n    storeme = (clf, scaler)\\n    pickle.dump(storeme, pkl_file)\\n    newclf, newscaler = pickle.load(pkl_file)thanks!hi all! if your team hires undergrads directly into data roles or has undergrad internships, could you message me or share a link to the internship application? \\n\\ni\\'m speaking a an undergrad women in physics conference next weekend and would love to share stories of opportunities at the undergrad level. thanks!and happy new year! :tada:thanks ! uploaded a file: i am trying to upload a powerpoint presentation online that retains all of its animation features, ( tried googledocs, ispring), any other suggestions?  (includes animations with movement paths)workaround: record  a screencast from your animation, upload it to youtube, upload presentation without animation to slideshare, insert youtube video between slides: reach out to  in the 2015b session — he found a way to do it with some animations he had. waiting to hear from you worked out for me. culturecatch up with progress in commercial ai -  shares info on deep learning in industry. anyone have experience with numenta and the hierarchical temporal memory framework? i\\'m impressed by the results i\\'m seeing from my poc but there are few details about the method. that\\'s a great uploada real great upload. i\\'ve talked to a lot of really smart people, and they\\'ve seen great upload, and they tell me that\\'s a real great upload uploaded a file:  and commented: does anyone have any good resources for (hopefully) a really simple problem. i have the shapefiles of the nyc taxi data and want to convert their given coordinates to gps latitude and longitude in decimal format. nyc tlc does not say anything about the initial format and i\\'m having trouble tracking it down. i\\'d like to do this in python, avoiding postgis or anything unnecessarily complicated. please pm me. thanks!has anyone managed to plot roc curves for multiclass situations where the features were text data?anybody have advice for interviewing a candidate who is to be your supervisor?ask if the person likes to give raises?ask if they’ve had any correspondence with russian ambassadors?or more to the point, ask them about something related, and see if the spontaneously deny that they\\'ve been in touch with any russian ambassadors... commented on ’s file : this looks like it might be ny state-plane coordinates, but i\\'m not sure.  not sure if it\\'s helpful, but you can reproject using pyproj. commented on ’s file : thanks so much! i\\'ll look into it. do you know if these things are typically a linear projection, or is that an unsafe assumption?i don\\'t have a good answer, but probably more \"strategy\" level stuff: what do you think is the most important thing for the data team to accomplish in the next 6 months? how should the team measure if they succeed in doing so?definitely ask about their management stylethanks for the replies. i thought of those kinds of questions as well. it\\'s supposed to be a technical interview though. :/reverse a linked list :+1:oh, technical! good luck!interesting. i wonder if it would be useful for you to pose a real-world problem you genuinely don’t know a technical solution to. if they’re supposed to be your advisor, hopefully they can advise you. think of a typical problem you normally encounter at work and see how effortlessly the interviewee solves it.. supervisors need to be able to trouble shoot situations as well as the people he/she oversees. if the supervisor-to-be has problems solving something that comes up normally, it might be a red-flag. :confused:here’s a relevant hbr article about exactly this: that might also help gauge how well (s)he will be able to judge the quality of _your_ work.  i would recommend plotly instead for interactive plots. its python interface is very convenient and can be used offline. can plotly take user input and replot things on the fly? yes. you can find examples of this from my insight project (offline now... but the source code  is still good). in particular, \"limelight/app/templates/parameter_importance.html\" has an example of  a 10-line javascript for a dropdown to get data from flask and update plot. thanks a bunch i\\'ll take a look!forgive me for using , but i believe this is an issue everyone should be aware, and hopefully already is. president trump signed an executive order banning entry for nationals from seven countries, including people with green cards and dual nationals. many of our fellow fellows are affected by this order. they can no longer enter the country for at least 120 days. this means if they leave the country, for any kind of reason, they cannot come back. \\n\\nhere is some more information: \\n\\nplease reach out to anyone you know who has been affected by this, and consider donating to the aclu or other organizations.\\n\\npolitical rant over.there is a protest happening at sfo this afternoon &amp; evening. i’m headed there now! write your senators and congressmen, too! have a look at matplotlibd3 as wellyou know it\\'s just going to get worse from here, right? these 7 countries are just the beginning. soon enough it\\'s going to cover all muslim countries, or worsecall those elected officials too! both local and d.c. offices this amounts to religious discrimination.   aclu has already sued.   best guide to political action from former washington staffers:   nuggetbot has no chill with how politically driven the supreme court is at time, i wouldn’t hold my breath unfortunately.hi everyone! i really appreciate if you would please call/write your congressmen or senators to stop this unamerican and unfair eo. the visa status of thousands of people currently on student visas, opt, work visas, and even those who have green cards depends on this! i’m one of them..: it ties up their resources to defend it.     is about how to influence your members of congress.  fortunately california is generally leading the way but feinstein, harris and eshoo need to know we are behind them.   senator feinstein:  senator harris:  representative eshoo:   just to piggy back off what  said: one of the guys who wrote the indivisible guide said congressional offices love it when their constituents call them to thank them.also call your state representatives as well!if voicemail is full and it frequently is you can send messages on their websites:  kamala harris diane feinstein:  anna eshoo:  : good luck to everyone from muslim countriesthank you!my friend and president of sbu’s grad student organization is being detained at jfk wow that\\'s shittyaclu just tweeted an emergency stay was granted i\\'m assuming in the immigration banremember to stay vigilant! keep track of whether the government actually honors the ruling.they still haven\\'t been allowed to enterjudge blocks part of trump’s immigration order  thanks for raising awareness about this issue. this is directly affecting 4 fellows at the current nyc insight data science program.everyone gets embeddings! an insight ai fellow wrote a post about how to embed any sort of data. let us know what you think:\\nwell, on the plus side you have to log out of your computer firsthas anyone played with facebook\\'s prophet library much? looking for any thoughts around how much data cleaning / quality checks / model validation visualizations come out-of-the-box vs extra coding to implement them welli only explored it briefly, but decent evaluation of seasonal components out-of-box plus well-reasoned prediction visualization. for data preprocessing, nothing that i saw (and fbprophet was a little fussy with the input dataframe).  has more experience with it than me.i think he means you show the create table on each table and you add comments about what each column means to people.basically yes. for example if the code that generates your table is in github, point people to that file as the data dictionary  ah got it. thanks!ask them how to measure something related to your company that\\'s hard to measure.hello everyone! does anyone have experience performing data integration with cloudera?i tend to like open ended questions that force candidates to stretch their thinking. i kind of don\\'t care how many python libraries they can bend to their will at this moment--that\\'s totally googleable. it\\'s way more important that they are creative and curious. \\n\\nour code challenge questions tend to be things like, \"tell us everything you can about what\\'s going on in this video. you have two days.\" a scaled down version of something like that is  usually what i shoot for in an interview. i really like the \"how to measure something...\" question.sure, going to go down the measure sectionthanksnobody let me do this, but i wanted to have our code challenge be, \"order these images of cows by descending cuteness. explain your cuteness metric and show how it is calculated.\"hahathat is awesome . i’m definitely going to use that it\\'s near the end of our fiscal year and we have leftover budget for tech, training, conferences, etc. who has suggestions on the conference to go to, the new tool to purchase, the course to attend, etc?are you an insight data engineering alumni who will be in the bay area/palo alto next week? insight is holding a reunion gathering for all de alumni at its palo alto office. let me or someone at insight know if you are interested in attending.‘what is the probability of a platypus?’jsm? it isn’t until july but you could pre-pay the registration/hotel/flights. bonus: it’s in vancouver this year!would there be one for ds alumni? i’m in palo alto, but did remote session, so i am not part of sv alumns (yet?).does anyone have an opinion on what\\'s the best linux-like command line install on windows, i\\'m looking at babun, has anyone used it? like? don\\'t like?windows 10 actually has bash. have you tried that?does it already? i thought it was yet to come.. havent tried it yet, im running windows 7you could install cygwin, but i was never a fan of the earlier versions. maybe it\\'s better?in my experience, it’s not.or just learn to live with powershellgasp!babun is built on cygwin but it\\'s a customized version with a lot of the useful tools pre packaged, &amp; with decent online reviews. i\\'ll try it for now..good luckcash looks interestinghope you like javascript!good gravy… i’d rather dual boot.hehor better yet, single boot.. in linux.yea me too, but not my call.. i feel your pain. i think i\\'ll be running windows too when i startmaybe i can run linux, and virtualize windows for an occasional appi would have a virtual machine running ubuntu within windows and that worked pretty well for me...i was using cygwin at work.  i would have preferred to virtualize linux but some places will lock you out from using hardware acceleration to virtualize.  if you find yourself in a restrictive environment, it\\'s probably best to ask the person sitting next to you what they\\'ve triedthanks. my co-workers have varying solutions, one uses git bash, which seems like a more limited alternative to cygwin or babun.. i\\'ll look into vm\\'s as well, it might be an optionbash for non preview versions of windows 10 will come in august. though this may interest many of you: , data challenge incoming?  group 4 is just here staring at each other :slightly_smiling_face:got it thanks!wrong team… sorry for the noise, anyone reading this…curious what you guys are doingfirst data challenge of insight!  i can forward you the email if you’d like?the answer is 4.haha, and they’re giving us 5 hours to figure that out! by all means!: are you going to get into it?found this on product hunt today, a way to “generate realistic data for your app”\\nhow progressive uses machine learning:   nuggetbot check on kenny_kwankenny_kwan has a happiness average of 0.5287356321839081that sounds about right :neutral_face:nuggetbot check on romanoromano has a happiness average of -0.36what?nuggetbot what?stupid nuggetbotnuggetbot check on mhoovermhoover has a happiness average of 1.8148148148148149: nice!nuggetbot check on kumbhanikumbhani has a happiness average of 0.8666666666666667so irrational:stuck_out_tongue: nuggetbot check on lenkatilkalenkatilka has no happiness average yethahaha nuggetbot check on pushkarinipushkarini has a happiness average of 1.1578947368421053all because of your support : i guess i am not slacking enough :smile: (pun intended!)thanks for all the previous help, but i have a new issue. my website has an embedded folium map which is rendered from an html. i see that after about five minutes that my app starts to reload old maps instead of rendering new maps, as if somehow the html is being cached. this is absolutely not something that i have implemented (and it is not something that happens on my local machine). the application is running on port 80 using gunicorn with supervisord on aws. i see that if i stop supervisord and kill all gunicorns and then restart it then it fixes this issue, but stopping and starting the application every five minutes is probably not viable. any suggestions?:  not knowing much about your app, my first line of debugging would be to explore the network tab on your browser’s developer tools. you can see the parameters that are passed with each request, as well as what is returned. see if anything looks different between your aws and local versions.no issue with the parameters that are passed. the address bar looks fine. it just loads the wrong map (seemingly at random from whatever was entered in the first 5-10 minutes that supervisor was running)also in the first 5-10 minutes it runs finereturns the right address bar parameters and the correct mapwhat i’d be most interested in are the http status codes of the responses (i.e. if you’re getting 304 wrongly). that’s the sort of thing that could cause weird caching bugs, and which might differ from one server to another.though on second thought, it’s probably getter to look at logs on the server side first. i just don’t remember how gunicorn logging works.i just refreshed ityou can play with itstay positive mamdayi’m taking a look…if nothing else succeeds, you can always set up a cron job to restart gunicorn every 5 minutesyeah, but having the web site constantly restarting and potentially throwing errors all the time could also be bad. better than having it return nonsense, but still not great.ah, for some reason i was thinking your app was refreshing the map via ajax. now that i see it isn’t, the network panel is certainly not going to be too enlightening.yeah. it\\'s foliumwell, now i just get \"this site can’t be reached.” i guess i broke it.hmmah. i think maybe that was the cron job. thought i turned that off...clearly still debugging that option...odd. it seems to cycle through stale results when i refresh.to me it seems like something is caching the map (for which the name does not change. it is always osm.html)i thought about assigning a large random number as part of the name of the map to see if it might fix itstring interpolate the datetime to the second?so i’m not that familiar with folium, but looking at the html source, i notice that each map has a unique identifier (something like map_d15934a15f6445b6b30a50e24bc94e20), but when you get the stale map, you’re also getting a stale identifier.yeah. and when the user clicks, it produces the correct map in the directory where i expect on the remote instance and it has the correct identifierbut the page does not load itand the map it is loading does not exist on the remote instancei feel there has to be some other storage that either supervisor, gunicorn or aws usessoumyadsanyal: i think i do not understand your advice?i still don’t really understand folium, but i would be inclined to vary the name of the map depending on the parameters, i.e. instead of  `map.save(‘osm.html’)` something like `map.save(‘osm-[hash]’)`, where `[hash]` depends on the address, the date, and the cost entered by the user.yeah. i think you are probably right. thanks :slightly_smiling_face:anyone know why a column declared as \\'datetime\\' aws redshift is displayed as datetime structure instead of a string ? for ex, (datetime.datetime(2016, 6, 30, 0, 19, 3), 94, 1541, 12382, 6770, 9) instead of displaying time as \"2016-6-30 0:19:3\"found a sweet little book on becoming a quant in the hackernews comments. the brain teasers are solid. :thumbsup:that is a good point. no one here is going to have preferred stock.side-note: i didn’t know sfers called it “the city”. has that always been the case or is that as a result of lots of east-coast transplants? it was already the case 15 years ago when i first moved there (and it annoyed me then)…my guess is it\\'s been that way for a while.good to know :wink:in an effort to improve data flow across the company, we are trying to integrate our testing procedures during manufacture with our general inventory tracking. does anyone have any suggestions of off the shelf software that would work for this? have you build your own systems? at this point anything is better than the scattered spreadsheets we now use, but i’d like to be systematic about it.hi all!\\n\\na friend of mine at a security analytics startup () is looking for a senior data scientist. you would be their second data science hire, and would be involved in growing the team and building the heart of their product from the ground up. i consulted for this company for my insight project. they’re tackling some very interesting challenges and have a great team.\\n\\nlink to job posting: \\n\\nif you’re interested, please apply and let me know! i’ll help to make sure your application gets noticed. i’m sure there are a lot of enterprise-level (i.e. expensive) solutions to this problem, but i’d think it breaks down more to the data input.a hack i’ve seen used several times is standardizing a google sheet as input and pulling in data from there (there are python libraries for that). but you could also build custom apps for data input that output csv or json or something else that can be more easily shoved into a data warehouse (i.e. redshift) thanks. that’s what we’re leaning towards for the testing. the inventory tracking is a pain, though!totally random question, but is there a chem or biochem person out there that could help me with a quick question? if so, pm me. thanks!many thanks to the awesomely talented  for help. :clap::+1: i know some biochem and if your question is too advanced i can also refer you to some chemiststhanks  -- i\\'m all set at this point, but appreciate the shout.any recovering biochemists want to do some enzyme kinetics assays just for old time\\'s sake? makes some great diagrams in this post on time-series modeling. good read, too. anybody going to database camp (free whole day db event) in the un on july 10? database.campthis weekend in san jose: hi insight alumni,\\n\\ni’m looking for a panelist for the data science panel at the beyond academia conference at uc berkeley.\\n\\nthe conference attendees are all graduate students interested in transitioning to non-academic careers. your job would be to speak about your experience transitioning into your career. joel swenson, the insight health data science lead at the san francisco office will help moderate the conversation with five other data scientists.\\n\\nplease send me a pm on slack if you’re interested in being a panelist. happy to answer any questions. thanks!\\n\\nmore info: \\nwhen: march 2nd, 2018\\nwhere: uc berkeley campus\\ntime: 1:30 - 3:00pm with short reception to followdear insight alums,\\ni\\'m currently in the jan/feb remote program, and i\\'m interested in connecting with people who have gone the leadership/management/product manager route. if you, or if you know other phds who have ended in up in that type of position, please let me know, i\\'d really love a chance to connect and hear about your experiences going that direction! thanks!!hey ml folks, i have a terminology question. say i have a logistic regression model `f: x -&gt; [0,1]` and a sample of labelled instances `{(x_i in x, y_i in {0,1})}`. so the number of positives in the sample is the sum of the `y_i`, whereas the expected number of positives in the sample according to the model is the sum of the `f(x_i)`. what do you call the ratio `sum(f(x_i))/sum(y_i)`? (does it have a standard name?)sounds like it’s the inverse of ppvor inverse of precision, equivalentlyi would call it the inverse of observed over expected?or are you leaving the outputs of `f(x_i)` as the continuous value while `y_i` are left as binary?yeah, i’m leaving the `f(x_i)` as is — not assigning binary labels ooh, ok scratch what i’d said — inverse ppv would be if `f(x_i)` is then mapped to a binary. i would too, i was just hoping there was a standard shorter term for it :slightly_smiling_face:yet another job alert (for senior candidates): a friend of mine is looking to hire a phd data scientist with at least a couple years of industry experience at the intersection of publishing and healthcare with experience in nlp and sufficient experience, and interest, in leading a team (i.e., developing a vision for the team, providing guidance to more junior team members). based in either new york city, philadelphia, or even amsterdam (nl).\\n\\ni cannot disclose the company (rulz!, but you can guess based on area + location), i think they are doing really interesting work and are, as a company, pivoting towards a \"platform company\" (distribution) driven by machine learning algorithms (e.g., recommendation systems, lots and lots of nlp work). pm me if interested! i would probably call it accuracy ( # correctly classified instances / # relevant instances, e.g.,  1s ). however, i would not use this measure since due to the fitting of the logistic regression, one would have the sum of f(x_i) to be as close as possible to sum(y_i), without regard to how well the classification performed.i don\\'t think that\\'s quite right, if i\\'m reading the parentheses correctly -- it\\'s the ratio of the number of predicted positives to the number of actual positives, without regard to whether they were correctly predicted or not.y = [1,1,1,0,0,0], f(x) = [0,0,0,1,1,1], that metric does poorly on this, no?# correctly classified instances = tp + tn# relevant instances = pf(x_i) is a probability, so this is before the threshold is applied. so one can’t really talk about “predicted positives”, i think.^ right. i’m thinking of `sum(f(x_i))` as the “expected number” of positives (in the sense of a probability distribution) rather than the number of “expected positives”and yeah there are definitely cases where this metric could be misleadinglooking at the example given in the wikipedia page (), one sees that `sum(y_i)=10`, while `sum(f(x_i))=9.9997`\\nas you can see, this metric will give a ratio very close to 1 because the model is fitted to do that.i think in practice, `sum(f(x_i))` and `sum(y_i)` will be very close to one another on the training set. this is because the intercept term will adjust so that the base probability of the classifier will match the base probability in the training data. on the test set, i suppose the two could diverge, but i think there are other metrics that would be much better.echoing , here’s a simple sklearn example:\\n\\n```\\nfrom sklearn import datasets\\nfrom sklearn.linear_model import logisticregression\\n\\n# create data\\nx, y = datasets.make_classification(n_samples=10000, n_features=20,\\n                                    n_informative=2, n_redundant=10,\\n                                    random_state=42, weights=[0.25, 0.75])\\n\\n# fit on entire dataset\\nclf = logisticregression(c=1)\\nclf.fit(x, y)\\n\\n# metrics\\nprint(‘sum of probabilities’, clf.predict_proba(x)[:, 1].sum())\\nprint(‘sum of observations’, y.sum())\\n```\\n\\ni get very similar results regardless of the weights (the base probability)also perhaps worth noting that as `c`goes down (the regularization strength increases), the two actually do divergedown or up? i’m probably confused`c` is the inverse of the regularization strength. from the docs ```inverse of regularization strength; must be a positive float. like in support vector machines, smaller values specify stronger regularization.```so it’s `loss function + (param vector norm)/c` ?no c multiplies the loss functionoh i see confused by how to interpret \\'strength\\'has anyone run a jupyter notebook on google compute engine?anyone familiar with some rules for choosing #bin /width for visualizing big data? looking at freedman diaconis i\\'ve liked:  for bin sizesthanks!here\\'s an excellent blog post detailing it (and giving a nice example of dynamic programming as well): i\\'m an \"alumna\" now! whoo!🤣hello everyone! my team at athenahealth in boston is hiring for a new senior product analytics position! if you\\'re interested in healthcare, want to work for a fun company, and do a wide range of analytics, please dm me for more info!\\n\\nthank you everyone for the awesome advice! :slightly_smiling_face:hello everyone,\\n\\n\\n\\nmy name is hyesoo and i\\'m a recent insight health fellow who received a phd from uc berkeley. at uc berkeley, i also worked as one of the organizers/officers at gdso (graduate data science organization) and they are currently looking for data scientists who would volunteer for mentoring in the data science workshop this summer (previously known as cdips workshop).\\n\\nit is a great chance to meet other data scientists, and also help the future data scientists gaining more experience. you can contact gdso () if you are interested in mentoring the phd / postdocs who are interested in data science.\\n\\nbelow is the email they sent to the previous mentors:\\n\\n\\n\\nthe graduate data science organization (gdso) at uc berkeley has taken over the planning of the annual data science workshop, previously organized by cdips. we\\'re preparing for the sixth annual workshop, and we are seeking mentors. we\\'re reaching out to you because you served as a mentor for last year\\'s workshop and hope that you may be interested in serving as a mentor once again.\\n\\n\\n\\n\\n\\n\\n\\nas a reminder, the workshop is a three-week opportunity for berkeley graduate students and postdocs to work on data science projects under the guidance of a working data scientist. mentors can take on varying levels of commitment. at a minimum, mentors should check in with their team twice per week (either virtually or face-to-face). the only fixed commitment on the part of the mentors is attendance at the opening and closing ceremonies, which will be held on july 21 and august 11 at uc berkeley.\\nif you\\'d like to serve as a mentor, please contact the workshop organizers at . more information about the workshop and the gdso can be found at .\\n\\nbest,\\n\\n\\n\\nthe gdso data science workshop organizershas anyone had problems using spark-submit w/ the --packages flag on emr? i expected it to \"just work\". it downloads the jars correctly, but they don\\'t seem to be on my classpathone thing where i’ve seen rnns beat cnn/bow is sentiment analysis, because syntactic order is so important to understand the sentiment polarity of a sentence.does anyone else out there work for a b2b data-as-a-service company? i’d love to connect and hear an outside perspective on the tradeoffs between model accuracy and data continuity, and how other companies have dealt with it.that’s awesome, thanks !has anyone systematically gone through and downloaded bureau of labor statistics data by metro area?glad i could helpglad i could help.\\nrespiratory issues tend to have low numbers so you either have to sacrifice geographical precision or temporal precision to get any significant values. in my case i looked at the yearly numbers since i was only interested in the long-term effects within a geographical location.\\na good idea could be to study the opioid crisis. there has been some projects around this subject that you could use as inspiration.“all of this advice can be implemented starting now, so that your company doesn’t miss out on the many benefits that come with a diverse environment where everyone — from underrepresented minorities to introverts to parents — feels truly included.”\\nhear hear! i turned it off ages ago, taking control of my own happiness! :wink:hi all, does anyone here have any experience implementing teacher forcing for lstm in pytorch?hi , i have. what’s up?yeah i saw these thingsits some real passive-aggressive bullshitstay positive mpanciathey did it and then they are apologizing for it. kinda seems stupidstay positive ashrithnuggetbot calmdownevery dollar they spend on their bs ad campaigns is a dollar they could be donating to public housing projects or somethingyep, agreed.for all those who got jipped by coin the first time. here is some good -&gt; thanks are in order?talking about jipped. for all those who never got misfit shine/flash to work -&gt; i would have loved to sit in on the design meetings for those airbnb ads: \"we should put up some ads to make people aware of what we contribute back to the city!\" \"great idea, and they should be passive aggressive, sf hipsters love that!\" \"perfect! you really get how the kids think these days!\" :wink:dear airbnb, \\n\\nwelcome to society.\\n\\nlove,\\nsocietyis anyone else going to the mit sloan sports analytics conference in boston this weekend?  i’m going and would love to meet up with any other insight fellows there.how buffer() evolved their pipeline: a blog post about predicting caltrain wait-times that i\\'m working on at svds : so cool, though i am partially biased because my insight project was about trying to predict delays (really ‘bunching’ as everybody in my session has heard 1000 times) in the muni systemthe point on sensor placement near burlingame is interesting; at first i thought it had to do with the bridge replacement in that area...`bunching`bunching as a servicebaaswe need not-bunching as a service:  was that attempt (muni only): yan will be there!does anyone have recommendations for good time series forecasting resources (i.e., books or presentations)?: time_series_analysis__with_applications_in_r-springer 2008 free pdf download.  i\\'m working through it now.other free pdf downloads:  time_series_analysis_and_its_applications_with_r_examplestime_series_analysis_with_rthanks don\\'t mention it.  :simple_smile:free one day training workshop on neo4j and graph databases:  i never knew multithreading and multiprocessing in python could be so easy:  now you knowif anyone has an opinion about fisa see if you can contribute : are you thinking of going to the geoweb summit?  if we get one more, tix are even cheaper :slightly_smiling_face:: sadly, i won’t be able to go this afternoon. but i understand why you’re interested. :wink::  perhaps you’d like to go with  ?: looks great but it’s a bit too short notice for me. enjoy!has anyone had weird pip installation issues on a mac where packages are installed in /library/python/2.7 instead of /usr/local/bin ? how can i fix this? thanks!i\\'m wondering if it\\'s related to homebrew path issues but i\\'m not sure: i haven\\'t come across this problem, but i\\'d recommend the anaconda distribution -- it plays nice w/ homebrew in my experience: you might want to check your ```$pythonpath``` and this might be one of the ways you could install things in local ```pip install —target=/usr/local/bin package_name```: just to add to the chorus, it sounds like you didn’t install python via homebrew (or your homebrew installation is busted, likely with a missing path change)even with another package manager like anaconda, you could run into similar issues, so it’s always good to check that you’re using the correct python/pip that you intend to. ``` ls -lh `which python` ``` and ``` ls -lh `which pip` ``` is a good diagnostic to check your setup is correcthad a bit of fun with tensor flow trying to machine learn what neuroscience sounds like. long story short, here\\'s a dnn neuroscience title generator :smile: thanks everyone! i will look into all these options! i\\'m pretty sure i had no clue what i was doing when i installed python/homebrew: \"amplingle-dependent define sexually behavior.\": \"experience of sexually bindyrin intraoxytics modulation of the action enhances taste inflammation.\"another favorite: \"motility and implicit information enhances orbitofrontal ischemic sex.\"could press refresh on that site all day. \"conditional nucleus in the cat/choice.\" is my favorite so farnicely done!does any1 know max download speed for ec2 downloading stuff from s3 in the same region?    only has upload :xit depends on your instance type and the networking mode enableddefinitely interested, thanks for adding me. work downtown. commute to bki work in union square and live in downtown brooklynawesome! i’m chasing several issues at work today so will be some latency in responding... but what do you think of meeting at insight for monthly meetings? \\n\\nseems like it would split the difference between downtown  / midtown and have reasonable office space to host us.\\n\\nif any other ideas (or office space you know of that we can borrow) please share!so the channel name is ml-meetup-cafeel free to joinwho’s going to be at mlconf tomorrow?just yesterday my boss recommended me a tiny book by paul d. allison ‘missing data’ (with some pessimistic view on the field in general :smiley: )\\nmust admit i haven’t started reading it yethi all, i was recently laid off at time inc. if any of your organizations are hiring data scientists in the greater ny-nj (or otherwise) area please ping me. i appreciate it.hey everyone, we’re excited to announce the launch of our new website! check it out!   hi fellows! i would like to do time series modeling using many variables (features),  anyone knows about good tutorial/book or links to do that using python?\\nthanks: you can change the weights of the edges or remove them as neededthanks everyone, i think the insight collective intelligence has lead me to a method for approaching this.anyone have experience using tpot to automate model selection and hyperparameter optimization? i’m just curious about how well it works., you may have your answer already, but 2 people in my session also did route finding with constraints.  or  might be able to point you to useful tools: this is not helpful, however your problem did remind me about microsoft’s “avoid ghetto” patent () that drummed up some news a while back. damn it, i guess i\\'m headed back to the drawing board.stay positive ddelosshi everyone! i am trying to combine 3 databases on mysql to be one database combines all the tables from the 3 databases, anyone knows how to do that?: you can export 2 of them with mysqldump and then use those files to create the tables and data in the third database: can you simply run: `create table newdb.table1 like olddb.table1; insert into newdb.table1 select * from olddb.table1;`? this is a bit faster than `mysqldump`, but slower than `load data local infile`. this also assumes that both databases are on the same server.\\n\\nif they aren’t, i would certainly try to use `load data local infile` for speed.thanks for getting back to me  and . another simple question, how to access  3 databases at the same time? because what i do, is use database ;  then i access the tablewhen i do that, i can’t access the other tables from the other databases: `use database` should not prevent you from reading/writing to other databases on the same server. you just have to specify db-name.tbl-name when accessing tables in a different database.ok.. that make sense :slightly_smiling_face:thanks  !hi all, i’m working with 3d accelerometer data to categorize types of motions from a wearable. i’ve got a lot of avenues i’m considering, but i don’t know which ones are worthwhile. does anyone have experience with this sort of problem? thanks!: that sounds a lot like ’s project from the january 2016 session.cool! i’ll look into it. thanks  !: here is her blog post about the project: oh this is perfect! \"engineering features from accelerometer data\"would anyone have any knowledge on how to go about funding a non-profit project ? would love to buy you coffee and have a chat :slightly_smiling_face:numpy/scipy tips for matlab users cheatsheet: insight health data science - boston 2015 session: just saw a billboard from google:\\n\"now needs data scientists\\nnext just needs you\"\\nuh oh...incoming jobs where we fix all the mistakes people make by using bad data.e.g. a company that goes and corrects everything on 538wow. i might actually find a use for a windows machine. :simple_smile:hrmshi there! does anyone have a good practical tutorial of using hive and pyspark? i will have an interview next week including working with a data set using hive and pyspark, which i’ve never used before. thank you!i don\\'t know of a good tutorial (tell me if you find one) but w/ spark stuff, i always start with the examples dir they have on their github.\\n\\nif you haven\\'t used pyspark much, this might be helpful. this pretty accurately summarizes me trying to refactor code when i haven’t thought it through….repartition(10000)i highly recommend checking out the community edition from databricks.  its free and they let you work with spark on a baby cluster they launch for you and there are tons of tutorials built in. was useful for me when i was learning pyspark ()someone cracked the code: oh no, i am getting 404 error when i click! :disappointed:that didn\\'t last long... here\\'s a google cache: stakeholders is my favorite! :octagonal_sign:thank you!   user research: we figured out a way to finally get paid for our ph.ds.does anyone have any recommendations for great data science/machine learning conferences?kdd, nips?odsc, pydata, strata data conferenceanyone have experience with using dbscan for outlier detection? i\\'m digging the netflix tech blog post from july 2015 but would love to hear about any real-world application    ^i think   worked with dbscanyes!! thanks   :smiley_cat: are you interested?late to the party, but this link is awesome for a/b testing: is anyone familiar with d3.js? i have a working scatter plot, and am literally trying to add a line using canvas, but it frustratingly doesn\\'t display.for my insight project, i\\'m using hdbscan (a hierarchical dbscan) to do outlier detection on medicare insurance claims -- www.fraudhacker.sitealso happy to answer any questions about either my specific application or the clustering method!does anyone use airflow for “applying” their ml models? i am structuring the process as load data, cleaning/transformation, loading trained model, predicting, outputting in a single .py file. i am wondering if there is a way to within airflow assign each chunk of the code as a task, without having to break each of these processes into it’s own individual .py file. not totally sure i understand, but yes, you should have different files to process different tasksis the discount restricted only to former insight fellows, or is anyone welcome to use the discount code?anyone can use it. please feel free to share with anyone in your network.thanks!does anyone have a recommendation on a good interactive vi tutorial?\\ntheano will no longer be maintained after the next yearyeah, a little sad`vimtutor` is probably the best interactive way to start with vim. if you want a short intro this should work fine ().\\notherwise, practice is the keymxnet!thanks!for all you fellows starting interviews soon:\\ndoes anyone know of a way to automatically reply to texts with an “away on vacation” type message on iphones? there doesn’t seem to be any option/third party app for this!#insightproject :wink:omg yeah- i guess a data engineering thing. apparently apple doesn’t want to let any third party apps do this for security reasons. wankershey all! i have a marketing funnel i\\'m working with right now to determine consumer purchase intent (classification problem). are there suggestions for models for predicting classes that are in hierarchical structures? thanks!just discovered the most useful function in matplotlib: anyone have experience with unsupervised learning on variable length input?this article is a nice encapsulation of what insight drummed into me when they said projects couldn\\'t be \"data exploration\". the simons institute for the theory of computing is hosting a series of workshops next year on “foundations of data science.” if you are interested, i recommend signing up for the mailing list to stay updated. hi all - has anyone here ever tried using kalman filter for missing data?can someone write a simple slack scraper or bot that has access to  and stores a message history to a database or external webpage or something? i hate when i see useful conversation get deleted over time.usually people ahve to pay for that. but you could probably make one pretty easily with crapyer scrapy: i\\'m trying to mess with the slack api but it\\'s currently \"disabled by admin\" for the insight slack. is that something that\\'s free to enable?cool idea!need to sell movie tickets? maybe you should round your ratings in a really bad way! stay positive alex_gudedoes anyone know of a good implementation or blogpost of “attention” neural networks in keras?have you seen this one? have you seen the \"counting your customers\" paper or pareto /nbd models? there\\'s also a package in python called lifelines that follows that methodology to create a probability that a user is alive given a record of transactions/events over timefb data analysis on the dress hey everyone,\\ni had a great conversation with an excellent nyu physics undergrad, chris ick, who has been doing some interesting stats and ml with prof. david hogg. chris describes himself thusly:\\n```\\ndata science and astrophysics researcher at nyu. recently graduated with a bachelors in science in physics, with extensive programming and technical problem solving background. research interests include computational data analysis, data visualization, and bayesian analysis. currently seeking work and education in data-oriented programs and positions, and furthering my understanding of data science and it\\'s methodologies.\\n```\\n\\ni felt like i couldn’t give him the most relevant advice on the current data-science job market and how best to set himself up for a career going forward. i have been off the market myself and we haven’t been hiring data folks here at greenhouse (yet) so i don’t have a sense of the landscape. would any of you perhaps be willing to follow up with chris ()? pm me for his cv if you’d like.hey  sounds like he could be a good fit for the insight data engineering program. we recently had someone with a very similar profile go through the program and is now working at bcg. has he graduated yet? that’d be the only caveat otherwise, i think he might consider applying for the de program.yep, he graduated last may, . would you mind reaching out to him about de?hi all, my team in google is hiring for a data scientist in mtv. dm me if anyone is interested in the role. hi all,  i am 3 weeks into my career as a machine learning engineer (aka data scientist) and have to put together a wish list for conferences i would like to attend in 2018.  can anyone suggest which conferences i should shoot for?  there is a finite amount of funds, so proximity to nyc is a plus.hi all. can anyone recommend a toolset for analyzing large datasets? so far i\\'ve been using pandas + scikit-learn for all of my ds work, but i recently started analyzing a &gt;4gb dataset that is resulting in \"out of memory\" errors. i was considering giving sparkml on a cluster a try, but i\\'m curious to hear your thoughts and experience. could try dask as a drop-in replacement for the pandas componentsdepending on exactly how big your dataset is, you might be able to get away with spinning up an aws ec2 or google compute engine instance with more memory, and working with your usual toolkit therethanks, will take a look at that!that also sounds like a good idea!yep -- would recommend bigger box, same tools as an intermediate stepit might not get you too far, but pandas has features like sparse data frames that are more memory efficient, too. depending on your dataset you might be able to just use a little workaround like thathi everyone -- current fellow doing nlp on a large dataset (200k entries) with a small subset (1k) of multi-class, multi-labeled ‘ground truth’ annotations. has anyone done semi-supervised learning on multilabel data? finding a fair amount of research articles but not a lot of python implementations.some great tips here tl;dr: downcast numeric types to save memory, avoid string-type columnsi haven’t used it for text data, but scikit-learn has some semi-supervised methods. you could pass your text data through `countvectorizer` or tf-idf as input into one of these models?i’m a fellow in this session, and i used tfidf as input for a naive bayes classifier to predict sentence classes (dog, shelter, neither).thanks -- i\\'ve been using countvectorizer to construct the feature matrix but the specific issue i\\'m having is that my classes are multilabel and thus don\\'t fit into a standard column vector for \\'y\\'scikit-multilearn has some implementations for supervised learning on multilabel data but i\\'m not seeing semi-supervised approaches in that packageoh, i misunderstood. for that i’ve never tried semi-supervised methods. i’ve only ever done what has multi-label support or neural networksyeah, i\\'ve investigated some supervised classification algorithms with multilabel support but training on 1k samples and extrapolating to 200k seems like i\\'m leaving some information behind. i may try an iterative classification approachdoes anyone have any advice (or things to avoid) for choosing a data warehouse?i also created a resource dealing with nlp classification in general that could be helpful: advice 1: don\\'t tell the engineers they have to build a data warehousethanks, i\\'ll take a lookhi all. currently for my project i\\'m trying to merge aggregate level data (percentile numbers per occupation/county) with lots of missing with individual level data (actual values per person, with occupation and lat/long) that is very sparse on the county/occupation level.  trying to form a complete pictures with very \\'holey\\' data so to speak. i\\'m doing so with imputation and then training a random forest, does anyone have an idea for an approach (that would be reasonable to implement within the timeframe).hi all! anyone have experience using plotly or folium for bubble maps? do you recommend using one package over the other? or some other package? relatedly, i am using geopy to calculate lat and long for us and international cities and it is very slow. i can run up to about 200 rows, when it is larger it times out before finishing. any insight into why it is so slow? thanks in advance for any input!: i’m working on a multilabel classification problem as well. what i ended up doing is using `multilabelbinarizer()` for transforming labels to binary values and then used excellent  ’s resource.sparkbeyond is hiring data scientists. i think they have remote positions for their singapore office. some thoughts for interviews: : we use tableau at glassdoor, moving all our internal reporting there. i like it sometimes, hate it other times.\\n\\nwith regard to connecting to data in many forms i find it works very well.  it\\'s also easy to make some nice visualizations, especially if it is simple and you don\\'t have a specific vision. but some specific things require too much hackery. aspects i don\\'t like which are mostly relevant to large projects:\\n1) no way to document the data\\n2) no solid revision history like you have with scripts in revision control -- i\\'ve made changes that broke sheets that i wasn\\'t looking at, didn\\'t realize they were broken until later, and found it very hard to tell how far back i need to undo even if i know exactly what you did.\\n3) so much clicking and dragging and it is trying to update every time you make a change. a colleague was excited about looker but i haven\\'t used it with any seriousness.thanks, alan! \\nmy tableau experience sounds similar to yours. if it is straight-forward query-to-chart it is relatively easy and quick, otherwise it gets murky fast.you can turn off the auto-refresh if you are building up some complicated queries and viewsyeah, i should do that more oftenit is easy to forget to do it until it is too lateis there an easy way to convert unix epoch time stamps into regular date timestamps in redshift postgres?\\n\\ni\\'ve found:\\ndateadd(s,somedatetime, \\'1970-01-01\\') as x\\n\\nand also some other worse ways to do it.\\n\\nto_timestamp() does not seem to work in redshift...\\n\\nthe regular, ugly, solution seems to be\\nconvert_timezone(\\'us/pacific\\', (timestamp \\'epoch\\' + somedatetime * interval \\'1 second \\'))::datetimecoconut is clearly the worst, but berry at #8! clearly their methodology is wrong and bad!stay positive alex_gudenice to know that cherry lime made the top 5: coconut #20 - guess everyone agreesweird, since coconut always runs out the fastest at my officecoconut is literally the worst.stay positive till_bereally need to rework nuggetbot’s trigger for thatstay positive johnrwalkyou’re not my supervisori’m surprised ‘not’ doesn’t set off nuggetbotthat and typing in all capsfor my project i\\'m going to to sentiment analysis on patients reports of medication preference. does anyone have any advice about sentiment analysis? #sentimentanalysis    hey all, meetup is looking for someone to do contract data viz in jan, possibly longer. we’re also growing out our ds, ml, and de teams. reach out if interested.a colleague at dia&amp;co intends to spend 3 months (jan to march, exact dates flexible) out at our la office and is looking to sublet her quiet and sunny one-bedroom apartment in brooklyn (bay ridge, subway r at bay ridge ave and n at 59th) during that period. if you are interested or know anyone who is dm me and i\\'ll connect you!i vouch for that apartment!kind of the opposite of my hep days: dry, yes, but packed with as many figures as possible. no time for context! let the convener and the one other person paying attention in your presentation interpret the plots as needed.haha, love itthanks  — we ended up asking questions quite similar to that, and they were revealing!a former coworker of mine is looking for data science help for his startup, avvir ()\\nthey are working on automating 3d architecture modeling and detecting real life variations from plans using laser scanning drone generated point clouds. sort of a mix of computer vision and anomaly detection.\\nif anyone is interested in hearing more, send me a message please.sounds really boring and not fun at allmy session may or may not have had a team picture maps where everyone used folium :partyparrot:aaaaaand another former coworker is looking for a data scientist:\\nif anyone is interested in the above, send me a message and i\\'ll intro you.score, thanks! most things i was finding for audio were music-focused, so this is great to find!: hey y\\'all!   another plea from me to sign up for alumni mentoring.  we\\'re in mock interview stage starting today.\\nnyc needs a ton of support as we don\\'t have a large alumni base like sv.  \\ntake a look at weeks 5-7: \\n:beer::beer::beer::beer::dart: :smile: imo the pgm course on coursera isn\\'t great. she uses some weird software to visualize/interact with her models.: i like this chapter from bishop for pgm: thanks  !  i completely forgot that bishop has a chapter on them! this solves it.538 put together a really cool app for the elections that lets you play with some of the underlying assumptions of their model and see how it changes the output! this was from a recent meetup in philadelphia and some really cool and impressive work at the command line to reduce work burden. \\nhey, does anyone know any good, publicly-available twitter datasets? i have a friend who wants to poke around with one.: the sentiment140 corpus is available for download here: \\n(1.6 million tweets tagged for sentiment)a friend…. riiiiiiiight+1 for  :slightly_smiling_face:  has a good point though.  i think the issue was that the scope of the conference is broad enough that it doesn’t really know what it wants to be, so the talks are all over the place.  moreover, this means some of the talks have to focus at a vendor/business level rather than being technical, but it seemed people were leery of tagging their talks as “beginner” or “nontechnical” so there were a lot of ostensibly intermediate/technical talks that were super basici didn’t go to this year’s odsc west conference, but the one last year was pretty good. at least it was much better than strata which, when i went two years ago, was a lot like   describes in boston: too general.welcome to all new fellows starting insight today!!!woot woot!any tips on building a collaborative data science tech environment? for the past year, i’ve been the only formal data scientist at the company, so my work has been largely quarantined in my own repo. now that we’re beginning to grow out the team (including remotely), is there any experience/wisdom/tools/best practices you can share to scale out wisely, promote collaboration, and minimize headaches down the road? any advice is appreciated!document your code extensively? *hides*use version control, write automated tests for everything you\\'d check adhoc, use continuous integration. do pair programming and code reviews.enforce style guidelines of some sort, refactor scripts into classes, export code from notebooks into e.g. python packages hosted as github repos, provide readmes, confluence docs, etc.you can run a linter as part of your automated tests to enforce style. but don\\'t try to do all this for all exploratory work necessarily.data dictionaries :slightly_smiling_face:thanks  ... those do look interesting.nice...i think i\\'ll probably be going to the ml conf too!i just started a  channel and re-posted these links there. maybe attendees could post their highlights there eventually too: feel free to start a channel for your session and invite members. private groups are also an option.nuggetbot: weatherweather for new york, ny, usa\\ncurrently: mostly cloudy -1.3°c/30°f\\ntoday: mostly cloudy until this evening.\\ncoming week: light rain on sunday and monday, with temperatures bottoming out at -1°c/30°f on saturday.nuggetbot: weather san franciscoweather for san francisco, ca, usa\\ncurrently: partly cloudy 12.3°c/54°f\\ntoday: partly cloudy until tomorrow morning.\\ncoming week: no precipitation throughout the week, with temperatures rising to 17°c/63°f on monday.nuggetbot: nuggetbotnopewhat exactly were you trying to do there joshainsley ?get meta with nuggetbotthe nuggets have nuggets!we can add functionality to nuggetbot from here: nuggetbot: pug bomb 2i think this is fantasticjonathan_whitmore: hope we all find it useful!nuggetbot: animate me welcome everyonethe is hereit\\'s happening.nuggetbot: image me jake klamkahaha nuggetbot\\'s the manjust arrived in nyc and ate a ton of pizza to celebrate - see you friday, andrew set the channel topic: :chart_with_upwards_trend: btw, thx for the alumni panel visit yesterdaysame to you  it was a lot of funnuggetbot: image me data sciencecould it be any other image?nuggetbot: image me insight data sciencehahanicenuggetbot: image me data engineeringhahaha lolthanks , i had a fun time and i hope it was useful to the current fellows. sorry i had to jet at 3, but i just barely made the train back to work.see you fridayok back in the fold after a loooooong time awayhi folks, is there a way to know in which way the feature is important in random forest? such as positively correlated or negatively correlated to the predicted value? xiwang: when i was looking into random forests, i came across this package that seems to interpret feature contributions for sklearn’s decision trees/random forest models: ^ i second using ‘treeinterpreter’.xgboost has a model.booster().get_fscore() function to look at feature importancei\\'ve used that. the f-score is just the number of times that a feature was split on in the tree; i\\'d consider that one way to measure importance, but possibly incomplete. you might have an extremely important feature that does its whole job by splitting once, for exampleone way would be in crude bootstrapping; assess importance by removing features and then ranking importance by assessing the difference in model scores based on a validation set. that\\'d be super time-intensive, though. probably better ways than that ...i don’t use forests a lot, so i’m not an expert, but my understanding is that the fscore is the number of splits in a tree _weighted by the number of observations going through that node_. so *if* our model splits on more senstive features first, it would capture the case of a very important variable that only requires one split. is that “if” satisfied in xgboost? i’m not surei don\\'t think so? wow~ so many replies. thank you, guys! i\\'ll take a deeper look. and get back to you!so there are variable importances (also known as vimp) and things like “minimal depth” for a variable. they don’t give you direction though, just importance. direction would require further tackling, as there could be multiple split points for the variable, with influence direction changing, i.e. if there is a parabolic relationship between the features and target variable. thank you! this is what i thought because each time the split might happen in different ways.if i remember right, randomforest in r has cool ways of visualizing the “effect” of the variable to see if it is linear, nonlinear, etc.here’s a beaut of a solution using ggplot2 :smiley: \\nthis sounds cool! will definitely try it! thanks!here is a great event about building your personal brand, from some friends at quora: hi guys, i had an aws instance running for three weeks. i could access my web app all that while. then suddenly two days back my app (webpage) wouldn\\'t load. so i killed all processes and reran supervisor. and it started working again! then today, the same thing happened again (page won\\'t load). i log in to aws and see my instance is running (no errors, alarms). so i\\'m not sure what\\'s wrong and what to even google for to resolve this issue. any help information will be appreciated! thanks .i do nmap -p 80 myelasticip and it says the host is up. i guess there is a problem with supervisor then. i have not used gunicorn and i admit i don\\'t know what purpose it serves :confused:stay positive pushkariniif you are using supervisor, you may be able to change the logging level. that said, if you continue to have problems, you might try a more robust server solution, such as apache, or nginx. additionally, if you really just want to keep it up and running, and don\\'t want a lot of extra work, you can try to log into aws and run your server from within a gnu screen session. hope this helps!thanks for such detailed writeup kevin! having a separate environment for each client is a really good idea.it might even be required -- clients will likely want to know that their data is in no way being used as training data in models that could potentially also help competitorshas anyone used secor (by pinterest) to consume messages from kafka and upload them to s3?hey ladies, i registered for the women in data science conference at stanford next friday (feb 3) but now i can\\'t go.  would anyone like my spot?  it was $350 but it will just go to waste if no one takes it so you can have it for free.  dm me!  thanks, , i\\'ll look into this as well!good morning!  does anyone have experience or a good understanding of what to do when you have highly redundant data samples with different target values?   i\\'m working on a consulting project with a pr firm to predict whether a journalist will click on an email heading containing a media pitch.  the data set consists of some emails that are only sent out once or twice while others are sent out more than a hundred times.  any advice is welcome!if you have a lot of redundant data, you may want to look into sampling to reduce the variance, or use a high bias model that ignores/sums some features. additionally, you might want to look into a bayesian graphical model if you want to hand engineer the relationships between actions and outcomesthanks ! those are helpful tips  have you tried jython? or jpype? that way you can at least run everthing in your python app. jython tries to replicate java in python, while jpype also the python vm to to the java vm. you could run a component analysis (pca/ica) on the set of features. you\\'ll lose the interpretability of the feature sets as the new eigenvectors represent combinations of the original feature set. that’s not entirely true, you can still draw on projections of each feature (depending on how many there are) onto the two eigenvectors being plotted and get an idea if clusters in the pca tend to fall towards one particular direction in the original feature space. however, those projections of the original feature space are certainly not orthogonal thanks for the suggestion. i had looked a little bit into py4j, but had not yet given up on finding something available in python. i will definitely look into jython and jpype as well.i\\'m trying to build a foreign language speech classifier for my project, but i\\'m having difficulty finding freely accessible audio data for this task.  if anyone knows of an appropriate dataset, i\\'d really appreciate the tip! thanks!do you need a specific language? a bunch of languages?this might be a decent start regardless: i worked on this project in grad school, which you should be able to request access to: . not a huge number of languages, but has speech samples that are comparable in each languageif you have a friend in a linguistics department, then the ldc catalog has some good stuff. unfortunately, not free hmm--i saw that, licensing those corpora is pretty expensive.  i\\'ll dig in a little bit, maybe there are some subsets that are openly accessible. allstar is actually perfect for  me. i\\'ll request access, thanks.  yeah, ldc catalog would be ideal, as you said, access it sticky.hi guys, does anyone here know how to render a plot in flask?  maybe even do it \"dynamically\" so that the figure does not have to be saved every time? you can try google chart api. i used flask and javascript to feed data into the chart from python. a simpler solution is plotly, but it has limited amount of access for the free version. thanks .  right now i have a simple python function that processes some data and makes a plot, and i just want to pass it back out of the function to an out.html file. just not sure how to save it and pass it back out. here is a tutorial: thanks is also nice for dynamic graphs+1 for bokeh. you can even use it with matplotlib plots.  not sure if this will help but project gutenberg has free audio books  both human read and computer generateddoes anyone have experience creating an interactive map using leaflet in r? i have some questions about creating dynamic buffer zones based on a user\\'s click...(i\\'m a new fellow in boston)does anyone know how i would go about scraping this website? : it depends on what kind of information you want to get. the search form on the product-search tab of the website is simply querying a php script: `&lt;form name=\"form1\" method=\"get\" action=\"/eng/product.php\" onsubmit=\"return checkwholeform(this)”&gt;`. so, if you have a list of products you wanted to search you could just query that php page and save the results.\\n\\na much slower way is to emulate a browser (for example using `splinter` module for python) and again save the results.thanks alex! i\\'ll look into it :slightly_smiling_face:: also, an easier way might be to just email them and ask for their database-dump. scraping might be against their user-agreement policy, might crash their website altogether, or be considered a dos attack. you never know how much traffic their php script is designed to handle.stay positive alex.tombergokay, good callthanks :slightly_smiling_face:i’m thinking of starting to organize a list of data science case studies after having a bit of pain trying to find some. does anyone know of such a list that’s already out there that i missed?stay positive joshainsleynookyesi willi guess kaggle doesn\\'t count? :slightly_smiling_face:sure! but it’s not comprehensive enough. plenty of companies are putting their data science work out there publicly, but there’s nothing out there that makes all of them easy to find.sounds like a good pitch for incoming fellows project? :wink:perfect! get someone else to do the work for me. that’s upper level management thinking right there.cc my god; what fantastic epicness!!anyone have any suggestion on strategies/documenting software to keep track of tables and their schema at their work?i\\'ve used alation () before, and it\\'s pretty slick.we\\'re starting to use aws glue, which crawls your database and s3 buckets to create a data catalog.  but i don\\'t think it solve your documentation issueso i have no real tips for documentation, but i highly recommend something like flyway for versioning schemas programmatically.thanks!has anyone used google cloud ml? may i pick your brain?we define metadata in the pipeline code that generates tables in bigquery, and then we have a tool like  mentions that scrapes all our tables and pulls the schema definitions and any comments or metadata into a centralized web resourceif any one wants some level of automation in standing up their aws take a look at this.\\n\\nthere is an example at the bottom, hope it helps.\\n\\npretty cooli also like  which you can run outside google with  eg on spark or flink.not sure if this is old news or not, but nasa has open sourced their a bunch of their resources, including data, tools, apis, and repositories:\\nnasa open data:\\nreally fun to deduce why each state has that top emoji:hello insight universe, does anyone have experience in biclustering? what is the main difference if you run biclustering on a matrix vs. do hierachical clustering on rows and columns separately and then combine? thanks in advance! uploaded a file:  and commented: we’ve hit 666 :smiling_imp:: correct, although the bug in question was specifically related to cluster correction (initially suggested in worsley et al. i think). had they been using anything else (e.g. bonferonni) or a different approach like mvpa would\\'ve been fine. does reinforce the notion of thoroughly understanding what method (and its implementation) is being used in an analysishi everyone! my company, blizzard entertainment (makers of world of warcraft, starcraft, diablo, hearthstone, and overwatch) is now looking to hire several more data scientists at lead, senior and junior levels. we\\'re located in irvine, ca. if you\\'re interested in more information, feel free to shoot me a message on slack! do you have an office in nyc?if not, would you consider opening one… please?btw, i have recently been using a notetaking software i wanted to share.  a nice alternative to evernote, and it supports markdown. :+1: sharing is caringanyone have the link to the recent draft paper from ds folks at facebook comparing methods of causal inference?nice job at sfiq today people who presented! :thumbsup:hi! i\\'m one of the current ds fellows and would like to set up a way to go around constantly hitting twitter api limits. i know some previous fellows got around this by switching between vpns- if you have some knowledge on this, could i talk to you? or if you know someone who worked on this, let me know. thanks!that sounds interesting , let us know how you go. i like hacky solutions to such limits.  sarah, you had a pretty good solution for api limits in your project no?  if its tweets you need there is a data dump i can help u get -thanks! i’d like to hear more about this data dump. so more specifically- i’m interested in users that follow certain groups (especially political), and would like to get a large number of tweets (200? but could be less) from each of these users. so if the data dump has user ids, who each user follows, as well as some tweets from them, that would be great! also can help you with a massive twitter data dump  -thanks for the suggestions!fellow windows sufferers: what is the best way to export data from ms access to mysql?i had decent luck with navicat when i had an accdb. was on a mac thoughso jealous is your access linked to mysql via pens?no what is pens?i converted to a database/excel format in access and saved it as then read it into python. are you trying to go directly? if you figure it out let me know!!!autocorrectodbcat least that\\'s how i used to do it back when the dinosaurs roamed... via the right odbc driver, you can use access to read/write to almost any sqldb as far as i knowbeen a nightmare trying to install the right odbc driver. after hours of frustration i realized i prob have the click-to-run version (wtf is that?) so driver does not install properly. stay positive jtsitroni dunno. windows 7 or 10?7this helped me... but i dunno. are you admin on that machine?does you windows machine (user/ip) have permission to connect directly to the db?could be a bunch of thingsi do but i think it\\'s the click to rum thing. i\\'ll take a look at your link. thanks!anyone have suggestions for improving performance of mysql queries beyond indexing joins, checking explain for anything fishy, and optimizing tables?can you be more specific about the kind of query you’re looking to optimize?well...they\\'re all select statements on 1-3 tables, many of them involving aggregation and filtering.anyone in the bay area looking for a good data scientist candidate? a work colleague is looking into moving out that way. pls msg me with your contact info and i can hook you up. thanks!anyone go to dataengconf last year? any good? thinking of going this yeari attended last year in nyc, but focused on the “ds” track. found it a bit more applied than some other conferences, e.g. spark summitawesome! thanks kevinwhat about mlconf in march?  anyone know about that?  :q !ah, vim. the easiest way for me to identify other particle physicists:heart:by far the best editor ever conceived.love vim! :smile: gave a 45 minute talk at work about why it\\'s amazing...not many converts still :confused:hear hearit’s hard to appreciate until you figure out how to not destroy all of yoru files and text documents when you’re not in edit mode &lt;-- a great read on \"thinking vim\"is vim is good. my life changed when i discovered `:!ls`also, spend 15 minutes a day here: i was in astronomy for awhile where emacs was king gives you emacs\\' extensibility and plugins with vim\\'s keybindings.question for you all: i have a canonical user-movie rating matrix. however, all users fall into one of five externally labeled classes (abcd or e, say). normally, we\\'d use matrix factorization to find latent patterns in the data, e.g., users who like horror, comedy, etc. however, the goal isn\\'t to predict new ratings, but rather use a new user\\'s ratings to predict which class he/she belongs to. there isn\\'t a 1:1 mapping between classes and latent genres, but rather class a likes some mixture of (latent) horror and comedy, while class b prefers another.\\nthe reason i\\'m not doing straightforward multi-class classification is because the data is already in this matrix form (term-doc, user-rating, etc.). i\\'m wondering if anyone knows an approach that could be useful for this case, e.g., could probabilistic latent semantic analysis be useful here?another analogy would be all documents in a corpus arranged into certain \"chapters.\" the goal is to use some kind of latent analysis not to predict the topic/word distributions, but rather which chapter a new document should be assigned to. do you want to write a quick blog post about this?halfjoking...:  it creeps me out you guys are investing in skin cream :wink:hey fellas. i want to record a training video, like coursera videos. i want to be able to record my screen and then edit / insert clips if any errors are madehow can i do that? what sw (free or on msdn) are available. platform windows 7for my o\\'reilly screencasting course () i used camtasiathey also gave a high quality microphone (headset)i recently saw a new course on data science from google, possibly through a linkedin group, but i\\'m having trouble finding it now. has anyone seen it or have the link available?i found their deep learning course on udacity, but i don\\'t think that\\'s it, i\\'m pretty sure it was from google developers.found it, it\\'s this series of videos: boston health economics () is opening a second office in downtown boston. in their own words, \"we\\'ve developed analytic software for large health care databases which were actively and successfully marketing.\\nwe\\'re looking for people with experience in both front and back end in thus field.\"   jordan menzin, the head of the software group, asked to advertise; he understands that he might be contacted by either graduates or program directors. lmk if you are interested - ill send over his email.is there any way to do a map-only job in spark stream ? it looks like every \"output\" operation on a dstream forces a shuffle. even map-side operations lke reducebykey() or combinebykey() eventually coalesce dstreams. i have an application where i don\\'t need any kind of reduce() operation at all. i just want to execute a function in each partition locally. apparently this is possible in hadoop streaming.: i thought just running .map on the dstream would work: does running forearchpartition after foreachrdd not do what you need?no it doesn\\'t. to clarify my problem, what i need is he ability to do an output function after after map and combine phase in the map-combine-reduce pipeline. but spark doesn\\'t let you do that. so eventhough i could do everything i need to right after the combiner phase i am forced to incur the cost of reducer phase which means data gets moved from the combiner partitions to reducer partition unnecessarily.someone should do an insight project to find the best way to redistribute tech people across the us to save on rent + change swing states from red to bluei have had this thought as well. as a native iowan who moved to sf, i feel the need for the latter issue acutely. the electoral college isn\\'t going anywhere.i didn\\'t frame it this way but...  :wink:hi insight fellows, i have a friend who has an opening for a ds-applied scientist on their team(amazon alexa), feel free to pm me if interestedwelcome new fellows! if you feel like this, it\\'s normal! \\n\\n\\n\\nwe\\'ve all made huge mistakes. :grinning::upside_down_face:a friend has an opening (security focus) at workday \\n\\nif anyone is interested let me know and i’ll put you in touch directlycome to hey  i work at nielsen on 6th at 21st, happy to chatanyone interested in a director-level position for data engineering?\\n what  said applies to me too.hi all, uber is looking to hire a senior data scientist on our crm team. if you\\'re interested do reach out to me :) thank you.what questions did you have?  though not sure i know much about \\'best practices\\'has anyone built a chrome extension before? some of us are working on them, but have been confused on how to get started so far. you did, right?did you find if there was a best way to preprocess images e.g. normalizing the color channels in a 3 ch (or more) satellite image?i’ve done some bare-bones stuff. i don’t know if i’ll be of use but swing by the ai group and we can talk.would any past fellows here have some expertise in building lstms in keras or tensorflow api?  specifically anyone with experience in encoder / decoder architecture with word embeddingsi’ve done some as well, a few years ago. not sure if things have changed much, but maybe i can help too.hey current fellows. i have a mentoring session from 6-8, but i\\'m still on the red line. i am on my way. sorry for being latemade it could someone let. me in?  you did pan sharpening right? i think the type of pre-processing you’ll do will largely depend on how you intend to process those imagescould you share more details about what you are trying to do with satellite imagery and what are the approaches you are thinking about?i agree with dan. it really depends be what you want to do.  or do you just mean turn you 8/16bit pixel  into into a float 0-1.0? i am not an expert but are about to work on it. let me know if you would like to discuss.this has some useful functions you can use or borrow.  has anyone recently could get caffe installed on mac?yeah, sorry for the vague question. and the answer very well might be “try it, see what works”. but, in essence, just color normalization. i’ve seen there’s different recommendations in doing this:\\n\\n - normalize every channel over the max value `img/img.max()`\\n\\n- normalize on a per channel basis `img[...,i]/img[...,i].max()`\\n\\n- normalize by mean and std `(img - img.mean())/img.std()`, now you have neg values\\n\\n- or, if using pre-trained layers (conv layers from something like resnet), just normalizing by their values.\\n\\ni was wondering if this became more important as you moved to multi-channel (4-16) satellite images which include the non-visible bands?if my memory serves me right, you don’t want to have negative values if using relusif you are using e.g. raw landsat images from aws, i’d suggest normalizing them on a per-channel basisyeah, that’s what i was thinking too. though it seems leaky relus are starting to gain favor, i’m not sure if that makes a differencei know each band has a range of values - people from geotrellis are empirically clamping intensity values to [4000, 15176] ()but i think intuitively, it makes sense to normalize on a per-channel basis, especially if you training from scratchotherwise, when you have 8 channels of 8k by 7k pixels,it seems weird to penalize nir band because there was an oversaturated pixel in the blue bandi did a couple of months ago.  ran into a few problems along the way.  dm me with where you’re running into trouble and i’ll see if i can help.okay, yeah that makes sense. thanks !thanks! we’re in the ds group in seattle right now. we’re stumped about how to get our models in python to talk to a chrome extension. we weren’t sure if we should start using flask and try to figure out if we can build an extension that way or we should be approaching this form a different direction.hmm never tried to get python working directly with a crome extension.. i always spun up a server (django or flask) and then made ajax calls from `background.js` in the chrome extension i have some experience in tf/mxnet/pytorch. if your going the tf way i highly recommend starting from the ptb dataset examplesorry to hear about this. i hope you’re ok. you can try contacting some of the higher end local bike stores and ask if they know of a lawyer who handles a lot of these cases. they can also put you in contact with the organizers of the local group rides. sadly, they will almost certainly know someone who needed a lawyer for similar reasons. good luck and i hope you heal quickly.+1 on spending a considerable amount of energy explaining ds, which is not necessarily a bad thing.\\nas a con i’ll say, make sure that leadership understands the investments and expectations of data science. \\nwhile i disagree that one should come in and demand certain things, i can say based on my experience that not being clear on what i can expect resource wise (setting up servers, or engineering support) made things challenging.how can we refer a prospective fellow to the new data pm program? any available link? you can use this form to recommend a fellow for any insight program: has anyone been to `ai nextcon`?hi. anyone knows of cities that publish live feeds of their transport system that include gps coordinates? most of the feeds i\\'ve found so far seem to only provide the scheduling per vehicle at its stops...allright, it seems like the boston data will do the trick: i\\'ll try fetch it nowhtml related question: i am trying to generate a simple bar plot each time a user enters an input. i currently have the plots saved as images with unique names but they are still getting placed on top over each other in the output. anyone dealt with this before? got it! added plt.clf() - not certain, but i feel like san francisco does this.  on a much smaller scale, i remember princeton providing access (if not publicly then on request) to live gps on their various shuttles.  uc san diego also does live tracking, not sure about feed availabiliythanks, will take a look: try this link for vancouver transit system (requires registration though): cool, thanks , looking into it: it turns out the vancouver api limits to a maximum of 1,000 requests per day... bummer for my plan, since i want to have real-real-time... don\\'t know if it will be possible with any of the feeds out there though...: vancouver api is designed for mobile apps, so that’s why the limit. can you use proxies to scrape more? i don’t think that’ll get you to real-time resolution, but 1 data point per 3-5 mins, maybe: sounds like a good plan. thanks :simple_smile:: berlin does it. &gt; help.exethanks , it seems though that the dataset does not expose the coordinates of each vehicle... also, i\\'m more interested in buses than trains :simple_smile:thanks  -- do you know of good resources/advice on how to evaluate search engine results?what do you mean by that, alexanderjerneckif you mean metrics for the quality of output of search engines, then there are a variety of information retrieval-theoretic quantities you can calculate  (e.g. ): it looks like there is some gps bus info here:  current remote session fellow with a question for anyone about using python 3 on a machine already running python 2.  am i better off installing a vm and running the python 3 strictly within the vm or can these two (and the relevant respective libraries) happily co-exist?  to give context, py3 is a specific requirement for my project, whereas py2 is what i normally use.  reply here or pm me.  thanks!far as i know, they can coexist, as the libraries have separate install pathways (i.e., pip installs to python2, pip3 installs to python3, at least on ubuntu/debian).  might be better off with a vm just for development best-practices though: anaconda makes it relatively straightforward to switch between python versions on a whim.: use conda to create an environment for py3, switch between environments as needed.virtual environments are also a way to isolate versions and dependencies : +1 for virtual envs over conda. in my experience, anaconda can give you trouble with the rest of your dev stack (my made-up statistic is &gt;60% of set-up problems for my session were due to anaconda)stay positive zkermishyikes.  okaybut definitely don\\'t mix conda enviroments and virtualenvs. both sides agree that\\'s a horrible idea. :wink:: \"don\\'t cross the streams\"\"it would be bad!\"stay positive alex_gudei\\'m trying to use stats.models acf and i keep getting an error that my data is numpy.ndarray and is not callable. anyone get this error before and have a fix?stay positive stephgline can you post the line that’s causing the error?----&gt; 1 monthcor = acf(dfmonth[\\'count\\'])\\n\\ntypeerror: \\'numpy.ndarray\\' object is not callablesounds like acf was overwritten to be a numpy array i just got assigned to a search engine team, and i\\'m sure (i certainly hope!) they have metrics - i\\'m more looking for someone with experience n the area that can give advice once i get into it. i seeinformal poll here — heard someone recently say it was pretty standard for company-based health insurance kick in after some waiting period, i.e., 30 days, 60 days, etc. that’s not been my experience before across a variety of sectors (education, government, international development, think tanks, non-profits) so was curious what others know from their past/current experience. feel free to dm me if you’re interested or if there’s overwhelming opinion on this, i’ll set up a new channel. thanks!: that\\'s terrible! i don\\'t have much experience, but i\\'ve never heard of that. what are you supposed to do if you get hurt or sick during the waiting period?stay positive berkeleyalmandhunter: you’re not helpful. :simple_smile:\\n\\n: exactly! was curious about a situation like that. seems like a big risk to ask of a new hire. i guess one could rely on cobra from a last job or something of the such, but that’s not applicable in all cases.: this sounds really strange. i\\'ve never heard of this. maybe if it\\'s for a role/company where they have high turnover and a lot of people drop out in the first month or two? for a tech job, this seems really weird.stay positive jeremykarnowski: i\\'m trying.not an insurance lawyer, but new job counts as a qualifying life eventwhich means you should be able to sign up for insurance immediatelyhowever, often, it\\'s the first of the month after you start: the turnover, i.e., someone doesn’t work out, was one of the reasons someone mentioned for doing it.so, if you start the first, it starts that day, if you start the 2nd, it starts the next month\\'s 1st.but it\\'s worth asking about to be cleartalk about bad timing, huh?stay positive mhoover: i’m the most positive person in the world. give me some love for that.also, maybe make it part of the negotiation process? that it should be available day 1? is it because of time to process paperwork?just to be clear, this isn’t something i’m facing — just something i was a casual observer to. didn’t interject as a bunch of others were also saying they’ve not faced the scenario before. but got me thinking... def a negotiation point though if it comes up!this happened to me at jp morgan and also happened to my wife when she got her job in advertisingi think it is just something that insurance companies will negotiate into their contracts with employersi think your odds of negotiating it would be pretty slimyep. i have seen a three month waiting period.can i start a poll? i\\'m new to data science and my boss asked me what my priorities were for learning skills on my new job. i have the following list, but i\\'m kind of intimidated by deciding this by myself. does anyone think i should add or subtract anything to/from my list?                                                                     machine learning\\nbusiness\\npython\\nsql and spark/hadoop\\npredictive analytics\\nmentoring/working on a teamwhat\\'s the job?i say work on the ones that give best roi.best roi for sure. but the python, sql, simple data analytics, and then (predictive data analysis and hadoop or spark).mainly because coding, sql, data analysis are the first things to focus. are there candidates? took me about 6 months to hire set of ds and de there. well, since you have found 2 people, we must conclude that there are :wink: my company also took about 5 months to hire a ds last year. we are currently looking for a senior de -- 5 weeks and counting.posting for a friend: \\ndatacamp is hiring for a data scientist position: \\nno problem! glad it worked out!some researchers collected 100,000 “happy moments” from mech turks. pretty cool:\\n i would be really happy to see a fellow project make use of that!^ i see what you did there.does anyone have a good guide on best practices that facilitate data science initiatives, like a data science project lifecycle, that you can share? thanks!maybe not quite a project lifecycle, but this has helped frame my group\\'s work: cool i’ll check it out. thanks!do you guys who work in data science prefer using dask to spark?i haven’t have the chance to use dask; i like sparkdask is nice if you data science team is doing most of their stuff in pythoncan scikit learn be used effectively in spark? (that is distributed) the wait was too long, and i was ready to give up. but yes it takes time!generally, yes. but there are varying levels of implementation complexity depending on what you want to do. there’s a package called spark-sklearn that lets you easily distribute a grid search across a spark cluster: cool!i’ve also done ensemble learning with sklearn distributed on a spark cluster, but it took quite a bit of work to make it happenhere’s an outdated example: the bootstrap module has been deprecated, but the basic concepts are thereamazing thanks!happy to help!that\\'s really cool! as a general caveat, i was told to be careful about mixing and matching python with spark unless it\\'s configured properly, because by default python expects a single-threaded processor. i\\'m working with fellows (and alumni) who are interested in toronto - if you\\'re still trying to hire people to your group i\\'d be happy to chat anytime!we used dask a bit, but then our de team pushed us to use spark and so now we’re all using (py)sparkspark in a jupyter notebook was interesting omg spark is so figidty!!! i’ve been using it in the last month and i’ve experienced so many quirks is dask able to handle bigger data than spark?i had to switch to scala because pyspark didn’t handle some of the features i needed at the timeduring insight,  suggested that i used dask to handle their data. i was doing a consulting project for them. but i never did use dask or spark back then. (my concern was that if i got stuck, i wanted the fellows and the pd () to be able to help me. so i stuck with sampling down and using pandas…i’ve been using (py)spark to handle 400+million rows of data at a time--that’s about 260gb csv file--with reasonable wait time between each execution command (&lt;10 min for whatever you wanna do, including joins). but i always wonder about daski only used dask a little bit, and only as a bulked-up version of pandas. i now use pyspark in databricks notebooks which attaches to decent-sized clusters and does a lot of pretty quick and efficient heavy lifting etl type stuff for me. it’s still a bit of a black box, but i’m starting to get the hang of pyspark and when you should cache dataframes, etc., to speed things up. and i’m getting ok at keeping everything in spark dfs (as opposed to topandas-ing everything!)and in terms of running ml algos, if using pyspark then do you guys typically use mllib, too?one of my teammates has used mllib, and they say it’s worked fine, but i haven’t heard much about it and never used it myself.i mainly use spark ml and stuff everything into a pipeline.i mainly use spark to clean data, then down sample to import things into pandas and do ml training and evaluation therelittle known fact... a little over 63,000 of those \"happy moments\" occurred in the nyc insight office. #themoreyouknow :rainbow:has anyone used/does anyone have opinions on snowflake? we are thinking of moving our rds postgres warehouse.we use snowflake.  our biggest complaint is that their pricing is opaque.  it\\'s so bad that we actually set up experiments to try to figure out how they\\'re charging us.one of our managers actually demonstrated that they are not charging us properly according to their own documentation, and showed this data to our sales rep about a month ago.  no movement yet.to be fair, though, we are still using snowflake, because their service is reliable and can handle our data.i also use snowflake.  i would echo the issues with the pricing and in flexibility.  if you spin a warehouse for 2 minutes  turn it of and then back on for 56 minutes you pay for 2 hours.however, the underlying tech behaves as advertised and is awesome and very fast.i love how data scientists data-science the pricing.there is one open source schema registry utility by yelp called schematizer \\n\\nthanks. i think roi is pretty difficult to determine when you are first starting out, but clearly focusing on coding and analytics are important.google is using recurrent neural networks to predict your messages: one interesting question about this: do they/how do they avoid leaking training data? one obvious way is to train user-by-user, but that would provide small samples.i can’t find it now, but this reminds me of an article i read which was anti- this idea.   the article was about facebook and likes and birthdays, and how it could be possible to have the computer wish someone happy bday on your behalf, and remove whats left of any human interactionnot sure if it\\'s what you\\'re looking for but here at ibm we adhere to the crisp-dm methodology thanks! i will keep a note on that. who else loves rebecca black??!! #friday#at-channelnochannel-tagging 1500 peoplewew lad:panda-dance:oh no! my desk is dirty!here comes the complaining about channel tagging followed by the complaints about the complaints about the chanell tag.:faye:also... spelling.and jokes about faye :stuck_out_tongue:somehow i needed this. which seat can i taayyyyyyk? :notes: set the channel purpose: this channel is for posting stupid youtube videos and @mentioning everyone.so bad, so, so, bad…odsc discount code message coming up soon :eyes::smirk:better stop using logistic regression in production nowman. these people are taught to use regression models for hypothesis testing and forget that regression can be used for prediction, too... and apparently so do their reviewers? oof. lolthat’s amazing!what\\'s the journal??lol j med internet res.no wondereven open source arxiv is way more reputable when it comes to this topics...careful, i published there! ... yeah it\\'s not very reputable.did anyone work through the nlp tutorial ()? on the first line of code from the github, i get the error unicodeencodeerror: \\'charmap\\' codec can\\'t encode characters in position 64-65: character maps to &lt;undefined&gt;it could be python version. are you using python2.7?3.0 --&gt; nevermind fixed it with: output_file = open(\"socialmedia_relevant_cols_clean.csv\", \"w\", encoding = \\'utf-8\\')this notification woke me up!!! and i ain\\'t mad :ship_it_parrot:ugh, this abomination of autotune again xdwell at least it makes me laugh#deadbtw (the upcoming) spark 2.3 should be much faster for converting spark df to pandas df. demos show up to 10x speedup for some cases. this is due to using apache arrow as a dependency, which can avoid the multiple serialisations.ohh!!!out of curiosity, because i’m not ds, why does one convert spark df to pandas df?haven\\'t tried it myself, but there is a topandas() function in pysparkhi all. what is your preferred visualization library? i\\'m thinking of migrating from matplotlib to  to make animated plots, however its documentation doesn\\'t appear to be very good... i\\'m not very good with javascript so d3 is not my first choice... this is really helpful thanks!you\\'re welcome , nice to hear from you!separate question: why is the pyspark api so unpythonic……??!!!i’d suggest using matplotlib’s animation module if you’re already familiar with matplotlib. otherwise, bokeh and plotly are good web-first alternatives.still using matplotlib myself (&amp; seaborn). what’s your use-case? for webstuff, there is a matplotlib d3 library that works pretty well. i’ve been eyeing other vis libraries.\\n\\na super overview of vis libraries and when to use:\\n\\nthanks for your reply. my use case is to implement a picker for a scatterplot, but the examples i\\'ve found of matplotlib seem to be quite rudimentary: i would love for example to have a hover mode so i know what i am picking, or to be able to do some polygon or lasso picking of my points...we tend toward seaborn and an internal tool build on bokehthanks guys.  how well documented is dash in your opinion?quite well in my experience i feel a lot more comfortable with pandas than i do with spark for manipulating dfs and getting information out of them. pandas is really powerful and it’s what i’m used to, so for anything that’s even kinda complex that i wanna do with a df, i *like* to convert spark df to pandas df, though when the df is huge it can be slow (or impossible to hold all the data in memory in a pandas df)ah i see, thank you for your response, it’s very helpful  np, and of course that just my humble opinion on the topic (as someone who loves pandas and isn’t super comfortable w/ spark)i’ve worked with some dataframes that are larger than memory - and dask dataframes explicitly copy the pandas api while enabling out-of-memory analytics on single cores. seriously good stuff — and it scales to multi-core and multi-node too!in my workflow, we occasionally convert to pandas when we want some function that we can\\'t do in spark. but it is limiting because afaik the pandas df has to load everything to memory at one time, as  mentioned. we can run larger data sets faster when we stay in spark.you may be able to accomplish something fast and dirty with jupyter notebook widgets wrapping a plot function.\\n\\nfor hover-mode and all that, i’ve seen better support in d3-based plotting libraries but haven’t used any myself.also, should mention - d3 based plotting implementations don’t have to be horrible - there are some d3 powered libraries that support a python api, including:\\n\\n* bokeh\\n* plotly\\n* altair\\n* matplotlibd3\\n\\nif you watch that video, there are demos for all that stuff which might provide some hints. its a bit of a time commitment at 30 minutes, but i think its a nice overview of everything.i have a friend who applied early decision to the ds program with ny as first choice. can someone provide me the link so i can write him a recommendation too? (assuming that is still what happens :stuck_out_tongue:)i’ve just sent my recs directly to the program directors…is there an official process other than that?yes! here is our alumni rec form. alumni recs are our best source of good candidates, so we love these. letting pds know directly is fine too.  pinned a message to this channel.thanks  ! uploaded a file: ibm lemon awardthey don’t have a lot of market share, but they make up for it in cost!apparently true! they posted the same revenue as aws last year it looks likebest trollhas anyone had experience with any auto-tagging softwares for unstructured data like meeting notes?pyarrow is pretty nicei think this looks promising\\ni\\'m glad paperspace is on the low end -- already adjusted to it (thanks to insight) and wouldn\\'t mind using it in the future!anyone have experience deploying logstash 5 through kubernetes? i\\'m having difficulty placing the config files appropriately :(have you used this  ?are you getting any particular error messages in the logstash pod?anyone here played with flink?  hassuper cool, will ping him...thanks  thanks for sharing,  ! really enjoyed the piececuz airbnb doesn’t trust insight?! jsilverman: i can’t get enough of airbnb’s policies regarding hiring and growth. hats off.\\n\\nhowever, i think the article is completely misunderstanding the purpose of this initiative. it’s not because they don’t have enough qualified applicants (come on now :slightly_smiling_face: ), it’s to diversify the types of people who work on data, which is a very smart and very beneficial approach.there’s actually a number of companies who do this.  i think cisco also does something similar for example.hi! my roommate and i are looking for a roommate in sf!  we share an (enormous) three bedroom, two bath condo right on golden gate park / the panhandle.  the open bedroom is big, and has a big closet and shares a bathroom (with the tidiest person ever).  we have a big kitchen, big nice living room, and roof access.  optional garage parking extra, but plenty street parking available.  and there’s a bus stop in front of the house that heads right to market st.\\n\\nif you’re looking, or know someone that is, let me know!  we’re just looking someone respectful and reasonably tidy, and not, you know, raging every night.  rent is $1850.does anyone have an insight on what the pros and cons are to work in a company that does not have a data science team to start your career?i work for a company with a small ds team. pros: 1) lots of low hanging fruit for projects, 2) small projects can have big impact for the company, 3) since you end up working on lots of different projects, you get to see lots of different types of data.  cons: 1) not very many people to brainstorm ideas with (hence all my slack and linkedin posts), 2) some times you have to build new infrastructure to pull data and push your models to production, 3) you end up having lots of meetings to explain data science jargon (i spend at least 50% of my time in meetings).thank you. but low hanging fruit point: how impressive to put those on your resume for the next job/company, etc..?definitely! my first project was to predict which of our 200000 dialysis patients would be hospitalized soon - both low hanging fruit and big impact.hmmm…this is also a unknown startupi can tell you for a fact that what’s impressive in resume is not complexity of project, but impact of results.  if you can spend a week and save someone hundreds of thousands that goes a much longer way than building a complex model for smaller gainsthere’s some jobs where that’s not true, like if you’re more interested in the research side of things, but for the vast majority it isthanks but how do those effects are measurable when the company is a small startup?that’s quite use-case specific.  you should probably evaluate the project’s value befor etaking it onok. thanks! :slightly_smiling_face:+1000 to the answer for \"but low hanging fruit point: how impressive to put those on your resume for the next job/company, \"impact &gt; many other things. data science is a team sport, and if you do some engineering, some dashboarding, some managing, some count+divide, and little modeling, but that delivers value, that\\'s how you starting building credibility on a small team. and with that credibility you\\'ll create more space for yourself to take on new projectshas anyone gotten into a bicycle car accident or have any friends that went through this and got a lawyer/bicycle injury lawyer? i need to find a good lawyer, but i don\\'t have a lot of friends who bike. please let me know if you have any recommendations. thank you!which state was the accident in? you\\'ll presumably need a lawyer licensed in that state. in any case, i haven\\'t hired that kind of lawyer anywhere, reviews on  can be helpful for vetting lawyers.\\n\\nalso, sorry that you\\'re needing to do this! :face_with_head_bandage:the accident was in california. thank you so much for the link above! i\\'ll def check it out see reviews from lawyers i\\'m thinking about.i found the o’reilly book statistics in a nutshell to be a good primer for some stats fundamentals that i hadn’t seen in a while. i have been using gan on time-series. while some of the techniques useful for images can be translated to time-series, others just make the model worse.i’m having issues connecting flink with cassandra. i get an io netty error like abstractepollunsafedan blazevski, dr. flinki wouldn\\'t normalize on a per channel.  then you change the color of the object you are looking at, and you will mess up things like ndvi.   if you want to adjust for exposure you should do that across all channels.dr flink :flink:could anyone review my lstm architecture and let me know if it looks ok to them?  i developed one in keras and one in tensorflow.   i’m basically trying to overfit to one sample, but the prediction is not quite what i was expecting and i’m not sure if this is normal or not.what version of flink + connector are you using?divide by 65535 this is the max value for a 16 bit pixel.the flink is version 1.4 and the connector is 2.1 i thinkdoes the code compile?it works on my local machine but not when i upload it to the clusteras in, you see data in cassandra when you write the job locally?does anyone have best practices for peer review of ml models? i\\'m looking for something like code review best practices, but for the model/algorithm itself, not the code implementation.does anyone have experience with django + postgresql set up?on ubuntuit\\'s insisting that my password is wrong and i\\'ve reset it a million times and googled my hands off.hello, has anyone used facebook\\'s api? i am trying to collect profile pictures, and how many likes they got for project validation. thank you!you dont need to scrape pubmed, it has an apiyou can also download medline if you apply for a licensethere are both python and r packages that will hit the pubmed api and parse the output sensibly for youwould anyone working on search engines be willing to chat?sorry.. i didn\\'t mean scrape. just use the api and parse the results into something useful.new public data galore: : i\\'ve been playing with this, it works for multiple search engines: if you\\'re getting any large amount of data though, you\\'ll want to run through proxy servers, otherwise google will ban you, or worse, all of insight (definitely not a way to become popular) :with open(\\'name.pickle\\',\\'wb\\') as f:\\n    pickle.dump(obj, f)can anyone recommend some good tutorials for node-red? i’m learning to build internet of things. thanks! do you love vim?alex_gude: i have been using vim for years only because i was too lazy to learn anything else :slightly_smiling_face: could you just run matrix factorization then use the user\\'s latent vectors as \"features\" into a multi-class classification model?oh yeah! a good vim plugin for pycharm is the only thing that convinced me to switch. :smiley:non-physicist vim enthusiast reporting in, anybody have a good solution when working in a jupyter notebook?:eyes: looks possible -- i haven\\'t used it though. also, their installation instructions might work, but it feels old to me.and of course there\\'s the  project which, in theory, separates the notebook pieces so that you can run via an editor (and not the browser). i know a few editors have used this plug-in some way, but your mileage may vary (and i don\\'t know of a vim specific one). that seems to be the most feasible approach for now. i recently found the following paper about predicting classes from a multi-class domain using matrix factorization, e.g. if movie reviewers had different ages, occupations, etc. sounds relevant, but i\\'m still working out the details -- not sure if it will end up being a better approach:\\na great use of deep learning: sv peeps — i have 2 tickets to the banff film festival in redwood city tomorrow (friday) night @ 7pm @ fox theatre. it’s super cool (a series of short films that are well done and usually related to nature, plus gear/product giveaways between films) and i’ve gone the last 3 yrs but i need to unschedule some of my wknd so i want to send the tickets to a good home. $24 each (i’d let you have both for $40). let me know if you’re interested! here’s more info and the film list: hey folks, got a job opportunity in austin, tx if anyone is on the job market and open to move. . the official position title listed on the jobs page is senior manager, data scientist, but it isn\\'t a manager position (schwab\\'s weird job title taxonomy). lots of really interesting data to play with. it\\'s a pretty interdisciplinary team, and financial knowledge is not at all needed. we make lots of internal data products. we\\'re starting to mess around with rnns and deep learning. python, scala, and d3 experience would be plus. dm me if you have questions or want to apply.to add more to this. i know that you can use a chi square test say if you can have association between two categorical variables , there is also goodman krushkal test which can be used  for this purpose. but from understanding there is no way show collinearity using the above two methods. just that they are associated.fellow long-time mac user now on windows.  i\\'d check whether they support windows 10, i\\'m stuck in windows 7 and it lacks a lot of osx-like features i notice in windows 10 (linux subsystem, multiple desktops, etc).  i\\'ll admit to being an apple fanboy, but, man i miss my old macbook :disappointed:i\\'ve used vif in the past:\\nvery interesting read- an uptick in down syndrome diagnoses was seen with the introduction of new machinery. quite buffling...hi, i am in the same situation. started a new ds position, into my 10th day. :joy:. long time mac user- needing to convert to window based- where do i start? follow-up: has anyone made a correlation/covariance matrix visualization where the features can be categorical or continuous variables?does anyone have a good tutorial/reference on how to identify important interactions using logistic regression?i’m looking into windows 10, and it actually seems like a number of the useful features have been implemented.  more than anything, i’m really partial to the magic trackpad, so i’m looking into windows implementationsis your question regarding collinearity between two categories of a categorical variable (such as cat, dog, other) or collinearity between two separate categories (such as animals vs weekdays)?hello everyone! \\ntwo weeks from now mya systems will be holding presentations on our nlp and ml research! there will be free pizza and drinks!\\n\\nhey  so i am talking about two categorical variables and not levels within a categorical variable. so take for example the categorical variables race and gender. in such a situation, chi square is a measure of association but the strength of correlation, which is need for collinearity, cannot be acquired from the chi square test. hence what does one use then ?in that example i think a chi-sq would tell you most of what you\\'d need to know (if the level of one var depends on another var). depending on how you\\'re thinking about modeling the data, you could dummy/contrast code the variables (ie convert to numeric) and then just use correlation to check if there are systematic relationships hey josh thanks for pointing this out. so the thing is, this works for the case when you are working with a regression problem, but when you have all categorical variables, you can\\'t really regress for anything. you can define an r2 to vif. am i wrong on this?  another approach i know is to use anova, if your independent variable is categorical and your target variable is continuous, this worksanybody expert in ascii/unicode? i’m trying to create a dashboard inside a terminal and looking at `termui` and blessed` tools.. or the least create tabs as ui within termansi graphics.. that brings me back to the good old days. yes, i\\'m old. : i\\'d suggest using a global optimizer to avoid getting stuck in local optima. nlopt is pretty good and has a lot of options:  . if you can\\'t compute a closed-form gradient, use a variant of direct, that has worked well for me for gradient-free global optimization in the past (but obviously having a gradient will make it converge faster)working part time as an undergrad in a geophysics lab in about 1996 i spent over a year developing an ansi graphics windowing system in c. those were good times (among many…). it was for a test stand to commission a satellite for measuring solar activity. one interesting thing is that years later i now work with a data scientist who spent time as a graduate student in that exact same small lab. awesome. thanks! have you looked at urwid or newt?  [  ]: thanks for sharing! :slightly_smiling_face: will be trying this out, this weekend@ryan: thanks a bunch. i\\'ll try your idea. but i am curious why the connection pool approach doesn\\'t work. i found that idea in the spark programming guide. it\\'d be really neat if it worked.today\\'s the last day to sign upnuggetbot check on ajmendezajmendez has a happiness average of 0: pretty low there, budnuggetbot check on dstonedstone has a happiness average of 0.12280701754385964i am, in general, a pretty miserable personnuggetbot check on meme has no happiness average yetnuggetbot check on higginschigginsc has a happiness average of 0.5227272727272727apparently i’m a happy dude. did not know you could do this: i doubt you will beat the jubilant nuggetbot check on vmgehmanvmgehman has a happiness average of 1.5161290322580645checks outthere you go: positive feed back. i am a happiness op amp.hey to all you more senior data scientists out there: what is your best approach for continued learning and skill building?: try newer analytical frameworks, use r! and try kaggle datasets on your own. also, look at usgs and nih, the routinely put out challenging datasets to work on.  there are also tons of great data science meetups that give talks and workshops.  its useful to see what and how people are using various technologies and pick up new skills that way.:thanks!   shutdownify, the shutdown notice as a service company, is shutting down: wowi literally can’t even.any advice for setting up my scala/spark development environment?\\n\\nthe problem is the split between my laptop and the aws node that\\'s running spark.\\ni can install an ide on my laptop, and use github to sync my source code, but how do i debug?: \\ni use pyspark with ipython notebook so i\\'ve never tried these but i\\'ve heard about people using\\n1) spark notebook (), \\n2) ispark () and \\n3) apache zepplin ()\\n\\ni assume these work similar to ipython notebook, for which you would ssh into the master node with w/e port the notebook is using and run stuff on your local browser (much much easier than copy/paste ur codes everytime in a spark shell or having to sbt build everytime when u just use spark-submit)\\n\\ni would give pyspark a try, the --packages flag now means u can directly get packages from thanks, guang.  i\\'ll have to check these out.  talk with you tomorrow night for mentoring.does anyone use weka? is it worth playing around with?does anyone have a pdf of the oreilly data science salary report they could send to me? thanks! thanskhas anyone ever worked on fraud detection? i want to discuss some implementation matters regarding the validation of highly imbalanced dataset.  thanks a lot!!nyc insight peeps i hope everyone is ok especially folks from the current session? that was awfully close to the insight office!!at my company since we\\'re often dealing with either regulated environments and sensitive data we usually do things on prem. that way we in theory should have access to all the data we need and they can control all the security policies. there are some cons with this (1. we often need to be on site since they won\\'t grant remote access or the remote access they do grant is awful. 2. we aren\\'t in control of the environment, including upgrades, installations etc. 3. we often don\\'t have access to all the data we need anyway).anybody in sf / the bay area need a...\\n- mattress? \\n- tv? \\n- bike/bike accessories?  (really need better photos for this)\\n- monitor? \\n- desk/table? (no cl link for this but it’s this, $10 )hi all, i\\'m in the process of learning nlp at insight, and i was wondering if anyone could help me out with this problem? i have a set of tokenized words in a dataframe (each set of words represents a single element). i want to tag each set with a custom label that i have, which are represented in another column in the data frame. anyone have advice about how to approach this? thanks!what are these labels based on, and what exactly are you trying to accomplish by tagging word sets?right now, just something basic. positivity and negativity of sentiments. i planned to build on top of that (or modify) once i can figure out how to run a classification algorithm.my question is more related to the fact that i\\'m unsure what data you have exactly, and what you\\'re trying to accomplish, so it\\'s hard to suggest an approach.  e.g. you have a df of labels, where did they come from?  do you have a set of words that is already labeled that you can train on, and then use some trained model to tag new sets of words?i\\'m not sure exactly what you have :slightly_smiling_face:got itdata: i have a large list of reddit titles i from various mental health reddits (~30k right now, but i plan on expanding this). the titles are represented in columns of a data frame, and i have tokenized them (using word_tokenizer()).i\\'m using reddit text to determine if a person is \"in need\" of additional referral to a mental health resource. i.e., a doctor would input some chunk of text from a medical record or type up something they wrote down, press a button, and the app would determine if the patient is in mental distress and should be referred on to another specialisti\\'m trying to start simple: so i\\'ve taken titles with any mention of being in therapy, and ran a basic (vader) sentiment analysis on them. from there, i categorized those titles as either having positive or negative sentiment. these are my tentative test labels....but i\\'m open to additional ideas regarding how to obtain the appopriate labelsfrom there i want to train a model to then apply the right labels onto a test set / new text. does that make sense?almost, what are these \\'labels\\' exactly, like \\'in need\" vs \"not in need\" and it maps onto a specific value that\\'s spit out by your sentiment analysis?i guess it sounds like your labels come from an unsupervised algorithm already, and then you want to use those labels generated by the unsupervised algorithm with a supervised methodwhich is kind of weird :smile:  so i\\'m probably missing something still.  mainly exactly how the reddit data is labeledthis dropped today and seems like it could be real handy dealing with biased samples: tech talk at airbnb\\ni\\'m having some trouble logging onto skype@par.r.callier thanks. we are using a pre-trained model for now. so will do simple addition (and maybe averaging) for now. but, i am thinking of training a new model with the wikipedia.anyone using luigi to set-up / manage their pipelines?\\njust watched this talk:\\nmake that  , i apparently forgot what year it is^ the channel i made for the 2015 jan pa insight fellowsanyone going to pa insight party tomorrow from sf?  would like to carpool pinned a message to this channel.for the current fellows using d3 in their webapps:  might be a useful toolhaven\\'t used it myself, but i do remember trying to learn d3 in a matter of days being kind of a dumpster fire during my project :stuck_out_tongue:rolling the dice, anyone here works at d-wave ? i don’t but have some grad school buddies who do (and interviewed with them many moons ago)hi  ! i remember you came to our session :slightly_smiling_face:  would be nice for a hookup to ask some questions dm me with your questions and i’ll see if i can get you in touchhi everyone - i have a data warehousing/data engineering question. we are re-working our data warehouse of large quantities (a few tb) of time series data. we get about 10 million readings x 4 types of readings per day. postgres is getting unwieldy in the loading process (we can’t take indices off because it takes days to put them back on) and we are going to be pushing up against the limits of an rds instance in a few months. has anyone else encountered this problem? anyone have a good (or halfway decent) solution? we are looking at dumping a lot of data onto s3 and using spectrum, but it is very new.any bitcoin people out there?kevinmercurio: they would prefer to maintain their anonymity.i hear someone just made 30mm shorting it :wink:ride the wave, friend:  seems like a redshift or spectrum issue depending on frequency of queries \\n\\ntypes now in python!seriously?lifesaver?awesome!this is great!!!30 mm oh my god….finally motivated to read that bitcoin paper!that’s pretty nice. i wonder why they didn’t build static type checking on top of the annotation system?i guess you can have pretty complicated types, and trying to do stuff with annotations might get really messy quickly.oh, nm, it does use annotationssilly me which paper ? i actually started working a bit on blockchain in ibm …. uploaded a file:  uploaded a file:  and commented: one of my colleagues here in tn posted an article of  its a small world and insight makes it smaller :simple_smile:hi, i\\'m another current fellow.  i\\'m looking for someone with experience using lstm rnns with time series data, particularly with multiple features.  others with general deep learning experience could be helpful as well.  i\\'m looking for help figuring out how to set up the data on tensorflow backend. from 2017c used rnns.new blog post by  - heart disease diagnosis with deep learning  and interested others, i\\'m working on a project to identify credit card declines with a rate of about 1%, so with imbalanced samples. does anyone have experience using smote  and/or similar classifiers? thanks!hi everyone, current fellow here. i’m looking at interactive mapping options. i have a set of locations (in the us) that each has a value associated it with it. my vision was to plot points with different sizes to denote the value associated with each point, but wanted to display further information when individual points were selected or even hovered over. any platform suggestions would be appreciated. thanks!`bokeh` can do overlays on a map with interactive annotationsthanks! i’m looking into that in particular can talk to 3rd-party geo mappingfor quick and dirty stuff the bokeh sample_data subpackage has patch coordinates to draw a usa state/county map if that helps as wellthat sounds perfect. thanks for the advice! i\\'ve used smote and some cost sensitive learning techniques for class imbalance, feel free to dm medamn. iphone x is $999!!!!,  you might look into using `folium` which is the python implementation of leaflet.jsi\\'ve used leaflet (in r) and found it practical to use for any mapping / gis work.thanks for the recommendation, i’ll check it outfor :  good to know. thanks for the heads up!i’ve been using it for a while at work, it’s super solid. anyone have experience with tracking web mentions? i have a consulting project this session where i want to see if a lot of small businesses that should be dead have any web activity (i expect some are still active). either on big sites like yelp, craigslist etc., or small blogs, or anywhere else. i see there are a lot of paid/quasi-free apps out there, but my idea was to simply set up a ton of google alerts rss feeds and monitor those, as well as try to tap into some apis for the big sites. is there a good free off-the-shelf app in anyone\\'s experience, and are there any known pitfalls with google alerts? what did you do?equifax, bowing to public pressure, drops credit-freeze fees question about d3 and python in visualizations channel.  thanks for the help.hi everyone! i\\'ve created a whole bunch of new channels by cohort if you want to talk to people who went through the session with you. of course, keep the amazing subject-specific and generally random banter going across all fellows as well! and keep creating aggregate groups if you feel those will be helpful too (e.g. sv-2015a)d3 + pythoni may be able to helptomv: they didn\\'t say shiny :simple_smile:aaaah andrew....irony has done a u-turn and smacked me in the facei\\'ve been flasking like a maniac for the last month and a halfall day everydayi miss shiny so much/giphy shinynuggetbot: image me not so shiny anymore10x\\'ing it! thanks obama!on a scale of 0-10, how many x is obama?here is the original source on medium that says nothing about a shortage in data scientists. the opening of the tc article is fake news.\\n\\nhello insighters! welcome to the new crew, and here’s a question for the alum:\\n\\n*does your company have a data science career ladder?*\\n\\ni’m working on a ladder for the ds team at enigma and i’m not finding a lot of existing examples to work from. if you have any experience or insights to offer i’d be very grateful! please reply in a thread to this message. \\n\\nthanks!  shared this last year, about glassdoor: thank you for posting that  -- i was looking for that a couple of months ago and couldn\\'t find itit lives!  -- we\\'re working on something similar for ds/machine learning roles, tooif you have multiple measured time points and want to predict a single value using all those (like group membership for a binary class) what type of methods do you typically use? or do you just ignore the autocorrelation and use each as a separate predictor?that’s very broad, you can do any number of things (fit some distribution, some time series model, blah blah).  you may get better answers if you’re more explicit.hmm... so say we have multiple measurements at multiple time points for a single variable (something like cholesterol), and we want to predict whether they\\'re likely to have a heart attack, it\\'s not really a forecasting issue and we don\\'t really want 4 rows for that individual because we want them all to contribute you a single outcome. what would a typical approach be using something along the lines of logistic regression? also, no black box models, and the sample size is fairly small.maybe a mixed model?i mean from a statistical view you don\\'t want to ignore the repeated measures aspect, but i\\'m curious if you don\\'t care about the predictive power of any single variable and only care about accuracy of the outcome variable.that\\'s one of the things i was thinking of, but usually you would have 4 rows for each subject and then specify an intercept and/or slope for each subject, how would you specify it such that all 4 observations contribute to a single outcome? mlm comes to mind but it\\'s been a long time since i\\'ve used it.not sure if you\\'ll always have 4 points or if that\\'s variable. i\\'ll assume it\\'s variable. one approach could be to engineer various features about the points, e.g. slope, intercept, min, max, variance, etc and use those as inputs to a classifier that predicts your single outcome.\\n\\nam i missing something more subtle about the problem?yeah, i was thinking about feature engineering. i haven\\'t dealt with time series data very much. i also did the standard insight program but wound up in the health field where sample sizes are much smaller than you\\'d like.i\\'ll always have 4 points. typically measured at roughly the same interval.i basically just wanted to make sure there wasn\\'t something obvious i was missing.does anybody have any experience with the facebook api? i’m wondering whether anybody knows if there is a way to download facebook social network datasets (friend-links) and user activity datasets (post likes, etc.) for the same user id’s… i can’t figure it out so far…for the new fellows that i met tonight during the jupyter notebook tips and tricks -- the updated notebooks can be found here: free mock interviews great article - disappointingly sad, but greatthat is disgusting and scarily eye-openingnuggetbot animate me mind your own businesslol, wow. language, nuggetbotnuggetbot animate me aint nobody got timea/b test your facebook profile pic, lol\\nwhile we were the fellows (i was a remote fellow), were we\\n1) an employee to insight\\n2) a student at insight\\n3) unemployed\\nor 4) self-employed?\\n\\ni need a clarification on some uscis related issue… thanks guys!it depends how you classified your stay at insight and what your visa situation was at the moment.you were not a student if there\\'s no i-20 from insighti would either classify insight as a conference which you were attending it (making you unemployed)or, if on an opt, doing some work (probably volunteer work) for insight. this will depend on getting some letter to the effect from insightdepends what visa you were on, but not 1 or 2 as insight is not involved with your visai have a green card - and applying for a citizenship - i don’t think they would care too much, and i think ‘student’ might be okay???unless you have an i-20 you are not a studentyou should talk to insight about this, they are pretty helpful but on a green card you\\'re not a studentde: i have been hoping to distribute my s3 file requests using spark 2.2+scala 2.11 and squeezing the client through a dataframe map, but i keep getting not serializable issues (from listobjectsrequest, even though it implements serializable natively!). i\\'ve done the normal \"wrap it and extend serializable\" routine, and haven\\'t had much luck. does anyone have experience with this in spark &gt; 2.0?guess you might have already seen these:       somewhat related to the article posted by : \\n\\n\\n\\nanyone know any product managers looking to work with data-heavy advertising products that want to work in nyc?\\nmy company (one concern) is hiring multiple data science positions. not only the candidates with domain knowledges (natural disasters such as earthquake, wild fire, floods and etc.) but also general data science experts (deep learning, ai etc.) will be highly appreciated if interested. \\nwe are located at palo alto and will be hiring at least 6 data scientists by the end of the year! \\nplease dm me if you are interested or know someone who might be. thanks a lot!\\n(i am a 2017a remote data science fellow)hi all,  anyone has suggestions for a good resource to learn spark ? databricks has a free edition of their platform - much like a notebook based spark environment with some how to docs, you might want to make an account and play around with that… (community edition) thanks  : i started it but kind of lost my way around - maybe will give it another go.i didn’t play around there too much but from what i remember they have nice “getting started” notebooks shared for all to copy/use\\nalso other users can share things with all, so try to look for some interesting examples and copy/play around with it in your own space...if you have any specific questions, feel free to ask... ...highly recommend this :nerd_face: :unicorn_face:   ... : \\n it\\'s a good beginner/intermediate level step-by-step guide to spark, and hadoop, hive, etc. -- just purchased this for myself on your recommendationthere’s also this free book: make sure you know the differences between spark 1.x and 2.x, they’re quite huge:smiley: :unicorn_face:any references for both learning spark and getting it up and running on aws?(i\\'m guessing a docker file does the trick here, but perhaps there are other less container-y ways to get started on aws?)this may be appropriate — anyone know how to launch a jupyter notebook server with all extensions explicitly disabled?hi!  is anybody good at edward, the python bayesian inference library?have you tried modifying the `notebookapp.nbserver_extensions` parameter in your configuration file? looks like you could pass in an empty dict.  maybe?is anyone using  ,  or  in their workflow? i\\'m starting to evaluate each of these. any input you may have would be great to hear as well.\\n\\nmy use case is mostly to run sets of sql/api queries and share the results in an interactive way with non-data-centric internal folks.i used tableau for a project. it worked for running sql queries and displaying a variety of charts based on user input. took a bit to get set up and the data munging isn\\'t too great.yeah, we\\'ve got tableau here, find it rather clunky but need to look at it again, and deeper, because we\\'re already paying for it :simple_smile:one of our clients is paying for it. works out well that wayrightanother option if you want to take a look at itused it?looked into it a bit since it was just made free, but i couldn\\'t figure out how to get it talking to a sql database. works fine from excel, but... yuck. excel.agreed: who all is going to strata this week?! join  if you want to sync up and meet other insight people!hi all, i\\'m looking for a room to rent short-term in nyc starting ~april 1, preferably (1) cheap and (2) close to the insight nyc office. i would look to rent for the month of april (preferably with the option to extend). don\\'t mind sharing an apartment with people, etc. please send along any ideas/leads!\"doctorates in physics use python and r, said jake klamka, a former physicist and director of insight data science, a recruitment service for data scientists. moreover, many physicists work with large data sets, such all particles in the universe, he said.\"the economics literature deals with this a lot. instrumental variables, propensity score matching, etc. the book \\'mostly harmless econometrics\\' provides a great overview. great, thanks guys! indeed, i own a copy of \\'mostly harmless econometrics\\', though i never made it to the part where this is discussed:bow:has anyone used the python lifetimes package? i have a few questions about how things are being computed and what they mean.\\ndidn’t use it myself, but pointed some coworkers to that code, which i believe is just based on the fader and hardie methods (see the papers on the bottom of the above page)x-post from  johnjoo: i’m sure this is way overkill, but i found this blog post super interesting oh nice! i’ll take a look!importantly, the author attempts to predict not-churn in that they are predicting whether or not an “engagement” event will occur over some future time window given historical infoyeah, i think that’s the line of thinking that i was going through too.fair warning, he wrote an entire thesis on the topic :slightly_smiling_face:oh! sometimes that’s easier to digest.  you wouldn’t by any chance know the guy so i can get a copy?it’s actually free on his website! if you click the hamburger menu on the upper-left of the blog post i linked, it’s in the menu.oh nice! thanks in genetics, similar ideas are also becoming popular. search \"mendelian randomization\".pearl\\'s books are very readable systematic treatment of how to estimate causal effects using causal graph modelsyou could potentially fill the “holes” by imputing with k-nearest neighborsi have used the google maps api to geocode addresses in the past and successfully calculated order 100k of addresses. they have a daily free limit of 100k i think. where can i find a great tutorial on the use of window functions?mode has a great tutorial: oh yes, that is a good onethanks guys!thanks! how did you plot the locations?these are great tips!  i was struggling w/ the same thing the past few daysi also like this handy guide from postgres\\nthe hive language manual page is a pretty good reference\\n\\ni used geopy with folium.  it was pretty simple.  one thing to check with geopy is what geocoding service you\\'re using.  geopy is just a wrapper for talking to a variety of rest endpoints, so latencies as well as rate limits may vary depending on the service.i looked briefly into plotly for heat maps (too briefly to have a real opinion about it for mapping) and ended up using folium, which i\\'ve liked so far! i too used the google maps api to get lat &amp; long for my data (~7k).random question: i don\\'t own an apple watch but know that they put a bunch of sensors in it to acquire the wearer\\'s health data in-situ. do any of you apple people know if the anonymized data is stored by the company? once there is enough data, i bet some very interesting patterns/trends can be found. thanks - i will check that. it looks like with folium you have to be online to render, so i am working with plotly.hey everyone, i\\'m working on a project right now that involves machine learning regression, but i\\'m interested less in having the best average error (mean squared error) and more in having the fewest possible large errors. i\\'m not sure how to set this up, does anyone have experience with this, or can direct me to where i can read about how to approach this? thanks!naively, why not just set up your loss function so that it is more than quadratic in the error? i.e. penalizes larger errors morethis is exactly what i was thinkingsklearn is somewhat lenient, depending on the class, about implementing custom loss functionshi i created a  channel for people who are working on fraud teams or who are interested in the field, please join!bayesian webinarto people interested in seeing some art this weekend: i\\'m showing at open studios at shared,  stop by and say hey if you\\'re nearby! (sat and sun from 11am to 6pm)cool, what’s your stuff like woah, these are neatdo you sell prints?yes i do :slightly_smiling_face:i was there tonight to see a friend’s art! sorry i missed yours!stay positive emily.mclindenshared (my space for this weekend)\\'s address is: 739 bryant street, san francisco, ca 94107import ggplot??i love xkcdhow about mode analytics?yep, looking into that as well! thanks.nugget-bot show me a unicornnuggetbot: show me a unicornnuggetbot: image me a unicornnuggetbot: animate me a unicornfantasticnuggetbot: mustache me a unicornnuggetbot: will you marry anasuya?nuggetbot: pug bomb 5nuggetbot: mustache me simon gelinasnuggetbot: lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me there we gonuggetbot: ship itnuggetbot: mustache me nuggetbot: mustache me omg hahahahalulznuggetbot: mustache me hi guys - just wanted to point out we have a great  channel going if you want to join - though i don\\'t know if mustached fellows count as cute animals... #ftw uploaded a file: oops, i should i added that here.anyone at dataengconf today?does anyone have work that overlaps with a definition of what they\\'d consider \"data architecture\"?i choose db schemas, s3 storage formats, and the pipelines that write to them. not sure if you consider that an architect or an engineeralso choosing what data to ingest, how to save it, how to pass it to different processes, data guarantees/policies, etcthanks! i\\'m trying to get a handle on how data scientists and data architects typically overlap. e.g. are there people who spend time thinking of what data the ds will need to build \"fancy model a\\'\" in 3-6 months, and then ensure that that the schema and pipelines exist to collect that data in advancea few of us from insight spoke today at dataengconf today. as a result, i have a few free tickets to the conference for thursday and friday, and likely won’t be able to use them. if anyone is in sf and wants to attend, send me a direct message and i’ll send over an e-ticketroommate wanted: 10mins bike/drive apt from insight. 4150 el camino way, palo alto ($1525). for insight\\'s sake, i\\'ll be gender neutral. :smiley:\\n ah this makes it much clearer, thanks for taking the time to link these. i see how especially for multiclass problems, weighted guessing or shuffling would give a much fairer baseline.  i wasn’t aware of sklearn’s dummyclassifier, it’s a useful wrapper for quick sanity checks!noticed this on an nih blog: request for input on their strategic plan for data science  chernoff bounds yes, negative dependence nope... in which context? just when the la croix wars had died down :stuck_out_tongue:we have always been at war with c******.and here you yelled at me for using the c-wordthank you.you should really add a content warning to this whole discussion. some people find it really traumatic to read about things like this. :snowflake: :coconut: :la_croix:in case you or anyone you know is looking for a data science position in la:\\n:pamplemousse:so this exists: account is dead, but some good stuff in therehey data science fellows, anyone looking for freelance data science work? we’ve got an interesting project involving communications metadata predicting attrition. get in touch and we can talk details.;) i spent 80% of time complaining that. i must did ds wrong.hi everyone!\\ninsight alum from the ds-idx-2017a remote session here. my company is hiring data scientists and data engineers with positions available in both victoria, bc canada (where i\\'m based) and la! please slack me if you have any questions or know of anyone who might be interested. \\n\\n this sounds like you!using deep learning to colorize the past using deep learning to measure pollution (!) from photos anyone who needs to install gobblin, eamon and i stumbled upon two bugs during the installation. one had to do with glib-gobblin gio-error **: settings schema \\'org.gnome.system.proxy\\' is not installed. to fix that error, we removed the -djava.net.usesystemproxies=true flag from the javadoc settings in build.gradle (main project) and then when we encountered the second bug, we just rebuilt gobblin but instead of doing a clean build, we did a clean assemble. there is a concern that we might run into runtime errors down the line. anyone have experience with gobblin you want to share?i have come to the conclusion that insight is a throwback to the 90s:  everyday is basically a lan party where we are all chatting on irc (slack) about interesting topics.on hiringthis week on hidden brain, shankar talks to google\\'s laszlo bock for insider tips and insights about what works — and what doesn\\'t work — in recruiting, motivating, and retaining a talented workforce.: have you guys started playing doom () on the lan yet? :wink:stay positive alex_gudewe meet again my nemesis...doom!hmm…i get nothing.nuggetbot has an affinity for me :confused:: throwback suggests this is the doom we are playing ()stay positive ajmendezha  ensures that the alex population is positivethrowback suggests this is the doom we are playing ()stay positive joshainsleythere we godata-driven design at the sf design week tmr evening in sf. anyone going? i’m on the waitlist but also an optimist. for the people of the new batch, you will need this when you start with the interview preps the dancing in specific, not the algorithms. silicon valley has a really obsession with hungarian folk dance... :wink:but of all the algorithms, you really _will_ need bubble sort to succeed here :troll:1 vote for bogosort bubblesort gets a folk dance, bogosort is a rugby scrum i guess?that or shouting fire in the theatreand then telling everyone to go back in and watch the film/playyou know, there is a lot of overlap with the 1st drunken passenger on the airplane problemclearly someone in #ds-sv-2016b needs to make a pod that is bogosort plus that probability problembest consulting project ever: welp, time to pivot...the worst sorting algorithm of all timestay positive mamday: boy you got until week 5 for when you start pods`jobinterviewsort()` is my favorite :wink: thankfully haven\\'t been there in reality, but insight practice interviews don\\'t count as reality.anybody here use aws cfncluster, specifically with the ~/.cfncluster/config `extra_json` option `cfn_scheduler_slots`? i am trying to force the number of jobs per node of the cluster to be 1 for memory reasons; i specify this option in the config file, but the scheduler still fills all threads on each node. i have tried using the mpi and smp parallel environments with qsub, since they use different allocation rules, but to no avail. any input would be hotdoes anyone have experience route finding with constraints? i know people have integrated google maps in their projects but i need some way to divert around certain areas.: would a simple a* work for you?possibly, i\\'m just trying to avoid building everything from the ground up. i know google can route around traffic but i wasn\\'t sure if you can add your own constraints with areas to avoid. it would likely only be directions for chicago, so i\\'m not sure if there are any open source mapping tools that would be useful here.you might talk to  he recently did his insight project using waze datahere’s how uber does it: shout out to  for pointing this out to me…he’s working on something similar now:this isn\\'t a data science related job, but if you know anyone appropriate, please pass it along:\\nthe job is running a software development education non-profit. i\\'m a volunteer instructor for the organization, and they\\'re really doing some great stuff!any one knows how to open big csv file , like 90g size, in pandas ? please have me an idea.you can read in batchespandas.read_csv with chunksize?: i think pandas reads all data into memory, if so, you will need a machine with 95gb (at least) ram — not impossible, but very expensive (though if you only need a quick thing, you can get aws machine and do your work quickly and stop it — bills by the hour)thatsomething like this : df = pd.read_csv(\\'check1_900.csv\\', sep=\\'\\\\t\\', iterator=true, chunksize=1000)also realize that if you want to do anything complicated with a dataframe that size, it\\'s going to take a long time to process. i recommend using dask… or go with spark :slightly_smiling_face:some docs here: thank you so much guys for all the response. ,  : thank you for the command line. i believe that i have tried the “ chunksize” is not really working. but i will try again. if i use chunksize, how it really works? does it read as a chuck? i would be able to use it as usual usage of pandas?   may be  right. i failed because of my hardware , ram. unfortunatley, i cannot take out my data out of our server.  what is the dask? could you please let me know how to use it?  i do not have enough experience on spark. could you explain a bit more about how to use it?: can you share the code you tried that didn’t work?: the idea with `chunksize` is that it makes `pandas.read_csv` return an iterable in which each “chunk\" is a dataframe of  `chunksize` rows. there’s a good example of how to use it in the docs i linkedspark is a distributed computational framework - you would have a cluster of machines that coordinate work. this is a bit hard to explain its usage in short time/text. if your organization has a data engineer, you might want to ask them and see if they can run your job as a spark job...: i do not have the code i have tried with me right now. :disappointed: but i remember there was “chunksize” setup, just like you gave me. and i remember that after i have tried then i was not able to call the csv file as a table.: i am not sure if our linux machine has spark in it. i probably check with the linux admin.: so once i do  df = pd.read_csv(\\'check1_900.csv\\', sep=\\'\\\\t\\', iterator=true, chunksize=1000), then i should be able to use df as usual? or do i need special command ?you can’t do it with just one machine (technically you can, but it will be too weak of a cluster to do anything large) - ask if you have a spark cluster setup that can access the needed data...: yes i will. if we have spark, then do you know any good qiuck start guide site that i can follow?: no, you need to iterate over it. so \\n`for chunk in df:` then within your loop, use `chunk` as a dataframe as usualaha!i see. so once i call in as a chuck, then call it in loop.ok i see, i will try it.: good luck!yes, you can simply run spark-shell (the interactive shell in scala) or pyspark (for python) and load/compute on your data. the amount of work depends on what you are trying to accomplish. if you need just some simple filter/aggregate type tasks, it’s fairly easy to do, if you need something more complex, it will take some time to learn the spark apithanks!   i will try it and let you know if i can manage it.: hmm i hope it would not take so much time to learn. i will check my admin and i think i need to be back to you for more learning about spark..:slightly_smiling_face:: looks like i am able to access to the data by using pandas. thanks !!! :smile:: i think we have spark in out system,  and the simple guide line tells me. now i will study how to use it as i need, so this would be able make me possible to run a massive job there, right ?i’m glad you do! yes, spark is for computationally intense jobs on large data.damn never should have wasted my time learning to code!stay positive markwanghey everyone! fellows &amp; alumni &amp; all. we added a happy hour to the calendar for tomorrow (tuesday) @ 5-6pm at the insight office. we\\'ll be hosting berian james, staff data scientist &amp; head of product analytics at square. come hang out! beverages and snacks provided.….and this is the silicon valley office, in pacific time :slightly_smiling_face:that’s coastism. where’s our party?technically it’s open to anyone on either coast, you just have to travel a bit fartheri guess that’s acceptable :wink::slightly_smiling_face::beers:skype in with drinks?or just drink.due to time difference east coasters will have a head start though, so sv had better pregamestay positive amerbergchief peeps officer. nice.hey guys- i just got a discount link for a geowebsummit happening tomorrow in bklyn (i’m on the last panel).  hello! do you regularly use github for work or play? are you interested in participating in a private beta test of a new github product that helps improve workflows for teams? if so, dm me your name, email, and company and i’ll add you to a list of potential beta testers. we plan on starting within the next week or so.hi all. i will be moving to the bay area next month and i have no idea about it whatsoever. i will be working in san jose (near to campbell). can you guys recommend of any places/websites/neighborhoods/etc all around the bay area. i’ll then factor everything in (rent, commute, neighborhood, w or w/o roommates, closeness to different places/activities, etc) to make a decision. any input is appreciated. thanks (i also posted this in )looking for places in the bay area is typically too high velocity for anyone to give you a list of places.rent: not the bay areathink people shy away from east san jose. cause it\\'s too hood or something like that. i don\\'t think most places in the bay are that bad, but it depends on where you come from and are comfortable with.on the offchance that you want to live in sf but work in south bay, potrero hill is a decent option. (relatively) reasonable prices to rent, and close to 22nd st caltrian station.are you going to drive? you could get an apartment in mountain view and have about a 30-40 minute commute to ebay (as an example of a place in that area). rent (for an old place) could be around $2100, lower with roommates.basically, there are *a lot* of variables. :wink: what’s important? cheap rent? short commute? awesome nightlife around your apartment?looks like campbell has “relatively cheap rent”, so living close should be doable! can probably recommend some places in colma, so long as dreary weather is appealinglol colma is #1thanks so much everyone. yeah i know there are a lot of variables. i just wanna have an idea of where to look for once i physically arrive there.where will you be working ?this just arrived at the sv office  uploaded a file: haha, hot topicif only i had that resource before my interviews!thanks  ! it will make a great addition to the de librarywe should nail down detailsmeet in person? or distributed / remote?evenings / weekends?adding    to the thread based on likes (is this how “adding\" works?)for another project idea, i’d be interested in exploring rbms and working on an implementation from scratch.it’s possible that in-person meetings with  the buddy on a weekly basis and then monthly meetings as a group could be a nice forcing functionthat does sound goodif some people / pairs have tight schedules / long commutes remote meetings could also workmonthly meetings in personwhere does everyone work? (midtown here, commute back to jersey -&gt; soon to ct)i\\'m in midtowni’m in west villagehey everyone — insight is hiring an office manager in palo alto! know anyone who’s a good fit? we’d love some quality referrals from our alumni. tl;dr on the role: \"the office manager will be responsible for maintaining a welcoming, comfortable, fun office for the team and insight fellows, as well as supporting the team with events, travel, and other ad hoc projects. to succeed in this role, this person must be a proven self-starter, an excellent task manager, and be excited about running a fun, productive office! experience with start-ups are a plus.\" i invited everyone who liked and commented to the new channel i created. please join so we can continue the discussion!cool! let’s maybe give it till eod or early tomorrow for people to filter in and we can figure out logistics...does anyone use jupyter and access data with elasticsearch? i\\'m trying to get things up and running and currently i\\'m trying apache toree for the kernel, but i\\'m having a lot of trouble figuring out how to get the elasticsearch libraries installed.hi  i did this about a year ago, i didn’t install a es specific library, i just used the `requests` package to hit the endpoint for the server we had es running onusing toree?ah, in pythoni use the python package `elasticsearch` - works in a python kernelbtw i find toree to be quite unstable, still. if you need scala from jupyter i’ve had more luck with spylonyeah, i saw that as well, a lot of people here seem to be concerned about the performance cost of pyspark vs pure scalaone issue with pyspark is that it lags behind in development. so some packages like xgboost only can work with spark with java or scalame too.  i havent used spark on over a year.  but i went to dean wampler\\'s \"just enough scala for spark\" tut at strata last week and his opinion was that the performance difference vs python wasn\\'t that great anymore.  can anyone recommend a good source of guidance for handling missing values for various machine learning algorithms?hey folks anyone done any work in estimating budgets based on past spend data? particularly i\\'m looking into marketing budgets for different businesses. any ideas/suggestions?thanks !hi all: my company is hiring a junior-level data scientist on my team if you know any folks in boston (or willing to relocate) let me know or share this link with them:  im using accelerometer data to quantify engagements with 360 ads, pm me!oh, just fyi. if you sign up for the free credit monitoring service from equifax that also removes you from any class action lawsuits wow. that is fucked up.oh what! that’s some bs, you’re on the case!haha, luckily i was involved in _a previous_ data breach from some other company and have access to their credit monitoring. :face_with_rolling_eyes:i just meant that if people could be alerted to that clause in the equifax terms of service, things would be much better! and beforeiaccept sounds like the tool to make it happen!note that freezing your credit report is unconnected to their “identity protection” products, and is legally mandated capability it\\'s hard to know how enforceable boilerplate terms of service are, though.it’s true that it might not be enforceable. doesn’t gain them any points though.serious question: if my data is compromised, how can they prove i was the one who signed up? isn\\'t the identifying information they require to enroll you the same information that may have been leaked?harder in this case given for some people they also have more info like driver license, etc. part of it will also depend on the exact info that was takenthat said, less use cases from other people to sign up for the credit monitoring.  at this point seems reasonable to look into a credit freeze. details vary between states but for ca it is $5.this is such an interesting event given i just signed my offer to do fraud and abuse detection in my job…yeah, i do ds for risk for squareoh cool! i’ll start with amazon in about 10 daysso i know close to nothing right now :wink:i guess that is also why they were asking for last 6 digits of social to sign upi am curious if there are good policy solutions that would protect people from identity theft, and other forms of cybercrime, in general... rather than 143m americans all signing up for credit freezes. at the very least it\\'s probably time to give up the charade that ssns are private information and reliable for authentication...btw, nice job in your talk! very well donewhen i moved to the us i was *stunned* that ssn is considered a password-level secret, despite the fact that it can\\'t be changed over my whole life, and i have to give it out to about 25 different entitities a year...and that it\\'s also inferable based on when and where you were born: i have always wondered - what can someone actually *do* with your ssn alone if they get it? (i agree that if the answer to the question is anything other than \"nothing\", it\\'s pretty disturbing, as there is no way to change your ssn)pnas, the journal for those who have a juvenile sense of humor lolan even worse situation is the italian codice fiscale, which i had to give out in many more places than an ssn, and which includes your city of birth and birthdate! (plus there can be collisions!)hi all, current fellow here. has anyone ever used the strava api? i\\'m very new to apis and am quite clueless on how to get started with this. didn’t you work with strava data? didn\\'t you?i didmostly scraped though what’s tripping you up with apis?  general working-with-them problems?happy to chat strava  , i’ve done a few projects with their dataomg fo realz??thanks for the help! my questions right now are very basic as i\\'m still figuring out my insight project. i don\\'t use strava, so i don\\'t know much about it, but i wanted to find some fitness data and it was suggested to me. could you first tell me a bit about the feature space? might be worthwhile to see if my ideas and the data work together before i go about gathering the data. also, what type of projects did you use the data for?hi joao - thank you for the tips!  i looked at hcup but the cost (&gt;$200) was prohibitive.  iowa releases county level data, but aggregated by month...not quite enough resolution there.  california has asthma hospitalizations and ed visits, by zip code, but only annually.so i\\'ve pivoted away from this idea, unfortunately...i was excited about it.  at least i failed fast.pivot, pivot, pivot...go &amp; go that is an excellent article, sina, thanks for sharing.this is incredibledoes any one know if there is a good online course for data modeling (for big data)?  thanks in advancewould love to know the resources you find :slightly_smiling_face:me too :slightly_smiling_face:just in case any current sv fellow would be looking for a more convenient place to live, there are two relatively affordable rooms available for a short term lease very close to the insight office. i stayed there during my time at insight, and i could not have been happier. send me a message if you are interested, and i\\'ll give you more details.,  thanks! i always thought cover letters were a waste of time but wanted to make sure that my sentiment was shared across it, and it wasn’t me being lazy :slightly_smiling_face:, well part of it is still me being lazy.we are currently interviewing for a director of analytics position, anyone has good interview questions for that level of position? thanks!i\\'d be asking: evidence of times you made an impact on a product because of your work, how you work with diverse stakeholders, how you prioritize projects, examples of times you had to learn/decide on something for which you had no existing data sourceshow do you balance rigorous data science/modeling work with real-life deployment requirements? can they come up with a roadmap for research/testing/analysis for the next 6 months, given a prompt about a business question or new product launch?what if you cut that down to 3 months, what would they cut from the original plan, why?:thinking_face: awesome questions!can they come up with a suite of kpis for your business? what dimensions would they be broken down by? what hypotheses could they generate from this list?also time prioritization between short-term ad-hoc needs and longer-term research needs is a tricky areasome use it to gauge applicant interest (have they done any howework about the company?) and alignment with company culture and mission.for director level, you\\'d also want to ask them some pretty pointed questions around how they think data can help their business needs...assuming you\\'d be the head of the data org, sometimes there are assumptions that aren\\'t grounded in reality for what data science / analytics can do for the company.if they\\'re asking the questions  highlighted, you\\'re probably already dealing with more reasonable stakeholdersalso to clarify are you the candidate here, or the interviewer :wink:hm, i am the interviewer :slightly_smiling_face:sorry i reread my statement and it wasn’t clear.ah, that makes more sense thennever mind :wink:quote of the day (on gdpr): dance like nobody is watching, encrypt like everyone is.lol\\n\\nwonder if anyone else here is gdpr-ing and using a strategy that is not encryption as a way to let users be forgotten ????freakin gdpr killing me manencrpytion doesn’t (even) count in gdpr i believewith europe from love :heart:gdpr… :disappointed: i’m glad i’m not in charge of getting our stuff in complianceyeah, i sit next to the people in charge of gdprdelete things?could you expand on \"doesn\\'t count\"?i need to spend some time reviewing the rules, sorry, it’s only that i’ve heard that even encrypting data is not enough to be compliantencryption by itself indeed does not count.  using it as a way to “forget” users is useful if you have a lot of services storing user datado you mean different means of storage accessed by different services?  but what if we still have the means to decrypt?  is that compliant?becomes a case of “easier said than done” if 100s of internal services are independently storing user data storing whether a user wants to be forgotten in one place and forcing all developers to store encrypted versions and when they want decrypted data, force them to call that central place to get that sensitive data and get a “sorry you’re outta luck” if that user asked to be forgottenwould be interested to know how others approach this ^yep same here :grinning:they all turning curmudgeonly? cause i am.deletion is the killer if you use kafka. any topic with a retention longer than 30 days will most likely need to be migrated one of the forst thigs should be monitoring. instument metrics so the emit things like disk space utilization, cpu, memoryyou’ll want o know when hosts are going into compaction which will spike cpu and memoryaws cloudwatch probably has that for you already thoughwe\\'re hiring a buncha people.  no data scientists directly, but data science related roles:    let me know if you\\'re interested or know anyone who is :slightly_smiling_face: agreed but company is not ready for awsthey want to try in house firstthanks for your interest in sharing your experience with the growing ds community! the slot has been filled.in addition to all the awesome suggestions (definitely case studies), i would ask questions like \"how do you best situate a data science team within an organization to set them up for success? and why?\" since there is no best set up follow up with \"what might be some of the downsides of this decision, how would you mitigate these downsides?\" and also, ask them about \"what makes good data science candidates?\" (this will have huge effects on your future team) and finally, a question from a different bucket \"how do you decide whether to go with a vendor or build in house solutions? why?\" finally, i like to ask something about their grander vision: \"where do you think our industry will be in 5 years? how do we best situate ourselves as a company?\" i have seen plenty people come up with disappointing solutions, you do want someone who can develop and advocate for their vision.tempalert is hiring a data analyst - i realize this is too junior for this list, but if you know of anyone who is looking in the boston area please pass this along! has anyone here worked with python notebooks on a windows machine - is it a good idea?? , i\\'ve used python on jupyter notebook on a windows machine, and i haven\\'t found any problems so far.worked fine for me when i did ithi  - i have not really worked with python on windows - do you have a resource to get me starteddoes anaconda etc workasking   as welli installed python 2 first and then used anaconda with python 3, after which running jupyter with python 3 is very straightforward, but i did have some issues switching to python 2 for jupyter.hi there, we\\'re having a bunch of openings at glassdoor that require a few years of experience, such as lead data scientist/analyst. if you would be interested or know of anyone who would be interested, please let me know! thanks:-)but using anaconda to set up whichever python and then running jupyter worked for me and was very straightforward to set up.i have also used anaconda/jupyter on windows without major issues; feel free to dm me if you encounter trouble.i used winpython rather than anaconda, but i know people who used anaconda and it wokredthanks everyone - i am installing anaconda right now and taking it from there - will shoot queries if i get stuck - thanks in advance :slightly_smiling_face:also sqlalchemy etc should work fine as well?i haven’t used that library, but in general, if you can get it with conda, i expect it will work. i was able to use keras/tensorflow and several numerical libraries without any problems at all. :grin:wow - i should not worry it looks like:slightly_smiling_face:anaconda on windows works great. for sqlalchemy, at some point you might have to install some special utils functions like sqlalchemy-utils, for which you can follow the instructions on anaconda: i ran into some issues with the latest anaconda with python 3.6. pyspark wouldn\\'t run and gensim wouldn\\'t compile with the fast version. i had to downgrade to anaconda3 4.1.1. which had python 3.5also, the first thing i install with the conda prompt is pip, after which i use pip install :grinning:...or let me know. [not because i want to steal birgit’s referral bonus. no, certainly not. i just like people to let me know stuff. yeah.]fight! fight! fight!haha, i don\\'t think we would get a referral bonus in this case, alan:-) seriously, glassdoor is worth checking out. fyi: we don’t sell doors. not that there is anything wrong with that.do you sell glass?:wine_glass: perhaps? hey birgit how are you?bits, bytes, and bowling - my company, together with accel and she++, is hosting an evening event at grace hopper conference. it would be fun to see a few insight alums. if you’re at ghc, come by and please, do say “hi!” register here: we just did a webinar (recording available) on how we use machine learning at crowdstrike to identify malware and stop/prevent security breaches: i\\'m using conda + direnv, so i don\\'t even need to remember the name of my environments. plus, i can get environment-specific environment variables, which has been really helpful.simply using miniconda while adding the conda-forge channel should be enough, while able to keep track of dependencies, and speeding up installs by using pre compiled packages.\\ni get chills when i see a conda environment file together with a pip requirements file.i put up a poll about a month back with a bunch of python environment options, pretty much all of them got half a dozen+ responses, seems like everybody has their own favorite at this pointthis situation reminds me of the old windows days where things would start breaking and the best recourse was to nuke the os and start overany insight alums working in insurance/reinsurance in the greater new york city area?we have a cluster of folks at new york lifeany pinterest people out there?:  pinterest ppl -&gt;     so many pinterest people out there loli know there’s more people than just 22 going to odsc east! join the  channel! we’re trying to organize a get-together. you should join the odsc channel!thanks  for tagging me in!!!’s partner isanyone knows if there is perhaps a standard template behind this web page? :star-struck: is it halloween yet?  or april fools day?  my worst nightmare for a company:\\nstay positive dalepiggy back on that: sing it, : wearing a vectra t-shirt is ironical and daemonical, canonical and comical, it’s polyphonical, catatonical, and downright supersonical!i sang it in the voice of jermaine from conchords!for all you market junkies: matar: great :slightly_smiling_face: i was just invited for his meet. i was not sure if i was going till now, but i will definitely be there to hear an insight alumnicool! always fun to meet up with other insighters :) hey everyone, we are hiring for several sr ds roles at grand rounds, a health tech company in the city.  please pm me and i am happy to have an upfront conversation about the ds group current and future. i’ve got a bunch of data engineering questions for anyone who might be able to help out with some or all of them.\\nfirst, are there any books, videos, courses, etc. that would benefit a junior engineer get up to speed with current best practices?\\n\\nhas anyone tried out third party etl tools like fivetran? what do you think? how do they compare to built in cloud solutions like aws kinesis?\\n\\nbigquery seems pretty darn awesome, but is it worth choosing over redshift if all your data is already on aws?\\n\\ni use docker a lot for my data science work. how useful do data engineers find it? are there alternative container options you like?: i’ll answer a few (it might be helpful to number them):\\n(books) there are a few books that could be helpful: for architectural point of view, nathan marz’s “big data” (one of the first books on the subject) is great. for db, i read parts of the “nosql distilled” and i think it is a good book to understanding when/why one should choose distributed db vs transactional.\\n(etl): i used (very briefly) a tool called nifi that made common etl somewhat easy (graphical setup) but i generally like doing things as low level as possible. aws kinesis is apache kafka’s private version and it is not an etl tool but a real-time messaging system (some processes produce messages and others consume). kafka is a very widely used tool and i would suggest reading about it if you have any real-time needs.\\n(containers): most data tools need as much out of the host machine as possible and as little interference as they can get (most need fast network, etc.). containers, like docker, make deployment somewhat easy at a cost of computer resources (i call this a docker tax) and in my opinion are not very good way to setup big data pipelines. instead, look into tools like ansible (there are other provisioning tools as well) to setup an automated server provisioning (run a set of commands against a cluster of machines to install/setup certain services/tools) that can be reused to recreate the same exact cluster anytime you need it.: i totally misunderstood the purpose of kinesis. i missed it was a private version of kafka. thanks! \\nthat’s great advice about docker. i didn’t consider the cost of using a tool like that.it’s not a full-blown virtual machine but it is an abstraction layer that still costs, especially considering some of the gymnastic moves they have to do for networking to work properly. hope this helps :slightly_smiling_face:so i have a repeated measures anova i\\'m trying to run, it\\'s a 2(between)x3(within)x3(within) analysis, however, there is a second outcome variable that i want to use as a covariate. it appears ancova isn\\'t appropriate, as it wants a single observation of the covariate per subject, not per subject by condition. it seems like a  mixed model might be the way to go, but i\\'m not sure about exactly how to specify the model. does anyone have experience with anything like this, or have a suggestion for another type of analysis that may be appropriate?: mixed-effcts are probably the way to go as they are more flexible options than an rm-anova. feel free to dm me if you want to discuss further.anyone here use  ?  i\\'m looking for something lighter than jira but more featured than trello.  also, the ability to self-host is a must.there is a lot of overlap, like with the work that hal varian does at google and pat bajari at amazon.\\n\\nin general, i would say that causal inference is a particular strength with economists, and they often have expertise with time series analysis that can come in handy tooalso happy to chat more - feel free to shoot me a pmpivotal tracker () could be a good option.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\sorry, that was my 1yo son. he likes to contribute.as long as he starts early… :simple_smile:hahalooks like he prefers windowswant to kaggle for a good cause? my friends ralph and lalit are looking to crowdsource solutions for the diabetic retinopathy challenge and win it with the resulting ensemble, donating the cash prize to charity i found a 20% discount code for the sf data science (for anyone who needs this) summit on july 12-13\\ncode is: sfdatamining20\\nit is from the sf data mining meetup groupnuggetbot check on mscherrermscherrer has a happiness average of 0.3867924528301887nuggetbot check on zkermishzkermish has a happiness average of 0.7317073170731707that can’t be right, there’s no way i’m as happier than  … he’s too happy for his own goodi’m pretty mean to nuggetbot though.nuggetbot check on soumyadsanyalsoumyadsanyal has a happiness average of 1.1071428571428572dang , you are one happy fellowi ran into a problem with sudoing as a user that i can\\'t figure out. i created a database that can only be accessed by the user mamday. therefore i have to be logged into my remote as mamday and not root to access it. however, to access port 80 i have to run gunicorn with sudo, and i\\'ve found that `sudo gunicorn` is then not logged in as mamday, and cannot access the database. mamday has sudo privileges and even mamday  all=(all:all) all in the /etc/sudoers, but to no availstay positive mamday: most databases allow you to specify a username when connecting: for example, `mysql -u mamday -p` or `psql -u mamday`well, my code is connecting to the database with psycopg2, but that connection will only go through if i am also logged in on the instance as mamdayotherwise it will complain that it cannot validate mamdayeven though psycopg2 has the username and password of the databasebasically i have set up an account and database like this: either this is a bug in psycopg2 (and since no one is complaining about it on the first google page, i don’t think it is) or you are not setting up you database connection properlythe solution would be to make sure that psycopg2 uses the username and password pair given to it to connect; not some environment variablei am not sure why it could only be set up with the user instantiated. trying to implement the user in psql without the linux user caused issueswell can you connect to your database with `sudo su; psql -u mamday`?no. with the setup in the link that leads to \\'psql: fatal:  peer authentication failed for user \"mamday\"\\'stay positive mamdayeven with --passwordsounds like an issue in pg_hba.conf, (you’re using peer authentication rather than password authentication)this works: psql mamday  -h 127.0.0.1 -d meetup_db . not sure what to do with it to connect with psycopg2the stack overflow you linked explains: \"this is then evaluated as a \"network\" connection by postgresql rather than a unix domain socket connection, thus not evaluated as a \"local\" connect as you might see in pg_hba.conf\"changing the pg_hba.conf did not work for me i thinkbut it seems that you are basically correct about my issuesolution is: gunicorn --user mamday run:app -w 4 -b 0.0.0.0:80\\napparently this is standard with unix daemons?is there a politics channel?want to discuss this more in depth but don\\'t want to spam generalwell anyways i started it if anyone wants to joni will say there\\'s a protest at sfo today and lots of celebrities are matching donations to aclu if people actually want to do somethinghere is a link to  in case people want one-click access to it.any body had similar issue like this with `apache flink` before: hi , look into the versions of your components and make sure they\\'re compatible. i think you might want to start looking into your build file (.sbt or maven) and then make sure you have the corresponding versions available during runtime. also, check the download_tech file in your pegasus repo for info on versions you have installed. i did check for the versions and even tinkered with downgrading.hey all .. just want to know your opinion of automation of data science using software like data robot or skytree .. and also, any experience to share about skytree? .. i have tried it and found it to be interesting!: alumni - let me know if anyone is interested in an opportunity at tesla (contact me direct).  software/data/devops engineerstopic summarization == visually explore a topic model like  ?, or summarize text in some other way?in case that is what you’re looking for,  is a python port of the package  linkedthanks kevin and john.  those are useful, but i was thinking more like a force directed graph with clusters and more info/structure at nodes.  most tools look kinda crappy, but trying to avoid building something custom if i can :disappointed:interestinganyone have any experience with sentiment analysis? specifically decent performance in out-of-domain predictions? training the model on the amazon, yelp, and imdb reviews.hey folks. any one have experience with predicting start time of the process in the time series data? i have data from the bed sensor and i need to predict when person went to sleep. please pm me. aside from renaming of the input columns, i don’t think there is a lot of preprocessing that is required. i use it to make forecasts for hundreds of columns and it does a pretty good job without a lot of data cleaning or parameter tuning. that said, in the future it would be nice if it had an option to feed in multiple columns at once and to return even a coarse type of performance score given the data you feed it. some of my time series columns have very sparse data, which it doesn’t handle well, and since i can’t visually inspect 100’s of columns, returning that type of metadata could be useful to get a better sense of how it is behaving across all my datasets (in the meantime i’ve written functions to perform some of those tasks myself).tomorrow networks is looking for a data scientist (phd not required). tomorrow networks is a new york city based mobile advertising company. feel free to contact me if you or anyone you know is interested. hi all, i’ve been having some issues with jupyter notebook autocompletion it’s randomly crashing the kernel. wondering if anybody else has faced this? currently using  jupyter 4.4 and ipython 5.5 on python 2.7. also what are your opinions on transitioning into python3? thanks!do you mean tab completion? it has become pretty unusable for large dataframes (~1m rows) in my experience. i haven’t had it crash the kernel but it will keep the kernel busy for way too long.facebook prophet is a pretty cool open source library.  quite stable and can handle outliers and missing data sufficiently well.  however, there are many parameters --intuitive parameters-- defined with default values to accomplish the task of curve fitting (it is not a temporal model), which could make a misleading forecast.  for example one useful parameter is the number of change-points that it defaults it to 30-40 (can’t recall exactly), and the reason being, they are dealing with facebook data of daily samples and 30 or 40 is quite small.  however, in case your sample size is small, the model would basically treat every data point as a change point and the forecast becomes very unstable.  the other nice parameter is the mcm_sample which you would only benefit if you understand its application in their model.  luckily, the paper that accompanies the library is very detailed and quite easy to read, so i would recommend reading the paper carefully before applying it to your work.  once you understand the model, applying the api is relatively a simple task.  the only downside i faced was the fact that the model needs to growing capacity for every data point if your function of interested for the growth model is logistic as opposed to linear.  in many of my use cases, finding that carrying capacity would require the entire path of research so somehow forced me to stuck with the linear growth function.: my transition to python 3 was seamless. the main thing you run into is remembering to put () around print statements and some things that used to return lists now return generators. and actually it has better stability than 2.7 (i had a bizarre problem with something resembling a memory leak in 2.7 which i found out should go away in 3). highly recommend switching. also, i think most companies are making the switch as well. regarding transitioning to python3, it’s not really an option anymore — all major scientific python packages are ditching python 2 in less than 2 years:  yes tab completion hm. i feel like i\\'ve been dealing with 1m+ rows for a while now, it\\'s a very specific circumstance that triggers the kernel crash. i guess i just have to get rid of my tabbing habit :(.  cool! the print part is definitely something that i\\'ll need to get used to. i am looking forward to not having to import division from future though.  thanks. i guess i knew python 2 was getting phased out in 2 years but hadn\\'t read into the scientific part which made me wonder. and f’\\' strings will change your life!definitely looking forward to that one as well :stuck_out_tongue:honestly as annoying as this is, its for the best.\\n\\nmaintaining things for slightly different versions of the same language can be a bigger pain in the butt than it needs to be. this forces people\\'s hands as well, which they would have preferred not to have to do, but as has been the case in many projects, if they leave the door open for groups to avoid refactoring, many won\\'t do it at all.what would you like to learn? hey michael, want to email me  and i’d be happy to intro you to the sv team if you’d like.it\\'s for a broad team including analysts, data scientists, and data engineers. and we have a pretty large amount of money, so i\\'m open to any suggestions.i am interested - if others from the last de batch are coming. :slightly_smiling_face:hey  did you get the invite? you should’ve got the invite emailed last thursday…lemme know if you didn’t and we can resend itno i have not seen any emails form insight. can you please resend it ?if anyone else is at scipy 2016 and feels like meeting up, feel free to dm me.:   game of thrones if it was happening in silicon valley (and job titles were houses).. okay yeah it\\'s a stretch but it was a lot of fun with photoshop :smile: *hey new york!* i’m organizing a data-centric meetup on august 25, hosted by my company, . check it out!\\n\\n*a million trees, a billion oysters: enumerating urban nature*\\nhow do you count and locate all of the trees in new york city? how do you measure progress toward restoring 1 billion oysters to new york harbor? a crucial part of any data story is how the numbers are collected, and the reasons for doing so. at this edition of data io, our speakers will describe their approach to enumerating nature, with an emphasis on the urban environment.\\n\\n*speakers*\\njacqueline lu, director of analytics for nyc parks\\npeter malinowski, director of the billion oyster projecttbd\\n\\nlocation: tba shortly (one of two locations in soho or nyu)\\n\\ndoes anyone have experience with embedding a dynamically generated html file inside another html file, particularly using flask?  i’m able to do this once successfully, but if i make any changes to the python code that calls flask, the changes are not recognized by my browser, even though i can confirm that the dynamically generated html file has changed.  it seems like some kind of browser cache issue, but i’m not sure.  any help is appreciated!hi  we have a dedicated channel for flask: askflaskcould anyone recommend either a person or ‘firm’ that could help us define data science architecture better? i work for a large org in the healthcare space that’s trying to create a data science layer that spans multiple existing product lines or verticals and we could use some help in figuring out some best practices/architecture. we’re hoping to create a flexible and scalable architecture that can ingest data from a broad variety of sources and layer ml predictions on top. for reference – we have all kinds of data – marketing, crm, content, patient, medical. our challenge has been coming up with an architecture that would work both in client deployments and in our own environment, and that can ingest/source data from multiple sources. thanks for your help and p feel free to dm me if you prefer that.i know some great folks at svds () including insight alumni   . they do that kind of workthanks, looking at them now!a couple of years ago, i followed a hashtag for a few months. the twitter api was super easy to use--i had it up and running in a day. sentiment analysis on the tweets wasn’t particularly useful, but the spikes in the hashtag over time were directly correlated to news events, etc. i’d be happy to pass along some scripts, but be forewarned that i did this pre-insight so it’s not so pretty…sounds like you might need some data engineers! let me know  if you or your company might be interested in insight de fellowshard to argue with gelman.  my impression is that if your trying to create something like that you should probably be hiring a team of des to design, build, and then maintain, because that\\'s a pretty massive undertaking.so there is no issue with centering positive only continuous variables at zero? e.g., what does it mean to have negative age or negative income? agreed on the de part and that\\'s part of the overall plan, but we\\'re also trying to understand what technology platforms we should be looking at, so we can hire the right mix of people.negative=below the mean, positive= above the meana quick update: not using the elasticnetparam (the example in the documentation is lr = logisticregression(maxiter=10, regparam=0.3, elasticnetparam=0.8)) helped with the model, the number of jobs triggered is still a mystery (i should dig in to the source code i guess)#drumpfstanford deep learning for natural language processing course is now underway: apologies for the shameless self-promotion, but this may actually be relevant to a few. if you’ve ever wondered how to get started with neural networks in python, i’ve made an intro tutorial (complete with a jupyternb) about getting starter with chainer. fantastic tj! looking forward to working through it carefully. love the idea of getting bonus points :simple_smile:absolutely shameless jejejeje sounds cool. i’ll take a lookdo you do workshops tj? :simple_smile:this is great  - thanks for sharing, i look forward to reading it thoroughly later!thanks everyone…this post was based off an internal workshop i did at stitch fixyeah i read this the other day , ; have you had a chance to play around with tensorflow at all?i haven’t had a chance yet to thoroughly investigate tensorflow yet, but i get the impression it’s quite similar to chainer in some ways: awesome! i was also going to ask how chainer compares to tensorflow and theanochainer doesn’t require any compile time like theanothis has the disadvantage of providing a bit worse performance, but you’re also not sitting around for 45 minutes waiting to be able to run your training script. so for shorter projects and prototyping i think chainer has a leg up.stay positive tjtorresthanks, nuggetbot…i plan to.:thumbsup:great job tj! quick question: why do the light blue lines bounce around to either side of the final magenta state? is the nn overshooting the minimum of the error surface? just wondering, since the learning rate seems rather small for this to be happening. then again, i know nothing about regression with nns :simple_smile:i’m not entirely sure, but i think in that example i was just going for using different optimizers and ended up randomly choosing a momentum sgd which would likely overshoot a bit in order to try and avoid local minima for loss surfaces which are more complex than the mse in that examplestay positive tjtorresah, got it! never heard of momentum sgd, but now i see that it’s proportional to the learning rate plus an additional “velocity,” which leads to larger step sizes than the learning rate alone. thanks :wink:and go lick a doorknob nuggetbotrecommender system blog post (good flow chart): :clap:hi kathy, salesforce also has undergrad/grad-level data science summer internships. i don\\'t know much about the beginning of the recruiting process yet, but here\\'s a general link: anyone know of good open source tools to visualize social networks? with ability to incorporate additional data, e.g. geographic?`networkx` in python. i wrote a r package as well, `ggnet` that’s at . you can use something like gephi as well.neat : there are also several other very nice r packages for doing this type of thing, see e.g.   yeah… i’d go with either of those two options over `ggnet` — those are really solid. thanks for the links npat the end of the day, if you want geographic stuff, you ought to just use cartodbthanks   hey all, we have a data analyst position at backflip studios in boulder, colorado. we’re a mobile gaming company that produces titles like dragonvale, and it’s a really fun environment full of smart people (and fantastic benefits!). if anyone is interested in applying or has any questions about backflip and the team, please email me at . please feel free to pass along my email to anyone you know who might be a good fit.gotta admit, i was definitely addicted to dragonvale for a minute there(edit) meetup canceled. sorry!! i think there is an online course that accompanies the book \"lean analytics\"its on udemyhi, everyone. did anyone have any experience on transferring xml file to csv file?i\\'ve done xml to json, but assuming you have a simple* xml it should be fine. what\\'s up?\\n\\n* the big issue you\\'re going to run into if you have a complicated xml file is xml is nested in general, and csv is columnar.hi all. i am tying to figure out ethnicity and/or gender from a list of names and no training data. a quick google search returns a couple of options at github, but without any metric for performance. does anyone have a suggestion for where to look, or (more generally) an idea about how reliable these algorithms can be? i do expect a multinational sample. thanks! on ethnicity/race i’ve come across:\\n\\n\\n\\nand\\n\\n\\n\\nhave you already looked at these?this looks awesome. thanks !you’re welcome  ! should be possible to adapt that to gender too, might take some work. i had a task at one point (never got done in the end, super low priority) to build a similar analysis for protected classes w.r.t fair lending.anybody know of python libs/code for entity matching?nicholas.k.baro: what type?dedupe ain\\'t bad stay positive mpanciathis is the most beautiful way to visualize machine learning i\\'ve seen: *heads up*: i\\'ve started  for planning group rides. hopefully this will be an improvement over randomly emailing people. join if you\\'re interested in riding! pinned a message to this channel.in case anyone wants to play around with some interesting python edge cases:\\n\\nplotly also has a library to bind to pandas dataframes called cufflinks that i think is great (very similar to jake\\'s pdvega, but more feature-full). folks are generally wary of  because of the online publishing aspect, but you can also use it in full offline modeif only they did a coconut one…we\\'re hiring an infrastructure engineer and a growth analyst at edmodo (). let me know if you or someone you know is interested at hi everyone!  am putting together a primer for my portfolio companies on diy data science.  would love your tips on how you wrangle/clean your data!nice ! but... no images of battleship boards? i want to see the match!when i have a bit more time i want to make a web app that allows you to playand thanks!what do people use for workflow management? we’re finding trello to be ineffective as the number of projects and people have grown. we use jira on bitbucket. it allows for more organizational complexity than trello.we use asana, but it isn\\'t great for our small teams. since we\\'re pretty self directed, updating it is our job, and no one is really on top of it. :confused:i can see how it would be great if you had a manager person doing itwe also use jira. it works fine, but, i\\'m beginning to think that using it too much is a process smell.i’ve become quite partial to pivotal tracker.jira here as well for the larger group, and confluence for some documentation / collaboration. though i still use trello for my own weekly planningjira for us and with github integration has anyone tried github\\'s workflow management that they launched a few weeks ago?not the workflow management .. tried their new pr review process though. yeah i\\'ve used that, it\\'s light, but still nice!we use jira. it\\'s clunky and bloated. the ultimate irony of a tool used for agile development!also not a fan of jiraapart from some clunkiness i find it’s a little too linear for ds developmentyeah, we have jira toojira/confluence/bitbucket here. it\\'s fine. nothing great, nothing awful. all in how you use it.yodiz/confluence/bitbucket for us - easy to get going. been meaning to look into getting jupyter and confluence to talk nicely - anyone tried any of the macros?hadn\\'t even thought of it! would love to see what you figure out.quick reminder from your friendly insight admissions team: applications deadline for insight data science and insight health data science programs is one week from today on october 24th! make sure to tell anyone you think might be a good fit for the program to get their applications in on time and please do send us your recommendations! . a friend needs help debugging an ios app for nov 1 launch. gig pays $5k. any takers? the app is sport technology - enables multiple smart phones to synchronize and take pictures (perhaps movies also) from different angles .. i have some experience building ios apps. i\\'d like to know more about the app and the bug.does anyone have experience implementing a genetic algorithm to solve the \"nurse scheduling problem?\" any suggestions for good references?dan would. he\\'s back friday?excellent, thanks - i\\'ll hit him upi have some experience if its just the basics. but to be honest its been over 10 years since  i\\'ve used oneare you using deap?i have two tickets for explosions in the sky - tomorrow at terminal 5.  anyone interested?  send me a direct messageiclr 2016 talks streaming on periscope: has anyone built a transit shed map using google maps api or perhaps mta api? i\\'m looking for suggestions of tools/libraries, preferably in python. trying to avoid using arcgis which is slow and buggy.  would you suggest using networkx for smthng like this?  any ideas?  thanks in advance! haven’t used this myself, but it looks promising — *\"dopelearning: a computational approach to rap lyrics generation”*nice, i started doing a project like this last year (with lackluster results)i wonder if \\'rack city\\' was in the training set one thing to consider is how fast would this need to be processed. how much lag can you tolerate 1s, 1h, 1d, 1wk? if you can tolerate several hours or days of lag then batch sounds simpler and easier to build. if the lag has to be lower, i’d recommend building a change capture system that pipes to kafka or a msg queue for other dowbstreams to consume and update their local caches or stateif you’d like to chat more about this later feel free to dm me +1if you just need storage of data that you\\'ll eventually query later in daily batches or partitions, the tools may be much different than if you need to take action on the data as it streams inwhat\\'s the source of the data initially?thanks a bunch  and .  i only need to update the tables nightly, but want to store one row per user per day.  the data initially is coming as a packet from a gsm chip, then stored in an s3 bucket either json, text, or csv (the software engineer i\\'m working with left that up to me).  then i want to take those numbers and calculate daily summaries for each customer.  (it\\'s for solar power systems in the field, so stuff like the max voltage reached by the panel on the previous day and the time it takes the battery to discharge.)  i can tolerate several hours of lag in processing the data and adding to the system performance tables.  we\\'ll likely be surfacing the most recent row for each user to an internal interface, but they\\'ll be accessible with a primary key so hopefully fairly fast.  we want to have historical summary metrics there as well, mostly for modeling (which can be on the order of hours, though would be nice to be faster.)  right now the amount of data is small (on the order of 100k users per day) but i\\'d like to build to handle at least a 10-fold increase without dramatic changes to the architecture.  does that seem reasonable, or should we just build for what we have now and expect to have to make changes?  commented on ’s file : if you go the docker route, dm me if you have questions. i use the jupyter stacks for many products at o\\'reilly and although i highly recommend, sometimes there are issues :)previously had a non-technical manager (background in finance and marketing). was a tough relationship because she never understood what i was doing. current technical manager is much better.not me. my manager is technicalhopefullyhey folks!  coming from academia where there was a very blurry line between professional and personal computing, i’m curious how/if people separate those two in the real world  - do you have a personal computer in addition to a work laptop? i’m debating whether to buy a new laptop to replace the university-owned one i have to return when i start my new ds job.personally, i\\'m looking forward to leaving my work computer at work when i start my first ds job in jan.i definitely have a personal computer because i don\\'t want to touch my work even remotely when i\\'m not at work/on holidayi keep a personal laptop and work laptop. while some companies don\\'t care if you put personal stuff on work machines, others have very strict it policies. you should ask once you\\'re employed. in general, it\\'s best to keep a personal computer as you\\'ll most likely have to return the work laptop if you ever leave..i have a work laptop but the info on it is sensitive enough (hipaa and company info) that i try to avoid doing anything not work-related on it as a safeguardalso depending on company, you can\\'t take your computer to certain countries if you ever go on vacation therehaha, nice - hep has had the same effect on me :slightly_smiling_face:if you want to do any remote working, you\\'ll need to be able to remote from your personal to work compthat would be a huge no go at my company...good point about returning it.  additionally, i’ve been a mac user for the last decade and will be using windows for work, so all the more reason to keep a personal (apple) laptop…something for me to keep in mind when i startwhere/how do you draw that line?  what about answering personal emails?  that’s the one i generally do from work on a regular basisamen to thathep\\'s effect on me: dry, text-laden presentation stylehaha, trying to overcompensate with gifs-a-plentyi just got a personal laptop separate from my work laptop. mostly because i want to do some work on my personal that i don\\'t want to get mixed with work. and my work laptop has an automatic back up that stores everything and i don\\'t really want them to save my personal projects.on a more subtle note, many companies have policies that state any software built using company resources (including work laptops) becomes property of the company. if you like to develop your own code and want to keep it, i\\'d use a personal machine.i do that kind of thing all the timei\\'ll watch netflix on my work laptop, etcbut i\\'m not going to do things like watch my bootleg nfl streams on it, hahahi all, especially those in the nyc area looking for a new opportunity! i have a couple contacts at the startup asapp, where they are interested in growing their data science and data engineering teams. if you’re interested in being connected with them, please send me a message! i’d be happy to connect you.\\n\\nsome info — asapp is focused on automated customer service (chatbots) - routing customers to experienced human agents, providing agents the appropriate background infotainment about the customer and call, as well as suggestions of what to say,while moving towards full automation to resolve customer inquiries/problems. they will be doing a lot of nlp understanding, prediction and sentiment analysis based on several decades of call center data, with the goal of saving both agents’ and customers’ time and importing the customer experience / time to resolution.haha, nice dividing line, good to know :slightly_smiling_face:oh yeah, wow, i didn’t even think about that!  thanks for sharing your experiences!seems like it !in my experience, this sentence \"machine learning engineers primarily come from data engineering backgrounds. they’re cross-trained enough to become proficient at both data engineering and data science. a less common route is for a data scientist to cross-train on the data engineering side.\" is not really true. at least at my company, it is the opposite.thanks romesh!has anyone used airflow with docker swarm in aws? i could use some adviceour company (glooko) is looking to fill a data/business analyst type role asap... we need someone who has analytical chops and communication skills. ideal candidate is a recent grad, bachelors or masters, with high growth potential into either data science or product management, depending on candidate\\'s interest / skills. would appreciate if you could refer anyone you know who\\'s interested! \\nwould anyone know any solid resources to learn xgboost fast? i would look at the video and slides from the author of xgboostthanks!survey data!  i just participated. there will be so much data in there. that might be a ds project for the upcoming batch :wink:just got an email about this if anyone is interested in fielding a team:dear friends,\\n \\nwe\\'d like to invite you to participate in round 1 of the big data analytics world championships 2015 (business and enterprise) on saturday september 25, 2015 (4 weeks\\' time). the current world champion is stéphane sbizzera (kpmg france).    \\n \\nthousands of the best data scientists, engineers, statisticians, computer science and data analysts compete in two online qualification rounds (4 hours each). the top performers are flown to austin, texas usa to compete in the live world finals. the focus is on business, mobility and enterprise data skills with real-world case studies, multiple-choice and short-answer questions. \\n\\nfeel free to share with work colleagues or friends that might be interested. register at .50% off ebooks and 40% off books from o\\'reilly with the code: authd  not sure how long it will last...in some cases i have gotten cheaper prices than amazon!call me crazy, but this doesn\\'t seem that revolutionary. they have really accurate clocks. what does that buy you? ^ yeah i got that sense too... but then it\\'s wired writing, so not that surprising.also \"faster-than-light coordination\" made me cringebuzzword bingo but strictly speaking correct - if you can finalize an atomic write operation simultaneously worldwide, it’s as if you had sent information from one place of origin faster than light. of course we know that’s not the way information flows.funny actually, atomic clocks is the naive answer to the consistency problem before you delve into all the practical ways of having eventual consistency but someone went out and actually did it. :smile:\"it’s as if you had sent information from one place of origin faster than light\" i\\'d say it\\'s more just assuming you sent it that way :stuck_out_tongue: but i get youyes, in practice it’s just a careful way of distinguishing event time, write time, and agreed-upon ready-to-read time (haven’t read the white paper so it’s probably called something else). pretty much what dataflow was for streaming, this is for databases. pretty neat still.yeah, physicsy grumbling notwithstanding, it works, i work at domino data lab, which makes a knowledge sharing, access control, automatic version control product for data scientists. we have customers from instacart to allstate. let me know if you have any questions or if i can direct your questions to someone elseon the subject of tools that would be nice to have at work, does anyone here know of tools for managing the integration of ux/ui designs with analytic/event instrumentation? especially one that has the ability to do version control for both designers and the back-end analytical schemas?anyone have experience with \"research repositories\" at their companies? along the lines of airbnb\\'s knowledge repo but less technical (aka supplying marketing/execs/etc with critical facts about company research for presentations and sales) fyi deeplearners wayfair has something called the knowledge base, which is mostly just a wikids opportunities in the uk with  - what kind of metrics do you guys usually use when you talk about model performance/accuracy/… to non-technical people? assumption here is that things like roc/auroc etc will be too technical.% improvement over previous effortsif talking to business stakeholders mapping it to a dollar benefit also goes over well, could also build a quick spreadsheet showing changes in funnel/process and cost/revenue numbers before and after.estimates of course i assume this works best when you have previous models to compare, would you use chance as the baseline otherwise?no, i woudl just compare it to not using a model, and sort of fudge it :slightly_smiling_face:  i’m assuming these non tech people are like c-level or upper managementthey want to see stuff they can tie to roi or improvement somehow i second .  a couple times i\\'ve come up with some baseline to compare to, even just \"assume this is a constant value\".  then you can say the model gives % improvement over a basic assumptions model.cool, thanks everyone. guess the question now is what a basic assumption model is :wink:  there\\'s actually a repo: ah, but it\\'s county level, not precinct &gt;_&lt;are there any canadians here who have any experience with getting a tn visa/ssn? could use some help (pm me)! thanks :slightly_smiling_face:you can also contact insight, they have immigration lawyers: ^^^i basically let adroll\\'s lawyers handle it but feel free to pm me any specific questions. ^thanks everyone :)  i think that\\'s what i\\'ll need to do or at least for the company that\\'s thinking to hire me how to think like a data scientist\\n\\nhow to think like a data scientist\\n\\ndoes anyone have a link to mike’s slides from yesterday’s presentation?thanks for the recommendation  looking at it nowhas anyone used this for photo projects before: ?seems like an incredible dataset: thanks ! :clap:can anybody suggest a good site to purchase a plane ticket to fly international (canada)?aware of the big guys (expedia &amp; co.) but looking for alternative choices...  i use  and thanks for the tips , anyone ever use sqlalchemy to automatically read a pandas dataframe into a mysql database using the \"to_sql\" command?: i just use : yesyes, i did for my project:  . search for engine and you should find some examples. no warranty of correctness or nonstupidity!that\\'s for   btwi used postgres because of the mysql/utf/python issue some of us ran/are running into.thanks  for the extra reference! i was able to get it working with  \\'s helphello! does anyone know about a public data set on precision farming, urban/indoor farming, or more generally plant growth in relation to environmental conditions?no, but perhaps you can find someone who works here to chat:\\nthanks! i\\'ll take a look.unemployment has similar issues, and you can get quite different numbers by using different methods. do you have any connections at bowery farming?no i dont, sorryawesome, good luck!who should i email at insight with referrals for partner companies?is it a silicon valley company?no - clearwater fl. probably something for the remote program.like the actively looking for work qualifier? is a solid go-to for all of insight, and  and  (katie@ and lee@ ) are the right folks for the remote program. :slightly_smiling_face:  thanks !maybe try \\nits a digital repository where lot of plant science journal publications ask to deposit data and its open. doing a quick search for the terms you mentioned, it does throw results with data files.... not sure of the quality though, and i suspect it will be a major exercise to clean and compile them togetherin case anyone is looking for housing in south bay(*cough* 18a fellows?), my roommate\\'s moving out in april. check out  for the post (posting here too cuz the channel looks kinda dead &gt;.&gt;)think insight could get the white house to be a mentor \"company\"? :wink:obama could come for the company visit: get on that!you heard about this, right? yesonly because i keep up with the dj scene:sound::sound::sound::sound:nuggetbot: animate me dj kitty does adding single \\' \\' like : person=\\'{0}\\' help? i also assume using read_sql and read_sql_query are the same? about security though, i think you can also use the sqlalchemy orm to avoid some of the security issues you mentioned. we here at grammarly are looking to hire a data scientist in san francisco. if you know anyone who’d be interested please let me know. the job posting is here:  i know some fellows who may be interested ;) i’m all ears katie!i\\'ll message you!hey, anybody at nips currently? we are hosting an event tomorrow and would love to see you there!a lot of my lab is, but none of the insight fellows unfortunately :disappointed:hoodline is hiring a data scientist! that’s strange to seeeveryone needs data scientists :wink: you willing to move to stamford, brad? :wink:heck nothough they do have an office in nyc too...stone cold svmshi fellows, i would like to use a big csv file, like 80g . i would be able to load as rdd in spark but i would like to use this as a matrix by using spark and run with my algorithm code in python.  anyone has a tip for me please?could you elaborate a bit on what you mean by use it at a matrix?also what have you tried doing so far?and by python do you mean pandas/sci-kit learn?but in general pyspark should work for thatexactly: load the csv onto hdfs first (`hdfs dfs -put`), and then read it from spark.the good thing about doing this is that it distributes your data in your cluster first.\\nonce you’ve done that, you could do `sc.textfile(”filename\")`hey crowd, anyone know any good data visualisation blogs? my preference is for simple data presentations or ranty blogs, but i’ll take anything :pray::skin-tone-2: and :    thank you for answering. i put the fule in hdfs  and was able to read it as rdd using  sc.textfile(\"filename\") . and i have a separate matrix calculation python script which i have tested with 10x12 csv file. now i want to load 80g csv file in that python code.  up to now, i am able to read 80g file and can do some basic command. rdd.first() , rdd.take(5) something like that. and also can make the first 10000 line as a matrix. but i have 1.5m lines and 1200columns in the big file.  and i cannot read this as a whole matrix which i need to do  in my algorithm python code.in the algorithm, i have to matrix calculation.     uploaded a file: here’s an interesting piece on ethics in data science: hi everyone! the boston fellows were given a document with references to many data sources. one of these references is mimic () - de-identified health data associated with ~40,000 critical care patients. did anyone use this data set before? if so, how long did it take you to get approved for the restricted access? thanks!: i think  used mimic in her project if memory serves me correctly. one of the boston pds may know if she did or not too. i\\'d check with them (and to also get her email just in case.)she wasn\\'t in my cohort but the one before itanybody has suggestions for 3d plotting in jupyter notebooks? i tried using plotly with jupyter before. if registering with an .edu account, you can get more plots access (or unlimited? i don’t remember)you can also use plotly for free in offline mode, or set up your own serverthere’s some limitations with sharing plots that way though, i’m not sure how easy it is to embed that javascript into the notebook itself, i had some issues with thatbut that was a while ago i know seaborn can do some basic 3d plotting. i\\'m not sure if that\\'s what you are looking for. if you peruse this github for islr () you will find some examples.also: i would assume that bokeh and plotly allow you do to 3d plots (more easily) too.those are more interactive, if that\\'s what your goal is. thanks for the tip!:thumbsup:jonathan_whitmore: the gift that keeps on giving!oh, one last thing : i realized the last link might be sparse in examples, but here\\'s a notebook where someone used matplotlib to do some 3d plots (which can be easily adapted into seaborn etc) good luck!i’m also interested in using this dataset for an insight project and i’m in the process of applying for it. not sure how long it’ll take…does anybody have experience with careerjet api or apis of other job searching websites?does anyone have experience in importing a video in python? pinned a message to this channel. pinned a message to this channel.what are you trying to do?opencv has support for working with video if you want some data on how much stock options can be worth it looks like it takes about a day (i was just granted access). i already signed up for a consulting project though. have fun with the data set!hi guys. does anyone have a lead on a user dataset that i can do churn analysis on?hi everyone! does anyone have experience with using rnns for machine translation? my consulting project has a database of ~5000 credit card financial transactions that include an often-messy string corresponding to the business name (e.g., \"united tx\", \"wells fargo ban\") and human translations of these business names (e.g., \\'united airlines\", \"wells fargo\"), and i\\'m wondering if this would be enough data to train an rnn to correct these sorts of mistakes automatically. the client has a strong desire to explore rnns for automating this task, but i don\\'t have enough expertise to know whether this is likely to work or is simply misguided. any input would be extremely appreciated!!i want to extract frames from the video i would start with some basic feature engineering or compute edit distances and then try a service like open calais for entity resolution (i’m assuming you’re interested in mapping company names to business entities).  in other words, try something simple and interpretable first.  btw, this is a problem we deal with a lot where i work so feel free to pm me with questions.thanks! i also changed my min:grinning:d..jeff: agree that opencv is the best python tool i know of for that. ffmpeg is the other workhorse - it\\'s old, but it\\'s been around for a reason and can take a billion parameters if you need to customize the sorts of frames you want to extract.ffmpeg = command line tool, not python, although you could call it with a shell command from python if you want.i use pyspark with python 3 with no problems…wonder what i’m missing :stuck_out_tongue:just have to ensure the driver and each node is pointed to the appropriate python binaryre the larger question, i’d say python 3.  it’s only a handful of packages that i’ve run into the package compatibility issue, and documentation/writeups for data projects is increasingly in py3do you want to build a desktop, buy a desktop w/gpu, or rent a gpu on the cloud?\\n\\nall options require you have a budget in mind. a great single gpu for deep learning will cost around $1k. i’m not sure what the status of windows drivers for deep learning is (it may actually be plug-and-play) but if you really don’t want to do a lot of configuration yourself, know that setting up the drivers on an ubuntu system w/cuda is a pain in the ass.\\n\\nyour best bang for your buck will probably be a gpu instance in the cloud, especially if its for a one-off project.\\n\\nif you want to do longer term personal research/play, rolling you own becomes more cost effective. some people need months, not days, to whip out three functioning apps :stuck_out_tongue: it was a week :stuck_out_tongue: :stuck_out_tongue: and bibli2 took us almost 4 months to setup  :stuck_out_tongue: :stuck_out_tongue:yeah that\\'s way worse than i expected of you. shame!i am getting older little pixipentreath, machine learning with spark.  it is a bit dated but has examples in python, java and scala.  karau, learning spark.  has examples in same three languages.  same comment.  edx has a good intro course all in pythonyeah, individually, by coalescing the labeled data up from the leaves to the node in question.we are looking into whether the leaf predictions can be strengthened via the ancestors models, like done here: out of curiosity is flask recommended for production web serving of dynamic content? or are you better off with a dedicated server with a python cgi plug in?i\\'m hearing tensorcon is a *scam*, so don\\'t sign up! () \\ntensorflow team at google have confirmed they are not hosting it and are not affiliated etca tensorcon con?tensorkhaaaaaaaaaaaaaaaaan!who could have predicted that?predicting cons with tensorflow: read all about it here….do they want you to connect through citrix or something?whoa, my slack was not updating, that message was meant for haha! i have no idea actually. they will clarify for me. but in the meantime i\\'ve found my own work around :) have you connected through citrix?yeah, i have used ssms through citrix or also via an install on my own computer with a vpn. i am sure you\\'ll have some reasonable options although citrix can be really annoying to use sometimes!ok thanks! good to know. maybe i\\'ll bug if i have some issues :)hello i was wondering if anyone had experience writing pyspark processing functions and could answer for few questions? please pm me!i have! ill pm you me too, dm menew blog for data engineering! also front page of hn!:+1: nice!has anyone looked into tools for estimating heterogeneous treatment effects, like causal trees/forests?this article just came out in political analysis (no paywall). includes replication code (in r) thanks! i\\'ll have a lookwe used the `causal forests` r package which was a goddamn nightmare. it’s now called `gradient forests` or something like that. i was inspired by j. faghmous\\'s talk @dataengconf. your team? i was talking to him after his talk and he said that susan athey came out with an updated version of her code ~6 months ago that might be better, but the original version they used was a bit buggy and he would consider using it as a beta sort of thing rather than a prod sort of thing:slightly_smiling_face: i ended up posting my code on their gitter, and somebody tried it and couldn\\'t figure out why it didn\\'t work right.but then i tried this, which does exactly what i want (fitting a gaussian distribution\\'s intrinsic width as well as its mean):  yeah that’s us. we just got a rejection for that paper too!hah, go figure - astrophysics to the rescue :slightly_smiling_face:interestingi\\'m having a hack week this week and thinking about working on a python implementation of something similar to return subpopulations with different responses to a testthat’s a good idea kevin. a hacky backup which i’ve never seen implemented but should work is to split the data, go fishing on one set looking for relevant subgroups using what ever algo, then test whether the subgroups remain significant on the hold out set.what sort of other methods did you try for the first part of finding relevant subgroups?for this project we only used the causal forests packagewhat about propensity score matching?  model the likelihood to receive the treatment then estimate treatment effect across similar groups with similar propensities.  i worked with this package long ago for this and it seemed decent: about multi-level models with varying slopes, one for each group? that way you can get estimates of effect for each subpopulationhey all, wondering if you could give me a sense of where data science sits in your company. what’s the team consist of and what’s the reporting structure like (outside of the team, not internal)?under the product org, which we’re not super happy about. manager of ds reports to director of data (who also has das under him), who reports to the vp of product who reports to the ceo.: you might want to look into aws glue.  i haven’t used it yet myself, but it looks like a promising method to use spark without having to worry about emr clusters.  i believe you can set timers for jobs as well within the glue interface or via the aws cli.all good ideas --  i think in this case we don\\'t know what the groups should be a priori, which is why we were hoping for a technique that could generate the groups. i suppose hierarchical models should be able to do this as wellah, interesting - i\\'ll definitely look into that!hi all.. any tips on how to present data science to senior management so they can come back  with  business use cases? im thinking of having an overview of ds in general in our area (regtech) and showcase some algorithms with some concrete use cases..  the goal is to make management rethink about their problems, pain points, areas of opportunity and frame these problems as data science/analytics problems so we can build a solution for them.  thoughts, suggestions, experiences?thanks jeffrey! we’re currently structured under a chief strategy officer (which i like), but trying to think of a new solution as the company evolves. looking for opinions on reporting to engineering or product or ?when doing this at zipcar we found that the best way, even if it was to solicit top-down project ideas was to show examples of completed projects, and additionally suggest projects we thought would be high roi and could be solved in a data driven way. this made starting the conversation easier, and made it an overall collaborative effort.thanks emmanuel.. yeah.. i was debating a bottom-up vs top down approach in soliciting ideas. but then settled on the top-down since they their buy in is e$$ential not just in getting started but also they are influential on the making the users use the intelligent solution or the data driven product.from my experience, reporting to engineering could lead you to be responsible for building your solutions into apis and data products, while reporting to product will lead you to do more ad hoc work and experimentation to guide the product roadmap via analysis and decision support. of course a company needs all types of work done, so both roles need to exist in one form or another. we have those two roles mostly on different “teams”, so for example a data scientist on a product team might analyze data and hypothesize that a new checkout flow would increase conversion, build a prototype, run a test, then the more eng-focused team may build out a production version of that prototype. we’re a small team (2-4 currently depending on how you count), but situated inside of tech / engineering. \\n\\nthe product folks operate in their own space between us and business stakeholders. \\n\\nlike kevin suggests, we’re very much responsible for building and maintaining production systems. we work alongside engineering + software development and all report to a cto. \\n\\ncto handles the back and forth between tech and the rest of the business, including product.a note about the top 10 example that niharika, mudit and i discussed this morning. if you look at the program am listing around page 62 an 63, you will notice that the mapper class declarer a map, that maps a \"reputation\" (which is derived from the input record) to the entire record. as it keeps reading more input, the map keeps growing. whenever the size gets bigger than \\'k\\', the \"first_key\" is dropper. after all records are procssed by the mapper, the map contains \\'k\\' elements. now the \"cleanup() is the critical part of the process. it\\'s this \\'cleanup() that takes the map and emits a (key, record) so the reducer can combine all these inputs and pick out the top \\'k\\' elements. if you look at the program, the mapper class has two mthods, map() and cleanup(). presumably map() gets called every time for each input, but cleanup(0 is called after call input has been processed. the cleanup is doing what a local combner does. but it is not clear where cleanup() is getting called. it\\'s clearly not called explicitly from the mapper anywhere. it must be happening behind the scenes my feeling is cleanup() is being registered as the \\'combiner\\' method. but this needs clarification.fyi to whoever is considering a project using airbnb data this session (i heard there was someone but didn\\'t get your name), my project used airbnb data and i\\'m happy to discuss what i did and share my code, i believe someone from the first boston session scraped data from strava. is that what you mean by “running records” or do you mean e.g. world records? trying to remember who it was.:  has done something like thisdoes anyone have a suggestion for a database ide like  ?  preferably something that is free and supports at least postgres, sybase, and ms sql server?: i wrote some scripts to scrape data from strava for runners. let me know if you’re indeed interested and i can point you to them. it wasn’t trivial!does anybody know of a data-set for mooc websites that would allow for dropout prediction? i know there is a kaggle data-set but i do not want to use that :slightly_smiling_face: thank you!thank you   , i will talk to eddie. i think  also scraped strava.i have an invite for the oneplus2, if anyone is interested expires in 24hrs…goneif anyone needs i’ve got an invite.goneif anyone is looking for sf housing, my college roommate has an opening for the master bedroom of a 3 bedroom apartment in potrero hill. dm me if you\\'re interested and i can put you in touch.: i’ll be going, if you want to meet up at some point.thanks , that\\'s what i suspected. make the top level \"god-emperor of data\"and they can have  as their slack profilethanks guys for the tips!and , i will definitely pass along your god-emperor-of-date title :slightly_smiling_face: don\\'t encourage him, he\\'s been lobbying for months to use this title for himself.:slightly_smiling_face:hahaha, sorry, i will stopdoes anyone have experience plotting timelines of overlaid events such as check deposit dates and phone call dates? i\\'ve used simple bar charts but was wondering if there were cool timeline features i was unaware of.and it\\'s totally not my github profile pic &gt;_&gt; i would add that we need gender neutral titles and representations, such as data dragon  fair enough :slightly_smiling_face: hey! i keep hearing good things about plotly, but i haven’t used it myself: any postgres gui recommendations for mac? currently using sql workbench j, which is a colossal resource hog. interested in something that might be more kind to my laptop batterypostico?  i\\'ve used pgadmin3, but it\\'s kind of a hot mess too...  postico looks cool, i\\'ll check that out there are no great solutions. you have to pick the least bad one, the one that irritates you the least.hey all, domino is putting on a data science popup in sf: use `datapopupvip` code for 30% discountpostico does look like it could be a bit more battery friendly… thanks everyone!datagrip is good, if you/your company has $200 to spare here\\'s a list of clients: also, i\\'ve used valentina on linux, it\\'s halfway decent.does anyone have a favorite gui for sqlite on linux (besides sqlitebrowser and firefox)?i do like datagrip i know all the insighters at dia and co will be there.  recent ai fellows   and  are attending and have accepted papers.  insight staff    are hosting a ai careers panel with the above alumni and  .  the plan is to maybe find a time for everyone to have dinner?  jeremy is the point of contact.  sadly i will not be going :confused:\\n\\nanyone else??since the panel is on wed, how about dinner that evening? lmk!ah, nice. i\\'m here with 22 of my colleagues so it almost feels like regular work days! i\\'d love to join for dinner if you figure out a time.i’ll be there and would love to meet some other fellowsmy coworker and i will be there.  would love to join.ha! i floated a very similar idea for my project, but ended up doing a consulting project instead.how about wed before the panel?when is the panel? i don’t see it on the schedule on the nips websitethe plan was to have an insight sponsored drinks and apps after the panel if people are downpanel is not on schedule. it\\'s 7-8 pm on wedsanother decisive victory for alphago! 3-0de people its a must read, i keep revisiting this article for perspective on data engineering. its simply awesome, talks so much about basics of de.the robot just has no idea who he is messing with. :wink:jimmy v. nuggetbot is a match i would watch. jujitsu that talking robot into submission!that robot has no idea how a good i am at dismantling things… and how terrible at putting them back together uploaded a file:  and commented: an astronomer colleague of mine sent me these job postings to share around. if you have an astronomy background and weren’t 100% sold on leaving academia in the dust, maybe these roles are a nice middle-ground.: dust or _interstellar  dust_? (sorry, bad joke; and possible one that doesn’t make any sense!)hahahey everyone! does anyone here have experience deploying a genomics web application? i have some questions about cloud service providers and data security.hey new york folks, my former boss is hiring a data scientist for the growth/marketing team at better mortgage.  happy to make introductions and pass word along.  job posting here: heard that was a neat company, also ^just mentioned in the  channel -  (techstars alumni) are looking for person 3 to join their company and lead their ds and ai efforts.  if you know someone who might be interested, please let me know (slightly more info in the channel) does anyone have experience applying gans to problems other than image creation? i\\'m curious to hear about other applications. here we go!uh oh :smile::fire:quick, clean your desk!this is my favorite part of this slack team!!*so it begins* i guess u can use them for clusteringget your announcements in everyone!at least it was here, not channel … ?don’t forget to sign up for alumni mentoring and in a couple weeks mock interviews!^stay tuned for more lightning talks and awesome alumni events in the near future!, if you\\'re not already in on it - there are a variety of (strong) opinions about using the at here or at channel tags, opinions which are generally re-aired whenever someone uses one of themdon’t forget to cheer for the vikings you have disciplesyeah, i slipped up and used our slack approach from work :papologies to the hundreds of you who just got spammedit\\'s never bothered me, fwiw better add at here to your apology to make sure everyone gets itseems like at here gets less response than at channelseems like a testable hypothesisbut i\\'d have to try it multiple times to get a big enough sample*psa*: go to `notifications preferences` in `channel settings` and tick `ignore notifications for at everyone and at here`here is for active users. channel is for all usersi believe a slack admin can limit usage of both at here / at channel through this page: should at this message :slightly_smiling_face:be my guest :laughing:has anyone worked with amazon sagemaker for deploying custom algorithms? any thoughts on it generally?has anyone worked with incorporating distance based correlations into a model. (for example, i\\'d expect data from people living close together to be more correlated.) or even just encoding zipcode/county/etc data into a continuous variable which maintains their distance. if this is for a project, try to keep it simple at first and use an average of the neighbors as a feature. if it ends up being an important feature, you can go back and change it to an weighted average (i\\'d recommend an exponential weighting) where the decay rate could be a hyper-parameter.i\\'m doing salary prediction for medical specialities per countyi have a lot of data from bls for most combinations, but i\\'m trying to fill in the gaps with prediction basicallyso i was thinking of doing kriging with the exponential model, but i wanted to also incorporate other covariates(from census data etc)which is why i am thinking of how to actually model it by incorporating correlation structuresif i\\'m reading this correctly, you want to predict salaries for where you don\\'t have enough data.\\nif that\\'s the case, devise a distance metric (for actual distance, population distribution, urban/rural breakdown, dates, etc.) and get the most similar cases to the county in case (these are your neighbors). use these to predict. you can later go back and apply an exponential interpolation using kriging or another method you desire.you mean just take a mean of the surrounding counties?the problem is only having mean data per countyso if i localize my models the sample sizes are going to be minisculeyou can also take a weighted average per state, or even by region. try to run a preliminary model with all of these averages and see which one might have a higher predictive power.\\nalso, remember that the geographical distance is only one of the distances you can incorporate.what do you mean?what other distances?i have been using a gan to create fake structured data. but will have a blog post out about it sooni mean how apart counties are to each other in other metrics. e.g., by population wealth.\\ncertain counties could be very similar, despite being far apart.oh yeah those are the covariates i would be including, which is why i didn\\'t want to use exponential smoothing and simple metrics like that.hello! i\\'m using convolutional neural networks for real-time emotion recognition from videos. i want to display the video in a web app while my python algorithm labels the emotions in real time. currently, i\\'m thinking to use the tensorflow object detection api with either flask or django. has anyone done anything similar? does anyone have any recommendations for platforms/tools to use? thanks!hi, i am trying to download a db share from onedrive, and it is 67 gb. i setup an aws account for it. i tried using curl but it concatanating the file instead of downloading which had worked before. does anyone have any suggestion on how to download a directory from onedrive to a aws volume? data engineering fellow in ny, pamela wu did something similari think i resolved it. thank you! :slightly_smiling_face:hopefully this is relevant: cool thanks!no, i did not. is there anyone to chipin?anyone have experience with  or something similar?yes, i tried that .. it was not great to be frank. i will suggest jupyterhub instead of this. but at the same time, it might fit your requirements better than mine.: we use it internally extensively, it fits the team need very well - we can easily share results with business partners, and keep a record of all the important past work.  haven’t used other alternatives, so can’t speak to a comparison there,a plus to knowledge-repo is that, one of our alumni actual is a main contributor to the code base!congrates on expanding to seattle, insight! some advice on choosing where to work: thanks josiahmwalton , stitch fix is killin it lately with the blog postsdoes anyone have a recommendation for a high-level crash-course or a good pedagogical textbook on quantum computing and algorithms? my phd was in hep experiment, so i\\'m starting from about halfway through peskin &amp; schroeder.still hard to beat nielsen and chuangnot an expert but at a high level data modeling looks like object oriented design without methods.  try stanford lagunita\\'s database class with jennifer widom.  it has a section on data modeling.  for big data specifically, kleppman designing data intensive applications has a couple of good chapters.  what do you guys think of the topic of this session   do you think it’s useful for implementing compliance to gdpr?using blockchain? :wtf:i see. so if you roll your own, then you’ll most likely want to automate the deployment process. be able to spin this up from scratch. pop in and out a hosts with 0 downtime. if you do require downtime be sure that you have multi colo replication so that you can do upgrades or what not to each cassandra cluster in each region without affecting data staleness or availability. for automation you can look into chef, puppet, ansible (config management) for provision of hosts you may need to write your own plugin for terraform. monitoring you can farm out if it’s easier, but look into what metrics cassandra provides already and which vendors they are compatible with (nagios, prometheus, datadog). if you’re plan is to move to aws alwasy keep in the back of your mind, is it worth it to build something specific to your datacenter or generalize it, so that it’s minimal work getting it onto aws.haha i kind of thought so too..our data science team at cz biohub is looking for someone to help us use phylogenomics and machine learning to detect, prevent, and understand infectious disease.\\ndynamic, collaborative, and supportive team.\\n\\njob posting: does anyone have experience with creating a churn predictor?  i’d like to bounce some ideas off of you!: i do. what do you want to know?same. feel free to ping me.same!does anyone know of a good tutorial, book, course or videos that  teach time series analysis. i understand some of the basics but definitely want to grow in that area.: ask :  is a good introduction to a lot of the standard methodsbut i wouldn’t recommend reading up on arima until you’ve also read this blog post: i have a really basic regular expressions question. i have a column with mixed entries some of which are people\\'s names and some are strings of mixed numbers and letters. i just want to filter out the names and tried this names = lambda x: re.findall(\"[a-z]*\\\\s[a-z]*\\\\w\", str(x))but it returned letters in the mixed cells as well: i recommend one of the online regex tools (like this one which also explains the expression you enter: ) to test your regex with some sample datathanks! i may have just solved it but will need this in the future!!another question, has anyone dealt with columns with empty lists in a dataframe? i want to replace them with nans but am having some difficulty. thanks!oh solved it!: also, if you\\'re not interested in time-series forecasting but more on the inferential side of things (same base principles, different applications and practice), you may want to check this out:  \\nthis one also has a full chapter on time-series () digested here too (): we use “quantitative forecasting methods” by farnum &amp; stanton, pretty decent and focuses and data mining aspects.\\ndoes anybody know someone who has been through hackbright? or hear anything positive/negative about the program? wifey is kinda keen to explore it..: insight is hosting a happy hour in la tomorrow evening. if you’re in the area, dm me! i’d be happy to get you the information a recent blog post by someone at my company may be of interest:   i am looking at it.  thanks! as far as i know everybody is fine, we did hear the bang in the office last nighthi guys -- does anyone have experience building bokeh apps in python, and would have a few minutes to chat?we have three positions open at travelers for those experienced in software dev in the ai and text analytics space. they are looking to fill these positions asap. be sure to mention that i referred you.   go to  and look for job id 9881br and 9560br hello! we\\'re looking for a data engineer who\\'s worked with big data systems at scale (aws, hadoop, spark), preferably with some experience deploying machine-learning models. if you know anyone, please dm me!\\nmonty python for me :wink:fyi, :cvs: is hiring a data scientist (~3+ years experience, so senior-level i guess) to lead some initiatives on the pharmacy operations side. the position is located at our corporate headquarters in woonsocket, ri. i\\'ve been doing data science work here, in support of the retail side of the organization, for six months now, and so if you have any questions for me please don\\'t hesitate to comment or dm me. \\n\\nhere\\'s the job posting:  if i remember correctly, 1 week of insight = 1 year of experience, so everyone from insight should be good!new principle of relativity when applied to insight?cheers!austin and i just posted a blog on cassandra that’s featured on hackernews.  it features fellow projects and code on how to run queries and setup cassandra with spark:\\nlink to the entry: &gt; we also got a great introduction to nosql and cassandra internals from insight alumnus mike grinolds and patricio echague, director of engineering at relateiq and a committer to hector - the java client for cassandra. he also discussed data modeling for cassandra, which requires a different approach than relational databases like mysql or postgresql.\\n\\nthat paragraph needs a rewrite.: thanks, it’s a bit of a run-on sentencefwiw: i guess the point would be not to make it faster for them but experience how it is to live like regular citizens i\\'m a data scientist working with economists.  does that count?i also know a guy who works as an economic fellow at the fda pm me if you want me to make a connectionthanks! i’ll dmknowledge graph @ apple is hosting a dbpedia community event: drop by and say hello! happy to answer your questions about what its like working at apple :slightly_smiling_face:for those in nyc and interested in causal inference : never mind. i got it to work. has anyone worked with facebook\\'s graph api to track daily social media engagement? so far i\\'ve pulled daily video views and timestamped comments, having some problems pulling daily sharedposts or other proxies of daily social media engagement (most fields can just return aggregated values for the lifetime of a post)so they’re including bonus and equity for employee compensation, but only include base salary for ceo compensation? seems highly misleading.even if that’s total compensation, where i come from that’s still mucho dinero.hi! we are hiring out our data science team at thirdlove. we are profitable/rapidly growing, just hired kim larsen - formerly stitch fix/uber - as our vp of data, and we are moving into a brand new building in sf next week.  if anyone is interested let me know! ( )anyone found a good forecasting library in python? i\\'ve seen prophet, wondering if there\\'s anything else. particularly interested in adding external regression variables to an arima. feel free to dm :slightly_smiling_face:hrm, i now see statsmodel has an arimax with `exog`. anyone have experience using exogenous variables in this way? completely agree. just pointing out that, unfortunately, in many organizations the structure doesn\\'t guarantee a project leader with the skills to advise on technical roadblocks. this is different from an academic setting, where almost by definition, your advisor has more experience/expertise and can steer the project in the right direction.strategic incompetence colin. :wink:anyone have any suggestions for picking up scala/spark? tutorials, courses, books, etc.there is an interactive tutorial with dean wampler called “just enough scala for spark”. if you don’t have a safaribooksonline account, you can use a free 10-day subscription to check it out: \\nthis is a great book, but not very introductory: because i know everyone asked this question every day of insight: are any insight fellows at the spark+ai summit right now?so what do people say about the whole github issue. for some reason most of the people around me already started migrating to gitlab ….  are going from my team! there are other insight alumni from intuit going as wellerrr…  is going, nick is on vacation. :wink:\"microsoft will need to really mess things up for me to want to switch\" said our team leadi see people saying “oh there going to pull off the same thing they did to skype and turn it to a crappy product” and others saying “this is a good thing, nadella has definitely been doing a good job bringing life back to the company”  \\n\\ni don’t use much of their products really (except linkedin + now github!), but their reputation + adoption seems to have improved recently.we had engineers planning the migration to gitlab out of paranoia, that conversation was shut down pretty quickly by our ceo ^we have two data analyst positions at rovi  in boston. if anyone wants to apply please send me your resume: .  i can confirm that it\\'s a great place to work. we build an analytics product for television networks that predicts their viewership and optimizes promo and ad placement. we get to work with big data, do machine learning, and all sorts of fun data sciency things.also, feel free to pass this along to other good candidates you may know. the position is entry level so for someone who has a b.s. or a m.s.hi all, does anyone have experience working with mobile phone datasets?  i’ve seen some interesting things come out of academia, but seems hard to obtain.ive heard there are european datasets that are obtainable, but never fleshed that outhi guys.. any data engineers/architects/scientists  in the health care field?  would love to chat about prod issues that you might have encountered in the day-to-day job.&lt;--question to data engineers about “staging data” in etl process, due to technical complexities we can’t have a day behind refresh for our staging databases. i am thinking that having refreshed data is not necessary, and there must be methods were we can test by randomly sampling data? i guess in general, i am interested in knowing how de’s go about testing their etl in staging. thanks!yeah, random sampling sounds like a good idea.for instance user 2017 conference live-stream link and schedule has anyone ever used coursera’s tool called dataducthi  thanks for the reply, when you are testing do have a suite of things to check off?or are you mostly just throwing the whole data set? in the context of datawarehousing.i also think the snapshot operations are not available to us, as i think our ops team would restrict us from doing that.what are the pieces in ur etl? mysql-&gt;redshift?combination of mysql, and oracleto s3, then s3 to redshiftuseful guide for effective t-sne viz: man, that thing has spread like wildfire across the ds worldfood for thoughti wonder which category insight would fall into… i also think that the people who do moocs might be very well practiced in answering technical questions in a similar format to . assuming they did the homework, this would mean 10-20 “practice” questions.i think everyone would like likely benefit from  practicing on  :slightly_smiling_face:the blog makes me think of claims like “people who buy baby books are good parents because they care enough to buy baby books, not because of the content of the books\"* nah, i\\'d rather coast on my sweet pedigree of the university of iowa. who needs to study and practice?yeah, i don\\'t quite believe the causal implications of that lost*posti don’t want to minimize the importance of wanting to succeed and taking action toward that purpose, it just seemed to completely ignore an equally, if not more, important (and very well studied) factor of practice makes you better at things.for the physicists: \"don’t underestimate the power of giving people a common platform and operating language.” \\n uploaded a file:  and commented: if anyone is looking for something different, general assembly is looking for a full time instructor for their new data science course in san francisco ().  they are also looking for people in the field to give lunch talks and work as tas after work hours.  if you’re interested in any of these things you can contact teresa todoroff at .feel free to spread this around.#makedonalddrumpfagain :smile:anyone going to this tomorrow night?\\ngosh i love 2fai think i\\'ll refrain from resetting everyone\\'s password herebut i encourage 2fa:tada:hi! we have an opening in our group at jpmorgan for  senior data engineer specifically with hadoop and spark expertise. feel free to pm if interested or know somebody.. :grinning:anyone have a good javascript tutorial for crawling websites? i\\'m working with data that\\'s stored on a vendor portal, and it would nice to figure out how to crawl the page and download each file without having to manually do it myself. :stuck_out_tongue:dk about javascript but i’ve done autodownload scripts in python with `requests`i did something using `requests` combined with `selenium webdriver` to go past javascript queries and reach specific pages i was interested in scraping. if that\\'s what you\\'re interested in, let me know.never crawled with javascript before, are you talking about scraping a page w/ javascript.  if you want to run full fledge crawler, scrapy and nutch are both greatwas talking to a consultant, and he says many companies just turn on a ec2 node &amp; have schema registry be hosted there, and producer, lambda, and consumer can all talk to it if it’s in the same vpc.after speaking with 50+ top applied ai teams, we wrote up some tips for academics who want to transition to applied ai roles. if you like the content, could you \\'applaud\\' the post on medium and share it with people who might find it useful? sort of.\\n\\nfor some context: the data i\\'m working with is parking meter data (how long a person parked at a meter, how much they money they spent at the meter).\\n\\nright now, the vendors for the parking meters have the data, and make us go through a webportal to download the data.\\n\\nthe portal asks for a the meter id, and a date range. then it will pull up all transactions for that date range.\\n\\nthere is a download button that creates another popup with some radio button options for how to dump the data. one of the options is just to simply dump it as a raw .csvi might have to pick your brain at some point.pretty amazing deal even if you\\'ll only use a few of the books.i assume it\\'s just the e-versions?yeah, just the ebooks (pdf, mobi and epub). though you also get to donate to a charity in the process, you can even choose to have all the money go to charity.perhaps doing the scraping with python\\'s scrapy would be easieris there a specific reason for using javascript?  would be happy to answer any dataproc question that you might have. my sister-in-law just started a new job that is a bit more ds/coding heavy than her prior post-doc and is looking for a good bash tutorial - any recommendations? this tutorial covers everything you would want to know as a beginner/intermediate. it is paid, but you can get a free 10-day subscription.  can’t compare it to anything else, but this book (you can download as pdf for free) was helpful for me: thanks  and !nice podcast of interview with  \\nhas anybody worked  with datasets, where number of features is growing as you do ml?\\ni understand that it is kinda like missing data handling. but in this case missing data has a very clear pattern to it, so many techniques won’t work.\\ni couldn’t find any discussion about this topic.anyone have book recommendations for a ds to learn user research methods?has anyone worked with pachyderm?like ux research?alums: how is data documentation handled at *your* company? can anyone share best practices? where do you store documentation (dropbox? github? google docs?) do you use version control?  what is this \"documentation\" you speak of? :stuck_out_tongue:actually, yeah. what do you mean by documentation? metadata? provenance? uses? change history?all if the abovelet\\'s with data dictionaries (metadata) and we would like to track updates to this documentation. *let\\'s start withi\\'ll just sum up our practices with: :disappointed:we actually don\\'t have a great way of tracking \"raw\" data, but i think that\\'s partially our nature as an org that does something completely different every 4-6 months. for \"raw\" -&gt; \"results\" tracking we use sacred: we have a similar problem to  in that we\\'re a consulting company constrained to whatever our clients have in place. i have yet to see an org do it well thoughstay positive higginscstuff it nuggetboti will say i have seen several startups that promise to help you do it; i haven\\'t found them to be great either. :confused: often it is a lot of overhead, and forces a major change in your work flow.we\\'ve had great success with mkdocs for both general and technical documentation.  super easy to write new pages (it\\'s all markdown) and if you put your site on git then you\\'ve got both version control and easy access for your whole teamanyone use readthedocs? just linked there from sacred, looks coolhost your static mkdocs site on an aws box (or your favorite server) and bam! documentation that anyone can access over the internet :) yeah, we looked at readthedocs but decided mkdocs was nicernice. thanks!  i’ve used readthedocs for academic code i wrote, was easy to use. also creates documentation from your python docstrings which is nice. i imagine mkdocs can also do that aswell.for both mkdocs and readthedocs, can these sites be private? can we set up so that only team members part of a bitbucket (private) repo have access?no idea, sorrydefinitely. all mkdocs does is create static web pages, so the hosting is up to you(it also has a built-in server, but that\\'s more for interactive documentation writing) gotchathanks all!hi all, now that slack has \"thread\" features, can we start using that more proactively in ? i find it hard to follow multiple simultaneous conversations some times and threads are a nice way to keep each conversation modular. please give it a try!agreed. the drawback is the lack of visibility for things that might be of interest to the whole channel. i also encourage the usage the checkbox below to post back to the channel when appropriate.hi insight! we are hosting am r-ladies meetup at flatiron health next tuesday. stop by! current insight fellow in the news!awesome article hey sf folks (although there are also satellite locations in every other place insight has programs), i co-organize a free event in town called astronomy on tap bay area and our next event is on feb 21 at dna lounge at 7:30 pm. it consists of 3 presentations from astronomers, an astronomy in the news segment (hosted by yours truly), trivia (with prizes), and free giveaways. no science background is required and all ages are welcome. because space is always better with beer! for any of you who moved cross country, can you recommend a good mover that loads/unloads. google uses ace relocation for their new employees. i didn\\'t look at the prices, but we had good service with them. data science conference at stanford via pat.r.callier: *”become awesome at technical interviews”**\"practice interviewing with engineers from top companies, anonymously. get better at algorithmic problems, find a job, or just see if you\\'ve still got it. it\\'s like chatroulette[1] for technical interviews.”*[1] but without the dickspretty great footnote: depends on which meaning of the word.i think it works in the senses i can think ofit was soooo awesome!!!! one of my fav conferences everis this a rogue, rabid ai trying to eat us?just make sure no one gives you demonetized 500/1000 rupee notes. when in doubt as for the new series notes.  old series:  new series: : sorry i missed this thread earlier. feel free to message me directly if there’s anything i can help with!when you’re debugging why commands don’t work, it can always be helpful to type “which &lt;cmd&gt;”such as “which wget”the result shows you the binary which is executed for the commandif you see nothing as an output, it generally means that the directory containing the binary is not in your pathi.e. $pathif you just installed something in a shell, but you didn’t reload the shell environment, you can always try opening a shell (which triggers sourcing files like “.bash_profile” and “.bashrc” which modify your $path variable to include searching for stuff you just installednot sure who to ping about this - but when posting a link to insight websites on pages which produce automatic url image thumbnails, sometimes it seems to just pick this one fellow’s image as the thumbnail. not anything wrong with that, i guess, but it might be worth investigating how to set up the page so that a representative image is chosen instead? just fyi. is this your department?just copy/paste a link to each program into a linkedin chat - you can see the behavior.yes, i’m not sure why that photo was randomly pulled from all the ones on our front-page (though i’m happy to feature alvin). i’ve spent some time trying to get this photo uncached, and replacing our social cards and opengraph profile, but never invested enough time to fix this. will look into it more - thanks for the reminder.thank you! i am originally an ubuntu user, and \".bashrc\" was coming built in. looks like, there are quiet a lot of differences between 2 systems. i fixed the bash &amp; path problem with installing brew (which was not straight forward due to the new mac os and new updates); and i was able to install the problem i was originally going for after. so far so good, up until the next problem! :slightly_smiling_face: thank you for all the replies. i appreciate it.how do those headphones even stay on?velcrodj kitty has commitment to his arrartnuggetbot: weather palo altoweather for palo alto, ca, usa\\ncurrently: partly cloudy 21.1°c/70°f\\ntoday: mostly cloudy until tomorrow afternoon.\\ncoming week: no precipitation throughout the week, with temperatures bottoming out at 14°c/57°f on saturday.nuggetbot: weather nycweather for new york, ny, usa\\ncurrently: overcast -3.5°c/26°f\\ntoday: light snow tomorrow afternoon.\\ncoming week: mixed precipitation throughout the week, with temperatures bottoming out at -4°c/25°f on saturday.nuggetbot: weather bostonweather for boston, ma, usa\\ncurrently: overcast -6.3°c/21°f\\ntoday: mostly cloudy throughout the day.\\ncoming week: mixed precipitation throughout the week, with temperatures rising to 6°c/43°f on wednesday.nuggetbot: image me data scientistanyone know a nice, simple, pythonic way to grab street maps by lat lon which dont look gross when plotted over?: you can use geoplotlib to overlay maps on top of street maps; using the boundingbox method lets you specify the (lat,lon) of the corners of the map, so you implicitly specify zoom as well. you should be able to just call the map, without any overlaying pieces as wellalso, more for  thanks for the rec! someone lost a phonestay positive thang and  : thanks for the help guys. for now, it seems like i have a good enough fix. will check sunlight if i need  better quality data.a good small tutorial on how to use flickr\\'s api using a python wrapper! a good tutorial on reading tweets and doing some natural language processing on them via python: flickrapi is great; it\\'s what i used for whereto.photowhereto.photo is great.  i wonder what happens if i try to find pictures of \"yoga\" in sf?or maybe i want to go to a \"fair\" in soma.inappropriate:disappointed:now i\\'m going to get firedi was going to let the inquisitive minds find it for themselves.  no need to make it easy for them.news-reading habits dataset: lol  your hot dog post disappeared!cia take it down?:wink:: for those interested in scraping data, this is a very easy to use tool.. just create an account and click buttons. no code required.\\n the example they use looks very apropos for  cool good read: looking in to it now - also conversing with  channel creator,  \\nwhat could go wrong with 500+ insight fellows having access to the api?  :wink: anyone interested in going to a deep learning/tensor flow meetup in sv?just signed up. thanks  !insight will be at strata this week! looking forward to seeing you all there! hey fellows! if any engineers are currently looking for new opportunities, my company rally health is aggressively hiring software and data engineers for our core team! here is my referral link: \\npositions for data engineers aren’t currently posted, but if interested please let me know and i can refer you in person. thanks! for 2016 you\\'ll have to scrape it from the secretary of state on every state, or ap\\'s election service api  not sure if still freeanyone using zeppelin with spark? or just using zeppelin in general i suppose?i\\'ve got the team here at placeiq going on it and i\\'m finding it pretty decent if you need a notebook that works well with yarn(we had too many problems trying to get jupyter working)we’re doing some work on azure, and you can configure jupyter notebooks that connect to a cluster thereso we connect locally and it uses yarn in the backgroundthough i do want to get into zeppelin at some pointi’m also supposedly learning graphx — have you all worked with graphs as well?i don\\'t know exactly the issue we had with jupyter on yarn but i think it was around user queuessince politics are part of that channel...here is an interesting article worth sharing about false info. super fun! neural nets recognize your drawings...   other onesreally cooldamn, quickdraw would kick my ass in a game of pictionary :stuck_out_tongue:stay positive ramalldfany headless-browser pros out there know how you’d go about building a crawler that can tell you whether a given site has a mobile-optimized version?well for crawlers i like splinter hey all: does anyone have any good resources for marketing data science (eg attribution analysis, customer segmentation)? looking for more methodologies than cheerleading (finding a lot of the latter)liverence:  papadimitriou.c: awesome! thanksi can share a bunch of academic papers with you via dropbox if you\\'re interestedmorning, are there alum working on online grocery delivery business? i’m working on a similar insight project and am eager to learn about this business. your two cents will be my precious gem! thanks! :slightly_smiling_face:hey jeff, i used opencv in python to get frames from a video for my insight project. you can see it here. warning though - really poorly written code: i am doing an insight project to identify trolls on twitter. i have some data and i’m pulling from the api but if any fellows have twitter dumps they would be willing to share, i would be grateful! i am particularly interested in @replies. i hear you might  … i believe had a twitter dump? has a twitter dump too: nicole wong () from my cohort did a similar project, might want to ping herthanks!i didn\\'t cache any tweets so unfortunately i don\\'t have any twitter dumps. happy to chat abt any other q\\'s you might have about the project tho! i have grabbed an additional pile of tweets that i’m using now so i may be close to having what i need. i am curious if you did any sentiment analysis. i’m going to be working up a model to classify users as troll or not.does anyone have experience extracting information from pdf tables (of a range of different formats)?kevinmercurio: yes please! email is i\\'m not claiming it\\'s comprehensive or well-organized, but there is some content :wink:willfrost: it\\'s hard. i used image processing techniques to clean up the pdfs first and then ocr (tessaract) to read in the datai used  for a small personal project and it worked well except that i had to manually tweak its automatic bounding boxes to get the last rows in my particular tables.  thank you! i will have a look into these options.hey kevin, mind sharing those with me too? email is i used pretrained vader sentiment analysis, it was pretty good imo. it was developed specifically for twitter and social media analysis. a friend of mine made this recently and it was a hot topic on hacker news. i haven\\'t used it though and don\\'t know if it will suit your needs. if you come up short i may be able to dredge up a tweet dump from awhile back but i am unsure if i archived it properlyplease feel free to contribute more :slightly_smiling_face:if you\\'re familiar with bash, i\\'ve used pdftotext with a combination with sed to get information from very old pdfs with tablesjust use `pdftotext -layout -eol unix &lt;file&gt;` to get the pdf translated into a text format that you can later filter the information you wantyou might want to check facebook as well as you can get the post sentiment and reaction built-in via likes and the sorts. especially for facebook posts from news sites and other public-oriented entities.  might be able to help.always interested in learning more. share away at \\nlatest that came to my table was :+1: for opencv. i\\'ve used it in both python and c++ for image analysis and features recognition, and works great!\\nalthough if you are only looking to get particular frames from a video, there might be simpler solutions to it. e.g., vlc is able to save any number of frames every x frames. it allows you to do it graphically or via terminal (as ffmpeg)have you tried fuzzywuzzy?maryamtaba: insight pds should have some example datasets and i are planning to go climbing at brooklyn boulders somerville, ma tomorrow at 11 am. anyone interested in joining us? check out stephaniemari: will do. thanks! i don\\'t have a twitter dump (yet), but just wanted to mention that i\\'m also going to be doing a twitter project that will (in part) involve sentiment analysis. so let me know if you want to swap thoughts/insights (pun intended) over the next few weeks. that would be great! maybe we can find a time where we both are awake and free next week and do a skype call? actually, i’ll message you and share my week1 files…cyucheng: no can do for this weekend, but i\\'d love to get back to it. my climbing gear has been collecting dust :cry: i have a blood feud with bkb.  come to metrorock instead :stuck_out_tongue: mind sending the link to ?thanks everyone! i was able to get what i wanted with moviepywould love to have a copy too. , thanks kevin! last time i listen to a ground hog...the snow/slush is starting to accumulate. at least in parts of jersey.^ that pic in the article is from a south indian movie “enthiran” (robot in other languages). i was hoping for some reference to it and found this instead: uploaded a file: omg what could this mean??(former) theoretical physicists out there: there’s some string theorists down the street (in sf) collecting 3 million dollars a piece:\\nhey all, i’d love to have some honest first-impression feedback about my resume from fellows in the industry. i’m not asking for a detailed analysis, just your quick opinion. pm me if interested, it’d help a lot. thanks!!is anyone familiar with negative dependence/chernoff bounds?does anyone have experience running pyspark jobs on an emr cluster using spark-submit?  i have a job that hangs indefinitely in \"running\" mode even though it should finish in about a minute (which i know because it worked ok inside of a jupyter notebook) and would love some advice on how to debug...maybe  \\'s territory? is it possible that another job is being submitted to the same cluster? i imagine that amazon emr will split the resources if there are multipile though. also see if you can get any sort of logs our of emr that may point to the problem. possibly permission issues accessing data.another austin! :wink:#replicaskubernetes pods of austins?sure!thanks everybody for your help!glad to hear it! could you expand on what you mean by random shuffling and what it helps with ? very curious about that.anyone at strata?  if you’re here come by the  booth :slightly_smiling_face:sure. you simply do a random shuffle of your test labels (`random.shuffle(y)`) and then compare against the unshuffled labels. this will give a worse overall accuracy, but it gives you a starting point for f1 scores for the minority classes. it is essentially choosing randomly given the relative number of each label in the dataset.hi there! any de\\'s mindd giving a noobie some advice? i think most of the de channels are kinda dead, but these seem active so i thought i might try my luck. specific questions get specific answers! try it out :slightly_smiling_face:i didn\\'t want to spam the channel w/ a convo, so i was hoping to just pm a couple people with specific questionsnobody knows if they\\'re the expert you need unless you describe the expertise you need!is anyone else a data engineer at a startup w/ less big data needs creating data pipelines from the ground up?i had some architecture ideas that use big data frameworks, but since there aren\\'t foreseeable scaling problems, i think in some regards they\\'re overkill, but as far as integration w/ other software they might be a good fit despite maintenance overhead / cluster management.mostly because glue has transforms that can translate into spark calls, andd those spark calls can be edited later as requirements change.that’s very cool, i’ve never encountered that before! do you happen to have any references to the method i could look up to dive deeper?whoa... will be checking out right awayiqvia is looking for a tech-lead for our data science platform. if you are a ml engineer or data engineer looking for a larger role, we would love to have you on the team. dm if you have any questions about the role or the company.\\n\\nrole is in philadelphia or possibly anywhere in the northeast  (remote)i don’t have any great references specifically on shuffling :confused:.  i originally saw the idea of shuffling in the context of bootstrapping p-values for hypothesis tests in mine cetinkaya-rundel’s statistics course (e.g. section 1.8.2 in [1]). \\n\\nfor a comparison of uniform random guessing, weighted guessing (which is very close to shuffling, but would be based on your training data distribution), and guess the most frequent; see [2]. also sklearn’s `dummyclassifier` can easily do all of these methods [3]. i suppose if you wanted a weighted random baseline, only predicting based on the training data would technically be best practice (and now that i know about `dummyclassifier` maybe that’s what i’ll do in the future). shuffling is just so quick\\n```\\ny_shuffled = y.copy()\\nrandom.shuffle(y_shuffled)\\nprint(sklearn.metrics.classification_report(y, y_shuffled))\\n```\\n\\n[1] \\n[2] \\n[3]  posted a nice tutorial:  fwiw, i\\'ve had good results forecasting thousands of time series using the `spark-ts` package. i think it\\'s available in pyspark, so low barrier to entry if you don\\'t work in scala. :wink:we (salesforce einstein) are looking for senior data engineers with scala/spark experience. see posting here: \\nfeel free to contact me if you have any questions!anyone here use jira to report bugs and track tasks? we currently use asana and very much don’t like it… we use jira for task tracking for data and engineering folks at my shop, asana for client facing folks.jira is also horrible but i don’t know which is worse since i haven’t used asanai\\'ve seen a shop that used a combination of github, trello and slack integrations for engineeringthanks for the feedback!if anyone is looking for an apartment in an excellent location (greenwich village), we are looking for someone to take over our lease sometime in the near future (trying to buy a place). no broker fee is pretty cool! or if anyone knows anyone... tell that someone! was looking for somewhere i think?anybody here going to data engineering conference in bk today and tomorrow? will be there! as wellyay! hope to see you there :slightly_smiling_face: i’m there now! hhh also is here! as well as jinchao and practically the entire capital one team! nothing special, ease of use mostly.  just want to play around with streaming and processing. wasn\\'t sure how easy/hard it was to get going with different brandsthat apartment is super cute, and on the plus side makes me feel less bad about my rent in silicon valley! :smile: best of luck renting it out! i think it shouldn’t be too bad. when you plug in the webcam it typically registers it with the computer as a camera option. you just request a frame from that device. maybe borrow one and try it out?stay positive jeremykarnowskii\\'ve had problems connecting to the internal webcam on macs with opencv in the past, but this was using the c++ interface and an older version of opencv (you using 2.4.13 or 3.1?)it seemed more stable in linux (which was a surprise).. sometimes it depends on the webcam manufacturer.. some high performance usb3 webcams are still problematic due to driver issues on macs.any experience on using arangodb. or are there any other graph or multi model dbs you can suggest?  any war stories on neo4j and orientdb?neo4j is really great in my experience, i can’t compare it to other graph dbsbut cypher is real nice for sure uploaded a file:  and commented: selfie courtesy of alihello all! does anyone have experience with domon dashboards here?#domopossibly i’m supposed to learn the capabilities as well :wink:this must be nuggetbot’s finest work. these days your isp is almost 100% determined by where you live. are you going to be in sf? nyc? at least here in nyc your options are either cable from time warner cable or fios from verizon (but only if it is available for your building). i would probably lean toward fios if you can get it but it is really just picking from the least of two nearly identical evils.maybe, (hopefully?), some other insighter has some other secret knowledge to jump in with?ah... in australia everything is federated, so one has a lot of choice.i\\'ll be jumping into the sf area, probably around san mateo... who are serviced by comcast. is adsl comparable in performance? more federated?(thanks btw)there are a few recent start up isps that have been trying to compete with the likes of comcast. i don’t think any have made it to nyc but hopefully the story is different in sf. i haven’t seen any talk of adsl in years so i think it is mostly dead in favor of cable or fiberah... it\\'s interesting that that is almost the opposite of the situation in australia.cable ultimately seems to have fizzled in favour of adsl / nbn (fibre)anyway... thanks for the insight.:wink:: might help: my rough (and highly opinionated) summary of the situation in the bay area is as follows:\\n\\n- comcast is the fastest and most evil\\n- at&amp;t is slower, but less evil\\n- wave is not so evil, and pretty fast\\n- sonic is not evil, but often just resells at&amp;t so the speeds are comparable\\n\\nsonic is the only one in the area without datacaps(i use sonic dsl in mountain view and get 22mbps consistently, which is what they advertise at least, but also as fast as they offer down here)thanks alex! looks like sonic is the best choice :)new insight data science blog post! check it out when you get a chance. eli talks about the need to look at your data as your very first step.  has anyone used scikit-multilearn with text data? hi, has anyone here had the experience of installing a virtual box on windows machine ( windows is the host)  and then using cuda-gpu on the linux guest platform?! i\\'ve had difficulties installing cuda on ubuntu! you likely need pci passthrough working, which tends to be temperamental. are you on windows 10? can you do the things you want in bash for windows?\\nits not possible i think. i use virtualbox on windows 10 (host) and run ubuntu as the guest os. the gpu is not fully exposed to the guest os. for example, the maximum gpu memory you can assign to the guest os is 128mb.your best bet is dual booting or using cuda sdk along with conda on windows. that’s true.. i shall perhaps think about ways to make comparisons with this. thanks for your advice :slightly_smiling_face:has anyone worked with trying to detect anomalies in time series data, or, for example, trying to detect statistically significant differences in a metric across ~100 ab tests where you have daily measurements? #multiplecomparisonfunhahaha if only there were such a thing! :parrot_mustache: :beers:thanks. i am on windows 7. i\\'d like to use pytorch, but i should have linux installation for that.thanks nishan, i think your suggestion of dual-booting is the most feasible solution for my problem.sheareraj: wut! that corgi is amazing... this is from 2013 :slightly_smiling_face: :hourglass_flowing_sand: what income on the side???  show me the :moneybag::grinning:greetings, i would like to contact former fellow yuhong wang to ask about a job at radius intelligence, but i can\\'t seem to find her on slack. anyone know the best way to get in touch with her?hahaha it actually belongs to i think she was a fellow in the silicon valley ds sessions. her name is remy :slightly_smiling_face:kevinmercurio: i did a lot of time series anomaly detection in grad schoolaxl002: hey alvin. yuhong left radius a few months ago. let’s touch bases in a biti see! perhaps we can chat this week?hi smart people!! i do financial forecasting at wayfair. one of the challenges i face is the financial info is constantly being updated, so i end up doing a lot of gymnastics to recreate the past. we\\'re in the process of scoping out a solution so that we have snapshots of the table across time. i\\'m wondering if anyone has a smart way of tracking changes to order-related financial data across time.  the data is currently all stored in sql, but is quite large (i.e., the base table is 400 gb) and getting larger. thanks!!take snapshot once every week and then store change logs the other times?sounds like the abc of functional connectivity analysis in neuroimaging :smile: what\\'s significant when you have pair-wise correlation of 15k time-series.my company, driver, is hosting a tech talk next week: \"curing cancer as a machine learning problem!\".  the event will be at our headquarters in san francisco on thursday, june 22nd from 6-8 pm, and presenters include our ceo and members of the engineering team.\\n\\nmore details and rsvp at   hope to see some of you there!\"this is the last century that our children will ever have been taught that one times one is one,\" he says. \"they won\\'t have to grow up in ignorance. twenty years from now, they\\'ll know that one times one equals two.” - terrence howard\\n\\nread more: lolwut^ i wonder if it’s possible to choose different axioms and construct an algebra wherein 1*1 = 2. what would the axioms have to be? what would the consequences be?axiom 1: do a bunch of shots.\\naxiom 2: accept terryology.the future is already here: brad.deutsch: in a ring (cf.  ) this can only happen if 1 = 0, just look\\n ```\\n1 = 1*1 = 2 = 1 + 1 \\n=&gt; \\n0 = 1 \\n```provided your definition of 2 is 1 + 1the existence of a thing is questionable, but there is a lot of virtue in behaving as if it does exist (at least if you want the thing to be a field -- there might be exotic rings that have strange zero divisor)bad news terrance: we already know why bubbles form spheres and not triangles... :disappointed:stay positive alex_gudehas anyone calculated nuggetbot’s false positive rate? \\n\\n: thanks. another mathematician friend of mine agrees. he suggests that this type of structure would be a semigroup since we can’t find an identity element, and would be fairly uninteresting. ah well, i wanted to give mr. howard the benefit of the doubt!not strictly ds/de related, but this paper shows mathematically that under very modest assumptions about error and bias that we should expect *most* medical science studies to generate incorrect conclusions. (no paywall)\\n\\nstay positive brad.deutschstep in front of a bus thanksgiving data from 538!\\nis anyone familiar with n-back tasks? for example you\\'re presented a color on each trial and have to remember if the color n trials ago was the same. i\\'ve been tasked with classifying the difficulty of different patterns. for example, for n=4, a pattern could be blue red yellow green blue. i have quite a bit of data, but aside from some arbitrary cut off of difficulty ranges based on average percentage of correct trials i\\'m having trouble figuring out the best way to classify their difficulty. i was curious if anyone had any suggestions for methods that could be used classify the difficulty of these different patterns for a wide range of n-back levels (1-12). another issue is that as the n-back level increases, so do the number of patterns, so coverage is light beyond n levels of about 4. one standard thing to deal with low sampling in the levels above 4 would be a permutation test on bootstrap values. as for the performance estimation goes… if this is a change detection task (you mentioned observers need to say if the color was same or different on n back trials) you could probably figure out a way to use d\\' as a measurement of sensitivity [d\\' = z(hit rate)-z(false alarm rate)]. another idea: to measure the performance on this task (or the classification difficulty) one way would be to reduce this problem to 1d space and play with the distance between accurately classified colors in n back conditions in the cielab color space. or perhaps look into using pse (point of subjective equality) which is basically the moment at which observers respond randomly to “same/different” question on your experiment. anyway, feel free to pm me, probably no need to get into details here :slightly_smiling_face:d3 question. has anyone ever passed d3 strings to flask? it doesn\\'t seem to like quotes. for example i have a list of dictionaries like {\\'label\\': \\'ruby\\', \\'value\\': 345}meant strings from flask!oh figured it out! {{var|safe}}why not use stan?oh nice, i see edward is at least 35x faster than stan and 6x faster than pymc3wow that\\'s quite a speed-upi\\'d be very interested in hearing more about edward... looks very promising...it does seem really powerful!  which is why i\\'m trying to learn how to use it, although nobody else at my office has done it.  the gitter forum for edward is good for help, though.does anybody have experience making customer-facing reports with embedded python scripts? suggestions for what to use?oh boy do i! what is your output format? `jinja2` is a good start for many use casesright now i want to just output to pdf. would you recommend it for that?caveat that i haven\\'t used it, but i\\'d probably start with the `fpdf` library.depending on what you are outputting, matplotlib has a pdf backend the library reportlab is more comprehensivei’ve actually used `simpleidml` to generate adobe indesign pieces and then gone to pdf from there, simply because indesign is much better at…designfor the purposes of making pretty reports, i meanthanks all!i’ve played with it a bit but found it very confusing. can’t tell if it’s actually confusing or due to my poor bayesian abilitiesknight foundation, omidyar network and linkedin founder reid hoffman create $27 million fund to research artificial intelligence for the public interest   hey insight people: looking or have a friend looking for a job? - i recently joined a data analytics startup called kofa in san francisco, and we’re looking for more engineers to join our growing team! we’re a self-service analytics platform that prevents companies from reinventing the wheel when it comes to data pipelining, analysis, and modeling. we’re backed by top-tier investment firms such as accel and sv angel, and the founders previously sold their first company to palantir. all of us are ex-stanford ee/cs excited about working with new tech and building a company from 0-&gt;hero. \\n\\nover the past few months, we’ve doubled our team size and nailed down contracts and pilots with companies across the country. we recently closed a six-figure deal and will be doubling the number of companies using us by the end of q1 this year with the intention of raising series a by decemeber.\\n\\nwe’re actively looking for talented and driven backend/data engineers/fullstack, and we’d love to chat about the product in more detail if you or any of your friends are interested. you can see the full job description at , and send me a message at  if you’d like to learn more. hello insight friends, our company is hiring machine learning engineer in san francisco, and here is the job description  feel free to apply using the link above or contact me directly if you are interested. thanks!friendly request that we not blast 800+ people in the general channel with `@here` or `@channel` messagesokay :wink:hello insight fellows:  ibm is looking for volunteers for ux/ui testing for a cloud platform.  there is an honorarium if you participate!  here\\'s the info:\\n\\nwe\\'re currently trying to interview some data engineers about this idea to find out what your biggest challenges are.   would you be interested in talking with us for 45 minutes about your experiences trying to ensure data quality, low latency, and scalability?  qualified volunteers will receive a monetary honorarium as a thank you for your feedback.  contact me at  or .  thanks!there\\'s a new movie about snowden if you\\'re interested in the silk road case, deep web is a pretty interesting documentary that covers the investigation and ulbricht\\'s trial.\\n\\n\\n\\ndomino is having a conference for data science leaders. nate silver, cathy o’neil, and wes mckinney will be speakers. there will be a practitioners track and a leadership track. use `dominodl` for $100 discount off the current early bird price.we’re also still looking for good speakers, so let me know if you’re interested or know someone who might be!what are the conference hours each day? 9:15-5:15cool, thanks!:+1:hi i am looking forward for this conf. is the dominodl a promotional code to get the $100 discount?, yes that’s should be the code. let me know if you have issuesok thanks for thishere is an upcoming opening for product intelligence manager in my current company (liveramp). let me know if you are interested:\\n\\nmanager, product intelligence \\nsan francisco, ca\\ndo you love working with incredibly smart people who inspire you to be better and work harder every day? do you thrive in highly cross-functional roles that work across engineering, product, and sales? are you excited to contribute to building a product that will transform marketing? consider joining us.\\n\\nproduct intelligence is a newly-formed group charged with deepening liveramp’s understanding of how customers experience our products and also with helping pms make strategic, data-driven product decisions. \\nyou will:\\nlead the product intelligence team, shaping its charter and priorities to maximize impact\\nprovide direction to stakeholders on how to measure the impact of product changes\\nscope and manage your own analyses and those of your team\\ncreate frameworks that enable your team to execute analysis quickly\\nserve as coach and mentor to analysts on your team\\ncontribute towards building a great product team and great company\\nyour team will:\\ndeliver actionable insights to pms, engineering teams, and other stakeholders according to the priorities you set in conjunction with stakeholders\\nbuild and maintain reports, dashboards, and metrics to monitor the performance of our products, understand the customer experience, and track relevant kpis\\nabout you:\\nyou have 3+ years of analytical experience in business intelligence, data science, sales operations, business operations, management consulting, or investment banking\\nyou have built models and dashboards for large audiences using tools like sql, ruby/rails, python, r, looker, tableau, etc. \\nyou are amazingly detail-oriented, but can also bottom-line analysis for an executive\\nyou can understand, manage, prioritize dozens of projects guided by a strong sense of priority and focus\\nyou can communicate clearly and quickly. you synthesize effortlessly, and always answer emails and calls within 12 hours\\nyou have a bachelor’s degree in computer science, math, physics, engineering, or other quantitative field.\\nyou have a type s(tartup) personality: smart, ethical, friendly, hardworking and proactive \\nyou have a desire to work in san francisco. we will relocate you if necessary\\nbonus points:\\nexperience leading an analytics team\\nbenefits:\\npeople. work with talented, collaborative, and friendly people who love what they do.\\nfood. enjoy catered meals, boundless snacks, and the occasional food truck\\nfun. we host events such as game nights, happy hours, camping trips, and sports leagues \\nstock. every employee is a stakeholder in our future. health and saving. receive the benefits of comprehensive health, dental, vision and disability insurance along with a 401k matching plan\\nlocation. work in the heart of san francisco and take advantage of our commuter benefits\\nmore about us:\\nliveramp is the leader in data connectivity, helping the world’s largest brands use their data to improve customer interactions on any channel and device. we thrive on mind-bending technical challenges and value entrepreneurship, humility, and constant personal growth.\\nthere is so much more that we want to build and that we could continue to improve. we value strong engineers who are agile enough to hit the ground running and tackle challenges.\\nto all recruitment agencies: liveramp does not accept agency resumes. please do not forward resumes to our jobs alias, liveramp employees or any other company location. liveramp is not responsible for any fees related to unsolicited resumes.liveramp is an affirmative action and equal opportunity employer (aa/eoe/w/m/vet/disabled) and does not discriminate in recruiting, hiring, training, promotion or other employment of associates or the awarding of subcontracts because of a person\\'s race, color, sex, age, religion, national origin, protected veteran, disability, sexual orientation, gender identity, genetics or other protected status. qualified applicants with arrest and conviction records will be considered for the position in accordance with the san francisco fair chance ordinance. is there an extended version with more details? :stuck_out_tongue:i\\'m guessing you don\\'t mean it but to tell you the truth, this is all we\\'ve gotit hasn\\'t been posted anywhere yet so pardon me for the crudenessjust read about youtube incident. hope everyone there is safeapplication for speakers: this worked fine, thanks for the promotion.posting this for a friend. please let me know if you’re interested and i’ll forward on your resume.\\n\\nit is a machine learning engineer role.hi all - does anyone have any deep experience with gsuite domain management? we’re running into a problem where we’d like to change our primary domain and email records with no service downtime (i.e. the company’s name and domain is changing) but it’s apparently not that simple. private message me or email me at  if you’d be willing to get on a call (for $$).sorry i\\'m late to the party gang. i applied for a tn in fall 2017, and tried to use the physicist category. the person interviewing me said that was a no-go because there isn\\'t any category that is just \"physics\", they need to tick a box for \"astrophysics\"/\"atomic physics\"/\"geophysics\" etc... none of these will adequately match the job description for data science or engineering.\\n\\nsimilar to the other fellows above, i had my category changed to mathematician, which matched up with a minor in math on my bachelors degree.if you have more questions about the tn application feel free to reach outmaybe not so applicable to insight fellows but still interesting ^+1 for that medium articleinteresting premise, horrible analysis:  from the comments:\\n&gt; i like it when statistics are properly applied to support a theory. unfortunately, the author makes massive statistical errors.\\n&gt; the new york city (nyc) graph shows a strong seasonal effect in a time series. the ordinary least squares regression line (i\\'m going to assume those are the solid lines) are strongly influenced by where the series starts and ends, especially when looking at series of just a few seasons.\\n&gt; for example, a series of temperature in new york city starting in january of 2011 and ending in july of 2015 will show a positive slope, one starting in july of 2011 and ending in january of 2015 will show a negative slope.\\n&gt; indeed, the real story of the nyc graph is that there are strong seasonal trends in taxi cab complaints (probably driven by nyc weather). without accounting for those trends, further analysis is meaningless.more importantly, the author never compared against the null hypothesis, which in this case wold be to evaluate a single linear regression (higher order seasonal trends aside) for the entire pre- and post- uber periodyes, clearly had a point to prove, data be damned!haha yesunfortunately this seems to be a trend at the atlantic: (long read) : just started  channel to cover all things rdbms/sql, inspired by my presentation/tutorial with the ds-ny-2015b fellows yesterday. \\nfeel free to make the first post!awesome: is your presentation/tutorial available online?the tutorial part is just ah it\\'s modei can post my notes in the  channelthankswe had derek heregave a little intro that perhaps was usefulproject-basedah, okay, so you went straight to the co-founder!:simple_smile:haha :simple_smile:i\\'m looking for a python guru to contract for a short series of on-site lessons/tutorials at my company in nyc, with an emphasis on data science workflows - any recommendations/referrals out there?finally, i’m a real data engineer :simple_smile:  congratulations david.  this is super awesome!what is the best way to make sense of json files using pandas dataframe? read_json() results in error ( valueerror: mixing dicts with non-series may lead to ambiguous ordering.) #json  #pandas: that is awesome!!! thanks for mentioning insight!:+1:&gt;this is another secret in the hacker community. written instructions and online videos are only useful up to a certain point; to get really good, or to make something really fast, you have to be in the same room with people and you have to talk to them face to face.\\n\\n\\n\\nfun read: i\\'ve never loaded json into a dataframe, but i have loaded it into a sparse matrix. if you load your file into a list (or iterator), you can json.loads() each element to get a list of python dictionaries. you can then use the sklearn dictvectorizer to convert that list of dictionaries to a sparse matrix. so this is good if you want to do machine learning or something, but not if you want to explore the data.a word of warning: json with integers (rather than strings) as keys is not, technically, valid json and json.loads() will throw an error. there is a yaml parser that can load such json, but it is pretty slow. just fyi since i happened to have this problem recently.stay positive miothanks  it was a technical warning, not a personal warning, but if you don\\'t shut yer bot hole...stay positive miololnuggetbot animate me high fivehahahah bonus plug for partially derivative because i once got a beer suggestion shout-out. :)new stitch fix blog post that i wrote just went live. deep style: inferring the unknown to predict the future of fashion. \\n\\nnice! pinned a message to this channel.:   here’s a free event in palo alto on quantum computing by prof. maria spiropulu:\\ni\\'m not sure if you guys have seen this, but i thought it was really neat. they can simulate how objects would move when interacted with based on short videos. : i’d be interested, too! for a good, in depth overview of agile workflows, i enjoyed learning agile  (in the insight library, btw, if you’re back in town).: try the mythical man-month (). good for developer and managers.good morning fellows, my name is yiyun, a current insight fellow at boston. my project is related to user retention and engagement and i am planning to quantify webpage traffic by drawing a web heatmap with a slider control. the heatmap will change as a function of time with the slider so it will show which part of the web is most hit at specific time by users. i have plotted heatmap before but not sure how to put these together. any suggestion is welcome. thank you in advance!: i’d use a jqueryui slider () with a change callback that retrieves data for the selected time and refreshes the heatmap accordingly. (i’m happy to explain in more detail if that’s not clear. let me know.): this looks awesome! thanks for the suggestion!:  this might be a useful way to visualize something similar : wow, that is fancy. thanks jonathan!hello. i\\'m one of the current boston fellows. anyone have experience with a modeling package written in python called numberjack? it\\'s supposed to include a c/c++ solver called mistral. the installation says everything installed correctly and numberjack import works but when i try to import mistral i get a \"module not found\" error.: i don’t have experience with numberjack, but for solving combinatorial problems/linear programming i use pyomo.  its super quick and the best i have found outside of commercial languages. not sure if this is what you are quite after or not but might be worth checking out.  do you know what class of constraints you have?  if they have a convex relaxation you could check a corresponding feasibility problem with something like cvxpy: thank you   and  ! those look like good alternatives. pyomo even has an example that\\'s exactly what i want to do.hello. has anyone used flask socket io? i\\'m wondering how to call an outside function, after i receive a response. i want to create a custom response depending on what\\'s been received. i tried some things but keep getting errors when running the app.thanks  and !hi all, does anyone have a recommendation for a digital marketing course? ideally, i’d like something that covers a good amount of theory and practice in digital marketing and analytics, perhaps using the google analytics or salesforce ecosystems, and not spending much time on rudimentary data analysis. thanks!happy thanksgiving, fellows and alumni :slightly_smiling_face::turkey: :+1:gobble,  gobble! happy tofurkey day!:stuck_out_tongue: virtualenv is your friend for multiple version of python.  the curse of minimal viable products... scary stuff. stay positive kumbhaniyou seem to like... nmf. have you tried item-item cosine similarity?anyone want to grab mexican food on/walkable from castro st. mountain view in ~ 1 hour?yes.hello! has anyone used the mimic data set? how long did it take to get access and how long to take the course? thanks!? i think you should request it right now just in case, as it takes a few days to access it. also used it in her project to my recollection tooi requested access last night, so i\\'m hoping it doesn\\'t take too long. the course takes ~2-3 hours to complete, depending on how fast you read.it took me a few days but i had to choose project before that happened so i chose something else. i haven\\'t investigated this beyond a quick google search, but i know that casinos have used facial recognition software for many years.there also seem to be a handful of face recognition api hooks offered by big companies. great for building a project quickly, but if you go that route, i\\'d think about coupling it with something you build yourself.and as always, think about your target audience and what business problem this addresses.i guess the thing i\\'d worry about *more* is how you plan to validate this.thanks guys!now get back to work everyone!!!anyone interested in using the mimic database should be aware that they have demo data available which doesn\\'t require a waiting time for access: there isn\\'t much data in there, but it does give you an idea of how the data is formatted.superwoman has woman only screening, now it has clown only screening.  victory for clowns!  and the best part is that this time mras can attend! #itburnhave anyone previously done an accelerometry-based activity classification project?try  for some relevant experience. from my session did something like classification of boxing punches based on data from accelerometers in boxing glovesthis is true! from my session did something like this, iirci want to think someone from one of the bos health sessions did something with fitbit datahi former fellows, does anyone know of a good repository for terms of service and/or privacy policies for popular apps/services? trying to stand up an nlp project for 2017 remote session. much thanks! used iphone data was that for hykso?yep! could you please send me a link to your app or demo?who uses rnns or cnns for nlp classification tasks? i have read a lot of articles on nlp classification and one thing i have noticed is that it is really hard to beat bag of words and the models that do are by a very small amount.  i\\'m just curious what other people do in practice. i work at  we provide moving help for packing and moving stuff. if it was a short distance move we could have tried to get someone to pick truck up for you. if you are still looking for help, shoot me a message or call us at  and someone at concierge can help. also, if you did decide to use us, i can shoot some coupons your way :wink:i still feel like that in week 8. :sweat_smile: welcome seattle!i would start by considering a discrete version of the kolmogorov–smirnov test, comparing the percentiles. if you\\'re not aware of the number of data points from each distributions, you probably have to consider the number of percentiles values as n.more info here: interesting, , thanks for the link! i\\'m not sure ks test is appropriate. i\\'m asking for specific percentiles. like given two distributions, is there a significant difference in let\\'s say the 90th percentile, etc. currently i have the harrell-davis estimator on the table because it gives l-estimators with standard deviationsbut it\\'s computationally expensivehey all, any recommendations for a large text summarization dataset (paragraph -&gt; sentence)?*any with fields for citations, click rates or commentsif you\\'re comparing just a specific percentile from 2 distributions, the harrell-davis estimator is definitely the way to go, as it gives a more accurate estimate of the percentile and an error, which you can use a welch\\'s t-test to see if they are significantly different.but given a sufficient number of percentiles from both distributions, you want to compare all of them at once, i.e., you have to take into account that the percentiles comes form the same underlying distribution.ks test might work given enough percentiles, but there\\'s probably a better way when one doesn\\'t have enough percentiles (e.g., only every 10%)there\\'s at least one reddit bot that summarizes news articles. you could scrape the text of those articles and the subsequent summaries.here\\'s a python package that implements several algorithms, i think: some are slow, i think. gensim also has some functionality like this.thanks!awesome! thanks for the links anyone getting through registration for grace hopper?does anyone have experience using pymc3 for time series? generally i\\'d like to include some kind of time lag feature but not sure what\\'s the best way and can\\'t seem to find good examples using autoregression in the pymc framework check in with austin rochford on twitter - he might have a good blog post on this! uploaded a file:  and commented: another question in the que! has anyone ever worked with character cnn? do you have a implementation or resource you could share for the following architecture?for fellows and alumni in boston: the city of boston is having a data challenge using data collected from speed feedback signs. the kickoff is happening at cic boston on july 26th from 6-7pm, on the 16th floor. there are prizes for winners! :slightly_smiling_face:\\n\\nlink here: also  has a lot of great nlp features including summarization.  i’ve always wondered if those worked…i didn\\'t realize they saved the data!if anybody does this i\\'m super interested to know how it turns out! vision zero is near and dear to my :bicyclist: heartof course you didn’t looks aroundwho, me? why would i know anything about government surveillance of cars?:stuck_out_tongue:demo: i use spacy and stanfordnlp for tokenization!also, it looks like apple has launched a ml journal today: ‘journal’apparently they do! if only there was some one in city government that would (not) do anything nefarious with that data....hey guys, has anybody worked with synthetic lift studies (where one tries to establish a causal link for the effect of an ad directly from the observational data, without performing an experiment)? all i have found are a bunch of papers by some yahoo folks. if you have any lead, please pm me or add a thread. thanks!i was like \"wait, i know someone who would be interested in that as well\" :joy:might be worth checking out propensity modeling to build a synthetic control groupwill you be there ?how does one tell if a query is cpu bound or more so memory/disk intensive?hey bay area folks- new insight sessions are around the corner in sf and palo alto and i\\'ve already had some questions about housing. if any of you are looking to sublease/rent a room to a fellow starting in august or september, send me a dm!i think i\\'m required to be there! :blush:quick question... are the lightning talks available to the general public? or just insight fellows/alums? that video is a big part of my motivation to make the survey in the first place. the other was this general unease/uncertainty about how to assess what a fair salary in the area. feel free to bring a friend / some friends!sweet article: it was nice seeing you this weekend….  for 5 secondspretty nice:\\n: that is both beautiful and nauseating: 5 seconds in heaven is better than 4 seconds in heavenhah.  heaven’s overrated theni’m looking forward to my 5 seconds in hellstay positive alex_rhee: tony is my colleague at h2o :smiley:nicewonder if anyone has seen this internal error in spark stream:  error jobscheduler: error running job streaming job 1467073992000 ms.0\\norg.apache.spark.sparkexception: an exception was raised by python.  it attributes it to an attempt to run foreachdd on a dstream: \"user_accident.foreachrdd(lambda rdd: rdd.foreachpartition(processaccident))\". and it\\'s fairly easy to reproduce. \"user_accident\" is a dstream from a kafka stream( a slow one) and it comes about after a simple map operation to extract some  fields and put out  a (k, v) pair.stay positive sathyabas i suspected python is generating an error when the dstream is empty i believe. i found this discussion in   in . the poster was asking if there was some way to avoid calling the foreachrdd(). is there way to check if a dstream is empty ?stay positive sathyabthe bootcamp setup has windows drivers by apple for the magic trackpad. you\\'re right, i missed that part.isn\\'t collinearity a concept only applicable to continuous variables? it implies that some change in one feature might result in a proportionate change in another. however, for categorical variables there is no connection between 2 different categories, right? that\\'s why mapping them to numeric and calculating the correlation is not valid.\\ni think in this case, you are best served with the simple chi test with cramer\\'s v test, or the anova test when you have both categorical and numeric variables.otherwise, depending on the number of different categories within each variable, one can one-hot encode the variables and apply the generic numeric correlation procedure.you mean, how to identify important interactions that you can then add to your logistic regression model? one can train a shallow xgboost (2-3 depth) model first, then feed the most important paths as nonlinear features.just finished the signal and the noise, really loved it! started on black swan this morning. :+1:yes, i\\'m interested in identifying important interactions.\\n\\nthat\\'s an interesting approach. thanks for suggesting it. \\n\\ni imagined i could interpret \"important\" interactions as those with high coefficients after fitting a logistic regression model with interaction terms. \\n\\nhowever, cursory google searches aren\\'t pulling up much on finding interactions using this approach. now i\\'m suspecting there\\'s something fundamentally wrong with it.\\n\\nthanks again for your input carlos!hi everyone! the insight sf health data science team is hosting a meetup this thursday at 6:30pm with invitae! we only have a few spots left, so sign up if you\\'re interested. food and drinks included! details about the speakers in the link below.\\n\\nif you\\'re referring to \"important\" interactions between a feature and the target variable, then the magnitude of the coefficients, as you mentioned, is absolutely it.yes, that\\'s what i mean :wink: did this during his project. from what i remember, he just created a metric boatload of interaction terms and used a regularized model to find the most important.hey insight data scientists and data engineers — do you love video games? i have an opportunity for a data scientist/engineer at activision blizzard in sunny santa monica ca, to work on building an analytic platform for some of the world’s most popular video games (e.g. call of duty, destiny, skylanders, etc). this specific role calls on the ability to foresee and support business and enterprise-level questions for franchises and titles as part of building a web-based data platform. this team is looking to hire someone very soon. send me an email at  if interested. you get to play video games at work! nuggetbot u so swoony. what a hot mess. this is really interestingamazon machine learning advanced course\\nwhile that blog brings up valid points, it wouldn\\'t address the not serializable issue. often times, that issue arises due to scope issues, such as creating a database connection in the master and then trying to pass that connection to a worker. maybe post a code snippet?thanks! yeah, those are why i thought it might be possible.i think you may be right---i was definitely creating the client on the driver and passing to a worker. this was what the blogs were doing, so i thought it wouldn\\'t be an issue. i\\'ll try instantiating it within the map unit and report back. thanks!that fixed it! lesson learned. thank you *so* much!great to hear ! now consider creating it within mappartition and then passing it into map. this way, you are creating it in a worker and then sharing it among the threads rather than instantiating it for every worker thread. the whole insight team is on holiday until jan 2, but  is correct, we\\'d be happy to help you sort this out! can you drop us a note in the new year? you can reach me directly here: thanks~ david from your office contacted me and explained things this morning….*awesome!* that is a cool optimization. implementing it got me a lot more familiar with how to effectively leverage mappartition.does anyone know of any companies doing internships in data science for undergrads?thanks for sharing . this is terrible but also a good reminder that these things happen everywhere and that we need to be vigilant. perhaps legendary entertainment in boston, i heard they have some internships at a panel discussion, but not sure of the level at wayfair we definitely have internships and co-op programs i know square brings in lots of interns for data sciencestate farm has some available  a call to all bay area folks and especially those who have experience working both on the east *and* the west coast: i am going to be on a panel to talk about the tech, data science, and machine learning community in nyc and i am curious to hear how others think it differs from the community in the bay area. what do you think?i have nothing useful to contribute, but i\\'d like to say i imagine you guys (in nyc) wear more ties. :wink:… haha, and fewer hoodies! :white_check_mark:this was making the rounds amongst physicist-turned-data-scientist friends:\\nphysicist turned data scientist is about 60% of my friends... i thought the article could basically be summed up as: \"why does deep learning work so well? because taylor series work so well at describing the universe! but we don\\'t know why that is...\"&lt;-- biases particle physicists who only knows how to solve things through series expansions i agree and i did not very much understand that jump in logic(to why this explains that neural nets only have to work in a very small corner of parameter space) yeah, my local theorist also passed that around. : ) i agree about the jump; _intuitively_ it sort of makes sense \"oh, the universe is in a small part of the function parameter space, so obviously neural nets only need a small part of the parameter space!\", but it sure isn\\'t a rigorous demonstration. i worry it makes us come off like this:  :wink:righti think theoretical particle physicists in particular, in the spirit of ken wilson, think the rg flow applies to…almost everythingin our defense, it was hugely impactful in condensed matter and particle physicsyeah… max tegmark is basically that xkcd in a nutshellformer biologist here.  i frequently see that cartoon passed around (half) jokingly.  =pyeah, max has gone outside of his realm of expertise a lot recently…i think the only reason that arxiv paper got a call out in the mit tech review is max is a prof there now…so grain of saltbut i was very amused to see the cmb and “cat pictures” on the same figurehe really has… like a less famous michio kakui don’t remember him being _that_ wacky when i took relativity &amp; cosmology from himhaha…indeed, but with probably more impactful work before he went off and started being a philosopher  \"a less famous michio kaku\" what an un-flattering comparison! (note: do not take that as me disagreeing with it. ;-))for solving tab completion/kernel hanging issues try this at top of your notebook: `%config completer.use_jedi = false`  &lt;-- solved the issue for me!hey , we\\'re looking for something like:  or have anybody used crunchbase api before? how long it usually takes for them to give you a user key since you submit the request?hey, i registered a couple of weeks ago and still haven\\'t heard anything...what kind of strategies do you all use for isolating your \\'production\\' database from your \\'research\\' database?\\n(e.g. clone + custom tables?,  read-only slave + ?,  data-warehouse layer like redshift?)\\nadvantages? disadvantages? they can work depending on your use-cases.  it becomes hard to optimize for very specific things or build custom versions of an algorithm that may be better-suited for a specific data set,  with these general-purpose model platforms.  on the other hand they can save time if you know you need something general, and i think skytree specifically i remember being fairly performant also.i have very little experience with it thoughanother way to think about it is that it is not aimed at someone who needs ‘superuser’ level of control of models, but you may not need that for many or most things you do. thanks till (and others) for posting this. we are all concerned and uncertain of our future in the us. my sister went on a vacation to visit our parents back in iran a few days ago and now she is not so sure what\\'s gonna happen to her. i am a de fellow this session and quite frankly it is very challenging to focus on my project with all the uncertainty revolving our lives in the us. seeing all the protests and support today was very heart warming. thanks to all for showing us your support. i couldn’t find an appropriate emoji to react to your message, but i just want to say that i’m glad you’re here and hope your sister can return safely. please know that many of us are fighting for you and all immigrants.then i don\\'t think i\\'ll be able to say good bye in person! best of luck back up north, and let us know if you come back to visit!hey guys, would anyone with some expertise relating to nlp be willing to chat about project validation? my project deals with identifying topics of interest from unstructured customer reviews and assigning a sentiment value to them. any help would be much appreciated! i don\\'t know if this has been shared already or not, but open data science is matching aclu or united nations refugee agency donations up to $5k per person.\\n\\nhi everyone! i\\'m trying to scrape some data from a javascript query that seems to send a submission form via post. however, i\\'m not sure how to get the responding page or what the keys should be. here\\'s the page: i\\'m trying to look at what possible keys they are to send through a  function, but i\\'ve been unsuccessful so far.does anyone know how to get that, or another tool that might be more helpful, considering that the resulting webpage is also a javascript page?joao: have you tried watching the \"network\" panel in chrome\\'s developer tools (view&gt; developer&gt; developer tools)? that will show you the different requests that are made, and you can click on them to see request and response data. why post when you can get!  \\n\\nfor instance, just a little playing around chrome\\'s developer\\'s tools and looking at some source code...   \\n\\ngeog = cty\\nevent = ased | asho  (for emergancy department vists vs hospitalizations)\\nyear = 2000 - 2014\\nrace = totl|afam|aspi|hisp|whit|othr (for all races, african-american, asian-american, hispanic, white, other\\nage = totl|0004|0517|1834|3564|65ov|0017|18ov\\nsex = totl|feml|male\\nmodel = conv|bbym (for conventional vs spatially modelled)\\ntype = r10k|cr10l (for age-adjusted vs crude adjusted)\\n\\nare the form names:\\n\\njust iterate through them all and profit!\\n\\nex: what he said! :stuck_out_tongue:thanks. i\\'m looking at it now trying to decipher it. from what i had looked before, it seems like the query page sends a post request via , but i\\'m not sure about the keys being sentawesome!thanks a lot just watch out since certain combinations of options may return an error (according to the website). good luck!by the way, where did you find the list of the form names?any advice on how to render a page with some javascript generated table, before being able to scrape it?i\\'m trying using pyqt5 but not having luck so farselenium?not sure if it\\'ll dump the pagehey sf bay area folks, anyone need a great bike? i bought one brand new from mike’s bikes in palo alto about 4 months ago, used it to just get around palo alto, and it’s in near perfect shape and i have all the original paperwork for it. it’s a raleigh 2016 detour 2 (), red, size 17. i also bought rechargeable front and rear led bike lights and a pretty heavy duty u-lock. all that stuff combined plus tax came about to just under $500 but i’d be willing to sell it all for $350 (or best offer!). anyone interested should feel free to dm me (and i have some pics of it as well).i was finally able to get it via selenium and chromedriverhad to get the chromedriver, but now i can simply render the page on chrome on demand :slightly_smiling_face:cool beans! don\\'t forget the little people when you become rich and famous off your demo!lol. i think at the top of my priorities right now is just to get some sleepfor future, you may want to consider splinter.   swears by it.  selenium is a very round about way.i do enjoy splinterworked well for interactive scrapping @ ny  help!  i need a place to stay from october 5th.  please let me know if you hear something.  thanks^ \"what it\\'s like to be on the data science job market\"anyone interested in checking out the mayweather-pacquiao fight tonight in mountain view? i\\'m thinking of heading to st. stephen\\'s on castro st.what time does it start?i think it starts ~8pm,so i was thinking of getting there ~7pmfor folks in bay area, i have a clipper card loaded with a caltrain monthly pass for september, zone 4 to zone 2.  i paid $190 and willing to sell at half price. pm me if interested.   anyone have experience with testing database schema? my team basically wants tests in place such that when someone changes one of the schema, our scripts don’t all break. i think that counts as an integration test? any suggestions or references would be super helpful :slightly_smiling_face:what type of database is this?exploring the spacenet dataset using digits: (shout out to my lab\\'s sister lab: cosmiq works!)further promoting: they just started blogging about some of their works on _spacenet_ here: sheareraj: what are “our scripts” in this case? etl?here\\'s a quick informal survey: what is your company\\'s policy on the use of and contribution to open source software packages? if you need to go through a formal approval process for certain licenses not pre-approved, what is the turn-around time? thanks for any input!biased answer: my company open sources all of my teams work; we regularly contribute to open source and the process is \"hit the pull request button on github\". i expect that is on the crazy open end of the spectrum. a little history: we do this because we work with a bunch of collaborators and \"we just open source everything\" was an easy way around fighting about ip.stay positive alex_gude yeah, etl-type stuff, analyses that get repeated periodically, really a little bit of everythingnuggetbot check on alex_gudealex_gude has a happiness average of 0.5905172413793104i know, i\\'m as surprised as you that i\\'m not negative yet :wink:i mean...nuggetbot check on scstarkscstark has a happiness average of 0.24100156494522693 ^turns out i\\'m just a lot more positive! :wink:also that\\'s a lot of sigfigsnuggetbot must be precisenuggetbot do you know the `rats()` matlab functionnuggetbot check on kumbhanikumbhani has a happiness average of 1.02  here come cheap ingredients for robotshi guys, i had a question about project validation i was hoping someone could help me with. my insight project deals with extracting interesting (most talked about) topics in online reviews, and assigning a sentiment to them based on the context of the sentence they appear in. the reviews do have a 5-star rating attached, but i was told they would like to get more \"granular\" and parse line-by-line rather than just using this overall rating.\\n\\ndoes anyone have any good ideas on how i could validate this procedure? i don\\'t have a \"sentiment-labeled\" review set or anything unfortunately.if in doubt, start with textblob: cool paper published yesterday on nature: \\napplication of deep learning to classify different types of skin cancer, with similar or slightly better results than dermatologists. ring a bell, ?how to not data science :open_mouth:\\nwait. someone thought it was a good idea to name a project skynet?gotta admire the gallows humor in the defense sector...on another note, does anyone know anyone with experience in data governance in hadoop environments — including the creation and enforcement of security policies within hdfs via ranger or other methods?. also curious about encryption-at rest and in transit- tools and other meta-data tagging capabilities. we use edgenodes with our internal hdfs cluster with active directory integration (i think via kerberos)\\ni’m not sure of the exact implementation details, but files/folders on hdfs have groups associated with them for which we get access to with a separate request.any former fellows out there have a good starter tutorial regarding working with flask? this might help . step by step instructions here hi guys! i’m having some problems deploying flask with aws (see screenshot below). if you guys have some time, i would really appreciate any help!!! thanks so much in advance!! :heart::sunglasses: uploaded a file:  and commented: what does this mean……?! thanks so much for any insights! what\\'s in your requirements.txt file? looks like it tried to do a pip install of a dependency and failed. did you check the log file?does anyone have any experience comparing data from two samples both from a negative binomial distribution? i\\'m considering a mann-whitney u test, but i\\'m not sure if that\\'s the best approach. i\\'m also considering negative binomial regression.i\\'m also thinking about simulation but i\\'m not sure of how to best approach that given that the negative binomial distribution has two parameters.  permutation test? then you\\'re not losing info by converting to ranks like with m-w.a pretty cool tutorial - \\n(warning it’s long)i\\'m not that familiar with permutation tests, given that the negative binomial distribution is not symmetric i believe that randomization tests aren\\'t appropriate, what specific type of permutation tests should i look at? you could probably use the kolmogorov-smirnov test after adapting it to your discrete data. however, i\\'d probably use the kullback–leibler divergence.on the other hand, if you already know the data comes from the same distribution but with possible different parameters, a m-w u-test or even a t-test will do the trick, as there\\'s no need to compare the whole distribution.here\\'s more on our upcoming 4-year reunion gathering in palo alto open to all  insight de fellows next week: my company (finance firm) is restructuring our office floor plans! do people have insights into what types of office set ups (eg, open floor plan, meeting rooms, other types of work/gathering rooms) tend to work well in terms of productivity and enjoyability?insights for insight office set up :stuck_out_tongue:but seriously, i found your nyc office in flat iron so enjoyable!that was open plan, with a few meeting rooms and a fridge full of beers  \\n&gt; private offices with doors that close were absolutely required and not open to negotiation.oh maybe i should clarify - i’m not working for insight. i’m a boston alum working for a finance company (ie not traditional tech office floor plans)read “quiet”, decide where your team falls on the introvert/extrovert/sensitive spectrum, then decide. finishes that reflect sound make open plans horrible because you can hear everything. white noise is often necessary (i’ve seen people install white noise machines in open offices, literally)    text to speech in python 3.  haven’t tried it yet.\\n\\na series of pieces by nyt called “reimagining the office”.\\n\\npt 1 is a really good piece about what google learned from its quest to build the perfect team.  also part 4 deals with diversity.  highly recommend.it really is a good series of articles. i enjoyed reading them.current idx fellow,  will be on all things considered today around 4:44pm et. she’ll be speaking to her academic position and how the brain responds to teleportation.\\npretty cool ()if i `pip` install theano on macbook air, how many cores will it use out of the box? is there a way to set this parameter in (say) a jupyter notebook? i know a comp sci graduate student at uic that might be interested, but its harder for him during the semesteryeah. its urgent. only 2 weeks timealso, someone with slack admin rights might want to delete old files since we are over the limit ()i need to post dog photosit’s urgenti\\'m not sure if everyone is already aware of these, but i found them quite a bit better than the default: ooo pretty\\nstill waiting on promised dog photos over here uploaded a file: the dogs of adrolli present mochi.her favorite things in no particular order are1.  food uploaded a file: this is lady.  she is from thailand.her favorite things are, in no particular order, are:1. food2. her owner alligood dogshe is also very fashionablethxi got you i’ve been stalking the office dogs for youalsothe white one bit meloli was poking it thoughand by poke, i mean petting it to get its attentionwho knows a lot about probabilistic graphical models (pgm)? someone i know (at mount sinai in nyc) is looking to chat with someone, who knows about those pgms. i believe it’s location agnostic … and … for a good cause!hi all. i am involved in a project where a non-profit is looking into possibilities to take advantage of their huge database for some (undetermined) advanced analytics in the near future. the current discussion with the database engineer is about various data warehousing/lake solutions, as the first step in the process. they have all types of data from sql server, to csvs, to unstructured reports.  i need to make sure that any solution proposed would allow analytics in the future. (they currently only display various information in their website and standardized reports.) . does anyone have thoughts about things to avoid or make sure are implemented for data storage? thanks!\\n\\nps. they are currently considering salesforce. delayed response here, but i use files dumped to a semi-organized structure in s3 and spectrum to join that data in to other sources in our existing redshift cluster and it\\'s pretty greatthe advantage of the files just living in s3 is you can also do more specialized analyses via spark in addition to having some standardized queries run regularly with redshift/spectrum thanks! i was reluctant to do the switchover based only on the press release (which makes it sound great). i hadn’t even thought of the advantage of using spark with it too.yeah, i was exclusively doing things via spark until spectrum came out and spectrum has made my life a bit easier (no worrying about tuning the cluster parameters for memory issues, etc) i had held a few bitcoin for a number of years but i actually sold them earlier this year to help pay for housing during insight~~zkermish: how do you decide between redshift or spectrum? do you use spectrum for data transformations that you then throw into redshift or are these just big one-off or seldom queries for stuff you’d rather not put in redshift?btw, we do similar dumps to s3 and loads to redshift, with occasional spark jobs for data transformations.the data we now link through spectrum was large enough that we had issues loading it into redshift…also, it would have brought our costs upwe have enough one-off queries to do that it’s worthwhile to link to redshift via spectrum and just run those as neededand eventually the hope is to have some regular queries done that right back to redshift for longer term use (overall descriptive metrics to condense the information in the raw sources) i guess for us it’s a good way to explore a larger source of data before having a clear process around it and a way to load in some distilled info from that data into redshiftall at a lower costawesome, thanks. we’re in the process of offloading some in-redshift transformations to spectrum (instead of spark) to keep redshift resources dedicated to our bi tools.and the migration from redshift -&gt; spectrum is much cleaner/faster than  redshift -&gt; sparkcoolhi  what is spectrum? ?“redshift spectrum”no  , it’s an aws thing: ah, i thought it\\'s called athena!i’m sure it’s all spark/hive/presto/whatever on the backend that they manageathena is very similarbut that doesn’t link to redshiftah!subtle distinction…they’re actually super linked. if you make a schema accessible in athena it’s there in spectrum too. the advantage is you can use your existing redshift cluster to run the queries / join to other data that’s therecost is the same too (they both just charge per tb scanned in the query)what is a good tool that you guys have used for schema management?but spectrum queries aren\\'t as fast as redshift, right?some things to consider, maybe being able to separate out personally identifiable and non personally identifiable data and store in 2 different db?spectrum queries use athena and redshift under the cover it’s all parallelized under the cover so it’s quite fast. the only thing missing is a dist_key and the associated optimization but my sense is spectrum approaches that by using compute cycles on the storage nodes…not entirely sure..but it’s all fasthey past fellows! does picking a la croix flavor get any easier? :confused: ending the 4th week and i’m still having troubleben_stclair: ask  , he\\'s a lacroix connoisseureventually i just went with one of the beer flavored ones.haha just try them allexcept lemon. ew.when in doubt, don\\'t feel blue, go green!citrus are the clearly superior options. berry used to be... but unfortunately i think it\\'s gone. :disappointed:there is only one la croix its all about the mango (or anything that comes in a tall can)it does get easier. lime is the clear winner.just stay clear or coconut and lemoncoconut and mango and berry are the only acceptable flavors coconut? what the hell man, i thought you were coolhahaha, i know, i’m very much in the minority on that one….thanks, everybody! this is really helping my decisions be more data driven :r: the nakamoto paper  … i’m very much a noob :slightly_smiling_face:\\n\\nwould be very interested in any references you could recommend though!definitely coconut. #teamcoconutprevious fellows: at week 4, did you start feeling excited for demos but convinced this real-world job thing would never happen? it seems to be a common fear in some circles.100% coconuti’m with cocoi go with the two flavor ones usuallypassion fruit for life!! :  maybe the current fellows want to start a channel to discuss their project ideas with the alumni?good ideaor even if they\\'re stuck with a particular issuemaybe someone can have some helpful input: it would be good for more alumni to sign up for mentoring slots too (especially us nyc folks!). jenainsley would really appreciate it!how many alumni are actually on here? i think it might just be me and british matt from my session.there\\'s maybe 10 from my session (nyc 2014) i thinki was really just pestering them :simple_smile:i just updated my profile to include insight info.  that could be one way we know who\\'s from where and which classslackers = a exp(-k(now - session date)).  find a and k!good idea talldave , i\\'ve done the samek = i: thanks for the mention!\\nif anyone has questions about signing up for alumni mentoring spots (project feedback or mock interviews) - email me! :sparkles:nuggetbot: animate me welcome everyone!:  is a good package. i used it in my project: www.nerddit.club/graphalso easy to use with neo4j :simple_smile:happy  international women’s day! check out our blog post! lol…i love that the two slack icons of women used to “like” my post above are called “ok_woman” and “information_desk_person”. how…ironic?: i used the ‘information_desk_person’ because previously a friend used it to indicate (in their words) ‘it’s like a hair flip heyo\\'so it was supposed to be chill:okman:oh, okay..if anyone wants 20% of the upcoming dataengconf in sf, we have a discount code: ids20xthey also run a regular (free) meet up for data engineers: and in ny as well: this match is fun to watch! deep learning + reinforcement learning against the world\\'s best! alphago beat lee sedol!i did not expect that.how is x distributed?fun data on bacteria in nyc subways he that was my supervisor :simple_smile: did you help gather data by licking poles in the subway tom? tell us how you guys wrangled the pole data!!? no, nooooo not *those poles* :speak_no_evil:gosh.also the github which has all of the code used with mimiccurrent remote fellow here: shot in the dark - anybody have experience with audio/speech processing in python?  i’m building a web app that processes audio of the user giving a presentation and does nlp, sentiment analysis, etc. does! or  have done some as well sounds good :slightly_smiling_face: might be useful to take a look at  in addition to whichever other tool you use (i recommend just going straight to `requests`)is any one going to the google cloud summit in seattle on wednesday?  trying to get in some networking as i look for my first ds job! pinned a message to this channel.current fellow here working on my project...have any previous fellows used the  api?  i\\'m having trouble getting the vehicle api key and was wondering if any past fellows might be able to help. thanks.  ? i did some for my insight projectgot it. that\\'s two strong reasons to go for it then (i\\'m using python). thanks!hello a new job posting in my company not exactly related to data science, but in case anyone is interested hello all!  i’m starting a ds position at a place that gives dss an option to work on a windows or ubuntu machine.  i am traditionally a mac user, and like the idea that ubuntu has much more overlap with mac — but am not sure i’m up for all the fiddling that i’ve heard can be required with ubuntu. what are your opinions on the two options?if it\\'s windows 10, you\\'d have access to the bash subsystem, so even less of a difference between them.i made the switch from mac to ubuntu last year. not much fiddling at all. 16.04 lts feels very much like osx did. and if you ever used homebrew on mac, the linux/ubuntu package manager system is easy to use. no problems at all installing &amp; using anaconda, tensorflow, etc.i have a windows 10 machine &amp; run a virtualbox w/ ubuntu. best of both worlds for scripting + ease of use with microsoft products (ppt, skype, etc.) that can be challenging on ubuntucongrats on the ds position! i was a long-time user of mac and linux from my time in academia, and i am now working at a place that operates exclusively under windows, but i was very pleasantly surprised by how minimal the hassle was for me to use windows to do my job. that having been said, i do wish i had the option to use linux. either way, i think the fears and concerns with one versus the other are generally overblown.very good post on ai...congrats on the new job! i would carefully examine how the it infrastructure supports ubuntu vs windows. daimler (my company) is historically a windows company, and has recently been transitioning to support mac. there are a lot of network issues that are purpose built to be easy to handle for windows and are ultra frustrating to deal with on a mac or linux box (i.e. proxy settings, email server, shared directories, etc). if your company deals primarily with web-based stuff, then i’d recommend sticking with ubuntu, but if you need lots of productivity software (ms office, etc) it might be easier to just use windows.just go windows. besides the bleeding edge, you\\'ll be able to use most data science tools just fine and you\\'ll have less hassle producing results you can share more broadly.if your company maintains its own fleet distro, then it\\'s a strong indicator in favor of linux - patch tuesday still sucks decades on-going.a contrary indicator would be if everyone\\'s account is office365 and activedirectory instead of say ldap.hey people, hope you are all having a great day. i just had a general question, for a classification problem, is there a way to check for collinearity between categorical variables. been looking around for a method, thanks in advancewhile this channel is still reacting to faye\\'s last @ channel blow-up, i have an actual question for everyone. \\n\\nmy team is in the process of interviewing for a new data analyst role, which is an entry level data position. \\n\\ni\\'m building a rubric to help my team judge resumes. i was wondering what you all look for in junior positions like this, at least on paper. any ideas would be great! :slightly_smiling_face:what are you hoping the person will work on? experimentation, dashboarding, ad-hoc work, modeling? how much will they need to get their own data and write etl? design experiments? do they need to work with engineers in python/scala? how do you share data internally, via tableau, jupyter, something else?frankly, they will be doing most of the above (our team is small, so most roles we hire for are generalist roles).\\n\\nthere will be a hefty amount of guideance in the new role, but i expect the new analyst to help with dashboarding, aid in modeling (even if they aren\\'t versed in it) and do etl as necessary in conjunction with our data engineers. \\n\\nwe are pretty platform agnostic, so any knowledge of r/python is a plus.\\n\\nusually data is shared in jupyter, rnotebooks, and tableau.\\n\\nthe biggest thing for this role is their willingness to learn and interest in government, since they won\\'t be on one single project with one focused task.anyone have any experience with speaker separation / diarization? i\\'m having moderate success with the typical approach of agglomerative clustering, gmms, and structured perceptron. would love to talk details with someone who has some experience though.samba tv is a long-standing insight partner, but we just had a de (insight alum) decide to move on to salesforce, so we have an opening: does anyone have a suggestion for a resource (anything from a blog post to a coursera course) to understand the basic things to consider when deciding on a data pipeline?  i want to set up a pipeline to handle a few tables with about 10m rows added per day, and i want to figure out whether a typical s3-&gt;spark-&gt;redshift pipeline can handle it.  i\\'m not a data engineer, but i definitely want to learn more!anybody interested in latent dirichlet processes? or if you have worked with them, please let me know:grinning:fantastic!does anyone have experience with the facebook api?  i\\'m pretty sure what i\\'m trying to do is really simple, but i\\'m getting lost in all the set-up documentation.what are you trying to do?i just want to get the number of likes for a list of pages.  i have the addresses already.hmmm...unless you have access tokens for all those pages, might be impossiblewhat endpoint are you trying to hit?does anyone have a link to the mentoring sheet for tonight?pretty cool:  uploaded a file: yeah! we were surprised by how well that worked...i really got to start trolling for meetups... :confused:hey everyone, if anyone is looking to pick up some o\\'reilly books, here\\'s a 40% off discount code, i just ordered a few and it worked fine: wcyazif anybody wants a chance to learn tableau without paying the $1000 for a license, you can get a free (and legal) key by registering for this competition:  . (the site doesn’t say how long the key is good for, but mine says it expires in mid-august, which is considerably better than the usual 14 day trial)for those of you still studying, there is a decent documentary on netflix called algorithms that explains some of the sorting algorithms, pagerank, and decision trees using fancy made for tv graphics.\"any bias contained in word embeddings like those from word2vec is automatically passed on in any application that exploits it\"tableau licensing is notoriously expensive. part of their pricing model assumes that all potential users of the product should be factored into licensing, and the scope of a potential user is very broad (e.g. an employee that may use excel -- in other words, everybody -- might use tableau, thus they should be factored into the license cost). it wouldn\\'t be uncommon to see tableau offer a site subscription to a small/medium organization at a rate of ~$1m+ for 3 years.consider your use cases and existing tooling... power bi is improving quite a bit and many organizations already have licensing arrangements with microsoft. also take a look at qlik, which offers their desktop product for free (windows only). the main limitation with the free product is sharing/publishing/collaboration.after talking with a lot of teams,  and i wrote down what we learned about transitioning from software engineering to applied ai. let us know what you think :slightly_smiling_face: does anyone every use any gui tools for debugging extremely long, hard to understand sql queries?awesome post.  but, what is ai? can you give examples that are ai, and not machine learning?  is reinforcement learning - ai or ml?we interviewed someone recently and reading my colleagues comments i was reminded of a gap i have (e.g. i could not have answered the questions they asked this candidate) --- particularly, the trade-offs between all the different ways we can store data (think broad here, not just sql vs. nosql, but that\\'s a piece) and how to think more deeply about schema choices.  when i asked my colleagues they uniformly said something like, \"i learned it on the job\" or \"key is to try the different options and see why it doesn\\'t work\".  i\\'m learning some of it on the job, but it seems like we are in a place where we already made a lot of choices and i want to understand why better... of course, i don\\'t have infinite time --- maybe a chart to start with, or say one section of an online course i can listen to on a bus... perhaps this can help: provided you\\'re using postgresany opinions on whether this is worth attending?\\nwent last year for free two of the days. some decent talks.i have a question about data science platforms .. my team requirements are simple: have a collaborative workspace supporting multiple languages and easy to deploy. i know of tools that solve the pieces of puzzle but i am looking for something more comprehensive .. i looked into dominodata labs and it helps, but i think its very expensive for something that is completely open source tools. any experiences or leads into other projects or tools? what do you mean by \"collaborative\"? does \"we share code on github\" count? or do they need to have like jupyter notebooks that anyone can edit at any time?predicting wheat rust in ethiopia with the bill &amp; melinda gates foundation: : lets say a big more than we share code on github .. commenting on files and general discussions .. i think many of the tools like bitbucket, gitlabs do that. but do suffer from a lot of complexity in ci/cd.i think bitbucket plus docker is what i am looking for ..seems likely. we used domino and went back to docker + github on our local servers, but we\\'re weirdhas anyone ever used the python package lime? my team is working on ways to easily explain individual predictions from a model uploaded a file:  we\\'ve been testing lime. we found it worked under... exactly the circumstances that they presented in their talk. :confused:it falls flat on anything complicated (or with dimensions that are not explanatory, like word2vec dimensions)thanks  any thoughts on how well it worked with random forest of (ballpark) 10-100k data, 50 features? other methods you have liked?i don\\'t believe we used it on a random forest (but that seems like a model that it would excel at). we\\'re still exploring options; our boss is a big believer that model explainers are the future. (which means it is one of the things we\\'re tasked with looking at.)more frost this morning...  uploaded a file: here\\'s a little gem:  sublime text key bindings for jupyter notebook!\\n(add the following to `.jupyter/custom/custom.js`)\\nhey, i\\'m looking at your chsi data hanlerand i have a couple questions whenever you\\'re around.what\\'s up?i figured it out in the mean time sorry i didn\\'t get a notification when you messaged me!it was very useful, i pulled out tons of interesting predictors.oh i did want to ask in your ridge regression, why did you weight by population size? is it because you were looking at state wide effects?if you\\'re running the regression to develop hypotheses of causal relationships at an individual level (which is a sketchy thing to do -- what with causation not being correlation and all that -- but it was assigned to me for a data challenge for a company i interviewed at...) then you probably care more about prediction error in counties with more people because they represent more individualsyeah, makes sense. i was just curious. usually we only weight by population size when we\\'re trying to weight survey sampling by strata.i wanted to make sure i wasn\\'t missing that i should do it with my county level inference. :smile:did anyone ever encountered virtual box problem such as \"the virtual machine \\'ubuntu\\' has terminated unexpectedly during startup with exit code 1 (0x1). more details may be available in \\'c:\\\\users\\\\selin\\\\virtualbox vms\\\\ubuntu\\\\logs\\\\vboxhardening.log\\'.\". my windows restarted itself and now my vb, therefore ubuntu is not starting.hyper-v has been much more stable for me than virtual box. i was getting blue screened every time i tried to launch a ubuntu virtual box instance recently.yes. pretty much any other virtual environment is more stable. after 4 hours of troubleshooting, i fixed it. i think it is time for me to spend money on some nice macbook, and never risk something like this again. btw: the problem is caused by windows 10 update, windows 10 firewall was attacking the saved ubuntu status files. for many people, removing the firewall was helping, but it did not help me. i fixed it by going back in time. :slightly_smiling_face: i\\'ve been doing all my development in wsl and it\\'s working a dream.might be an option for you.i will check it out! thanks!anyone want to do some nlp on emoji? witness the sputtering of the hive mind.that would be fun!anyone else interested in future insight alumni hackathons? once we (the current session) get some free time we\\'re thinking of doing this. join at  , did you ever find that voting data?anyone looking for room mates?if so,come join the new  channel!!!does anyone know a good *zeppelin* tutorial that walks through working with a dataset? i\\'m tasked with comparing jupyter and zeppelin, and i\\'m a complete zeppeling n00b. :wink: most i\\'ve found so far focus on installing it, but i\\'ve got that solved with docker.horton works has a gallery of notebooks that seems like a good start: : check out toree, which literally runs in jupyterit’s still in the incubation phase though...anyone know what happened to nuggetbot?don’t wake the beasti\\'m not sure how to stay positive without his friendly remindersph\\'nglui mglw\\'nafh nuggetbot r\\'lyeh wgah\\'nagl fhtagn.agreed stay positive, jollyhrothgar! all will be well.:stuck_out_tongue: dyfrig - could you please send the name of the textbook for time series analysis?: : you could also check out google scholar - they show h-indexthe arxiv has an api too, im not sure if it has citation information: the arxiv does not have citation or precomputed h-index infoinspires is much more rich, but limited to astro and hep: many thanks. google scholar does not provide any api, and it blocks if requesting too often -- based on the info i search online but didn\\'t try out myself.i got something to start with from inspirehep. thanks a lot, guys i looked into this at some point. all of the citation data are owned by either scopus or web of science. google has a deal to make it available basically as a community service, but they\\'re not allowed to have an api. for medicine/biology you could scrape a well formed pubmed search to get publications. :  if you need to scrap pubmed talk to   he did some nice things with pubmed data. here is an example scraping pubmed with ri have a perl script for it as well... did anyone (or know of anyone) go into machine learning/deep learning academic research in industry? i was curious what the career trajectory and market compensation is like. please dm me. thanks!can anyone help with getting a postgresql database set up with aws? i have an e2 instance running ubuntu, and i\\'m able to connect and load my app. i also have an rdb instance running postgresql, and i\\'m able to connect and query the database on my local machine with sqlalchemy and psycopg2. however, when i try to run my app on the e2 instance, i get the following error after running `gunicorn flaskexample:app`,\\n```operationalerror: could not connect to server: no such file or directory\\n    is the server running locally and accepting\\n    connections on unix domain socket \"/var/run/postgresql/.s.pgsql.5432\"? ```\\nif i comment out the line where i try to connect to the database, it will run. i\\'ve changed the permissions on amazon rdb, so it should be able to connect. \\ni\\'m stuck, can anyone help?hi all, i fixed it! it turns out i wasn\\'t accessing the rds database as i originally thought. i was still accessing the local database, which is why it didn\\'t work on my e2 instance.hi everyone,  does anyone knows a good tutorial to deploy flask apps in redhat?  (beyond the ones you find googleling)does anyone have a suggestion for an hourly rate for data science consulting work? a large range -- perhaps $100-250 depending on if your client is a startup or fortune 500 :wink: many many factors can influence the rate thoughthanks! :expressionless::expressionless::expressionless:\\n\\nthoughts?\\n\\nit’s strange how we see computer scientists, politicians, and economists arguing that gender differences are a result of biology, but never population geneticists.note that google’s response in internal memo is on the motherboard article now (as is the full text of the original) .in terms of social issues, i find that it\\'s (un)surprising that we hear critique from (presumed) white guys who feel like they aren\\'t treated equally. regardless of background. we\\'ve always used \"biology\" as a way to argue for exclusion historically, unfortunately. \\n\\nyou have to wonder that if there\\'s a general consensus  about what this guy asserts, where are different minority groups in supporting this statement? \\n\\n/sjw\\n\\nseems like a typical corporate response. i wish they took a harder stance. :thinking_face:i found his screed to be an exhausting rehash of garbage ideas and talking points. for nearly all of his arguments: [citation needed]. and i\\'d bet the farm this guy subscribes to red pill and men\\'s rights on reddit.yes, i thought the response sounded like bsjust read it. i thought those [#] markers in the google email were citations.turns out they were footnotes with more ranting.lolcontroversial male opinion: just read the whole thing. i think the pseudo evolutionary science explanations for differences in representations are bullshit, or at the very least difficult to prove. however, i agree with his point that there\\'s a culture of trying to shame people who think non party line thoughts on diversity and that it\\'s unhelpfuli find that pretty suspect and am curious what your examples would be. for this specific case, the medium article is pretty clear in describing how toxic this kind of ideology is.  it is important for leadership to take a stance against it so the organization can continue to function instead of tearing itself apart with suspicion and lack of collaboration. there is rightful anger against this person for perpetuating  the sexism and racism that has troubled our society for so long, and if they’re shamed by that, that’s their own fault.i\\'m going to sorta sound like a bitch here, but i\\'m going to say it anyways: there\\'s a distinction between compassionate educating and \"justified\" calling out. \\n\\nbut i\\'m generally, if you are perpetuating sexism, racism, islamaphobia, homophobia, transphobia etc, an someone calls you out, i\\'m not gonna massage your hurt feelings. sometimes we say things or believe things that are hurtful (and/or wrong) to others. and sometimes we need to learn from those mistakes. i place myself in this statement too because i have also have done and said things that have been hurtful or off base.\\n\\ni usually don\\'t bring my own identity into the discussion, but i will here. i applaud aggressive inclusion practices and believe we need more of it. as a *trans* person, i have no effing patience for people who perpetuate oppression or discrimination. i\\'ve spent my past and current life being on the fringes of society, and puke when people who are not trans or have not consulted w/trans people try to speak for me.\\n\\nanyways, hopping down my pedestal now. 🤣i guess my example of would be the reaction to this postin the medium post itself the guy says \"i need to be very clear here: not only was nearly everything you said in that document wrong, the fact that you did that has caused significant harm to people across this company, and to the company’s entire ability to function. \"the idea that this one person\\'s opinion can cause significant harm to people seems like a pretty strong assertionand if we\\'re working on that assumption then the follwing \"if you feel isolated by this, that your views are basically unwelcome in tech and can’t be spoken about… well, that’s a fair point.\" seems justifiedbut i guess i question the underlying assumption, and think if we really do think he\\'s wrong, we should engage with him, not threaten to immediately crucify him and anyone who might be within 50 feet of himie, don\\'t be like yale i don\\'t think the crux of this debate is whether callouts are justified, it\\'s whether this employee (and perhaps others who think like him) should be straight-up fired (and perhaps blacklisted by the industry) via the usual social media dynamics. how far do we need to shift the overton window in order to have an environment that\\'s considered sufficiently inclusive to whatever groups?so it sounds like your argument is that you think his statement is not that harmful and therefore he should be spared all of the criticism he is receiving.  statements that support the kinds of discrimination and oppression that are the norm in technical careers (and elsewhere) are absolutely harmful. that system is absolutely harmful and we are trying so so hard to improve it. every time someone makes hateful and uneducated statements like this, we take a step backwards. given the public forum his statements were made in, criticisms absolutely must be in public forum as well. if someone said this stuff to me privately, i would argue them privately but not post it on the internet. but he is the one who made this the the forum. i have zero pity for him. \\n\\nalso, the fact that you claim people  “crucify him and anyone who might be within 50 feet of him” is absurd hyperbole. he is being countered, as he should be. he might be removed from his job, once again, as he should be if he has prevented himself from being a functional part of the organization by attacking his colleagues. \\n\\nwarning i am not going to  respond any further to this thread since i don’t actually know you and don’t want to go down an internet argument black hole.also, not to continue to belabor the point (against my better judgement), but what reaction would you expect if someone aired their misogyny (or other awful opinions) to a wide audience? would you expect everyone to be all ok with that?\\n\\ni think it\\'s worth understanding why many women got up in arms about this manifesto and made their grievances known, rather than being reactionary about it the grievances themselves. this is generally also a good model to follow for any social justice issue.\\n\\ni would say that there is room for both anger and critique on the discourse we have about gender disparities (and other disparities in general). it\\'s not an either/or. \\n\\ni find it slightly disengeniuous to compare gender disparities in tech (especially with regular headlines through this year) to an incident at yale. we know there is icky stuff going on in the world place at many tech companies, and is actually a good reason for people to be less than pleased with every new piece of news that comes out.\\n\\nlike  said, i have no sympathy for this guy. he aired his opinions company wide. he should\\'ve been prepared for critique, angry or not.i think i misspoke in my original comment so i\\'ll clarify: yes, of course people should be able to get angry. but in the medium post that was also posted, yonatan says, \\n\\n\"you would have heard part (3) in a much smaller meeting, including you, me, your manager, your hrbp, and someone from legal. and it would have ended with you being escorted from the building by security and told that your personal items will be mailed to you.\"\\n\\nwhich basically says he would have been immediately fired for his opinions. judging from the near universal praise of that piece, i think many people here think that\\'s an appropriate response, and i don\\'t agree. \\n\\nthe original piece might be wrong, but i think instantly terminating him doesn\\'t remove the underlying biases that lead to the piece. it just drives them underground so the next person who feels this way will just talk about it in redpill on reddit. if we\\'re serious about trying to convince people that diversity is good, how can we ever hope to convince anyone if they\\'re afraid to share their opposing views because they\\'re afraid they\\'ll get fired.so uh, mea culpa for making it sound like i think it\\'s wrong to get angry or critique him. that\\'s a totally appropriate response and no one has a right to tell anyone not to get angry. i just don\\'t think he should be fired or sent to the basement for rat clearing duty.+1 to that answer!hello all! can anyone recommend any tools for labeling data sets that have worked well for you? (and possibly ones to avoid) thanks!what type of data are you labeling? i used a java tool to do image segmentation… i can try to find ithi everyone (if i get enough thumbsups, i will hit the dreaded at channel on this) \\ni am aggregating data on parental leave policies in tech startups.. hard to find data on small companies, and since scraping glassdoor is not the first thing i would like to do :wink: i am trying my luck with crowd sourcing! it is extremely important for startups to have parental policies in place if we have any chance at truly diversifying tech! women (and men) should not feel the need to work at select big companies if they want to make babies.. especially since having a family and kicking ass in tech are not mutually exclusive! so please help me aggregate some data since we all know data is power :slightly_smiling_face:  i started a sheet with the big companies, please add to it any small company whose policy you know of \\nand any more data on big companies the better!wow super useful. thanks!so if we work at a non-startup should we enter our info or not?yes! please also enter! but would love to get some data on startups.. so please also include the number of employeeswhat’s the :+1: threshold for going at-channel?can we get more music fanatics in ?if you want a pure python solution for image segmentation, bleeding edge scikit-image has that capability \\nwe\\'re experimenting with  for text labellingthe developers are super helpful with deployment/customizationhehe.. good question soumya.. 30?anyone aware of openly available data on hiv anti-retroviral treatment (art) adherence with associated social/demographic/medical factors? i have an old project i’m trying to revive and get a publication out of, but out outcome (art adherence) was poorly measured and subject to a lot of bias, and i’d like to try and train a model on decent data of art adherence that i could then use for these empirical data we’ve collected to predict art adherence. i’ve been reaching out to researchers based on publications that might have data willing to share, but wanted to tap into this collective bundle of genius as well. i’m open to anything, but if it happens to be data from sub-saharan africa, all the better. if you’ve got any thoughts or leads, pm me. thanks!for those who do a lot of exploratory analysis or ml algorithm prototyping, what kind of tools (internally developed or externally available) do you use? the reason i ask is we are thinking about building some infrastructure for this internally (leveraging what\\'s available as much as possible). what are the considerations you would suggest before embarking on such a project? i\\'m thinking about, for example, infrastructure for database connection, scaling compute resources vs. quickly seeing if an analysis is promising vs. how easy it is to move from exploration to production. those might not be all or even the most important things to consider, though. basically, before we waste time building infrastructure that turns out to be useless, i want to see if there are any best practices for an exploration/prototyping platform?i have a potential co-founder opportunity if anyone\\'s interested. i\\'ve worked with christian for over 2 years now and can attest to his judgment and ability. he\\'s recommending his friend (sam\\'s) soon-to-be-startup:\\n\\n```sam is launching a startup and looking for folks with deep data science/statistics backgrounds, to be a cofounder or an early employee. he\\'s building machine-learning tools that help companies make better people management decisions, starting with hiring. the venture is very new, but they\\'ve signed up 6 relatively large companies for pilots and have assembled rich workplace data on 50,000+ employees. they\\'ve got a few hundred $k in funding and are opening an sf-office this summer. if you\\'re interested, sam\\'s email is \\n\\nit\\'s simple: if you want to start a company and build a data product in this space, you should do it with sam. he\\'s everything you\\'d want in a co-founder, and also everything that you didn\\'t know you wanted but would if you knew the future. he\\'s really smart, super organised and hard working, and most importantly posseses an outstanding character--he will always have your back. the only reason i\\'m not joining him is that i already have something else going on. do yourself a favor and reach out to him if you or someone you know is interested in building a data product that could improve how companies hire.\\n```i think that python 3.4+ has been supported since spark 1.4thanks everyone, this has been super helpful!thanks !postscript: i tried applying the updated `prophet` r package (v. 0.1.1) to my forecasting task and it no longer crashes. hahathat would be funny uploaded a file:  commented on ’s file : kevin take some more for me? thanks!videos? uploaded a file: here\\'s a short clip: what\\'s the conclusion/summary from this salary slide?my takeaway was that salary differences by gender for outgoing insight fellows is much better than the industry norm  uploaded a file: not to be a buzz kill but is that true? i was under the impression men/women mostly got paid the same for the same positions but just men got promoted sooneranyone in this group interested to form a team for this event? or if you know someone who might be interested? might be a long shot, but i think any phd who currently maintains student status (even defended) is still eligiblepm me if interested :slightly_smiling_face:::  \"note: the nba basketball analytics hackathon is open only to undergraduate and graduate students studying at accredited universities in the united states.”you can do it anthonyif you believedon’t dream it, be it~/giphy rhpssquare is hosting a data science meetup for anyone who interested\\nhey guys, there are various ml meetups in the bay area; i was thinking to start a ml meetup with insight fellows. we can meet sometime on the weekend at the insight office. let me know if there is interest!on ^, who wants one for nyc?anyone have experience with large-scale product merchandising? we have a client who manually curates groupings of similar or related products that would like to start automating the process i have some experience, not sure if applicable, but feel free to dm me with details so i can see if it’s relevantwould be interested!same!i’d be interested. the quality of meetups in nyc is very…mixed, so i think this would be an improvement.super! let’s collect names and set something up.agree with varun. there are so many meetups of very little utility. how can we ensure ours is unique + useful? … because _we’re_ uniquely awesome?personally, i’d be more likely to attend something that involved actually writing code or building a project over time to learn a new skillsi upvote that(heading into meeting, will respond shortly)like working in a group of 2-3 people to build some new prototype for a month, then sharing it with the group, then switching projects and switching partners:+1: :+1:how about open source issues?perhaps as one option, though in general that might be too narrow for me depending on the librarymore options the betteri have some issues i’ve been wanting to finish for statsmodels, maybe also a small contribution to mlxtend or sklearnwhat other sorts of projects would you want to work on?user embedding?potentially, though i’m not quite certain just yet as things will change soon i love the idea of building a project on a small team. count me in!looks like we’re gathering a crowd!tell your friends!interestedi would! but live in oakland.  if we switched between the  two offices that would be preferablesign me upi\\'m interested, but i am a simple data engineer. what would be focus of meetup? so there is a lot of cool stuff happening in the ml world right now, and basically all of it is freely accessible. \\n\\ni work on some stuff on my own, but i think it would be more productive and more fun to do it with peers :slightly_smiling_face:\\n\\nfor example something that we could do is do the stanford deep learning nlp course that is available on youtube. but tonight i\\'ll make a new channel and we can discuss what people\\'s interests are and what they hope to get out of something like this. we\\'ll come up with something :slightly_smiling_face:we can also form a kaggle team :slightly_smiling_face:all of our conference rooms have glass walls to encourage transparency and openness--if those are important aspects of your corporate culture, i highly recommend them! also i love the gentle hum of the industrial hvac  :blush:does anyone out there know of a good e-mail dataset with labels other than spam/non-spam?if you work near campbell, i would recommend living in campbell or even better at los gatos. campbell should be more affordable, though.have you looked at the enron email dataset? not sure about labels thoughhey  i’m going to be working at velo3d (4 year old startup on 3d printing) as a ds starting in july :slightly_smiling_face:yes, so far that has been my best option. i would be interested to hear about the tech they use at velo3d. i did research involving metals 3d printing in grad school :slightly_smiling_face:they are in campbell ? then i\\'d def live in the south bay or peninsula, or even santa cruz:thumbsup: uploaded a file:  and commented: machine translation is hard.(how does dishonored 2 become rayman 2?????)hey all, i\\'ve got a question on classification (and possibly anomaly detection): i have five known classes with labeled objects. in the future i want to classify objects as they show up, but there is the possibility that something will be anomalous (in a way i can not predict now). such an anomalous thing should not be put in one of the five classes, but should instead get a \"weirdo!\" label. what are my options? any good readings on this?two different models might be a good option. one for classification, one for anomaly detection.that certainly seems reasonable!i’ve had a similar problem in the past, and that was my solutionit worked out fine as this wasn’t something that needed to be done in real timedo you have any weirdo’s in your current dataset?no :confused:stay positive alex_gudecan you simulate some?how complex is the data?i can actually tell you the whole problem. :wink: we\\'re looking (well another team here is) at network traffic between devices. classes are things like \"web server, dns server, phone, laptop, apple tv...\". basically, everything on the network is already know. if another laptop is added we\\'d like to be able to say \\'that network traffic looks a lot like a dev laptop!\\', and if something weird shows up we\\'d like to say \"weird new thing on the network!\"so simulating anomalous network traffic is outside my knowledge basei am making this up on the fly now but what would be the problem if you were to model 5 models. 1 for each class and if all five models are pointing towards not its own class maybe there is your weirdo?just make it really weird then. add 1 billion to everything:wink:\"anything that connects to port 1,000,000,022 is weird!\" :wink:hahawhen i did this previously, i used a one-class svm for the anomaly detection where all the example data was used for training. then i used a multiclass model for the actual classification. can’t remember that one...yeahthat seems like the right way to attack itit\\'s sort of inline with tolgao\\'s idea, except instead of training n classifiers, you train 1 that says \"anamolous\" or notit wasn’t something running on streaming data, so the added time from that two step process wasn’t a big dealyeahyeah, in theory our group hopes to run these classifiers on the sdn hardware... but they\\'re looking for ideas, so anything helps :wink:hello smart insight peeps.  i need to get up to speed on tableau quickly.  any suggestions?run away :slightly_smiling_face:seriously though, i never came across any great learning materials out there, what was most useful to me was to reproduce tables and viz that i had produced using other tools like sql+pandastableau has some nice tutorials nowthere is a feature where you can set up the data source and then look at the generated sql, i found that usefuldamienrj: got any links?looking for the one i usedi liked this one, got me up and running pretty quicklythanks. jonneff: what are you being asked to produce via tableau?not sure yet.  :grinning::grin: got called into boss\\'s office at 4 pm to ask if i could repurpose an existing tableau app and write some python to automate alerts.  saluted smartly and said yes sir!it\\'s going to be a poor man\\'s rule based expert system.  anomaly detection without the statistics. alerts without statistics sounds a little dangerous to me. we’ve had issues with some “dumb” alerts in datadog that were wrong often enough that people just ignore them now even though they may indicate an actual problem some fraction of the timestay positive andrewzirmit\\'s rules like \"chip card in chip card country being used as mag stripe = fraud\"fair enough, as long as you aren’t building the alert-that-cried-wolfi will say, jonneff, having lived in “chip card country”, sometimes you just gotta failover to the mag stripe. :slightly_smiling_face:i think that the problem will likely be that it is a real wolf but he doesn\\'t approach the pasture the same way next time.  :eyes: \\ndat wolfdoes anybody have experience with nginx? i\\'m using it as an http proxy to make calls to a rate-limited api. the api service gave me two more keys, so i can now make three times the number of calls from three different servers. if anyone knows how to configure nginx to rate-limit _across_ concurrent http proxies (not individually), please dm me. thanks!fyi, this _may_ be what i was looking for. still playing with it, appreciate comments if anyone has done something similar.\\n sure. i’ll give you an idea once i’m there. i did my phd on this (3d printing + ds) so i’m excited about it yes they are in campbell. thanks for the advicefun post from yhat:  dear all.  happy hour tomorrow thurs, starting 530p @ b-restaurant and bar in sf.less data more drinking!2016a-sv is hostinghello! has anyone had experience opening outlook email .msg files in python? i\\'m having trouble figuring the best way to do it. thanks!!!(  \\n )thanks!!has anyone used databricks? opinions?it\\'s sweet: i have and use it quite extensively at work.  its a great and easy way to launch and maintain spark clusters for various data pipline tasks and analysis.  the notebooks are very similar to zepellin and work well with different languages.  they have a pretty complete and free community version that you can use to test it out.i took a spark class last month that used it. it’s pretty nice for handling a lot of the background configuration required by spark auto-magically in the background. i get the sense though that if you need to do something with a special or unusual configuration, it might not be so nice. that is wild conjecture though—it could work just fine.\\n\\nalso it’s written in scala, so things tend to work in that language a little better and a little earlier than they do in python. i didn’t have any problems using the python bindings though.awesome. thanks for the input! i\\'m playing with the community version now and like it so far. i\\'d like to play with sparkling h2o also.&gt; i get the sense though that if you need to do something with a special or unusual configuration, it might not be so nice.\\n\\ncan affirm ’s hunch. we have a some pretty specific requirements that we struggle with on databricks… but i will say, their support is really pretty responsive. they have shown us some backdoors that are useful. i would also second the use of scala over python (we use python almost exclusively) — there are things we have done in scala because it’s possible and not  possible in python. but, all in all, most days, we are thankful we have db managing most of the backend magic for us.  : reddit popped up in my seo demo. would\\'ve been funny if that happened a day earlier during the actual demo there. can you say \\'awkward\\'? :flushed:hey all, have to move out of my current place on july 16.  if you know of any available shorterm furnished rental opportunities please let me know!  thanks these are just my thouhgts so far.. you suggested s3 -&gt; spark -&gt; redshift.  given your goals this makes sense to me but i\\'d want to understand more what do you plan to do with spark?  if you are planning to work mostly with tabular/structured data (csv one option your engineer mentioned) then it seems to me that all you need is s3 -&gt; processing daily in python/pandas on a relatively high ram machine -&gt; redshift.  spark will be needed if you want to build ml parallel models using mllib, and/or if your data is coming less structured and you want to do some parallel processing of it... otherwise it may be adding more infrastructure w/o need for it... on the other side redshift can be considerably more expensive than spark and if you are planning to grow to 100 mm rows per day you\\'ll need some relatively large redshift cluster... if i were you i\\'d try some combination of this, starting with the simplest s3 -&gt; pandas -&gt; redshift and based on that decide... for that i have used this with no problem...  good point about spark possibly being overkill.  i can definitely write the code first with pandas, then switch to pyspark later if need be.  thanks!does anyone out her have a favorite weather information api? i just need basic stuff like temperature range, humidity, air pressure, precipitation (basic local news weather report stuff). it would be really great if their coverage also included more rural areas, and not just urban centers.i’m also thinking of heading in this direction for a project, so i’m interested to hear what you have found so far!noaa has an api.  if you’re still working where i think you’re working, then we have a similar use case\\nsee \\nfor example. i use the `requests` library to go hunting and parsing for information; i’ll leave the rest to you\\n\\n(ps you’ll need an api key from them)if you are going from audio files, bilstms have been state of the art from my understanding (a paper example here ). you can basically cluster on their hidden states once they are trainedi recently joined a top-vc backed ai &amp; cloud services early stage start-up in palo alto. it is in stealth mode so i cannot share so much, but it is an amazing opportunity to work with people who have had several successful exits in the last decade. basically they know what they are doing. and needless to say, the stock option is going to be pretty good. we are looking for senior backend data platform engineers in java / scala to build data stores, pipelines, micro-services, data orchestration, communication, distributed systems management, availability and scalability.  if you are interested, dm me.\\nand machine learning engineers and data scientists, as well.nice. do you know if the noaa api  has wether information for just the us or all over the world? in either case, i\\'ll look into it! thanks, !great question. it is a national service…haven’t tried internationallyi\\'ll let you know...looks like it does... which is nice. did some of this for his insight project.first to chime in on the jira discussion --- (1) it can integrate with github and atlassian and for that it is nice that i can write a spec doc and link my tickets. (2) it is not really designed for data science, but i find that it can be helpful to find info on things my colleagues are doing that i need by reading their descriptions and comments --- when they fill that in --- so please do that. (3) i am in the middle of my first \"self-assessment\" as part of our annual assessment and i found it really helpful to look at all my tickets since i started (next year since my last review).  it took some help from an se with more experience to know how to get that, but now that i know, it is pretty cool.   i don\\'t have experience with other systems outside of using trello in insight for personal stuff.second: i have googled and googled and googled and i need to sleep, so i need to ask for help and i thought i would start here.  i have found lots of pages that note that using string formatting for sql queries with sqlalchemy (and more generally) poses significant security risks, but what i can\\'t find is how to properly identify the parameter in the string and in the sqlalchemy call.  what is most important here is that i\\'m using mysql.everything i\\'ve tried either throws and error, or returns an empty dataframe and i know that the query itself is fundamentally sound, it is simply formatting the parameters correctly that is the problem. uploaded a file:  and commented: here is a super simple example.  assume i have already successfully set up the engine.  the question is how to correctly indicate the parameter in the query string and now to correctly enter it in `read_sql`.  i\\'ve read the docs, so know that tuple, list or dict are options, and i\\'ve had the same results with `{person}` and`@person` and it has to be a basic formatting issue --- everything suggests this.  but none of what i\\'ve found is mysql specific.try `%(perons)s`i think the interface is fairly standard and i know that works for the mssql connector (used it literally minutes ago).hi fellow fellows!\\ni\\'m looking to hire 1 (maybe 2!) data scientists for my team at coach, inc in nyc.\\ni\\'m a director in the customer intelligence and data labs group. we\\'re focused on customer-facing analytics and data science to grow our multi-brand (coach, stuart weitzman, and maybe more soon), global fashion retail business.\\nthere is a great team in place right now with 4 data engineers and 6 people focused on analytics and some basic data science. it\\'s still a pretty green group as far as machine learning goes, but everyone is very excited to dive in and learn. they have really invested in strong, clean internal data and coach itself is a very numbers driven organization. i report to an amazing vp (best boss i\\'ve ever had) who reports to the ceo. we\\'re well positioned to get some really cool stuff done, it\\'s just too much work to get done myself. if anyone is interested, i\\'d be happy to share an official job description, answer questions, meet for coffee, talk about our employee discount, etc.stahpkeep it classy bart\\nwatching a spark tutorial and wondering why the instructor keeps referring to some package called \"skippy.\" later realize she\\'s talking about scipy... :open_mouth:hi,does anyone have any experience with armax models?i do  anything i can help you with?hi all, i’m wondering if i can pick the brain of a data engineer who has experience with in-memory databases (redis, memcached, etc.). i have a few questions regarding why you would choose one service over the other. this looks like a good read for comparing redis and memcache . i\\'ve only worked with redis in the past and it\\'s super easy to get started. we have had issues at insight with security in that the default port (6379) on redis has been vulnerable to getting hacked  i\\'d be interested in learning about your experience with armax models! i created a #time-series-model channel if you don\\'t mind having a conversation therethanks austin, i’m hoping to take things in the redis direction, but i have a few co-workers who want the decision made with a bit of evidence, this is perfect, thanks!what if we\\'re not trying to convince people that diversity is good but trying to send the message that we don\\'t tolerate supremacist ideology? because i think you might be mistaking the point of these reactions.ok i read everything before weighing in.  \\n1. biology is not destiny.\\n2. if we devalue empathy, why should i give a rat\\'s a$$ that a conservative feels stifled?\\n3. google makes tons of money.  affirmative action isn\\'t hurting their business.  \\n4. another clueless male with a high paying job complains about being mistreated by programs that help people who have been held back by centuries of prejudice.  \\nps i predict appearances on fox news, a book deal and a job offer from trump\\'s equal opportunity office. i don\\'t feel sorry for him.  great resource for redshift tuning my team has implemented some hierarchical classifiers in the healthcare setting (classifying onto a medical ontology that is hierarchical).\\nin a tree-structure, we have implemented hierarchical classifiers, where each internal node has a classifier for its children, and the classification is done by starting at the root, and traversing down via the node-wise classifiers, until a leaf-node has been established. \\nwe found that this is usually a bit better than a flat classifier on the leaf nodes.\\nwe are also looking into some alternatives. let me know if you find some sweet ways of doing this.hi leifur, thanks for your msg! do you mind my asking how did you train each classifier at each internal node?individually or you can somehow optimize the entire tree of classifiers all-together?any bostonians in this channel interested in presenting their insight project to a broader audience at pydata boston meetup? i\\'m in contact with one of the organizers, who says there\\'s a slot for a bunch of alumni/a/ae/arghcannotevergetitright to present at the end of august. are you after international companies?no just the usa.. the rest of the world has paid leave.:sad_parrot: :burn:thank you for doing this! also this is depressing :sadpanda:hi all! does anyone have insight on advantages/disadvantages between chartio and mode analytics as bi tools? any specific reasons you would choose one over the other?\\nwould also love to know if anyone has used aws quicksight and has thoughts on it. thanks!hi all, has anyone done basic calculations on geospatial data, let\\'s say i have a table with lat, lon and a float (i.e., phone signal), and then i have a shapefile with boundaries, and i\\'d want to know the average(float) for each polygon in the shape file... the table with lat, lon could be billions of records (so could be quiet large)... i considered postgis as an option but if my table is too big it\\'s not the ideal dbcan’t edit notifications away  :stuck_out_tongue:loli later remember the non //here policylolhow fast do you anticipate your table to be growing?not that fast... the table currently has 1 bn+ recordsat most 100 mm / moor something like thatif it’s for exploring/prototyping, i’d suggest giving postgis a try firstfrom what i’ve heard, it’s usually not the number of records but the total size of your table that slows postgis downsince you only have point data, it should be able to handle lots of objectsif postgis, doesn’t work, you can try geomesa for which you can choose various backends (e.g. cassandra, accumulo, hbase, etc)but of course, it’ll be a lot more overhead to maintain itmmm nicelet me give this tools a tryof course, if you have lots of unused gpus lying around, you can give mapd a try :slightly_smiling_face:(they have since made mapd open source)my team uses chartio, but now we’re looking to host our own bi tool\"current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong ai. to achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. to demonstrate the essential role of such models, i will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.\" - judea pearl ()thanks for your response ! any specific reasons you weren\\'t satisfied with chartio?because we want to reduce an extra step in the data pipeline where chartio needs to copy over data into it’s own data store, and also for cost savings.  we found open source tools to be pretty feature complete as it is.i work for the city of ny and we get zero paid leave. in fact, after cuomo\\'s latest paid leave law went into effect in jan, unionized city employees are the only workers in the state, private or public, without paid leave. oh and i just had a baby, used all my vacation and sick days and am already back at work 6 weeks later. yea i it writes to cassandra locally. could it have to do with maven?hello everyone - i\\'m currently executing a pyspark batch job on a cluster. my data transformations are a series of map, flatmap, and reducebykey operations. it seems like my driver is the executor for every single task, and the reducebykey operation is taking a long time. i know that reducebykey is a wide task and will require shuffling, but i\\'m confused as to why the driver seems to be doing everything. any thoughts/gotchas i should look into? make sure you have executors alive on the spark master ui and when you spark submit be sure to set the spark master flag to the master ip:7077. it’s possible your job is executing in local modethanks! didn\\'t set the --master flag, so i was running this on local modenice. glad it worked! hope your week 3 goes great!i’ve checked out lifelines but it still seems to require the definition of a “dead” user before the modeling phase. but i have not seen the pareto/nbd paper. will check it out!this is such a perfect depiction of how i think we all feel in seattle right now!!oh actually, i have checked out the pareto/nbd models too. can’t remember why i thought they wouldn’t work now though… i’ll have to revisiti have not, but will take a look. thanks ayseni, for one, welcome our new algorithmic overlordsi’m surprised nuggetbot didn’t have anything to say about that, carloswhy would nuggetbot have anything to say about algorithmic overlo- oh my godhi all, some good friends of mine at ucsd are beginning a funding campaign for their non-profit startup that is focused on bringing the online education revolution to offline communities. they\\'ve been at it for a few years now and made huge strides (near the beginning: ). this is their first public fundraiser. they would love to get some donations before monday (when they will make a more public push) so those that come to the site will see funds already being donated. if you are at all interested in bringing the education revolution to everyone and not just to those with internet, please consider donating. also, if your company does matching contributions, that would be awesome for them. thanks! : well, it was nice having a job while it lasted. :wink:\"stay positive alex_gude\"(where is  when you need it?)nuggetbot helpnuggetbot adapter - reply with the adapter\\nnuggetbot animate me &lt;query&gt; - the same thing as `image me`, except adds a few parameters to try to return an animated gif instead.\\nnuggetbot echo &lt;text&gt; - reply back with &lt;text&gt;\\nnuggetbot help - displays all of the help commands that nuggetbot knows about.\\nnuggetbot help &lt;query&gt; - displays all help commands that match &lt;query&gt;.\\nnuggetbot image me &lt;query&gt; - the original. queries google images for &lt;query&gt; and returns a random top result.\\nnuggetbot map me &lt;query&gt; - returns a map view of the area returned by `query`.\\nnuggetbot mustache me &lt;query&gt; - searches google images for the specified query and mustaches it.\\nnuggetbot mustache me &lt;url&gt; - adds a mustache to the specified url.\\nnuggetbot ping - reply with pong\\nnuggetbot pug bomb n - get n pugs\\nnuggetbot pug me - receive a pug\\nnuggetbot the rules - make sure nuggetbot still knows the rules.\\nnuggetbot the rules - make sure nuggetbot still knows the rules.\\nnuggetbot time - reply with current time\\nnuggetbot translate me &lt;phrase&gt; - searches for a translation for the &lt;phrase&gt; and then prints that bad boy out.\\nnuggetbot translate me from &lt;source&gt; into &lt;target&gt; &lt;phrase&gt; - translates &lt;phrase&gt; from &lt;source&gt; into &lt;target&gt;. both &lt;source&gt; and &lt;target&gt; are optional\\nnuggetbot weather - get the weather for nuggetbot_dark_sky_default_location\\nnuggetbot weather &lt;location&gt; - get the weather for &lt;location&gt;\\nnuggetbot youtube me &lt;query&gt; - searches youtube for the query and returns the video embed link.\\nvictory - display an victorious imagenuggetbot animate me ninjas fighting(i’ll accept it)nuggetbot tell me rules of the internetfyi we have a channel dedicated to nuggetbot  *raises hand*but ask  i think he\\'s taught a class on itare you trying to track a fixed value or will your set point be moving around a lot?yep i have! not an expert but i\\'ve mentored students to run pid controllers on roombas that get discrete location data my strategy was to hold off mentioning this until the end, otherwise you risk getting cut by an hr person who doesn’t really know how easy it is. getting a tn visa is easy easy easy and your company will hire a lawyer deal with it. \\n\\ni gave the guy the letter from the lawyer, talked to him for like 5 minutes and i was in. lol, i keep telling people to use it but no one listens to me.everyone wants to do some optimal control bs.    i’ve also heard some stories of major delays with tn at the border, so just be sure to do your due diligence on your specific situation :slightly_smiling_face: the set point will be moving around a lot.  i’m just looking for something simple, better than the baseline of nothingmoving around a lot compared to how long it takes you to compute the control signal?if the computation time is orders of magnitude faster than your dynamics you should still be finemodulo some nonlinear thingscomputation time is much fasterdo you know of any frameworks that would fit this problem?what problem?  pid is a pretty general control algorithmthe problem for most engineers is gain-tuningi.e. picking the parameters of your controllerthere are some automatic algorithms, but most people can get away with heuristicsthe algorithms won\\'t do much better anywayone of the big advantages of pid over more sophisticated methods is that you can tune the parameters online and by handi can pm youre: tn, definitely give yourself lots of time for the application. i got pulled into secondary screening at yyz and had to run to the gate to avoid missing my flight. never had any issues once i actually had the tn stamp though. on lawyers’ advice i’ve brought my original diplomas with me every time i cross the border but i’ve never actually been asked to show them. was also given the tip that they usually have more experienced immigration officers on duty during regular business hours (9-5 m-f) and you’re less likely to run into issues with them, so book your flight to the us then if you canstay positive bradit’s hard sometimes, re: tn, don\\'t snooze on it, there may not be a nafta agreement in the near future that\\'s a happy thought :disappointed:  .. though if that were the case, it would still buy 6 months (jul 20) at the very least, if it were revoked on day 1 of the trumpian era... so more than just 6 months...demandbase is hiring a data scientist and data engineer! our triumphs in machine learning end up directly affecting the quality of our product, which has been super exciting (demandbase is one of the pioneers of account-based marketing). let me know if you\\'re interested, or if you know anyone else who would be! graphs, python, machine learning, spark, aws, etc.\\n\\nhas anyone used open street map data before for routing and encountered issues where the shortest_path function gives really poor results?does anyone know if the data is bad enough that roads in manhattan could end up being disconnected?stay positive mattkrems:  had a similar problem with his project.: my issue was that searching for the nearest node to an address would often return something like a node on a foot path, which didn\\'t seem to be actually connected to the street grid... i \"solved\" this by instead querying for the nearest intersection node (node with more than one associated way) and using that as the origin/dest of the path search. not sure if you\\'re facing the same problem, but hope this helps!if anyone is using natural-language processing for their projects (or alums that still work with it), just made a  channel for sharing ideasanybody have experience with/currently working on  clustering (hierarchical or otherwise) that involves non-numeric data (ie string-delimited categories)???? can you give an example of the kind of data you’re interested in?what is the usual procedure to standardize images of different sizes and shapes to the same size and shape, if i want to feed the pixels into svm etc. as features?  resize the images and pad using white pixels?hi , feels to me will be best to go into the fourier domain and then revert back with fourier interpolation onto the plane region you aim to cover. personally do not like the artificial white background you\\'ll be adding. note i have no idea how an svm will respond to your patching idea.: i’m not sure what the usual procedure would be, but photoshop has some pretty good options for batch resizing. i’m sure you can specify things like “keep aspect ratio” and fill background with white (whatever tasks can be done by clicking in ps can also be automated with a script)  i\\'m trying to cluster data that involves fec filings/ super pac expenditures - some of the data is numeric, but i also want to factor in expenditure type, candidate, ect... i agree that the padded white background is not ideal... i think i get the rough idea of the fourier transform idea; do you have a good reference or good package to perform that on images?  \\n i have the images as numpy arrays so padding should be relatively straightforward, i hope: one good way to deal with categorical feature is the \"one hot\" encoding scheme which transforms one discrete feature into several binary features. sklearn has a function that does that: : pandas has a feature pd.get_dummies() which is more straight forward than sklearn.preprocessing.  if it’s really sparse, you might want to consider using sparse matrices: of the top of my head all ones does is ft, zero padding in the frequency domain to satisfy your size requirements and then back to real space. you could use numpy to achieve all this but  the  suggestion may save you some time.  as to references      best bet is to google for fourier interpolation of @d signals and similar2d signals i meantdear all, does anyone has some publication data with bibliography information which could be used to calculated h-index? many thanksbtw, tangentially related, careful when doing interpolation in the frequency domain (if you ever need to do so).  fourier amplitudes decay fast with frequency thus posing challenges to most familiar interpolation techniques. here a link with a nice comparison (only sending the conclusions page):   thanks!: what field are you looking at?  has loads of particle physics and astrophysics data, and an api to boot(and counts citations and computes h-indices of authors)many thx, dstone.  any field would work. i\\'ll check it out.is there a channel for posting job listings?my parent company is looking to start a new data team and want to hire an ‘experienced’ data scientist to lead itis that a job with the tender team? i would love to work for tender.classic example of \"too much hackery\" in tableau: my colleague wanted to show zero rather than nothing in a table of numbers when there is no data. instead of a friendly formatting option \"show null as zero\" or similar the standard prescription () is a new calculated field with zn(lookup(sum([sales]),0)).\\n\\nfor tableau fanatics this is second nature \"sure, that\\'s easy, just create the following three new auxiliary calculated fields...\" but for most this cuts into the easy-for-anyone aspect of the product. my colleague who is a very smart, hardworking guy just said \"uh... no\" and now hates the product a little bit more.oh, also i forgot he also went and looked them up and asked, \"so, how is it that tableau is $6.5b company?\" my response: we just paid for an enterprise license.thanks ddrum001 and johnjoo ! (and andrewmo for posting)got my last job from someone i met there. :grinning:also i remember a session on hidden markov models that was pretty good.  of the general data science conferences i’ve been to, odsc was the best. that was odsc west two years ago. sounds like either data architecting (higher level) or data modeling (more detailed).  for architecting try covert, \"how to make sense of any mess.\"  for schemas try data modeling section of stanford\\'s free database course on lagunita. my friend justus, is coming to palo alto on aug 23. he and i will rent a place together starting sep 1st. between aug 23 and sep 1 he will need a place to stay. anyone has a room to rent out on such short notice between aug 23 and sep 1? or a couch to surf? thanks!anybody in the bay area with experience and interest in building recommendation engines in production, hit me up. there\\'s a role opening up on vevo\\'s (music videos) personalization and recommendation team for a senior ml engineer. hi all, i\\'m looking to make a transition from my current position, so if anyone knows of any opportunities for an experienced data scientist/machine learning engineer at your company in the nyc area, feel free to pm me. thanks.a thing i’m working is is identifying events in remote places as they happen. anyone here do super hyper targeting of individuals and events? i’m wondering what sources of info you might use outside of twitter/fb/youtube/google trends? - i’m looking to hire a spark/scala guru for part-time consulting, needed now through the next week or so at 10-20 hours/week, with opportunity for ongoing work at fewer hours. we’re in nyc but remote is fine. pm for more info such as rate and scope. thanks! have you looked at gdelt events data?this looks very useful, thanks !: acled is another event dataset you might like thanks ! , 80gb doesn’t sound that big. can you just put it in  big aws machine? they have instances that have 2 tb now  thank you, yeah, i agree, i do not think 80g should be handled by spark.  the matter would be, the manipulation in spark. do you know any good source to study this?why are you trying to manipulate with spark?if you’re trying to prototype, you can either run spark locally or use something like pysparkling, if you’re using spark with pythonnot in the spark, maybe, but in my python code, there is matrix manipulation. the problem is , i think , how do i load my matrix  from my data.i’m saying that you could just load up an 80gb dataset in a big ec2 instance and use python instead of having to deal with sparkdomino has all this set up for you. pm me if you want to try it out as john suggested in the past i’ve just used a notebook environment on an ec2 instance for stuff like that, i forget what such an instance would cost to know if it’s in your budget, but it’s probably the most straight-forward way to work with that data in python. i have my data in our system , cannot take it out side of the system. but i can work interactively. i may want to try what you said.  by the way, what is the different between running python script and running interactively?if you mean script vs notebook, the only differences are what you want to do - i didn’t read your initial goals, just the memory aspect.  i was just saying in the past when i wanted to do exploratory stuff in a notebook with a similar data set size i was able to do it on a notebook environment set up on an ec2 instance with lots of memory.  \\n\\nan alternative is to see if you can come up with an algorithm that allows yo uto somehow do your operation iteratively, not sure if this is possible, but if it is you can then maybe stream the data bits at a time.  thanks harry, “if it is you can then maybe stream the data bits at a time.”  i am now thinking if i can make this happen, do you think parallel  calculation would work?it depends what you want to do - for basic linear algebra computations like matrix-matrix multiplication you certainly don’t need the full data in memory all the time so if you can figure out how to partially load the parts you need, do the relevant parts of the computation, dump them out of memory, and load other parts, then you can get that to work.  i would bet someone has already written stuff like this evenif you’re using python, you can use daskat some point it becomes worth it to use spark.: thank you, what is the \"dask\"? dask:   thanks!!  a team at ebay needs to hire data science engineers in nyc.  it\\'s not my team but i work with them.hey all, i\\'ve got my first post up on the lab41 website, would love if you\\'d take a look! it\\'s about the datasets we\\'re using in our recommender challenge.  disclosure: i have an editor writing the headlines, and he is a bit enamored of buzzfeed :wink:hey all, if you are interested, here are the sentiment results from the super bowl last night for every 5 minutes of game timei am still working on mapping the plays to utc time and found a better avenue using the coaches film for each playi\\'ll confess mickey: based on the words being used by the panthers fans in my living room, i think your sentiment might be \"overly positive\". :wink:haha you need to tell them to get on twitter more!the most likely outcome of that would have likely been phones thrown to the floor, so perhaps it is best that they were not.: where is the positive spike for puppymonkeybaby?one of the best salary negotiation posts i\\'ve ever read, fwiw: twitter news:  +1 for the floodinghi jeremy - if you are still looking for some indian rupee change, give me a call or e-mail.  i am not as active in slack, so bit slow to respondhas anyone worked on spatio temporal models for irregularly space data (i.e., wind turbines in a farm field located in random positions, some meters away, some kilometers away to the closest one).  .. on a more general topic, anyone working on spatiotemporal models?oooops keeps forgetting not to put the at here sorry everyoneandrew ng’ s new book---machine learning yearning - chapters 1-14 —  a colleague combined krieging and facebook’s prophet modelhi all, i\\'m a bit confused with the newer spark api (dataframes and datasets)... does it still make sense to use mapreduce or is it better to let spark handle the operations under the hood? thanks! good hint... makes sense, sounds like a good approach, would you have some more details on the models used/approach, etc.sure, she wrote a blog post. (incidentally she is also an insight alum) \\nnice article!  love the classic \"stay tuned for part two, coming... never!\"lol, indeed. the work is done, but she just needs to write it up :slightly_smiling_face:wow nice!thanks for sharingyou bet!hey program directors - i’d like to see if its feasible to drum up a consulting project relationship between insight and the smart-electric business unit at mercedes. they have a pretty rich data set which i’ve been working on as a side project and it seems like an fruitful place for fellows to get some experience with a consulting project. might also lead to job opportunities for a associate to mid-range data scientist position. ping me if there’s some interest and we can set up a meeting.so cool!nice! this should be interesting.thanks for mentioning it,  could probably help arrange thiswhich language are you using? if  python, you probably want to stick with the dataframe api as much as possible. that’s because you will get near-native (scala) performance when using the dataframe api with built-in functions.also, for machine learning you’ll want to be using dataframes, since  will only support the dataframe api going forwardhi everyone! i\\'ll be a speaker at odsc east in boston in a couple of weeks, if you are attending, come and say hi! excited to meet other current and former fellows! is anyone familiar with apache hive? i’m trying to run a cross join with a temporary table, but having some errors.my company was just acquired and our new corporate overlords are not doing a great job of assuaging the doubt and uncertainty that comes with. it doesn’t help that there is a cultural divide (it turns out the job market and job culture are very different between a corporation in minnetonka and a start-up in boston) and they are taking away many of our benefits. has anyone dealt with this? any advice for dealing with the stress/knowing when to jump ship/anything else?i was at a publicis-owned ad agency when it merged with another publicis-owned agency. it created a fairly long-feeling/anxiety-producing period of about 6 months where everything felt stalled.all goal setting at the company level had to be re-evaluated and took ages, people expecting promotions were basically told, yes it\\'s still going through but wait, keep waiting, then eventually they came through, which was great, but it took a while. several higher-ups left as they didn\\'t have a clear place in the new org.recruiters usually pounce on people when they hear the news as wellafter that 6 months, things were pretty much back to normal in a cultural sense though. my girlfriend also went through something similar with her company having had a majority stake go to a pe firm this year.we also both jumped ship at least in part due to the inertia / uncertainty that it causedthanks . this is really useful.somewhat -- at least to the extent that spark supports hive queries.\\n\\ni think the general wisdom is to not run cross joins without any join conditions, but i\\'m aware that may not be helpful :slightly_smiling_face: i feel ya. i just went through/living through that. my company was a child company and hq decided last year to fully integrate. imo it was/is a poorly planned process. many left, i have visa issues  that made the jumping ship harder. i am on month 8-10 and things haven’t been back to normal, but on the positive i have started to do ds which was something i was unable to do consistently when i was part of my original company. to be honest the uncertainty kills, and oftentimes i’ve just wandered figuring it out how to advance my career within the chaos. also feel that this was not helpful :stuck_out_tongue:probably best to look around in the job market and leverage it to keep your benefits? most of the times in these takeovers, the most valuable thing is the employees, not the product.\\nnot sure how to deal with the uncertainty though.i\\'ll send you a dm.yeah, cross joins are to be avoided unless absolutely necessary.\\none advice is to have all selects and where clauses done before and then use the cross join on the resulting tables. if you have those conditions after the cross join, the cross join is done first and then the result is filtered.that shouldn’t matter. predicate pushdown should actually do the right thing regardless.i’ve actually seen worse performance trying to optimize predicates in ctes or subqueriesthough it’s been a while since i’ve used hivethank you so much, will definitely experiment with ithello insight community! looking for a great way to give back to stem education? check out letters to a pre-scientist, a pen-pal program connecting professional scientists with middle school students in high needs areas! this program has officially obtained fiscal sponsorship by a 501(c)(3) organization and is pursuing their first ever fundraiser. check it out and spread the word: this is useful - i’m in collecting data phase right now (because i have to be a scientist about this) but hearing different experiences just normalizes everything i’m feeling and seeing. check-ins with me from 10:30-12 tomorrow. reply to this thread if you want to chat - i can plan to see 9 peoplelol here we gooh no i know what\\'s gonna happen now:popcorn:ahahahahah no, i\\'m good.woooooo!  real career advice time!clean your desks everybody :)i\\'m down to chat!:oncoming_police_car: @-channel alert :oncoming_police_car::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice:ruuuutttttthhhhhhhh:bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji:2016a has still got the only emoji game in towni’ve been honing my skills for 18 monthspoor faye 🤣twitch had a  guy-fieri-themed @-channel incident a few months back. it was carnage for 16 hours straight.the frosted tips jokes alone must have lasted for weeks...my mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the chat. uploaded a file: ohmygod, i am so sorry. this happens every single time i want to @ channel our session from my phone. am mortified. [goes to corner and cries]nuggetbot pug bomb 10did not know pug bomb was a thing; i support the pug bombmixpanel, amplitude, and heap do a half-decent job of ingesting events data and letting you do basic analysis on top.  however, you always end up running into their limits and wishing you had more flexibility and direct access to your data.\\n\\nsegment is good to collect events from your site and it can get them into redshift for you.  or you can use their webhook to ingest it yourself using kafka or kinesis.\\n\\nif you want data from multiple sources (events, apis, other databases) there are companies like treasure data, fivetran, astronaut, or alooma.  but they are just piping this data in and tossing it into your redshift cluster, you get to deal with creating a system to update derivative tables.\\n\\n(this is where the pitch starts :slightly_smiling_face: ).  i work for a start-up that’ll get all of your data from these disparate data sources, put it into s3 and redshift (soon spark, maybe hive), and make it easy to create derivative tables that are automatically updated, hook up jupyterhub or a sql dashboarding tool (metabase), or any tool that can hook up to redshift.  and! pipe it back out again to another database or apis.\\n\\nhopefully some of this is useful.  i got involved with this start-up because i was tired of doing all these things instead of actually working with the data.  i wanted to build this tool so that i would never have to deal with it again.nuggetbot animate me  oops. i had trouble searching \\'@ruthtoner \\'. try later.as luke was told to go to dagobah, so you shall be told to go to nuggetbot animate me this is finegood bot invitae bought good start genetics (boston-based) in august and we\\'re learning a lot about how to do things better next time. \\n \\n- reduce uncertainty by making sure everyone has full-time offers (except the people that you need to cut right away)\\n- benefits etc... should be clearly communicated possibly honored the old style through the year and then transitioned to the new company\\n- people are needed to maintain existing systems and the company should be willing to pay to keep people around.  if you lose too many key people then the point of the acquisition could be meaningless.\\n- going from a small company to a big company is hard.  people won\\'t have full insight into all the products and that\\'s just the reality because with a bigger company things are more complicated.  that\\'s hard for people to get used to.\\n- speaking as someone from the other side, if you can see they are making an effort (and you believe in the mission of the company), i would try it out for 6 months.  it\\'s not too hard to find a new job. \\n- you can also use this as a time to demonstrate value to the new company by stepping up to help with the integration and make yourself more valuable, if you believe in their mission.   \\n\\nhope that helpsdoes it need to be in person? can we do it by phone or skype? :laughing: :faye:thanks guys! cross join was running fine in athena, but it\\'s giving me error on hive. also i am trying to cross join a temporary table. still don\\'t know the exact reason., i\\'m flattered that you want to check in with me to see how i\\'m doing, but i usually reserve that for partners that i have more investment in (maybe after the third or fourth date?)\\n\\ni wish you the best though!aren\\'t there close to a thousand fellows on this channel? i think you might have your work cut out for you if you\\'re only doing 9 at a time.hey all, you using data science for social good? come share it at bloomberg\\'s data science for social good exchange.  abstracts due aug 1st! looks like \"the data science handbook\" is finally out. preface by , featuring alumni chris moody and diane wu:\\nwow very coolit’s all about topo chico.hi all, just curious, what are the policies regarding database access for you? my company decided to hand us 90 days database credentials that are ugly and random just wondering if this is the norm for data teams. :slightly_smiling_face: thanks!/giphy topogigiowait… you only get access to your data for 90 days?if so, that’s kinda… broken.: does that just mean that each 90 days you get a new set of credentials?yeah :disappointed:every 90 days i have to request a new set of credsit’s self service which is not that bad except i have to update all my creds for python and other sql stuffstay positive kenny_kwanwant to know the standard for people like usthat sounds annoying, but not unworkable.:stuck_out_tongue:: i just started a free trial of the slack “standard plan” that runs to october 1st. so, enjoy the unlimited message history and other perks of standard while it lasts!\\n: create a file called `.credentials` in your home directory and store all of your credentials in there as bash export commands, yaml/json-encoded python dict or whatever. add a line to your ipython startup script and to your .bash_profile that reads this file and loads its contents into memory as either a python dict (in ipython) or environment variables (in bash). now your scripts will always work as long as you have a valid `.credentials` file, on any computer you work with. thanks! i am already doing so i don’t have to copy them around, but was interested in knowing more about what the community faces. another point that i have thought is that probably in startups these things are more lax, whereas in my case where all our policies are set by the parent company, then things have policies and all that fun stuff...: that\\'s great! while we have it, could you also perhaps setup archiving at ? that way we can continue to have full history even after the trial runs outon second thoughts... that archive is public :confused:: yeah, i was just looking at that. do you know if they have a private version?: 90 days is better than 60 days :slightly_smiling_face:there’s this: good documentary, i watched it this weekend. made algorithms fun and accessible. upcoming data eng meetup at netflix on wednesday april 27 at 6pm: : is the meetup recorded?i do not believe it is: confirmed, it will not be recordedsad.. okay thanks a lotwow this has been there since 2009.  interesting algorithmwow what a break through with deep learning… i read about itinterestingi’ve built a couple slack reporting apps using r and python. they report some various metrics on a daily or weekly basis. the upfront time investment is minimal and it’s a very easy way to keep interested people at my company informed.`corgi_gif_generator %&gt;% slack_spammer `stay positive mpanciathe american statistician association weighs in on p-values and hypothesis testing misuse:\\nthey make 6 statements. summary:1. p-values can indicate how incompatible the data are with a specified statistical model.\\n2. p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\\n3. scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\\n4. proper inference requires full reporting and transparency (e.g. p-values and related analyses should not be reported selectively).\\n5. a p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\\n6. by itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\\n\\n*other approaches*\\n\\nin view of the prevalent misuses of and misconceptions concerning p-values, some statisticians prefer to supplement or even replace p-values with other approaches. these include methods that emphasize estimation over testing, such as confidence, credibility, or prediction intervals; bayesian methods; alternative measures of evidence, such as likelihood ratios or bayes factors; and other approaches such as decision-theoretic modeling and false discovery rates. all these measures and approaches rely on further assumptions, but they may more directly address the size of an effect (and its associated uncertainty) or whether the hypothesis is correct.\\n\\n\"no single index should substitute for scientific reasoning.\"thank you for the summary,  !there was a reporting article on nature:\\nwe need to have a go board at the office, : i have an extra one! i\\'ll bring it to the office on friday and leave it there!alphago and lee sedol are playing game 2!hey, if any of you are attending the nips conference in montreal and want to hang out, let me know!current ai fellow here. anyone interviewed and / or interviews at netflix? i have a onsite next week (and it\\'s the last opportunity in my pipeline). looking to get as much help as i can get!  :pray:\\nrole: analytics engineer for original content promotion, \\ntopics: product metrics, a/b testing (stats). \\nthanks in advance! (please dm me if you or you know someone else that can help). i\\'m very very new to them, so just getting started was throwing me. i found some helpful beginner videos though and am feeling much more confident about them now. just finished week 1 of this fellowship, so i\\'m sure i\\'ll be back here freaking out again soonnot now nuggetbotformer fellow (16a boston) tamas madl launched a website tonight for the debate that will be checking and plotting hillary clinton\\'s and donald trump\\'s heart rate in real time.  tamas’ insight project involved being able to detect an individuals heart rate using only a webcam, based on the subtle fluctuation of skin tone on an individual’s forehead. the url is awesome! i look forward to watching it with that in the background!^in week 4 i was excited for demos and terrified of not getting an interview while simultaneously being terrified of getting an interview (because i was incredibly intimidated by the interview process). btw, i got a great freakin job as a ds at samba tv in sf wholly because of insight.also, how is there not a coconut emoji?! i think the first demo (and the leadup to it) was the hardest just because it was such a shift from the project phase coming out of week 4 (and because i was still tinkering with my presentation right up to the wire, so i didn’t get to practice it much :stuck_out_tongue: )though it’s definitely a breath of relief when the first callback comes…joyceschan: schema registry is great. we use a lot of kafka+spark streaming internally and sr was the obvious choice. it\\'s a common belief at week 4, but stubborn reality always proves the belief to be wrong. you\\'ll see... by week 8-9, you\\'ll start to wonder why were you ever worried in the first place. \\nalso... past fellows seem to be determined too to prove the belief wrong! :wink: [that said, ny fellows: looking forward to your demos in a few weeks!! ]i think at this point, i was still tinkering with some visualizations on the webapp, but was ready and excited to show it off already. the most stressful part was waiting for interviews (not preparing, just waiting), especially since it\\'s such a contrast to the fast-paced project building stage. once the first interview comes, it\\'s a sigh of relief :relieved:\\nbesides, imagine where you were 4 weeks ago. probably can\\'t even fathom how far ago that was. imagine now 4 weeks from now. that\\'s what insight is all about: quick and sudden changes :laughing:you will get call backs after week 7. :blush:\\n\\nalthough, getting call backs and getting a job is a different issue. for some folks, a job comes within a few weeks to a few months. which is awesome!\\n\\nor you could be in my shoes and have to go essentially three months before landing something. not to be a negative nancy, but the road has a possibility to be long!to piggyback..it is a matter of fit in many cases and some will find that sooner than others but don’t let that discourage you.  i took a bit longer than most to land something but in the end i found a role that was well worth the wait! :slightly_smiling_face:for those where it took a bit longer, how did you spend your time? more work to refine the project? other learning on the side? or perhaps get some much-needed rest? any suggestions (just in case!)?also...focus on the things you can control and don\\'t worry about the ones you can\\'t. there\\'s lots of variables at play how companies choose their employees and a lot of it has nothing to do with you (like...office politics).  that being said, your interview performance and attitude (positivity) will play a huge role obviously- so make sure you are ready for interviews and nail them! ^ def keep up the practice :slightly_smiling_face: we did a round-robin setup of giving walkthroughs/lectures/whateveryouwanttocallit on topics we had particular knowledge of during interview prep, e.g. one fellow goes through more straight-cs algorithms for coding interviews, one does stats, etcthere’s always time for that :stuck_out_tongue:i think in week 4, this feeling kind of creeps in  because after the crazy pace of working on the project, ranking and demoing at companies reminds fellows the real reason all of this is happening. i agree that it is best to work on what you can control and approach things positively. i was having a lot of visiting and demoing companies because it is a genuinely exciting experience getting a behind-the-scenes look at all these companies that play a role in our daily lives. advice re: beyond the 7 weeks, i enjoyed preparing for subjects i was weak in because it was a real opportunity to acquisition new skills and do it together with others around me. it helps to do it with others. also callbacks and interviews are pretty unpredictable, in my case, most fellows in my session already had jobs by the time i got callbacks, but i ended up with multiple offers coming in, in one week and ended up at the top company in my wish list!study and practice practice practice..but def take time off, or even a weekend off to replenish yourself.  its a draining process!#teamcoconut :team-coconut:  with regards to callbacks. given the variance in the process, trying to figure out how it all works will just result in a headache (at least it did for me). i know it\\'s difficult since we\\'re trained to look for patterns (more so than other job seekers), but without knowing all the relevant parameters, you tend to over-fit based on your small sample size. :stuck_out_tongue:  remember, as long as this process takes, it\\'s still far shorter than what you would have to do in academia.: i was doing alot of studying for my weak areas, specifically cs and whiteboard programming. usually working on leetcode questions everyday and/or reading about data structures and algorithmsi also structured part of every other day or so to comb through job ads, and bringing them to the insight pdsmichelle_tat: great ideas, thank you! only because you mentioned it: do not work on refining your project. it’s really important to focus on simply presenting your results (however crappy they might be…mine were) as best as you can and move on to practicing interview-relevant skills. everyone thinks their project can be a little bit better with another 1-2 days of work (or that their project is horrible and needs another month) but hiring managers rarely care about / notice the marginal improvements you’ll be able to make now.hi everyone, we’re looking to collect some feedback from a handful of practicing data scientists.  this would involve you taking a 20 minute survey, and the results will be used to directly inform the roadmap for our product.  we’re also offering a $25 amazon gift card to everyone who participates.  if you’re interested please dm me :slightly_smiling_face:  thanks!^lack of coconut emoji fixed!thanks everyone for your interest, our survey slots are now full :slightly_smiling_face:  appreciate it!hey everyone! one of the ai fellows just posted a blog post about his work doing audio super-resolution (recovering high frequency audio signal from just the low frequency audio signal). could you please heart on medium? interesting, there was a similar attempt at google brain but in video:\\nthe coconut one smells like sunscreen :laughing:what my grad school education consisted of:  applied math, optimization and estimation theory, orbital mechanics and control systems.  what it should have been:  how to make pretty charts in powerpoint.  hey!  hope all is well! i was wondering if anyone works at nordstrom or has any experience doing a data scientist screen with them. i\\'d love to hear about your experience...thanks much! :slightly_smiling_face:\"the result is called a tensor processing unit (tpu), a custom asic we built specifically for machine learning — and tailored for tensorflow. we’ve been running tpus inside our data centers for more than a year, and have found them to deliver *an order of magnitude better-optimized performance per watt for machine learning*. this is roughly equivalent to fast-forwarding technology about seven years into the future (three generations of moore’s law).\"i wonder what physical considerations go into optimizing for that sort of thinghow much of a step is it over designing gpus to be optimized for linear algebra (low-rank tensors)they don’t make it clear in the blog post, but i believe these do 8-bit integer processing and probably only get the 10x performance / watt for inference tasks. for learning tasks gpus probably are still the way to go. there is some discussion of this on hacker news.caveat: i know nothing about gpu designi know that graphics cards were also pretty powerful at bitcoin mining and were most commonly used by most miners. however, asics have been around now for years that easily break 1000x the performance of the fastest gpus ($1300 asic vs. a tesla  gpu). as long as the task is specialized enough i could easily see how an asic could outperform gpus and be cheaper in the long run. it may require optimizing the code to run on it, but i don\\'t see why people would see it as being hard to believe.here\\'s a breakdown of the speeds of asic/fpga miners, you can see graphic card speeds at the top by clicking on \"non-specialized hardware comparison\". almost by definition, an asic implementation can always be faster than fpga or gpu or an any pre-structured architecture. the problem is that the fixed costs of asic are huge, and redesigns are equally costly.stay positive alanasic -&gt; application specific integrated circuitasic -&gt; fpga -&gt; general purpose processor  gpu / cpuin laymans terms, a dedicated chip for your specific use casethe costs of building your chip include many phases of application definition, chip design, lithography/tape-out/fabwhereas with an fpga or gpu/cpu, you are using something that already exists and writing an application (e.g. code) to work within its architectureyes (though you can’t assume fpga is always better than gpu or cpu. they are more flexible of course, but for what they are designed for a gpu or cpu is always going to be faster.)with an asic, you are allocating, by design, all of the registers, gates, etc to implement the logic for your applicationwith an fpga and your hdl + libraries, you are allocating a finite number of reprogrammingable registers+gates to implement the logic for your applicationwith a gpu/cpu, all of those elements are already allocated into a the processor design and you are writing instructions to be executedyou have execution units in a gpu/cpuin real world practice, the ability to develop good software/applications is affected by things such as the developer/development team’s ability to work with the tooling/language, speed to execute and iterate, etcwhile an asic or fpga offer theoretically better performance, the ability to materialize that benefit in practice tends to not happen as oftendoes anyone have experience on  how to build a sci-kit classification model that mix numerical/categorical features and text related features.(that generated with tf-idf)?brad: uh oh! does this mean you\\'re leaving us! :disappointed:yeah, i’m moving back to the 6ix and working remotely from there permutation tests make no assumptions about the underlying distributions/data types, which is their beauty. (also, the m-w is actually a randomization test, but it converts data to ranks first).it looks like if there isn\\'t random assignment it requires symmetric distributions. i\\'m still trying to determine if their randomization technique is sufficient. i\\'ll have to look into it more.well best of luck! when do you run off?maybe more relevant in a few weeks for the new fellows :wink:these are actually good / fun questions, i wish that’s what my experience was when interviewing at most places haha, instead it was mostly dice / coin problems and reversing python stringsyeah, a lot of silly code questions.  still worth knowing though, i reckondoes anyone here use crisp-dm or similar methodology structure?  interested in getting some thoughts about that from people using it.thanks! i fly out on tuesdayhey guys, had a question. i need a python program to call a function \"outdata\" from a different python program called \"process_data\" (\"outdata is within \"process_data\").\\n\\nif i say \"from process_data import outdata\" everything works fine. but if i *change* the contents of \"outdata\", python does not recognize those changes even if i import again. any ideas?you are sure the path you are importing `process_data` from is the same as the path where you are editing its contents?also what changes are you making? on the fly?if it’s in a jupyter notebook, you have to explicitly re-load the package, re-running the cell with the `import` statement is not enough^ use `reload()`yeah reload did the trick!hey all, i am organizing a hackathon to download government data before it disappears :scream_cat:   eventbrite link to be added to the event, but feel free to share in the meantime! for nyc folks, there’s an event on feb 3, but it’s all full i thinkanyone looking for a mattress + bed frame? practically brand new, $100 each or best offer, i\\'m just looking to get rid of it!  jackieb: it’s coordinated with the other efforts out there?yep!all run by a lab out of upennafaik no other bay area events yetok, so i need to writeup a quick example of the various ways to use the ipython magic `autoreload` but this gives a good starting point:  regarding \\'s point -- in python 3 (if you\\'re using it) `reload()` is no longer a core function, it must be imported explicitly before use: `from importlib import reload`anyone in east bay, my dog got let loose by maintenance. 17 lbs blond freshly groomed female lhasa apso yorkie mix with purple harness. named circe. piedmont area kingston ave. please call number on tag.if anyone sees her pinned a message to this channel.found her!glad to hear. that was fast. hey guys, does anyone have experience using bokeh for apps/dashboards in python? they have some really nice example templates, but if i try to alter them at all (i.e., try to plot a histogram in the app rather than a simple line) the code breaks. is there a good tutorial on customizing these apps other than just the bokeh site itself?took 3 hours, but glad it ended wellone of my friends team at uber is hiring for a backend software engineer role. please pm me if interested and i can put you in touch with the right person. here goes the description \"if the world of maps interest you, there\\'s a position open in uber maps team for a backend engineer role. the position is similar to this one, and focuses on maps data. #softwareengineer - #maps #infrastructure, #data, and #services... \": it might be that the page (or part of it, say the map piece) is being cached by either the browser or the server. you should look into disabling caching for both. there are some html tags that do this (it’s just a suggestion) for the browser and there are settings that are server specific. i’m not familiar with gunicorn settings, but try to find the caching and disable it.making the map name different every time worked, so i am positive it is being cached somewhere (but i am positive it is not the browser). i will look into aws caching...i have found that some files are turning up in /var/tmp so that subtly suggests to me that it is probably supervisor/gunicorn (but i am not certain)linkedin open-sources a url detector and parser using finite-state machines. blog post by tj from the anti-abuse team: \\nstay positive carlosfahamanti-abuse nuggetbotstay positive carlosfahamloldude someone needs to rework the sentiment analysis nuggetbot usesseriously!nuggetbot check on mscherrermscherrer has a happiness average of 0.3925233644859813i’m so super happy, life is so wonderful like rainbows and unicorns. amazing!!!!nuggetbot check on mscherrermscherrer has a happiness average of 0.5370370370370371: more decimal places pleasedoes anyone know of a clever way to read files out of an s3 path with wildcards like you can do on a local filesystem with glob? i poked around on stack overflow and came up with nothing. i’m doing a bunch of string manipulation in python and kind of getting there, but i can’t shake the sensation that i’m doing this the hard way.the  —include —exclude flags?yeah, that’s kind of the best way to do it. i keep foolishly thinking of s3 like it’s a file system...i’ve gotten spoiled by how easy this is with glob. sorry to bother everybody...anyone use the databrick’s spark-xml parser and elasticsearch run into problems with accessing an s3 bucket? we have a fellow who successfully uses the spark-xml parser but the minute he tries to also import elasticsearch libraries, he gets runtime errors complaining about s3. i know there are sometimes problems with accessing s3 with spark and hadoop, and we\\'ve been able to fix that (setting the classpath to add the aws jars) but the minute he tries to import elasticsearch, he gets the error. any suggestions?stay positive reporterhoacan anyone recommend some good java tutorials?umm, i normally use this:\\n\\n\\n\\ni hope it\\'s helpfulany nyu alumni that want to go to the nyu alumni day on oct 25th?nyu alumni day? what\\'s that?: just some fun activitieshey all - i\\'m a current health data sciences fellow in sf; looking to get less-limited access to the locu api for some restaurant menus. if anyone has an \"in\" with a contact at locu, any chance you\\'d be willing to put me in touch? thanks! -andy hey alumni rockstars, did anyone use tensorflow in docker and do you know how to grab the output so that it can be used/displayed in a web app?you can exchange data between flask and a web front-end using jquery/ajaxis that what you\\'re looking for? you can mount a data volume and access data from outside the containter, there are a bunch of examples explained here:\\nthank you guys for the links!hi everyone! we are looking for a spark expert with hands-on experience to consult on a project. we expect around 10-15 hours. paid. please dm me if interested or know anyone who might be interested. thanks!dear sage data scientists, how do i extract a representative data subset (~200k rows)  from a large sql table (20m to 90m rows)?  what would be the best way to proceed if i need a representative data subset from  four or five related sql tables, each in excess of 20m rows?`select * from table order by rand() limit 200000` ?rand() ! brilliant. thankstake care if you have some rare but important things in there :slightly_smiling_face:mdhardy: some db use random()i think this solution has somewhat poor time complexity (o(n log n)), because of the sorting. there’s some discussion in this thread \\n\\ni believe `select * from table where rand() &lt;= .3` (replace 0.3 with your actual percentage) should be better if performance is a concern.(although this solution has the disadvantage of not returning exactly the desired number of rows.)friend of insight karl heibron on 23andme\\'s parkinson\\'s team is launching a microbiomics meetup in sf:it will likely be rather data heavy! definitely check it out on february 27th if you can!i will definitely be theremy girlfriend is looking for a job, ideally in boston (where i am!). she\\'s an nlp engineer with three years of experience in search, a master\\'s in computer science, and a phd in linguistics. she also speaks french and german fluently. much appreciated if you have any leads, e.g. friends working on search teams.not too get too political again, but all fellows currently on opt should contact their employers to explore alternative visa options. there is talk that opt will be either scrapped or modified, so extensions might not be possible by the time you need one. see: , paragraph: \"limiting legal immigration\"varunrajan:  thanks! rand() &lt;= threshold-percentage worked nicely.hey guys, had a question about jupyter notebooks. i looks like some of my figures aren\\'t rendering in \"dashboard\" mode after a certain point. anyone else run into this before? i have included the %matplotlib inline. could you give more details?in my notebook i\\'ve written a bunch of code snippets that generate figures in different places. if in the notebook i click on \"view\", then \"dashboard preview\", they all show up fine up to a certain point. then for some reason subsequent figures don\\'t render past that point.okay, now it does show up somehow. i have no idea what happened: wanted to add one more thing. i had in fact tried the idea of \"rdd.foreachrdd(lambda rdd: rdd.foreachpartition(processrdd))\". it appears as though that should take the rdds from a partition and apply processrdd() to each of them without causing any shuffle. but it does cause approximately the same amount of shuffle read and shuffle writes (you can see this in the \"executor in the :4040 page of the spark master) as an alternate approach that was doing the processing after the reduce phase. what it means is that the foreachrdd() happens *after* the reduce phase. we want it to executed *before* the reduce or rather in lieu of it. when i try rdd.mappartitions() instead, it thinks there\\'s no action  on the stream stream at all and ignores the input completely with the result that the incoming rate of this stream drop to zero !does anyone have any tips about getting a unicode data frame in pandas (collected through webscraping) into mysql? i have a function that uses sqlalchemy and pymysql which works, but not for this dataframe in particular. i\\'ve tried a number of data type conversions, but can\\'t seem to get anything to work. looking for any kind of work around, i only need to get one df with 10 or so columns into mysql. thanks!:  this is what my pandas to mysql code looks like (using mysqldb):def pandas_to_sql(df, database, table):\\n    import mysqldb\\n    db = mysqldb.connect(host = \\'localhost\\',\\n                         user = \\'root\\',\\n                         passwd = \"\")     \\n\\n    db.query(\\'create database if not exists \\' + database + \\';\\')\\n    db.query(\\'use \\' + database + \\';\\')                                                                                                                                                                                    \\n    df.to_sql(name = table,\\n                    con = db,\\n                    flavor = \\'mysql\\',\\n                    if_exists = \\'replace\\')                                                                                                                                                     \\n    db.close()\\n    \\n    return \\'sent \\' + table + \\' to mysql.\\': did you look at my post  ? i show how to use utf8 encoding to get unicode into mysql. in some cases you need to set the mysql character set to utf8mb4, not just utf8 (see e.g. ) mentioned a file: .thanks so much   this looks great! i\\'ve been struggling with unicode to mysql for way too long :simple_smile:what\\'s reddit\\'s button all about??\\n: the button!  :smile:our ceo (who’s non-technical but really smart) asked for resources to learn more about data science so she could be more informed when in discussions with the ds team.  does anyone have suggestions for books, blogposts, or talks?this has been making the rounds recently:\\n some of the relationship managers at insight gave positive reviews of data science for dummieshi team, i am building a classification model for imbalanced data (1% positive prevalence). we have a lot of data so i am subsampling the negative cases to match the number of positive cases. i plotted the calibration probability plots following this link () and the subsampled train &amp; validation curves look good. however, the test data (not subsampled) is essentially just a flat line near zero (slightly upward sloping). does it make sense to plot the calibration curve for imbalanced test data? has anyone used calibratedclassifiercv with imbalanced data? the use case for this model is to provide a ranked list with highest probability at the top. unfortunately due to the imbalance, the number of false positives greatly exceeds the number of true positives at the top of this list. any recommendations would be helpful. i can provide more details if needed.if you have high false positives, i would recommend using an ensemble model approach. train several different models on different undersampled subsets of the data, and then take a majority vote of all models. just make sure your test set is not in the training set for any of the models.you might still be able to do a calibration curve if you have a mean probability cutoff instead of a voting cutoff for the models. one huuuge caveat - if you\\'re using a boosted classifier this will likely push all your probabilities to 0 or 1, and i would not recommend using a calibration curve, or using a probability cutoff instead of a voting cutoff for your ensemble model.thanks  and !thanks. i forgot to mention my \\'client\\' wants the top 5 contributing features to provide insight into interventions. i\\'m currently using xgboost and grabbing the top 5 features when pred_contribs =true. i\\'m unsure of what pred_contribs = true does - but you might want to be a bit wary if you have correlated features. in feature importances for a scikit tree model, that will affect their rankings. we are looking for ds and de interns at tempalert (also in boston).thanks for getting back to me everyone! very helpful.hey guys, a technical question: is there a way/algorithm to partition a continuos variable into a series of intervals, while maximizing the predictive power of this new variable? let me make an example: you have a time variable that you want to discretize into, say, 10 intervals. how do you choose these intervals so that most of the information of that variable is retained?haven\\'t tried this before, but off the cuff, can you just do a series of binary splits using a similar information metric as in rf?would it be relative to one dependent measure?you might be looking for something like bayesian blocks: sounds very much like what we did in physics with continuous physical variables to try to maximize signal / background ratios across several bins of a given variablewe did it similar to what   recommends — automated a loop over several possible binning configurations and chose the best one. perhaps there’s a much smarter way, thoughthanks guys, this is helpful. ideally, i would like to have an algorithm choose the best splits, as  suggests, rather than testing a bunch of binning configurations. will look into the bayesian block stuff, this is new to me. thanks again!you can probably use sklearn’s decisiontreeregressor (or classifier equivalent) to find the first split, then search the two resulting splits for the next best split, etc. but beyond the first split, i’m not sure if you can somehow pull the information metric out of it so you can compare the next steps.  but it’s probably not too tough to figure out how to get that.got it. will give this a try and maybe post an update in a few days...i’m very intrigued by the bayesian blocks thing, very cool way to do thatthe bayesian blocks is pretty awesome, i think this is the current up-to-date implementation: originally here ya that looks righti was trying to find an implementation that wasn\\'t buried in `astropy` but couldn\\'t find oneyeah the astroml package is linked in the article you posted ``deep learning ftw uploaded a file: that\\'s hilariously terrifyingthis is the stuff of my dreams. and nightmares.wowjobsalaryreminds be of meowpurrdylast day for abstracts!has anyone dealt with issues with decoded_latlngs = google.maps.geometry.encoding.decodepath(latlngs)?what sort of issues are you encountering,  ?are job postings on-topic here?hi, ii\\'m current fellow, and i need twitter graph data (who follow who ) outside of snap. any help is greatly appreciated. thanksdoes anybody try using scikit-learn model across different  http requests on flask? i want to train a model based on the first request and predict something based on the second requst. new donald trump programming language seeks to make programming great again\\n\\nicymi, a recent de.ny fellow just posted a nice post on cassandra for “newbies”: dear respected elders, do you have any suggestions for a new fellow like me on how to best pitch an api key request to a company?  if you were at an api-key-granting company, what sort of request would you get excited to grant vs. what would be red flags in a request?  i humbly submit this question to the oraclenot sure, but likely the words \"academic purpose\" could be helpful yeah? right on.  this company states “we share our api with a limited set of corporate, education, and research partners”they’re more into academic projects than business ones? i guess i just don’t understand why they’d turn somebody downlike, they’re worried i’ll take their data and use it better than them and make all the money?katie suggested name-dropping insight, but that kinda seems contrary to your suggestionour company would view the insight connection as positivewe consider our data to be both sensitive (it has pii) and monetizable. so we would be looking for partners whose use wouldn\\'t compete with us, and who wouldn\\'t disclose our data (especially to the general public) irresonsibly.nice, thanks! i’ll try to angle it with that in mind!hi guys, i am trying to get tripadvisor api key but there is a limitation: \"tripadvisor grants a limited number of api keys to official tourism organizations and select other websites. it does not grant access for purposes of data analysis, research, testing, or similar uses.\"  any advice?is there a specific reason why you are trying to access tripadvisor data? is there another data source that\\'s similar you could use?yeah, i guess i can use wikitravel and expedia data instead but tripadvisor also has these reviews that could be usefulwould scraping the data be a viable alternative?hey cats - anyone at google? just put in a request for google flu data - hoping somehow that could be approved or expedited. also hoping to know the geographic/time \"resolution\" of it (zipcode/weekly/etc.) would greatly appreciate any help.hi fellows, has anybody seen a dataset that has diagnosis or medical records that have valid date and geographic information? i\\'m thinking specifically for respiratory issues and zipcode or hospital level spatial precision, so i can correlate with metereological information. (the mimic-iii database \\'dedates\\' the time information and adds a random offset to the date so it falls between 2100 and 2300, making it unsuitable for this purpose.)  this data set is likely not for public access because of the risk of identifying individuals.  thank you!what kind of reviews are you looking for? yelp has a handy data api. also, google might as well.welcome new fellows! anyone who has even the slightest chance of using the mimic-iii database should request access right now as it takes a couple days to get access to the data.you could probably try hospitalization data from hcup or the children\\'s database at kiid and associate the hospital zip code to the patients.\\nhowever, depending on the condition you choose, it might be very hard to get a breakdown by zip code. much more likely to get it by county, but is very dependent on each state\\'s department of public health.\\ntry to focus on a single state and, if given enough time, expand to the us and eventually the world!\\ni actually did something similar studying pediatric asthma.hello fellows, i\\'m in the first week of insight. still brainstorming the project idea. i would like to propose a project doing the face classification, i.e., like different disney character, or different famous people, etc. any advice is welcome. thank you.this blog is a great resource for image classification that doesn\\'t require nn using histogram of gradient features,  hello! hope you\\'re hanging in there during your first week. i guess my question to you is, what is the use case for doing face classification? i imagine this has been done by several fellows before. \\n\\nthinking about your use case and what kind of practical and/or business value of your idea should be your mindset when thinking about your question.there is a huge celebrity face data set, celeba is this for the ai program or for the data science program? if it\\'s ds, think about how to productizeeither way, actually disney!!! :slightly_smiling_face:hehe, i don’t think i’ve seen a disney-related project ever before…thank all for your comments. very helpful. p.s. this is for ds project.the local bike community or cyclist union should be the best bet for that information. i\\'m only aware of the ones in boston though.new blog post by  featuring some alumnae giving advice to their younger selves. check it out: hey friends - if anyone\\'s interested i\\'m giving a talk on using data science to disseminate off-grid solar in sub-saharan africa in sf on april 4: people who work/live in the south bay, if you\\'re interested in and enjoy art and work in technology, the palo alto arts center foundation (paacf) is starting an initiative of art and tech ambassadors. if you (or someone you know or work with) is interested, sign up here: i\\'m on the paacf board of directors so feel free to ping me directly if you have questions. :point_up:encountered this today, missy elliot came to the rescue. nuggetbot animate me missy elliot flip it and reverse itberkeley grade average by department: also, cal data: hey, domino is doing another data popup in nyc. sv folks know angela, who comes by to advise fellows often. she posted a blog about the popup domino did in sv earlier this year: .\\n\\nthe nyc data popup is on june 14, . if you want to attend, here’s a 20%  off discount code: datapopupnycinsight alum, , will also be presenting!new seattle fellows?? :grin:woot! welcome seattle :slightly_smiling_face:dear seattle fellows,\\n\\ndon\\'t worry if you start feeling like this very soon. it\\'s normal. :laughing:\\n\\n seattle fellows, plus the first cohort of ai fellows in new york!:wave:coolthis is awesome. what\\'s the location of insight seattle by the way?seattledamn, edited too fast!right by pike place market, i\\'ve been toldyep, right by pike place - 1st and lenora. come visit? :slightly_smiling_face:are there snacks?there are! :la_croix: :cookie: :coffee:i live close by (3rd and battery) will do yay! i want to come visit too!hey all, random expertise request.\\nis there anyone here familiar with tv programming/advertising in argentina?\\nplease dm me. thank you! pinned a message to this channel.has anyone here done significance testing with comparing percentiles between two different distributions ?does anyone have any recommendations for an egpu setup? if you\\'re mac inclinedapple is also has one has anyone generated google static maps requests via python’s ‘requests’ library?i think its mutating the parameters passed in via the ‘params’ keywordi.e. r = requests.get(&lt;api_url&gt;,params=some_python_dict)tried unicode encoding, no dice, =(oh, nevermind, stupid error on my partplease ignore the above, :sadkeanu:yeah, it is pretty handyi really enjoyed nate silver\\'s book. he does a good job of showing that some fields have become excellent at updating models over the past 20 years (e.g. weather prediction) while others have not improved but recognize that (e.g. earthquake prediction) while other fields still have lots of players engaged in puffery (e.g. politics/elections)another book i recommend is \"the visual display of quantitative information\"has anyone here filed for a tn visa? i had a few questions? what category do i file under ? i was planning to file under physicist.fyi: my phd is in physicsawesome, thanks ! i\\'m in chapter three right now and really enjoying it!  and thanks for the tip on the tufte, i knew it was famous but not sure if it was actually worth a read, i\\'ll put it on the list!depends on your job.. many data scientists are under mathematiciani came in under mathematicianah thanks guys ! if you have a phd, you should file under the activity you are performing. uscis is very particular about this. make sure that your employer\\'s letter with a job description matches as closely as possible nafta\\'s definition for that category. start by looking at -&gt; \\nto \\'s point, mathematician (or statistician, which falls under that umbrella) is a commonly aligned description.  \\n(also, you may want them to file that in your behalf to ensure that it gets approved easily, as of this year, uscis has started to get really picky about tns)hi everyone,\\n\\nanother boston 18a fellow here like pawan. what if my phd in physics is still in progress? i have an undergraduate degree in engineering.\\n\\nthanks and regards,\\n\\nabdullahthanks marco !+1 to applying ahead of any attempted border crossing. they still asked a lot of questions (which they continue to do every time i re-enter the us), but it was a big help.out of curiosity, has anyone ever heard of “equal first-author swap” on a cv/resume list of publications? i just saw this for the first time and after some googling it appears that it’s a thing people sometimes do when the first 2 authors of a paper “did equal work” but the second author is “allowed” to list themselves first on their cv/resume, even though they are listed second in the actual reference.\\nmost people online seem to think this is super sketchy and i tend to agree… thoughts?why do you think it’s sketchy?  is this not a situation where the publication supports co-first author?many publications have status for that, where it does in fact mean they did equal work, without quotes :smile:it does seem odd to me to do something other than publication order… but then i come from a field that puts little emphasis on author order for papersi had this come up once in a very specific instance... during grad school, another graduate student and i ended up doing nearly identical experiments, completely independently, and unintentionally. when it came time to publish, we talked about the idea of publishing our results in a joint paper and explored this as an approach to having equal credit as first author.\\n\\nin the end, we decided it was simpler (and more impactful) to publish separately, so we ended up publishing in the same journal at the same time.\\n\\ni could see it being a reasonable approach sometimes, in fields where author order is important.(i\\'m coming from ecology and evolutionary biology)i come from a field that puts emphasis on the alphabetical order of authors last names. first author swaps are not allowed. at least in astro, if you list a paper in your cv/resume and you’re in the top ~3 listed authors, then it’s assumed you did a lot of work on it. if you’re listed first, you’re assumed to have done the plurality of the work. if you’re in the alphabetical section, then you probably didn’t do much. the sketchy view comes from the idea that what is stopping me from saying i “split” first author credit every time i’m second author or a trio of authors when i”m third?i think it’s totally fine in cases wehre the publication supports and validates co-first authorship.  at that point i might as well be questioning their finding or p-values.  but if they do it on their own it’s sort of weirdif you want to explain how much work you did when you’re “only” second author, then add a bullet under it? i would just read it and assume, ‘cool, second author, they did a decently large amount of work’yesterday a paper of mine got accepted in a journal where i share first authorship with another postdoc.  i fully agree that we put in equal amount of work and were equally invested from the beginning.i’ve never seen a publication in astro support this in any waythink of it as pair programming.there’s a bunch of quality ones for biology stuffdo commits of pair programmed code have both author’s names on it?i’ve done second author work that can range from a week to like 1/3rd of the work^so have i!if i am co first on something (i’m not) it woudl be because its’ very close to 1/2 and due to much fewer papers in that field it matters(monkey work stuff)no, they would only have one.i know in deep learning/ml papers they often have little bullets saying “authors did equal work” for the first `n&lt;=3` authors. i suspect that is what some people are saying when they say “the paper supports it”but coming from a field that doesn’t have author ordering… y’all weird! :wink:i bet its a function of “time / resources per paper” with the more required, the more likely for co-first :smile:`academic politics are so vicious precisely because the stakes are so small.`does anyone know of any good vendors/services/workflows that keep the docker image together with the dockerfile that created it? have you looked into kubernetes?have you looked at jfrog artifactory? others that come to mind include nexus sonatype and aws\\'s container registry (ecr). i think you can use ecr without actually using ecs.i’m not sure if i follow the question exactly, but if the main concern is linking a docker image to the code that originally generated it (perhaps long ago), then i’d: store the docker images in any container registry (docker, jfrog, ecr, etc.), use git to track changes in the dockerfile, and then tag each created image with the hash of the latest commit in the git repo containing the dockerfile. \\n\\nthat way, if you have a random image and want to find the code that generated, you just check out the repo at that point in time. yeah we use kubernetes internally but that’s more container orchestration. i’m thinking about a more dumb service than that , yeah we currently use a combination of circle ci, github, and  for this. i saw that  would let you build images on their service from dockerfiles but was disappointed that you couldn’t link the image id with the dockerfile so that if you changed the tag, everything would follow along. i would have thought there’d be something out there that did that elegantly.  i’ve used ecr but not ecs with ecr. my impression of most aws services is that they do a low level thing really well but don’t really take features beyond that. i’ll check it out though  they have a link to an 8h-long youtube video of the whole thing(which is actually a playlist of many 30min talks)anyone else hanging out at the scipy 2015 in austin?does any one have experience with parallelization for assessing model performance on ~100k different data sets? spark seems like a good option, but it seems like it would be limited in terms of library selection. (i\\'m using python)\\n\\nhelp! there seem to be so many possible options and approaches while it\\'s not memory efficient, you could run them via multiprocessing if you have a decent machine that can handle it. how big are the datasets? can you hold nvirtualcores x dataset_filesize in memory? if so, the model is static and you\\'re only applying it to different data sets, you could pass model and the filenames of the dataset and let it go. if the data live in a db, it\\'s even less io intenstive. do you need to run it on a cluster?does anyone have experience with creating technical training materials for software use at a startup? my company would be interested in doing a lunch &amp; learn :)i’ve done some, what technical level are you thinking?more specifically, i’ve given the “lunch and learn” talk about some tech i’m using (but that’s kinda one-directional), as well as created some training materials for our analysts you could try`luigi` as well, i’ve used it for task graph specification.  sounds like you basically just need to queue up the same model run for a bunch of different self-contained datasets?  like said most approaches are going to be memory-limited, but at least luigi makes it pretty friendly to run arbitrary python code for the task nodeswhat about dask?i faced a similar problem during my phd research. i concur with lonnie, dask is an excellent way to approach this problem. it has a really easy syntax for constructing arbitrary directed acyclic graphs using dask.delayed, interfaces well with scikit-learn, has distributed dataframes like spark,  and scales well up to about 1000 cores on a cluster.can some insight fellow make a chrome extension that reads your jupyter notebook and automatically suggests pages on stackoverflow that are relevant?brilliant!!! but... how do you validate??? :stuck_out_tongue_closed_eyes:copy-paste til it works:wink: for building it, i\\'d say you have a feedback loop sort of like how search engines determine what links are effectivewould be hard to build the model / solve the cold start problem maybe*consulting solicitation - image classification guru*\\ni’m looking for an image classification guru to consult on a short-term project, 2 - 4 weeks, starting asap (like, next week). it’s a very interesting project for a fortune 500 company, hourly compensation at competitive market rate. pm me for details!surely google can help solve the cold start problem?assuming they’re logging everything you type into chrome...?anyone around who can help me think through a problem with time-series data? :point_up:does anyone here have experience calculating regression prediction confidence intervals with sklearn models? although this doesn’t answer your question, i believe `statsmodels` has prediction intervals, so if you’re doing something simple, that might be a good choice. happy to help  as well if you need ithi all! does anyone have experience with javascript? i have none. what i\\'m trying to do is:\\n1) have a drop down menu populated with states.\\n2) once a state is chosen, have a second menu populated with counties.\\n3) once the county is chosen, query a database i have on the (county, state) and display the other values in that particular row.\\ni would like to do all of this without refreshing the page since there is a relatively expensive map that takes a little time to render. feel free to dm me if you think you can help (code snippets, good tutorial, examples, etc...)!look into jquery, that should be able to help youhi guys! any podcast suggestions for big-data? i have about an hour commute per day and would like to use it listening to something more meaningful :simple_smile:partially derivative is fun and good about giving you and update on the recent news in data sciencewhat’s the point from fivethirtyeight is also good about current topicsi’ve also been listening to linear digressions and talking machines which seem to focus more on teaching specific techniques/algorithms/toolsdang, beat me to it on those.everything else i tried was just… not very good. so that is a list of 4 (ranked in order of how i’ve enjoyed them) culled from a list of like 10there’s also select episodes of: oop, you got that one too! nice job, paul.brennerhaha! i’ll dance on your grave!or… maybe that was a bit too intense.haha, yesnuggetbot: animate me that escalated quicklynot the one i had in mindsorry, i’ll back that up… but i’m laughing at least looks good but i haven’t listened yet^ oreilly data show/giphy anchorman that escalated quicklyi just couldn’t get into data skeptic. the subject matter is great thoughthanks guys! i’ll give those a try...*hi all* - i’d like to connect with ds/de alum who are working in *data-driven finance* or *commercial insurance*, and who’d be willing to chat a bit about sourcing data. pm me if you can spare a few minutes to talk at some point. thanks!for the ml people in sv : :  try cca since you have an output variable.   this is better than pca in this case. while pca will give you components which explain variance in x, what you want is the components of x which explain variance in y.    pca is not appropriate for this and may bring in features that have unnecessary noise, or don\\'t correlate with the output.hello! geopandas question! i\\'m having a weird issues where i now can not import geopandas in ipython. i\\'m getting errors like failed `cdll(/usr/local/lib/libgeos_c.dylib)` and warning:shapely.geos:failed `cdll(/usr/local/lib/libgeos_c.dylib)`\\nfailed `cdll(/library/frameworks/geos.framework/versions/current/geos)` and warning:shapely.geos:failed `cdll(/library/frameworks/geos.framework/versions/current/geos)`\\nfailed `cdll(/opt/local/lib/libgeos_c.dylib)`  i\\'m really bad with directories and knowing if i am missing folders or if my libraries are in the wrong place. any ideas?stay positive stephgline: we already had a channel for scala! it is called so fun: this algorithm unpacks the rappers whose rhymes influenced the musical hamilton .  plus some details on how they built it here: : nice! we\\'ll migrate hi people. does anyone have any good book recommendations about project planning and management? :partyparrot: good talk from last week’s spark summit regarding data culture\\n you should also send this info directly over to    or anyone else on the data engineering team at insight! it\\'s the best way to quickly find which current fellows might be interested in joining your team! :raised_hands:san diego is trying to poach tech talent from the bay area.event on feb 1: (the event’s in mountain view) thanks！oh dang, i\\'ve been going around saying thistimeseries finding trends example: computing rolling averages for timeseries: another good tutorial, the resample function looks incredibly powerful: a chance for free ebooks:\\n\\nhas anyone heard of predictive analytics innovation summit? is it worth attending or being a speaker for? nicolestrang: storing the changes of the table across time should do the trick (similar to a version control like git). however, it would decrease read time, as you\\'d have to get the base table and then the cumulative changes. nonetheless, depending on how little it changes with time, it would save plenty of space. take a look at `log triggers` for more information.dear data science sages: i\\'m interviewing w/ a few companies and have seen a couple of startups using a ruby on rails stack. what would be a good way to integrate python into such a set-up? - micro-services.  most companies are polygots (multiple languages / stacks etc) and let teams choose the one that they are most productive in.  so the way i have worked in such environment is make the data science model as api and let ruby on rails app just communicate via the api and get a json response just like any other prediction api services out there.anyone have experience passing google maps autocompleted addresses from javascript to python?sampathweb: - thanks ramesh! i was thinking micoservice applications but wasn\\'t sure; it\\'s great garnering perspective from a professional - thanks! i really appreciate it.sure. when/how?classic papers determined by googleif you’re ever alone on a friday night wondering what to doanyone have any tips or favorite packages for creating web dashboards? i\\'m currently looking at bokeh, data will be coming in from mongodb and postgresql. videos, tutorials, etc. are appreciated.shiny is pretty quick for adhoc stuffrshiny depending on what you want to do with itsuperset is pretty awesome. the built in admin and roles is hard to pass up especially if you\\'re deploying for your own company. user interface was a bit strange but gets super easy once you\\'re used to ittinkering with superset now, debating that vs metabasenb it doesn’t *yet* work with nosql databases. i’ve had issues with mongo. there is active contribution for elastic search (which i’ve never used), thoughinsight ai fellow  shares her project on separating overlapping chromosomes with deep learning: who\\'s on  !!???so i\\'ve found that, when preparing for contests where you are hopelessly out matched, it is best to talk a lot of trash. so: come get some you junior university trees! go bears! :wink: :bear: :football: well that worked out poorly. are there mercy rules in college football? :wink:so cold in nyc... must be nice to be in the valley...probably warmer than nyc, but colder than i\\'m used to!if it’s any consolation, boston is colder than nyc now. but i’ll take a bit of cold over measles outbreaks, dysfunctional transit, and absurd housing policy. (signed, a former bay area resident of 6+ years)&lt;— smug former boston resident in sv here….i saw it was in the high 40s this morning... a real weenie shrinker (la story reference)this is a great series of in browser demos of deep learning with javascript: pretty fun to play withwhoops, sorry the above link was to a specific demo. here is the full list: the paper has a bunch more, and is open: : i thought that plot was really important tooevery difference that they list before that last one they report it like: (62.7%) (χ2 (df = 2, n = 2, 473, 190) = 492, p &lt; .001).or 74.6% [74.56%, 74.67%] 95% confidence intervalbut they don\\'t list the values or the confidence interval for the gendered outsider difference (it just shows up in the plot). so is this not a statistically significant difference?haven’t read the paper, but reading the bbc pr\"\"women have a higher acceptance rate of pull requests overall, but when they\\'re outsiders and their gender is identifiable, they have a lower acceptance rate than men.\"maybe their paper makes this clear… the figure doesn\\'trather… when people don’t know somone’s gender and  know the person (insider) men and women’s rates are equal within their errorstarting the y-axis at 60% also distorts the overall size of any differenceswithout scrutinizing the stats, the outsider numbers seem “significant” but the results are either within the error bars or very close to being within the error barsstay positive jsapangood try nuggetbotgoogle embeds more words in more vectorspaces making *swivel* (obviously they tried pretty hard for that acronmy): knock if off, dudesleave words as strings¡backronyms ftw if you are interested in attending odsc west in san francisco (11/2-11/4), please use the following link to get 60% off your tickets : . \\n\\n(the discount code is insightdatascience)woooo! please see some earlier comments regarding using the at channel here. you effectively just pinged 1.3k people from a lot of different time zones.#thoughtleadersso much is wrong... starts with the hair@thushy works for insight. i think she meant to ping the entire channel. just sayin\\'.i’m sure she’s super sorry about providing 60% off tickets to one of the largest professional data science gatherings in the usi don\\'t find the notifications that annoying? but i\\'m biased ;)clean your dishes it took me way too long to figure out what odsc stands for. define your acronyms people!!!this.any data engineers or data scientists working with streaming pipelines that are attending strata in ny this week? interested in attending the apache beam dinner/talk on wednesday night? let me know and i can send you an invite :slightly_smiling_face:: sorry for the 2 notifications from me today. trying to share the discounts to as many people as possible.lolwill the @ channel vs never-@ channel debate become the great tech rift of this decade, much like vi vs emacs or tabs vs spaces?or the great la croix wars of 2017? it’s telling that there’s only an emoji for team coconut edit: nm there’s a pamplemousse we\\'re trying to keep this channel a civil and safe place, so please don\\'t use the c-wordi only have a few truly deeply held convictions, but most of them are about flavors of foodi think there is a civil war happening at fellows desks right now.\\n\\ni suggest that all current fellows clean up all their crap. :stuck_out_tongue_winking_eye:actually all of them are about flavors of food except for the one about the direction toilet paper rolls should be fed on the roller thingsi think the tp thing is only important in households with cats or toddlers#provemewrongdoes clark count as a toddler?we both know the answer to that questionspaces. always. spaces!!!!!team coconut is tearing this insight family apart...uhhh husbands who do it the wrong way are definitely a problem #justsayingmy wife thinks i do it the wrong way, but i\\'m not convinced there is a right way.oh do the different colored la croix cans taste differently? i never noticed. team tap water :potable_water: why is there a debate about the direction of toilet paper rolls?because people do it wrongbut... there can be only one way!exactly, that’s the right way* unless you have a toddler or cat that\\'s why you get one of these: not the best of reviews thought...this could be gamechanging...if you’ve forgotten about all the mvps we keep obsessing over at insight, we wrote a blog post to remind you to always start with a baseline :slightly_smiling_face: \\nwould love any thoughts you all have on that role^ tell yo friends? how excited are you about the ipo?yes. great post!\\n\\nfor a really simple baseline, i also like a random shuffle of the labels. i find it gives me a useful starting point in multiclass problems.\\n\\ni liked how you mentioned the baseline you choose depends on the data and the task. i feel like you could write more great posts (e.g. your post on approaching nlp problems) with that idea. for example, i didn’t know i should reach for a u-net if i have an image segmentation task.canadian fellows: what can i do in vancouver in the next 36 hrs?jira is the bane of my fucking existencei use jira for backlogging things and loosely organizing sprints and it’s fine. that said, i’m glad i’m not managing it and setting it up. seems a pain. convenient when it’s integrated with github.haha i don\\'t mind it in itself so much. ppl closing my painstakingly created tickets without actually finishing the work would drive me to tears of barbaric nausea however we used to use jira... painful. switched to zenhub (which integrates with github), amazing! a happier life is possible :slightly_smiling_face:i definitely prefer jira to asana, but there’s probably options that suck less than either of themexcellent info! what pain points did zenhub alleviate? (hate jira too)^yes! it seems like no one’s a real fan of jira or asana, but maybe zenhub is good?does anyone know of tools to track per-dataset bigquery usage across a gcp account? i.e. identify which individuals in an organization consume which datasets?it doesn’t seem like anyone is given any concrete reasons why they hate jira but i would give my impressions. my experience: we use jira at glassdoor but when i started we were using rally. i’ve tried trello on my own time and heard some talk about asana (my wife has used it a little in non-engineering contexts).\\n• it is built to be extremely flexible and do all the things but for many uses this is way overkill (boards, filtered boards, projects, epics, versions, labels, issues, sub-task - how do i map each to our org and workflow? why are issues not called tasks, why are sub-tasks not called sub-issues?)\\n• its defaults and structure are very engineering oriented, but as an engineering org it is still going to require much configuration\\n• advance search is the only easy way to find stuff, but of course there is learning curve before this is actually easy.\\n• when i got started i found i could fairly easily create issues and then not be able to find them, this is because it is super easy to create issues that won’t be visible in the current context. in my experience - shared by the team as well - jira is extremely inflexible precisely for the reasons than  points out: it is built with software dev/engineering in mind, and much catered to the agile religion... it requires task hierarchies, not built to be very intuitive, things get lost, blocks certain functions if not all task classifications are filled out... a lot of complications and unnecessary time investment if you need a simple tool to track your backlog, report issues, and track progress in a very flexible manner.\\nwhile zenhub _could_ also cater to the agile way, it it not built to trip you if you don\\'t. so very, very flexible. ui devs in our team use it as jira-like, the ds team uses it more as a backlog tracker, the general team uses it as a mixture of both. since it\\'s natively integrated with github, it\\'s seamless to link your backlog with commits/pulls. we also use it to document issues and the solutions that have been developed. this is extremely useful when you have offshore teams in different time zones, contractors coming in and out of projects, or kt when a project transitions to different teams within the org\\nyes, it sound like pie in the sky... but for whatever it\\'s worth, the whole of ds and it here transferred to zenhub, and we\\'re not planning to go back...awesome...thanks  ...makes a lot of sensewe’re sending out dev setup recommendations to incoming fellows and currently suggest python 2, vote below if think we should stick with that recommendation or should start suggesting python 3. feel free to comment if you have strong arguments either way!bmregner: while i think 3 is unarguably better, i\\'m still seeing 2.7 in much wider usage.i’d say 2 since more packages are compatible with it.  i try to use 3 when i can so i’m not part of the transition problem though :slightly_smiling_face:i’ve heard the package compatibility argument, but i haven’t run into that problem at all, any example packages worth noting?the big ones are mostly compatible, but there’s various small ones that aren’t - though i don’t have a list handy.  should be able to do most non-niche things with python 3 packages.we have some internal stuff that isn\\'t py3 compatible yet.also spark i believe only works with python 2.7yep, unfortunately pyspark is only compatible with 2.7though maybe that’s less relevant/giphy such visiondocker and ci question: does anyone use (or know of) a continuous integration service they like that is \"smarter\" about building docker containers? none that i know of cache the various layers, and so a change anywhere triggers a complete rebuild (at the cost of about 15 minutes).great question — ?i don’t want to speak for austin, but worth noting he is out of office through next weekis anyone here going to be at nips in december? that gif is from `heat vision and jack`, btw, which everyone ought to watch immediately: did u try go-cdgo has job, stage and task layerswill take a look!quick question, does anyone have any recommendations for a web cam that is good to use with something like opencv? you should be able to connect to any cam connected to your computer with opencv (afaik). anything particular you need from one?yay!on another note: internet, you\\'re grounded!y2.016k bug.hi all. i\\'m going to be arriving back in the us soon , moving from australia. can anyone recommend an isp? and which modality (adsl vs cable) i should best consider?ddeloss: you could probably use an arima model where each measurement will influence the final result dependent on some parameter and how long ago was the measurement taken (akin to an exponential decay with a certain half-life). that way you can sum up all the time-related information of one variable into one quantity and use a simple logistic regression afterwards.created  in case anyone wants to share tips and materials on back-end architectures for ad-serving, chat about targeting, audience-building, yield optimization, etcthis is interesting:\\n\\n\\nand here is a related  nytimes article, “what a million syllabuses can teach us”\\n: i also stumbled across plotly’s dash () and dashboards () a week ago and they didn’t seem bad ( limits the number of private plots though unless you’re a paying customer…depending on the balance of engineering time and desire for interactivity, it might be cost effective). but haven’t used them myself eithertoo bad the data from the open syllabus project is so limited. :disappointed: cute for playing around with their dashboard, much less useful in general though.i am working on a project where i need to determine the utc time of nfl plays. i have the short text description of each play and the time of the play on the game clock. i am trying to match each play to one of the 5 minute intervals from game start to game end. for each 5 minute interval, i have the text from several hundred tweets within the interval.\\n\\nso far, i can predict distinct events like halftime by using a tf-idf/sgdclassifer pipeline in scikit-learn. however, i am having trouble matching specific plays to the intervals, since the description of the plays are very similar to each other. can anyone suggest alternative methods for converting game clock time to utc time besides brute force?:  pandas has some functionality to help with this.  thanks for the info! i am able to bracket into 5 minute intervals with pandas, but i need help matching the game time to utc based on the tweet text and play text.:  can you convert the utc time to the appropriate time.i think an example of a row of data would be helpful. :wink:: hey alex.hey!so i have a list of plays in an nfl game with the time on the game clock, which does not correspond to real timeah, so like \"4th quarter, 5 minutes left\"?exactlyand i have a pandas dataframe of tweet texts with corresponding utc time bracketed into 5 minute intervals: can you show us several rows of the nfl data time stamp.: you can just copy and paste into here. uploaded a file: : yeah i can see this is very hard. do you have any reference start times for the clock and times for commercials.so your big issues is going to be that the play clock doesn\\'t match the real worldunfortunately, there is no timestamp associated with each play - only the time on the game clocki tried treating it as a classification problem and i do a good job of predicting distinct things, like halftime: yeah without a reference time to index against - and also without knowing times for commercial breaks. it seems like a hard problemyeah, i’ve been struggling with it - i might just estimateadd 2 minutes for every scoring play for commercials or something like thati guess i should have chosen the other football for my project :stuck_out_tongue:i don’t see a way to do it without knowing a lot more. injuries, time outs, the occasional power loss, etc.stay positive joshainsleyi have injuries and time outs as wellis pulling from another data source an option? or are you trying to infer this strictly from tweets?i checked other sources, but none include the actual timethe closest i got was the nfldb, which has utc time of database insertion - varies wildlysince this sounds pretty difficult to do accurately, is there something you can do to approximate? just add on times from the expected game start time?: i think going with your heuristic is the way to go. ..or getting another data source.or even figure out if it’s really necessary and if there is another way to get at your goal.i’m leaning towards just adding up the time and adding time for breaksi was just wondering if there was a more elegant way to do it: without reference times or another data source, i don’t imagine that there is….you can determine the average number of breaks for games and the average length of breaks and use that.. with some cis..use all the data to create a distribution of how long each type of play takes and then probabilistically sample from it? is that elegant? :stuck_out_tongue::  i like that idea…: good luck!!i like that too, i only have to get each scoring play within a 5 minute windowthanks for the advice!i hope it works out! good luckhi everyone ! does anyone have any experience deploying a shinyapp that contains rcharts? not in years, but have you seen this? keld_lundgaard: pricing for pro accounts is prohibitively expensive for a team of this size. something like (at least) 2-3k / month. if losing message history diminishes the value of this slack enough, we’d probably have to charge users and i think that may be a non-starter as well. the other option, that a group like this should be able to handle, is to build a searchable message archive via the slack api.sounds like an awesome de project :wink:my office uses rocket.chat, an open source slack knockoff. the downside is it needs to be self hosted, which involves some expense and maintenance. does anyone know the problem with\\n```-bash: brew: command not found```\\ni have a new mac and can\\'t seem to work with standard bash commandsneed to install `brew`\\ni was doing that but this line is giving a problem, even though the directory exists: \\n\\n```$ ls -l bin\\nbin/wget -&gt; ../cellar/wget/1.16.1/bin/wget```you’re just typing `ls -l /usr/local/bin` right? you’re not adding the other stuff?i was adding the other stuff as wellbut i think the error was popping because the homebrew is at 1.19.4_1 version now, and it comes prelinkedi followed the other steps and then tried installing the package i was trying by brew install ...  looks like it worked.cool!very random, but from what i understand, women entering workplace/gdp numbers are very flawed because unpaid housework isn\\'t counted as gdp but paid housework is, so if a former house wife enters the workplace, takes a person\\'s job, and the person becomes a paid houseworker, then gdp has now gone up even though no additional output has been addedbut that\\'s only something that could be known if you knew the details of how gdp in this instance are calculatedanyone know of other statistics that are only seen as misleading if you dig into the methodologyi used dbscan mainly for cluster detection with the purpose of not assigning clusters to a lot of the \"noise\" in my data. it\\'s really helpful if you don\\'t know / don\\'t want to know how many clusters you\\'ll have beforehand, and also if you don\\'t want all your data points assigned to a cluster.hey all, does anyone have any good tutorial/reads on how to model rare events with historical data using statistical inference and creating simulations?you can try zero inflated models, like zero inflated poisson, or negative binomial. you can make inference and simulate from such a process using bayesian models. i can\\'t think of any specific tutorial nowgood start for me to look into, thanks tadaishi.hey! i\\'m leaving tomorrow and would like to get rid of a cheap working bike. i\\'m at the insight office (in sv)  now. anybody who might be interested please message me directly.fyi the rally positions are in sf and dc. perhaps all hiring messages should start with a square bracketed line with a 2 letter country code an optional region, a city name, and a general capacity (role). eg:\\n&gt; [us az mountain vu - cashiers (wfh)]\\n&gt; excitingly creativ naeme is rapidly expanding and looking for people to help process the bulk transport of large cash assets. familiarity with usps and envelopes a plus.woah i\\'m in:  interruptible map function for multiprocessinghi all, anyone here with a computer vision background? (physicist asking) looking to define feature vectors to characterize images details in binary images (black/white) while keeping the vector spec as robust as possible to geometric attacks such as rotations, scaling, translations, etc? hints?...  might helpthanks for that . will take a pick. still on the mvp phase thus building very simple features that my common sense can recognize. thinking about histograms of sizes for connected (contiguous) areas of black/white patches.  the winning entry for that is a conv net  . it probably wont be very useful carlos unless he comes up with a decoder version - carlos doesn\\'t have labels.  this is doable but tough.  however, would recommend reading the forums.  there might be good tips there  apologies should have clarified, take a look at the forums, specifically “so what were your approaches” some of them didn’t use convenets, hope it helpsdoes anyone have experience with svm handling network derived metrics (pagerank or community (louvain/walktrap derived)?i come from a more “statistical” background and am more comfortable with permutation based/ bayesian approaches to modeling networks.thanks in advance: here is a cheat map that i find helpful for choosing  ml algorithm according to the given data:\\nanyone a master of putting nested json in to pandas? — dm, i do this for work pretty frequently: used recently with twitter feed data. you can email me @ @general: happy to connect with you on linkedin. feel free to send an invite. welcome new fellows!: thanks - it\\'s really helpful. i was looking for the map a few weeks ago, but didn\\'t know where to find it. really basic strptime question, how would i parse out year month day from 2014-06-05t15:48:00:000? the time stamp part is making it tough. thanks!!the boneheaded way is just do yourstring = 2014-06-05t15:48:00:000; year=yourstring[:4];  month = yourstring[6:7]; etc.ahhhhh! thanks!in general, i have used the parse function from the dateutil.parser package which is pretty amazing in parsing dates.`pandas.timestamp` has a very robust parser toothanks so much!```import pandas as pd\\nt = pd.timestamp(yourstring[:-4])\\nt.year, t.month, t.day```no problem :simple_smile:i am trying to import a string into a text datatype in a mysql database in python, i get the following error: \"unicodeencodeerror: \\'latin-1\\' codec can\\'t encode characters in position 0-5: ordinal not in range(256)\". it is due to trying to enter: \"богота\". anyone know how to fix this?hi guys, here are some cheat sheets for python, matplotlib, and pandas that i\\'ve found extremely helpful. hope this helps: has anyone played around with scraping reddit data in python using praw? 2 points: a) i think mysql is not very good with unicode/can handle it with the right config (feel free to correct if anyone knows more).\\nb) try pythons decode () with error=\\'ignore\\' or the like to get rid of it problems in a simple, hackish way.  : you might be interested in looking at textons. a good start would be to look at the visual geometry group at oxford (many other nice little piece of code to get you from zero to hero) :wink:hey all, in our fall session of insight, we made some custom insight-themed emoji for our slack channel.  i\\'m adding them here as well.  examples::insight_left: insight :insight_right::insight_left: john :insight_right: :john::insight_left: wafa :insight_right: :wafa::insight_left: zach :insight_right: :zach:if you had other pds you\\'d like to add, the process is super simple and takes about a minute, see iderjerneck i was trying to avoid the hackish way but might just go with that for now. thanks!whoops, above was meant for does anyone have experience using the meetup api? i\\'ve posted on unicode in mysql and python before in the sql channel. if the infos not there anymore dm mea bit like kaggle competitions, but for forecasting world events and trends: does anyone know a good, automated way to get approximate latitude,longitude boundaries for a city?like an approximate square area (for example) if you can get the boundaries of cities, try the python shapely library anyone know how to integrate variables into urls you are reading into python? i\\'ve been  using requests.get(my url here) but i am using a url in which you can query so i would like to add variables within the url but am having trouble concatenating them. thanks!can you give a specific example?( ^^):  try requests.get(\\'\\' % yourvariable) # yourvariable\\'s value is substituted into where \\'%s\\' isalso : maybe you can move your questions to ds-sv-2016a?what, and deprive the rest of us of the opportunity to help? :stuck_out_tongue:but yeah,   would be more appropriatealso, check out `str.format()`. you can use named tags like:\\n\\n```s = \"hi, my name is {name} and i’m a {occupation}.”\\ns.format(name=‘brad’, occupation=‘data scientist’)```: i got (very) approximate city bounding boxes from google geocoding: does this help:   (at least for the us)does anyone know if there if there is a not completely roundabout way of getting postgressql running on an aws linux instance?`sudo apt-get install postgresql`?fair enough, they just had a quite involved white paper about the whole thing, that i probably need to just pick through a bitbut thank you , that’ll probably take me 90% of the wayand apologies for the sillinessjust spin up an ubuntu ec2 instance and 3-4 commands later you\\'re good to goayedoes anyone know where to find granular (e.g. precinct) voting data for the 2016 election?judging by the sources listed here you have to get it from individual counties\\nor beg the authors of that page i had a great experience with two men and a truck.  they are on the pricier side, though.is anyone interested in a job as a data scientist at a consulting company in nyc? my former employer is looking for someone new. the work would be focused mainly on the energy and pharma industries. send me a pm if you\\'d like to know more.an old but relevant blog post by  hahaha, i was just thinking about my insight project as we were rewatching the force awakens on friday. “this movie got me a job!” :nooice:thank you for the kind shoutout!spoiler alert:  random forest works pretty well for nlp. :grinning:does anyone in the bay area have indian rupees they\\'d like to exchange with me? i\\'m heading to india soon for a trip.this came out, has some interesting things perhaps even for those that are experiencedyep! especially on the attitudinal side like surveys, interviews, diary studies, ethnographic studies, focus groups, participatory design*hot off the press*: we just starting looking for more data scientists at samba tv in sf! samba’s an awesome place to work and long-time friend of insight, so if you might be interested in working with us on gobs of tv data, dm me! \\nhi alumni,\\n\\nmy name is joel swenson and i’m a program director for the health data science program at insight. i would like to invite you to our san francisco office for a data science meetup featuring color genomics.\\n\\nfood and drinks will be provided.\\n\\nplease rsvp here () by this friday (11/10) as space is limited.\\n\\nhope to see some of you there!\\njoeldomino is doing a data science management survey. chance to win $250: has anyone had experience with saving and embedding an image from pyldavis in flask?nevermind- found a solution!:  :wink:i\\'ve always thought that the last post in the thread on this comic should say, \"nevermind, got it to work!\" -- would be more true to life.agree!for anyone curious about setting up neural networks with gpus in windows i recently published a post about it.\\nprofeshcould someone help me interpret something… i’m trying to install xgboost on osx following these directions:  and have run into trouble at the last step. i think it’s because i use osx el capitan. \\n\\nit says: \"note: if you use osx el capitan, brew installs gcc the latest version gcc-6. so you may need to modify makefile#l46 and change gcc-5 to gcc-6. after that change gcc-5/g++-5 to gcc-6/g++-6 in make/config.mk then build using the following commands.” \\n\\ncan someone help me interpret what that means?i think it is just asking you to edit line 46 of the makefile in the xgboost directorythat might be out of date, it looks like that change has already been made in a previous commit but i’m on yosemite and had no trouble running makestay positive andrewzirmalso, what brad said :slightly_smiling_face:ok so i guess that wasn’t the reason why i’ve been having an error, but i’m going to try something else… thanks for the help!thanks mike. i saw the supervisor log file and noticed an error right before the site stopped loaded. apparently there was a problem with flask (i found this on googling the error message). updating to new flask has solved the problem right now or it seems like. i will look into your tips if i have more problems.stay positive pushkarinip.s everyone else in current session, you may want to update your flask (you may notice an update in your werkzeug installation).glad to see that solved the issue!nuggetbot animate me \\'log file\\'haha :smile: :smile:hi! i am interested!! i am at an airbnb in east palo alto and there was a shooting and cops outside my door last night…i gotta get outta here! thanks!: article on recruiting data scientists (and how we do it at sailthru)\\n:thumbsup::thumbsup::smiley_cat:thanks dvats, very interesting articleyes, now get everyone in the world to change their interview process please :simple_smile:\"working on it\" -- data scientist at  :simple_smile:re-post from  :nice! sounds like a couple of interesting discussions\\n\\npreetika [11:18 am]\\ntamr cofounder michael stonebraker won the turing award..neatsean\\'s project!hahgood read. insight gets mentioned 2x!! also, interesting point about code and data sets being considered in academic advancement. at o’reilly jupytercon in nyc last week, there was much discussion about how jupyter notebooks will help with curating and publishing such code / data. (if you have a subscription to safaribooksonline, you can access the conference talks in about 2 weeks.)\\n\\nanyone work with ads and dealing w gdpr? i know you\\'re excited to talk with me about it\\'s impact on the ability to personalize advertising for a better user experience if anyone’s interested in entering product management, one of my friends is giving a talk about being a pm for linkedin not sure, but  _might_ be using / have used gcp dataproc feel free to dm me. work as a data scientist in a prod capacity at health tech startup.thanks  and  ! will dm.. thanks  for the cartodb suggestion! :simple_smile:yes, cartodb is wonderfulhappy new year! hey all, i have a good friend in the bay area (and who’d like to stay here) who’s been doing webdev/ui/ux/design stuff for a few different companies for about a decade now and she’s really not pleased with her current company. she’s also recently gotten interested in and a little experience with doing some analytics (usage stats, etc.) related to website designs and stuff like that. if anyone knows of any possible positions that she might be good for, please dm me. thanks!airbnb seems to have a great data science on boarding education. lovely! they also just open-sourced their data exploration + viz tool: ai and the competition for talent in silicon valley uploaded a file:  and commented: really great piece of research, just the best, reallydoes anyone know how to export hive query results to parquet format?i have used pyarrow for sql to parquet. might be worth a try. thanks dr neff.how about this:  create table a stored as parquet as [sql].  then pull the file off hadoop fs.  you can use spark to do this :slightly_smiling_face:that\\'s amazing seiji, thanks for sharing! uploaded a file:  and commented: my company gac r&amp;d has an opening for a research position oriented on ai, experience in robotics or control desired but not required, in the automobile industry (domain knowledge not required), see the post here commented on ’s file : good ideas! thanks!any one has a good way of building data dictionaries for their databases?hire an intern?  i actually have yet to see anyone have a good data dictionary.  i always have to build these out for whatever data i have to use, and it\\'s a painful process of exporting schema and then poking at engineers and jira tickets to fill it inwhat do you document it on?just a google doc?a spreadsheet with an intro sheet describing each table and then additional tabs, one for each table.  i color-code them based on data source if the warehouse is integrating multiple sourcesahi was hoping to avoid spreadsheets but…it is the inevitable thing to do.what else did you have in mind?i feel like describing a table is easy to do in a... table :smile:ha! good point. i was thinking i guess of something more in the lines of having it being part of a general database wiki document.you can always link to it but i feel like document format for something like that would be pretty impractical to navigate around for a complex warehousei would recommend having the data dictionary live as close to code as you can, so you don\\'t have to update field descriptions in multiple places when things changequestion: i need to explain random forest to a bunch of analysts in my company. i want to convey the main idea and not get bogged down into the nitty-gritty. does anybody have a good reference for this?i always hear it explained in layman terms as \"wisdom of the crowd\".  analysts may deserve better though, they\\'re probably somewhat technical :slightly_smiling_face:i\\'d like to go further than that (but not too much)... ideally i would like to show some simple visualsi like the idea of drawing an example of a 2-d feature set + classification label, then thinking about how you can draw lines in those dimensions that would help you best separate the classesthanks , will check this outand make the connection that the lines are the splits in your decision treei.e. just start with one treethen explain random forest after that have you seen the r2d3 demo? don\\'t think it necessarily talks random forests but it\\'s good for visualizing decision tree learninggood ideas guys, i\\'ll check them out. last time we did a mini-lesson on decision trees, so that should also helpa long while ago i made this for a business client who wanted to know wtf i was doing uploaded a file: it\\'s very rudimentary but it worked pretty well, they \\'got it\\' they feltcan you elaborate on the “as close to code as you can” part?hey, i got roped into doing an technical interview for an analyst position. anyone have a analytical question they like? commented on ’s file : not sure what you mean by linear projection.  as i understand it, state-plane is based on \"flattening\" the state\\'s map so you can do point-to-point distances in 2-dimensions.  i don\\'t think it would translate easily into wgs (what google uses) commented on ’s file : thanks! i expected it was not so simple a problem. i\\'m looking more into pyproj today and will let you know how that goes. the help is much appreciated.supervisor does not an advisor make this was just open sourced. looks like it might be a cool alternative to bokehoof. outstanding name!hey fellows, i got a friend from the uk who\\'s thinking of joining the program. he was doing a phd in vancouver, but he\\'s grown rather disenchanted with it. he\\'s looking for options for visas, and i\\'m wondering if anyone here has been in a similar situation, and how they made it work. let\\'s start a thread, or reply to me privately!actually, i think that is a critical role of a supervisor. they have to have useful thoughts on the path forward.update for those curious: i basically just did an interview i would give an insight fellow but at a faster pace, covering more topics but trying to stay at a relatively high level. no whiteboarding, but she did bust out a pen and paper to start writing some equations and sketching graphs. i did delve a bit into management and recruitment style. i also did a longer real world scenario of an experiment i\\'m planning to pitch on a real product we\\'re launching. apparently she was so impressed that she talked about it in her interviews with other people. that\\'s all cool--but now i\\'ve been roped in to interview two other candidates tomorrow and monday :cry:: i saw this and thought of you. we\\'re hoping it sends people to our blog! :wink: (where, of course, our lizard people mind control rays will get then!)anyone thinking of giving it a crack?i do not have time at the moment, but if anyone decides, you can ask me for tips. i did my phd related research in helioseismology. there is a overlap with planetary seismology. is reddit famous!hey team insight, long shot: anyone working as an economist, or have economists in your team? i’m interested in what they’re doing. - recent fellow  was working as an economist at the ftc before joining insighti’ve used it, but i don’t have much brain to pick. i retrained the tensorflow object detection api for new classes. training was expensive, i whistled through $250 without even realising. \\n\\nsince google wrote the training scripts it was easy. i can really appreciate the workflow they have set up.glassdoor has a chief economist (andrew chamberlain) and one data scientist assigned to his team. much of their work appears at i worked as an economist pre-insight :raising_hand:\\u200d♂️thanks ! that’s super useful :slightly_smiling_face: hey jesse, do you have any thoughts on econ v ds? like, is there much/any use for econ roles in industry?meh. they would cause such a hassle with security it\\'s probably worth not having them take it. i would be curious to hear the thoughts of someone with a neuroscience background on this article. boffins and luvvies eh? was this article written at hogwarts?: !: i also felt like this article needed to be translated from english into american. i don’t know what a boffin or a luvvie is either.: smells a lot like the look elsewhere effect (and related p-hacking/p-value misunderstandings) being abused: in my personal opinion i would say that article is about 80% accurate. there are some brain regions (mostly sensory/motor) that activate pretty reliably but a lot of the behavioral economics and social neuroscience research is on shakier ground. i don\\'t trust the results of a neuroscience study unless they\\'ve been replicated at least twice, and that happens very rarely. probably partially bc results aren\\'t real and partially because there\\'s little grant money and interest in replication. and of course no one wants to publish null results or replications. ugh. and  that is pretty much what i thought. thanks!stay positive pamela.clevengeri totally missed the article that started the thread  (and  and )  i just brought us full circle.  sorry about that.stay positive pamela.clevenger: have you seen similar issues with erp? i was in linguistics/cog sci and n400 and n600 are used super frequently\\\\m/hey, i remember brianna talking about not trusting neuroscience results during the session!: i don\\'t have enough experience with erp to know, but i\\'m sure there are other people who could comment!this isn\\'t just about people not correcting for multiple comparisons, the bigger issue, which they only link to and barely mention in the article, is because of a software bug that existed in very popular fmri software. there was a  bug in the software that was used to estimate the probability of false positives, so it means even some people who were trying to account for multiple comparisons could have spurious results because the software they used had a serious bug in it....for 15 years.  i am hearing terrible news from new york. i hope all of you guys are safe and sound.  everybody here is safe!: thank you. it is getting uglier everyday.    glad to hear everyone is ok. stay safe!!anyone know about the command line split (to split a large json files into multiple files)?i\\'ve used split for json before. what\\'s your question? so when you do (for example): \"split -b 40k myfile segment\" does this mean each segment will have 40k at maximum? so if my file is 165kb will it make four 40kb files and one 5kb file? is there a way to split it into four even (having the same amount of data) segments?do each of your lines have the same amount of data or is it variable? you will need to determine if your ‘json files’ contain a single record per line of the fileif that condition is true, you can use ‘split’ to divide that file into chunks, by line (see the options for ‘split’ on how to divide a file by lines)divide/partition  yes the files did contain a single record per line so that was the perfect solution. thanks!ny! thanks for your hospitality to us boston fellows today. great to meet you and see some familiar faces too. if you\\'re ever in boston, hit us up.wardrobe question for the dudes: my husband is interviewing for a job at a startup, the role is on the business (as opposed to technical) side. would khakis and a button-up be too casual? was thinking that a suit wouldn’t fit with the culture. definitely wouldn\\'t do suit, i\\'d say+ sportcoat! (not the top part of a suit, but an actual sportcoat!)+1 for sport coat/blazerand very shiny brown shoes that match the brown of beltthat plus button down, nice high quality jeans and the right shoes have a magical ability to look professional and casual at the same timeidk if i\\'d go shiny shoes thoughthinking somewhere between tier 3 or 4chukkas are a go todepends on where the startup is locatedeast coast more formal, west coast less formaldo any of you data scientists develop your ml workflows in an object oriented programming paradigm? my code gets messy very quickly so i\\'m thinking of adopting an oop design for my projects, and i\\'d be very curious to hear about some existing paradigms (what classes and methods to use).i use scikit-learn’s pipelines[1]. they mirror the level of abstraction i would use to describe my model to someone on a whiteboard. i’d also suggest writing your code such that the data structure passed around to different transformations is _effectively_ immutable (i.e. your transformers should take in an object and output a new object instead of mutating the old one). this makes parallelizing the code easier, and, at least for me, makes the code easier to understand. \\n\\n[1] +1 to pipelines, or at least using them as inspiration for rolling your own classes+1 on the immutabilityhi fellows, we just launched our latest insight fellows program:\\n\\ndevops engineering\\n\\nmany insight data engineering alums are moving into the field, and we\\'re seeing a lot demand for these roles, so we\\'re going to run a dedicated fellowship starting in july in silicon valley. if you know of any engineers or systems folks who are excited about getting into devops, sre or infrastructure roles please send them our way. if you\\'re working in this field currently, please definitely reach out as well. very excited to be expanding our engineering fellowships!wow ! you guys are on a roll with launching new stuff!a :trophy: to the person who can most succinctly define devopsthe team holding up the walls and ceiling around product / ds:austin_and_his_puppy:^^ shortest definition of devopssuperherosanyone know java? can you point to good starting materials?“site/database reliability”head first java is pretty good\\nthanks guysyeah, that one was good\"devops is intersection of lover of cloud and hater of wake up at 3 in morning.\" allbirds, good jeans, button updevops: the people you call to redeploy your code after it breaks from changes made to the environment by devops people. :troll:me, i like apache beam\\'s abstractions; but they\\'re not necessarily oo.hi all, preetika just wrote a great blog post on how she chose here project and technologies.  check it out here:\\n:thumbsup:hi fellow fellows!! my team here at memorial sloan kettering is looking for  a comprehensive public/government database of death records. the social security death index used to be the best source but since 2011 they have stopped integrating state level records so are woe-fully incomplete. someone else suggested the ndi maintained by the cdc but the timeline for accessing that data is doesnt work for us. the data cannot be an api and has to have identifying information. any thoughts or suggestions would be great!!: ^^i would also be interested in this, , let me know what you come up withfacebook algorithmic factory in 3 parts - a neat read: hi folks.  i know a marketing guy with lots of experience in tech startups who\\'s looking for a new position.  if you know of any openings feel free to pm me.  thanks.  at fast forward labs, we are hiring one machine learning engineer and one data engineer for a short, 6 months engagement to develop a novel prototype using quite fun nlp techniques, clustering, outlier detection etc. it’s a fun opportunity to develop an entirely new product from scratch and an ideal role for folks who want to dive into new techniques to e.g. level up for the next role. we’ve got strong partners: prehype (an incubator) and “an international news organization” (i can’t say the company name but, let’s say, you’ve heard of them). here is more: send me a pm if you’re interested!we’re hiring another data engineer for the data science and machine learning team at crowdstrike:\\nmy roommate is likely to move out by november. if anyone is looking to share a 2b/2b condo in central redwood city ping me!  ^not sure if this has been talked about here or not: \\nanyone have experience building interviews for data leadership positions (ie the person running a team)? looking to create a list of questions/discussion topics for an interview and would love some advicedoes anyone have experience with `sklearn.cross_decomposition.cca` module, especially in the ways that it relates to the `canoncorr` implementation of cca in matlab? i have some matlab code that i am trying to reproduce in python, but answers never seem to be close. thanks in advance for any pointers! i asked this question a couple weeks ago. i can give you advice about how it went for me. what is your relationship to the candidate?hey, oh awesome!that would be great! basically we have a ds team and a ba team and we are looking to hire a high- level person to both run ba as well as deal with upper management on behalf of both ba and dsdamn, looks like my question slid off the back end of yeah, insight too cheap to buy real slack :wink:yeah, sounds pretty similar to my situation. we are/were hiring a director of analytics and insights (de/ds/ba). i was interviewing for technical depth in terms of data science and analytics. what worked well was to run through a series of high level scenarios that mimic the kind of work your team handles (or that you anticipate you will handle). ask them how they would approach the problem, and basically ask them to explain the approach as if you were a business stakeholder, then a junior analyst/ds who would have to do the work, then dig a bit into their technical depth on the problem, then bring it back up to the stakeholder level--asking how to interpret some fictitious results or ask them to communicate some bad news to you (my favorite) as if you were the stakeholder.that way you get to see how they would communicate with management to receive requirements, how well they can explain things at a basic level, how deep their expertise goes, and how well they can synthesize and be diplomaticyeah that totally makes sensethis is what i was imagining doingwhat about more high-level stuff? questions that assess management style, culture fit, that type of thing - nontechnicali don\\'t have any advice in particular about that, sorryi think these are the women who will be speaking at/organizing this event, not all the women who work at the company :wink: . also, two of the eight are directors, two are data analysts, one is a data engineer, one does analytics, one does data visualization, and the others represent different roles that are integral to building data products. it doesn\\'t seem like the lack of a specific title is diagnostic here.all good manthanks for the advice on case studies, i think that’s an awesome direction to take  it seems to have decreased how big figures and fonts appear for the same size values!hooray backwards compatibility! :smile:\"the available shortcuts are currently limited to command mode, but we invite contributors to add support for edit mode!\" \\n\\ni would kill for a good vim implementation....+1 on the case study questions. i\\'ve had to interview someone for a position like that on my previous team (and we also designed case study questions that were almost identical to ones we were trying to solve on our team) and i\\'ve also been interviewed myself for a position like that (they too asked me case study questions)it helps both sides to understand the problems and whether the person is the right fit for it or notin terms of management style/meeting deadlines, they looped in questions like \"if you had a problem like this, what would your thought process be like? how would you set goals/deadlines for your team?\" etcyou can also just be direct and ask them about their management stylesome people will give vague answers like \"i like to follow the agile process\", but be sure to ask them to elaborateha, just read \\'s response. yup, basically sums it up, well put! :slightly_smiling_face:i think culture fit pretty much becomes apparent through their mannerisms thanks so much! :smile:all good points, definitely understand and will keep all of this in mind (we’re hiring a head of data science)hi , cape analytics is hiring data engineers!  we use machine learning and deep learning on geospatial imagery to help property insurers \"see\" the built world at scale.  if you or anyone you know is interested, feel free to ping me here or at  and i\\'ll  tell you more :slightly_smiling_face:there _is_ a vim mode (though it hurts for me to say it since emacs is da best). the edit mode uses codemirror.js, so you can just turn out the vim mode there: that feeling of disappointment when you get a slack notification and it was an @ channel :sadkeanu:  that or wondering why the hell someone from work is slacking you at this hour.  like getting one from your boss and you’re just like “i’m at a bar, i just did a shot of hot sauce, _we will deal with this in the morning_\"shot of hot sauce?like tabasco or is that a type of shot?no like literal hot sauce…i am not a clever manshot of johnnie walker would have been betterthat’s a little on the nose, don’t you think?just pulling your leg:stuck_out_tongue:but i’m glad we see eye to eyei’ll be that insight staff member and caaaasually point out that anyone on this channel can always hire a data engineer / scientist _through_ us instead of posting a job announcement directly to the network and connections we’re providing for you :slightly_smiling_face: * refreshes inbox for eager alumni emails who want more team members * * but seriously you know where to find us and obviously this is what we do *anyone know of a good online/remote masters program for data science?  i have a work colleague (who\\'s a good data engineer) who wants to do a masters focused on ds. u. illinois cs dept has an online ms in data science for $19k.  i can\\'t vouch for the program but the cs dept has a good rep.  also georgia tech has on an online ms cs and you can choose a ml or ai focus.  it is much cheaper and still a good school.  hi  arm is hiring in us for two roles: an analyst role and a data scientist. if you know of anyone that is interested, please ask them to mail me at . here are the links : , sorry to do everyone here ..i think uc berkeley has a great ds online programhey  -- please scroll up to see my message a couple posts up. this looks like a role we could help you fill via our current fellows (it appears to be entry level / new grad?) you can message anyone from our team directly when you have open roles, rather than posting widely to this network that we connected you with (including new fellows we\\'re currently working on placing).hey  .. sorry did not see your message. was in a hurry, will get in contact with team for these positions. do you want to learn about quantum computing as a general thing ? or something specific ? if general then your really can’t beat  nielsen and chuangok i am never testing anything on vanilla mnist again \\n  nielsen and chung is good for basics like knowing what quantum gates are and such but depending on what you are interested there are huge varieties of good intros.. are you interested in annealer based algorithm or universal related algorithms? are you looking at particulars like shors grovers hhl and such.. or even more basic like techniques?   i don\\'t know what any of those things are. so probably basic.  i guess then it depends on what your interest and how much time you want to invest in it. neilson and chung is great but it is a whole textbook. if your interest is more casual there are some great posts by rigetti computing staff. i personally like ti read review papers and wikipedia when i need helpanyone here working in ds roles in canada (vancouver, toronto, montreal)?i\\'m not but i\\'m interested to hear of anything/anyone particularly in mtl or toronto (any other :maple_leaf: folk feeling like moving home lately?)friends of mine are looking to hire a data scientist in toronto. let me know if you\\'d like an introduction.canadian feeling vancouver hard here.  hi! is anyone good with debugging website issues, or know of anyone who is good with this? the web app i built for my project was working perfectly a week ago, but starting monday i started having really weird bugs with embedding instagram posts to my site, and i can’t seem to figure out what’s going on. any help would be greatly appreciated! -- you may also want to have a chat with , she\\'s got a pretty good grasp on the canada ds scene! but he rarely checks slack any pointers in the logs in the webapp?i’m currently looking through the errors with dev tools in my browser, as i’m not generating log files for the webapp i am in toronto, but i don\\'t really know any companies that are hiring atmany errors in the chrome console?hi , i’m getting errors like the following: uncaught error: minified exception occurred; use the non-minified dev environment for the full error message and additional helpful warnings.\\n    at k ()\\n    at ka ()\\n    at \\n uncaught error: minified exception occurred; use the non-minified dev environment for the full error message and additional helpful warnings.\\n    at e.exports ()\\n    at v ()previously, embedding posts worked fine with the embedded html instagram’s api returned to me, but then all of a sudden this week it stopped working. you can check out the (broken) web app here: the formatting of each post is what’s not working. they’re all different sizes, and overlapping. previously they were coming in perfectly formatted yep if there\\'s a military parade on all the tv channels i\\'m going to definitely do a few searches on job sites...i can see in the dom that it\\'s assigning a fixed row-height of 26 to those \"overlapping\" bits; unsure if it\\'s inheriting that from elsewhere.looks from your github that you tried to manually loading the embed.js, but when you do that you need to get the html you have to use the omitscript parameter in the request url some of your embeds get the js from  and some get it from . see what’s different between those posts toothanks for taking a look! and yeah, i did try loading in the embed.js manually. when i did that i also used the omitscript parameter for the request url, and it didn’t seem to helpdid you get any idea about why some of the embeds link to a different js file?no, and i suspect this is the cause of my issue. i have a window open from last week, when the app was displaying properly, and it only doesn’t have any errors and lists embeds.js from within a “scripts” folder. now, my app with all the errors points to an embeds.js from an “extra scripts” folderperhaps the instagram urls look different for these two embeds?  too!has setting `set node_env=development` helped?, no, as far as i can tell, the urls look the same. i’ve been spot checking a bunch of them, and nothing stands out. , i haven’t. i’m not familiar with that, but i’m willing to give it a shot. where exactly do i set it?this would be in the shell that you started the webappso you’d have to take it down, then run `$ set node_env=development`, then bring the webapp back upit should provide you with more debug info hopefullyawesome, i’ll give that a shot!after a whole lot of frustration, i was able to get the site back up and running! , , , i just wanted to thank you so much for taking the time to offer some help! i really appreciate it!anytime!: one of the fellows from my session ended up doing something along those lines -- the startup he was working with ended up giving him anonymized text messages that were like `aaaaaaaaa. aaaaaaaaaaaaaaaa :)`, he ended up using the emojis a lotp(excitement | number of a’s)?very highimodoes anyone have experience with mobile app analytics platforms like segment, mixpanel, amplitude, mparticle, app annie, and appfigures? i\\'d like to hear your impressions of any of them.shit, i’m supposed to be using amplitude right now but haven’t played with it enoughstay positive lafonjstay positive lafonjstay positive lafonjsorry, had a little issue with the robot...stay positive i try but a robot telling me how to act...stay positive *data scientists in healthcare:* i host a weekly youtube live show on ml in healthcare and am looking for someone to interview this week. the show is on thursday from 11:30-12:30 pst (including sound set up). if you’re interesting in discussing:\\n- your work\\n- ml in healthcare generally\\n- the past or future of ml in healthcare…\\nplease let me know! we’d love to have you on the show! ?\\nhas anyone attended the open data science conference? if so, would you recommend it? i’d like to attend at least one conference on the west coast this year, and was wondering if anyone had any suggestions on conferences to attend or ones to avoid. thanks!btamadio: i just went to one at boston. i don\\'t plan to go again. the one in boston was mostly targeted at business folks, with a mix of vendors and some practitioners as speakers. audience was mostly business folk new to data science and students. it was ok, some specialized talks, but most were very general, i would suggest looking at the schedule of the boston conference. dm me if you can\\'t find it. of the workshops at the conference, only the platinum pass would have been worthwhile (spark training) for me. i signed up for the gold pass before knowing what they were training on, it was on scikit learn. good refresher but not really worth the time and money.btamadio: if you can tell me more about what you\\'re looking for, i can tell you if it\\'s a good fit\\u200b or notfor what it’s worth, several in the boston insight crowd have decided to submit talks for next year at odsc. it was a little rough this year, so we want to make it better :slightly_smiling_face:hi all, due to visa issues, i am going to be leaving the us this  august. does anyone know of any companies that are hiring for remote work? also, not necessarily opposed to other countries outside of the us as long as immigration work can be done. thanks!this announcement may seem familiar to some of you! it\\'s a re-post with different start date.\\n\\na colleague at dia&amp;co is looking to sublet her apartment for a few months, here are some details:\\na quiet and sunny furnished one-bedroom apartment is available for sublet in brooklyn (bay ridge) for 2-3 months, starting early/mid february. actual dates are flexible.\\n\\ndm me if interested and i\\'ll put you in touch!i’ve never specifically worked from onedrive, but when i had to transfer large files (100+ gb) i always used `rsync`hey, if there are any alumni looking for senior data engineering roles in boston, please let me (or someone at insight) know. thanks!is there a way to download all the county data sets at once on census fact finder when you\\'re looking at the demographic (age vs race) tables.you can download specific tables and variables here: and if you\\'re an r user, the tidycensus package is quick and easy yeah i\\'m using the factfinder download center uploaded a file: so for example, i get to this page: so it is making me go to each county through that dropdown to download it, i was wondering if there\\'s a way to expand the table so that it has each county as a columnhi guys, i am making mood-based playlists for indian classical music for my insight project. i am able to pull out data from spotify api and analyze them using clustering techniques, however, since my data is unlabeled, i am considering getting \"human in the loop\" as a mechanism to establish ground truth. simply put, i am asking you guys to do me a small favor and fill this survey form for me: . i would send out one form every day; each form needs you to listen to 10 songs and provide labels to each of the song wherein you rate a song on the basis of energy and positivity. i am also thinking of extending my work to give user-based recommendations for which i will ask you whether you like a song or not. send this to whoever you think would be interested in participating in this project. a big thank you to all of you in advance! :slightly_smiling_face:sounds like a really cool project!  so, are you aware you can get energy/positivity measures from the spotify api?positivity -&gt; \"valence\"yup :slightly_smiling_face: i used them for clustering my data points :slightly_smiling_face:cool, so then you\\'re trying to validate spotify\\'s measurements of those quantities, is that the idea?yes, in the context of indian classical music. i may have to come up with different set of features eventually since indian classical is quite different from genre-based western music.i see, very interesting!thank you! :slightly_smiling_face:playing around with that r library, looks promising. thanks.planning to listen later, but had a suggestion on the form - having dropdown menus with the expected response choices to choose from would standardize the inputs and could make your data much cleaner...  even though we\\'re all phds in my experience people aren\\'t super consistent with following complex instructions...that\\'s a great suggestion! i was considering that but then i will have 50 items to choose from for each song. that\\'s not very pretty :disappointed: any thoughts on how to optimize selections?i was picturing just three drop-down menus per song, one with [\"calm\", \"mildly calm\", \"neutral\", \"mildly energetic\", \"energetic\"], one with {\"dark\", \"mildly dark\", \"neutral\", \"mildly positive\", \"positive\"}, and one with {\\'like\\',\\'unlike\\'}or the \"linear scale\" response option might work tooeach song could be a \"section\" in the google form, with 3 questions in itjust from experience, if people have to type things in there will be misspellings, capitalization differences, etcchanging the format now! thanks a ton, jamie!! :slightly_smiling_face:great! glad to be of help :slightly_smiling_face:done! :slightly_smiling_face:would something like this help? just fyi the order of the questions doesn\\'t match the spotify playlistnow it will. i had asked it to shuffle before which i removedthat website was where i took the screenshot from. :slightly_smiling_face:hi guys, i\\'m working on extracting features from blood pressure waveform signals and performing classification (e.g. diseased vs. healthy) based on that and i wonder if anyone has ideas about what features might work best for the classification task. i\\'m thinking about morphological features and frequency domain-related features. please let me know if you have any insights. thanks!what’s the pm track?hi all, i was wondering if any past fellows have worked with satellite images? i have a couple of “best practices” questions as well as questions about available data that i was hoping to askinsight started a project manager track for “ds ai ml” stuffjust when you though they couldn’t fit anyone else into palo alto :wink:interesting. i guess i assumed that woulda been announced somewhere… has all the details... it\\'s kind of a pilot.yeah, i only learned through the grape vine; it think it’s very much a betahappy to chat with anyone who is interested - just message me.^i’d be curious in the executive summary, personally!hello current and past fellows. would anyone who\\'s worked in the ad-tech space be able to have a quick chat? i\\'m looking to verify that my project idea is a practical use-case. thank you!this is so importantplease let us hire them ;)  we can talk tomorrow too i have, briefly.  did a project using satellite imagery.thanks. i spent two years at the oracle data cloud ad-tech firm, so let me know if you want to chati\\'m technically in an ad tech space and curious to hear about your project!not sure if this talk would be beneficial or not (in terms of available data): thank you all for your quick replies.  and  i\\'m currently part of the nyc session. i\\'m open to chatting over any platform  (i.e., slack chat or skype call) that works best and can accommodate your schedule. thank you again.awesome, thank you all!:  we just contracted domo for a few use cases for internal clients. their pitch is flexible aggregation, storage and great visualization. i am talking to then about options for hosting code on their platform which they don\\'t support currently. unless you have a small dataset to download from the api and compute on locally and push back results, it becomes impractical very quickly. they are open to supporting a few use cases, and are currently doing so for julia. python is still not supported.anyone know a good reference for statistical testing?i found these notes to be useful: i’d like to share interactive matplotlib figures with business people. does any one have suggestions on mpld3 v. bokeh v. something else?never used mpld3, but big fan of bokehi just learned tableau via . you can very easily create interactive graphics via data you load into it. it\\'s all click and drag, no coding. udemy thanks guys!i’m getting errors in converting my complicated figures to bokeh (“unboundlocalerror: local variable ‘axis_l\\' referenced before assignment”), so i’ll see if mpld3 just workstableau’s good for simple plots but i find that if you want to customize it much then you have to do weird hacky thingscant stress enough how valuable this post is, especially if you are in the startup world.  shared a file:  commented on ’s file : this better not be a rick roll: well there is only one way to check :trollface:aww. only snow on sunday...anyone has got access to windows event logs - dataset and is in a position to easily share it? could you let me know? thank you.naive regression question, has anyone ever used lowess regression to predict future values? if so any tips on implementing that in python?:  (though statsmodels is notoriously buggy, so if the module doesn’t work out of the box, i’d google other python solutions…chances are someone else has implemented it)how important is it that we show error bars or p-values in our figures since we are working with less than ideal data especially if large (yet significant) error bars take away from the aesthetic presentation of the data?: as all things in life, it depends: this seems like a philosophical question of fundamental importance.  i’m with keats on this one, for what little my opinion is worth.  \"beauty is truth, truth beauty\"hello, is there anyone who has successfully gotten a bokeh server working on an aws ec2 instance? i can get it working locally to produce interactive plots on my site but am having trouble once deploying it to aws.anyone at o’reilly strata london this week?anyone have any favorite resources for learning scala, most directly related to data pipelining jobs?scala cookbook!!has anyone had to use mongodb for analytics and dashboards? from what i understand it\\'s kind of a pain, but i\\'m curious if there are any decent resources out there for learning it. the best i\\'ve found so far is this: folks, thanks again for participating in the comp survey. results have been broken out into sheets according to seniority. \\n\\ni\\'d like to make this survey a somewhat regular thing, to serve as a resource for all fellows during performance review and job offer evaluation. if any of you have feedback on the kinds of questions you\\'d like to see on the survey, clarifications on the results, or other feedback - please pm me (or better yet, start a thread so people can see the discussion).\\n\\nthanks again!hi everyone, i was just wondering if anyone had resources for good measurement ave attribution techniques beyond ab testing? *andwhen i google measurement analytics without ab testing i only get results about ab testing :dgraphical models? amassari: looks promising, i\\'ll take a look. thanks andrea!! when did you stop being maxos :di’ll never stop being maxos! i’m maxos on the sv16c slack descriptive stats global\\'s total comp min and max are wrong because the row order of the base comp has min and max swapped, but the cell references used to compute the total comp point to the opposite cell. also the total comp min and max (didn\\'t look at the other summary stats) are not correct as computed. the totals should be calculated per individual and then the max taken on that--not taking the sum of all respondents\\' max for each individual categorythanks for pointing that outerrors fixedi;m getting a giant black box for some of the statsmeh, if natural language generation ever gets up to a human level, software developers will be out of a job, since the expressiveness of human language is a proper superset of the expressiveness of programming languages.gac r&amp;d (the company i work at) is looking for a senior data scientist: with experience in the data engineering part at production level (with knowledge of bigdata technologies) .\\nhere is the post:\\n\\nplease contact me if you are interestedhas anyone had problems adding maven packages to their spark runtime w/ zeppelin? i tried adding comma separated repos in the maven format to the\\nspark.jars.packages option under the zeppelin spark interpreter, but it doesn\\'t seem to be pulling in the library at runtime even after a restart.does anyone happen to have any resource materials on how to parallelize python lambda functions consuming from kinesis firehose ?get data scientist as a role. anyone who cares will read your resume, anyone too dumb (riding the hype) or lazy(or overwhelmed) to look into it will glance over it. \\n\\nalso this:\\ni\\'m a data engineer though, so, that\\'s just my grain of salt.a good list of machine learning resources on hn today: went for the 15$ package :slightly_smiling_face:lots of talk around this, similarly lots of talk on the pending cookie-pocalypse coming soonholy moly! this is amazing! thanks for sharing! $15 for medoes anyone have any connection to ds/ml in fashion industry in nyc? would love to grab coffee/chat. pm me if available. thanks!macys, dia&amp;co? talk to  , toodoes anyone know if avro schemas can handle unknown fields?  i know that json can do that.  was tying to find some info on this.does anyone know if this is an accurate representation of dl research?can\\'t comment as i\\'m a developer but i know the swordfish hacking scene is a pretty accurate portrayal of my life i haven\\'t used it . i came to know about it while i was presenting my insight project at yelpdoes anyone have a clear sense of what sort of data will be \"off limits\" for targeting? it seems to be a bit opaque from what i\\'ve heard. for example i don\\'t want to target anyone based on their pii, but if i have data about a user\\'s past engagement with ads (e.g. you click on a lot of native ads in finance) it would seem strange to consider that off limits for targetingthe youtube video shows him working on ipython, noice!addictiveyou\\'re welcomewhat have you done here you goi grew up in la crosse, wi! the brewery there makes crap beer, but at least we did something right.stay positive bmregnerthat sounded pretty positive to me nuggetbot. shut your mouth.poor little nuggetbotnuggetbot is a beer egalitarian and didn\\'t appreciate my beer snobberynuggetbot, o\\'douls rocks!nothing.nah nuggetbot is a bigger beer snob than the rest of ushi guys and gals. i\\'m looking for insights into this error metric called the \\'gini coefficient\\'. its routinely used in kaggle competitions (particularly related to insurance). how does one train a machine learning algorithm differently in order to get a high gini score (as against say mse or roc)? any insights will be appreciated! more about the metric here: the tree based methods in scikit-learn minimise the gini coefficient  by default, maybe that a place to startthanks chris. the \\'gini index\\' (also called gini impurity) is used in tree based classification methods to determine what a good split is. i\\'m not sure it minimizes the overall \\'gini coefficient\\' which is a different quantity. also, i\\'m working with a regression problem.okay, i see.: the _gini_ _coefficient_ is a measure of variance of a random variable. basically, it is 0 for a constant r.v. (uniform distribution) and 1 for a delta function r.v., all other positive random variables should fall in-between. i always thought of it as an approximation to the entropy. i am not really certain how you can use it to measure the _error_ of a regression.: i agree! but since that\\'s the metric used in the kaggle competition, i wanted an insight into what kinds of error metrics would give a good gini coefficient. essentially, what i want is predicted outcomes yhat which have a \\'distribution\\' similar to the actual outcomes y. the exact values f(x) don\\'t matter, only the distribution of all the yhats does (and this distribution should be close to distribution of the y\\'s). thanks.i don’t think it’s so much the distribution of yhats that’s important as their _order_so, obviously, if every yhat was exactly the corresponding y, you’d get a perfect score. but you’d get the exact same score if every yhat were double the corresponding y. and in fact, even if every yhat were the square of the actual y.however, the extent to which you’re penalized for deviations from the perfect ordering _does_ depend on the _actual_ y values.anyway, i don’t have any real answers here, but i’ll think about it a bit. uploaded a file: : this book chapter might be helpful to your questionhi all - i’m speaking at strata+hadoop in san jose on march 15 about my insight project. i have a friend of a speaker discount to the conference - dm me and i’ll share it with you!i am speaking there on the 4th. if you guys are deciding on a place, then may be bukowski, or the pour house boylston/newbury?would love to meet the insight boston crew can bootcamp be deployed on pc hardware? i was under the impression bootcamp was just for running windows on your mac.i found this to be a pretty good speed comparison of different methods to save/load dataframes: \\nanyone got some insight on how `feather` compares?i did some quick benchmarks a few months ago. for a 50 gb dataframe i tested, mixed categorical, string, and float data, pyarrow to parquet was the fastest (available in dask and recent pandas versions). the second fastest was feather.i think pyarrow was faster because it compresses data by default, and feather does not. i could be wrong, but i think they use the same in-memory representation.anyone have experience with facebook\\'s prophet forecasting tool? i\\'m trying to determine if the scalability aspect is as good as claimed.thanks, `parquet` is indeed a good option if given a large amount of data.\\ngood to know that `feather` compares really well to all these other methods, although it\\'s still not recommended as a permanent storage method. lasso is another approach that could help me identify interactions. \\n\\nto be clear, i\\'m trying to find interaction effects between features for a classification problem. the point is to both tell a story and to identify the most predictive interaction terms to add to a different model.very excited to launch insight in toronto this morning! expanding to my home town has been something i\\'d always hoped we\\'d do, and today\\'s the day. to read more about insight  in canada check out the blog post we just published, and if you know any great scientists or engineers who want to work in data in canada, definitely send them our way  anyone familiar with the hidden markov model and know of a good resource to learn quickly? my boss suggested it today and i had never heard of it before.hi  dm mehi  --i have some experience with prophet.  happy to discuss.  i found the first few versions to be buggy and crash-prone, but it\\'s gotten better.  we\\'re running it in production now to forecast many thousands of time series, each with weekly or monthly frequency.i’d love to hear any responses/suggestions you get.you don\\'t need to run bootcamp on your pc.\\nafter bootcamp boots into windows, it has a mount of special apple drivers (for windows) just for apple hardware. that\\'s squirreled away in some part of the bootcamp image.\\nwhich recommends:  but ymmv because that assumes you\\'re trying to make windows the main os on your mac hardware; which is not what you really want.it begins: you’re gonna scare them off_types and quickly erases “blood for the blood god\"_ data scientists for the data science godlet\\'s save the sacrificial references for tomorrow\\'s panel, ooookaaayyyy -_-i will bring the microphone and special effects boxwill there be cartoon hammer boings and fart sounds? i really hope so...for those interested in black box inference : has anyone ever tried scraping data from sites that consistently change their css class names (i guess in an attempt to block scraping...). this site i\\'m trying to get data from seems to be generating new random class/id names every hour or so, which means i can\\'t find the info i\\'m looking for that way. any clever strategies to counter that? can you drill down from a static class or id?hi alums+, anyone work or worked with google analytics data and have any tips? particularly about how to access the raw data? thanks!thanks, i’ll check this out!does anyone know something about the deprecated method .inc(word) of nltk\\'s freqdist() function? in particular, i have heard that word_freq.inc(word) has to be replaced with word_freq[word] += 1, but what if there is a second variable (which is an integer), as in: word_degree.inc(word, degree)?your probably going to want to store degree in a separate dictionaryi.e. word_freq[\\'word\\'] += 1, word_degree[\\'word\\'] = degreeor word_degree[\\'word\\'] += degree depending on what your trying to do with degreei suggest you take a look at what freqdist() does to better understand how you should be using it:  i access google analytics data via big query fairly frequently, and it works well. that may be a premium only feature, though. i am not an expert on ga generally.hi all, does anyone have experience working with druid? i\\'m trying to access a db using pydruid, but i\\'m getting stuck almost immediately...thanks a lot!i\\'m trying to create a random subsample of data from a dataset on bigquery as follows:\\n\\nselect * from\\n(select rand() as random, * from [fh-bigquery:reddit_comments.2015_01] order by random)\\nlimit 1000\\n\\nhowever, i get an error saying: \"resources exceeded during query execution.\" what am i doing wrong? is there a better way to accomplish this task?has anyone run into a python blockingioerror? doing a lot of image processing and think it’s running out of memory.: if you are an r user, i highly recommend the rga package. for python, i would recommend the python version of the api  you can probably put the limit 1000 right in after order by?bigquery has issues with running queries that return a lot of rows, you can circumvent that by storing the results in your own table temporarilyso many punchlineshillarioushi all, i was asked a question about the size of our corpus needed in order to have statistical significance in our models. i\\'m not entirely sure how to do that... so would you just train models with different sizes of corpus and see at which size the performance starts to level off? this also depends on how well the data represents the domain and the type of algorithms right? does the training corpus normally has a statistically significant number?what type of models?unspecified - it was a general question posted to our sales team. in general we do binary and multicass classifciation, and regression.are you thinking size of corpus needed to train a tfidfvectorizer/word2vec or number of samples needed for classification? regardless, i would plot performance vs size and see what it looks like. if it continues to increase, you could add morei\\'m thinking samples needed for classification (which we need to determine which target to classify). so this doesn\\'t sound like applying the hypothesis test to corpus right? that\\'s the other place i\\'ve heard of \"statistically significant\" being used.an academic paper on this topic that i was reading recently that you might find relevant:\\n\\nthis is more about classification performance rather than statistical signficance, so as  said you might just subsample and look at p-values (or whatever significance test) as a function of subsampling size and look for an asymptotei ran into that paper too. thanks.the other key stats concept to read up on and keep in mind is  \"effect size\", for example, in regression, what\\'s the strength of the relationship you\\'re hoping to detect? if the effect you\\'re trying to measure is big, then you don\\'t need as much data to confirm it exists, but the more subtle the effect you\\'re trying to test for, the more data you\\'ll need to collect.always wish my stats were stronger... thanks i will read up on it! is it worth ordering a hardcopy of this if it is based on java 5.0? i was going to make a similar comment :slightly_smiling_face: even in social science a lot of people don\\'t use/don\\'t know about effect sizes.this would help get acquainted with a bunch of the object oriented principles of java, which i found helpful.  \\n\\ni basically complemented this book by google searches when looking up specific syntax, which after java 8 could be a bit different w/ all the fancy functional stuff it introduced in java 8.  can’t say that’s worth getting a hard-copy (but if it’s like $20, not a bad investment for a physical copy)this is so cool. hackernews didn’t really love it, saying “why not just use the api this businesses have?” but that’s the whole point, most business do not have an api except the phone!anyway, i hate calling businesses so i hope this works as well (eventually) as they demoed)can an insight employee let me know who is the best strategic partnerships point of contact (email) for desv and whatever ya\\'ll are calling devops sv?^^ yes! david@ is working a lot on devops too, for msp it\\'s ericgreat, thanks kathy!agreed, plus businesses robocall me all day every day, time for me to return the favor!hi all,hi all, i\\'m wondering if anyone has used apis for peer2peer lending sites such as lendingclub or prosper before?: ^also\\nwelcome to all the new fellows!ozan, i\\'ve used the lending club api before. shoot me an email at  for more infothanks  !i’m thinking about doing a project with the angellist data, maybe we can discuss nice to meet you all today - feel free to email me if you want feedback on project ideas or if you have any questions about silicon valley data science. cheers!  if anyone interested to do project on a bitcoin blockchain please let me know. there is a trend to use the blockchain as a public ledger for storing timestamps and signatures. i am thinking about analytics on the custom data that stored in the blockchain transactions (so called op_return type of transactions):  has, recommend dropping him a line at if interested, we use azkaban + gradle here at linkedin ()though probably luigi is much faster to setup and get runningi recently found a price of registrar by tld comparison site that i think is handy.  .nyc is there too but has restrictions.happy mid-autumn festival to all!!!!supermoon + eclipse tonight!\\nyeah! it was always cloudy in austin for the eclipses... it is happening!moonrise is in like 20 minutes i think. sounds like it\\'s pretty cool on the east coast right nowit\\'s about 50% covered now!amazing sight from the east coast !it\\'s super cool!what do people here use for serializing and persisting machine learning models? sure pickling is great for a handful of models but what about a large number of  different models that are each being loaded and trained by an end user and then saved again?i have tried yhat, azure ml, and custom deployments written by software engineers i work with. i’m not sure about how many models is large for you, but those methods can work for tens of models without too much trouble.maybe someone at a bigger company will have better advice than me. :slightly_smiling_face:not at a bigger company, but this is something i’ve been tinkering with.  i’ve been having good luck with modeling frameworks that have inbuilt serialization, especially cross-language implementations (xgboost, h2o)alternately, could try passing around pmmls?: have you tried pmml?   just wasn\\'t sure how well it worked in practice and how extendable it was to custom models i.e. models they dont already have the xml defined for. also thanks for the tip about xgboost and h2o. will check them out. i haven’t, just something that came up at the new job.  at least for out of hte box stuff i think it has apis defined for the common frameworks, sklearn, xgboost, etc.xgboost and h20 have java apis so i can dump out to something that works in our engineering backend with like no overhead, it’s nicei\\'m mostly using python 2.x for no real reason. jupyterhub looks like it might be a good enough reason to switch to 3.xhaven’t used jupyterhub yet, thanks for the link. i’m in the midst of scoping out collaborative ds tools for a growing team, is anyone in love with any of the platforms out there?i am also interested in thisas a ds team of one, for the time being, i haven’t evaluated any of the collaborative tools yet, i have looked into several bi tools (looker, chartio, zoomdata, mode analytics and tableau)there was that thing that came out recently that does everything, what’s it calledalso, i’m in the python 3.x camp as of last falli imagine there’s fairly broad interest, or that it will be a topic that comes up for many teams at some point. i’m scoping out the ecosystem and talking with reps from many of the providers - i’m glad to post what i learn.i messed around with it a little, it was janky but maybe that was a result of it just having been posted to hnyeah,  looks interesting but i haven’t taken the time for a deeper dive yetdoes this deserve it’s own channel, or would it get lost as channel 28+?stay positive tcmorannuggetbot is trying sentiment analysismy initial impression of  is that it’s a pretty nifty mashup of notebook-style collaboration and github-like version control, but it’s killer-application isn’t totally obvious.experiment: fuck you, stay positive tcmoranit worked!hahahai’m a happy unicorn, no love.do you want nuggetbot to tell you to go negative?fishing for positive reinforcement. a little atta-boy, that’s all.:simple_smile:nuggetbot: image  me atta-boyanyone at this event today/yesterday? : i assume you’ve seen this too we use python 2.x with from future import thingsml in mario!  uploaded a file:  and commented: hey there! if you or anybody you know is looking for a new opportunity in europe, brainpool - a london startup founded by a founder phd colleague - is looking for a head of machine learning. feel free to pm me for an introthis is really great, snappy thoughts from a google engineer who has seen many ml systems go into production: hi to all fellows (past/present)~ i\\'m doing a project on removing blinks from photos and i was wondering if anyone knows of any databases with photos of people with eyes open and closed (with the same people so that i can test my method)? i\\'ve found some with different people but that isn\\'t quite what i\\'m looking for. do you know of anything like this? sorry, i\\'m not up to date on available databases. cool project, though ;-) thanks so much! hopefully they can release the data :smiley:hi all! new health data science fellow. i\\'m working on a project using nlp. i\\'m still waiting on getting the data for the data i\\'m consulting for. i was wondering if anyone in the ether knew of a nice primer on doing nlp in python? thanks in advance! :slightly_smiling_face:check out `spacy`, and in particular this tutorial: `spacy` is nice, but in my experience has a pretty big learning curve. depending on what you want to do, you can also look at `nltk`, `textblob` and `gensim`.definitely a +1 for `gensim`. in fact, the tutorial i linked to uses it heavily, especially for topic modeling (lda) and word2vec. `nltk` is best for general language processing (textblob is basically a wrapper around its capabilities iirc), `gensim` is better for numerical heavy lifting with vectorized texts.  there should be a copy of the nltk book in the insight office libraryi\\'ve also used `fuzzywuzzy` for fuzzy string matchingyeah `textblob` just has a slightly nicer api than `nltk` in my opinionoh yeah, simplifies all the defaults by far.  nltk is if you need to get into the internalshas the vader sentiment analyzer built in too, doesn\\'t it?oh also  could try reviving the  channelguys has anybody experience how one can read or add the *.bacpac file (microsoft azure sql backup) to my local database using unix and linux platform i\\'ll echo what others have said recommending nltk and gensim.  nltk is kind of bloated imo so i generally restrict to tools that are also in the corenlp package.  you may also find somethings in here useful: the algorithms in the datasketch package aren\\'t really optimized for performance but it should give you enough breadth to experiment at least.  for instance, we use a variant of minhash to detect algorithmically generated text...has anyone here used the isic archive? i know staplet did a skin lesion project, but not sure on the dataseti was thinking of more focusing on the melanoma vs. benign nevus anglehi y\\'all, in case anyone is thinking about buying a new mac, i happen to have a pretty big apple giftcard that i am looking to cash in :slightly_smiling_face:whatever you do, don\\'t buy a new macbook pro. those things are trash trash trash. i\\'ve been dealing with them for 4 days since starting a new job. one died on the first day. there are no (useful) ports, the touchbar is a travesty. just..ugh.what types of models do you guys typically use for a/b/n tests for seo data with factorial designs? i was thinking of a zero inflated negative binomial regression but there don\\'t seem to be any packages in python for this. does anyone have any experience in this area, and if so can you point me in the right direction?aws emr question — i am using pyspark with python 3.4 in my config as i created a new cluster.  i am using spark streaming.  i already set my pythonhashseed in all my nodes and restarted my slaves in ec2.  as i run a simple spark-submit (not as a step, but in the console), i still get `randomness of a hash string, should be disabled in pythonhashseed` error.  any thoughts?my next plan of action is this starting a new cluster and adding the pythonhashseed in configurations.can you confirm that the worker process can “see” this variable (value)?\\ni’m not sure how you set it, and most likely there is a description on aws, but it is possible that it was not set as systemwide env varso i think this was asked, and then fell off the edge of the world, so i\\'ll ask again:\\n\\ndoes anyone know where to find *district* level elections results for the whole us? 2016 necessary, further back a big plus! :smile:thanks  .. i was able to see it as the `hadoop` user in the `hadoop` env (confirmed it by running a simple script that hashes a string). no bueno there with my spark-submit job even if i see it.  but setting it in `hadoop-env` which is apparently different from the user `hadoop` did the trick.  i added the export var upon start of a new cluster. so happy dance for me. for now. :slightly_smiling_face:: i’m curious about beaker. let me know if you think it’s worth trying: i really like it so far - it runs on top of ipython notebook (and you can even open ipython with it), but it allows you to use multiple languages and share variables between the  languages. the other thing i really like is that it is easy to annotate your cells and collapse whole sections of the notebook.: thanks! i’ll check it out: i unfortunately don\\'t have a solution for you, but would be interested in hearing about anything you find!spent half a day on these tutes yesterday:  \\npretty great documentation for an initial release: did you do anything with it beyond their tutorials? i\\'m wondering how easy it is to make something new with it.as long as it’s easier than theano, i’m in.not yet, just tinkering around the edges. i\\'ll let you know what i find offline if you\\'re interestedyes, i\\'m definitely interested. thanks: can you make me a recurrent neural net real quick? i’ll wait...: i\\'m assuming that they did better than theano. hinton is in this group, right? maybe they learned from past projects. it did look easier from the brief glance, but i won\\'t know until i try something. :simple_smile:it would be fun to do something with people using this!twist bioscience in san francisco is currently looking to hire an experienced data engineer (this would be a great fit for someone looking for their second post-insight job) to help organize the data collected by their custom dna synthesis manufacturing pipeline. their product is incredibly cool and the company is growing rapidly. here\\'s the actual job posting: please contact me here or at  if you or anyone else you know is interested.if you have job opportunities you\\'d like to share with the alumni network, please email or message the insight team directly! we may also be able to connect you to data scientists and data engineers with *senior-level* skills and experience! pinned a message to this channel.&gt;4+ years experience in pharma/biotech, or experience with biology/chemistry datathat\\'s a tall oder:)what about more junior level positions? i frequently get asked about jobs at that level and i’ve posted for a few. i know they are not aimed at insight alumnae/i specifically, but they are useful connections to make.great question! i think it\\'s true that most of those positions wouldn\\'t be a great fit for insight fellows or alums. feel free to post :slightly_smiling_face:and one more question - if we are interested in a senior level position, should we contact insight? is there a specific contact person?if you are interested in a senior level position, for yourself, anybody at insight is happy to talk about career growth with you! you can email any team member from your session, or send a slack message, and they can help you get connected to the best party.looking for ds infra advice\\nhow big of a box (or cluster) should i spin up for jupyterhub? for now, mostly python with some spark. eventually gpus for some dl stuff. a data team of ~10 including a mix of de/dshey bay area ds folks! the data science teams over at fitbit wanted us to extend an invitation to insight alum for a ds happy hour they are having at their office next thursday night. more details and registration here ^sweet! i can finally ask about my interview status since they never said anything to me!playing around with neural networks\\nwould anyone who did the remote program be willing to chat with someone finishing their phd?hey everyone, we’re announcing that insight has officially opened applications for our first data engineering session in new york city, which will begin in september!  please spread the word to any one looking to become a data engineer on the east coast:\\n:  don’t drink and derive. cool graphs: pymc?????? help neededdoes anyone have any advice about sites the are difficult to scrape ( using  software): try a headless browser like selenium for pythoninteresting article, but worried about significance of finding and authors jump to causation (\"the more they buy, the less likely the product will succeed\"):\\n\\nfor all the physicists in ds: a follow up to our discussion in “women in tech” panel: : rand wilcox (usc psychology) and his students have done a lot of work on this (c.f., _introduction to robust estimation and hypothesis testing_). from what i recall, they generally recommend 20% trimmed means, based on simulations that attempt to jointly minimize type 1 and type 2 error rates.20% kind of makes sense conceptually, too. standard parametric tests (e.g., t-tests) generally use all the data, which is 0% trimmed. standard non-parametric rank-based tests (e.g., wilcoxon) generally use the median, which is 50% trimmed. 20% trimmed comes close to splitting the difference, erring a bit on the side of keeping more data.hope that helps.does anyone know of a decent probability course? aside from khan academy i can\\'t find much out there that has full lectures and is reasonably in depth.hi everyone! i wanted to get feedback on a way to deal with a machine learning problem that involves text classification with multi-labels. so i have text data, and they are labeled according to certain categories; so imagine for example they are labeled as: “delivery”, “complaint”, “account”, “cancellation. ”however, some of my data has multi-labels assigned to them. in other words, we have some text that is assigned more than one label (for example, it’s about both “delivery” and “complaint”). for now, to train my classification models i have been ignoring any data that has multi-labels assigned, so that whatever i fed into the model would be “pure”- aka one label. but now i’m in a situation where a significant portion of my data has multi-labels. i was thinking of trying to treat this as a regression problem- so for each label, try to predict a score… does anyone have any feedback on this, or has anyone tried other approaches? thanks!oh and regarding the above post- i would rather not go for unsupervised or generative methods for the moment...you can do any type of multi-class classification and rather than getting back the class pull out the probability for each class (labels in this case) sorry i should have mentioned i’ve been doing that… but there isn’t a way to input multi-labels for one training sample…how about using the same corpus with both labels in the training? i have to think about the drawbacks of this. what is the final outcome of the model? multilabel systems should be able to handle appropriately formatted multilabels, like with `sklearn.preprocessing.multilabelbinarizer`.  going from that to naive bayes seems naturalassigning only one label to the output or it can be up to 4 that you mentioned? the final output should be a probability score for each label... also to clarify: one data pt can have up to 3 labels assigned to itis the issue that you want the ‘target’ of the training to be all of the labels?  essentially you can train multiple classifiers one on each label in that case, if you don’t have a ton of labels.  \\n\\nor use some type of matrix completion methodand if the labels carry information about each other you can pass in the other labels as features when you train a model on one of themnot sure if youll have that data in your actual test data though (e.g. if some labels may be present sometimes but others missing).so you probably don’t want to do thatthe other thing you can do is to create new labels based on the combinations that take place, like complaint&amp;cancellation can be a separate label. specially, if on the business side, complaint and cancellation alone have different meanings that two of them happening at the same time. look up multi output classifier in sklearn, or train a bunch of binary classifiers in general. i did that in keras for my ingredient recognition model. that would be a great idea, except i have a lot of labels (20ish) and not all of them are in “natural” groupingssorry to throw deep learning at this, but this naturally falls onto a neural network architecture where you want to predict a final \"layer\" of your model consisting of 1\\'s and 0\\'s mapping to the multiple labels. there\\'s a conversation on the keras github about this -  it\\'s probably overkill if your data is small, thoughyeah if you want to go the nn route it’s more suited to train on vector targets where each vector is a series of 0s or 1s for each of your labelsinterestingly, that was the first thing i did with an nn, but the vector target was series of outputs from neurons in monkey brains :smile:nns on neurons - how meta :slightly_smiling_face:thanks everyone for the input!does anyone know where i can find a shapefile for all of nyc not subdivided by boroughs etc. i just want to see if coordinates are within nyc or not and all the little bridges between boroughs do not overlay with the borough shapefile. thanks!osm has a list of ways to get various shapefiles: \\n\\na few places claim to have premade ones (like ) but unclear what that covers...oooh! thanks!hope it\\'s helpful! i haven\\'t worked with city level shape file in the past. mostly country levelthat was an excellent read  and it\\'s a channel by channel setting!!yes, +1 on attention to detail for slack ui teamanyone around with some experience engineering features from / analyzing cookie data?does anyone have any experience accessing the fitbit api from the aws server?solicitation for advice on distributed modeling:\\n\\ni’m finally getting around to doing my first actually large size distributed model training and could use some advice. i’ve got a local xgboost model that i’d like to extend to a distributed training framework. i know xgboost can be run in a distributed fashion through spark or flink, but i also know there are other algorithm options out there that may be more conveniently built in to a distributed framework (such as spark mllib or h2o, both of which have gbm algorithms).  \\n\\ncan anyone speak to the pros/cons between these choices? does spark mllib or h2o have a significant ease of use edge? are their gbm algorithms as good in performance as xgboost? any thought are welcome. thanks!\\n\\np.s. - let’s keep responses in a “thread” to avoid channel spamdanjump: dask has an xgboost implementation. from what i\\'ve seen it\\'s pretty quick and makes distributed computing a breeze. there\\'s a pycon 2017 talk about it, it seems to have matured pretty well.quick question for insight folks: considering the limit of messages in the free version of slack and the growing number of alumni, should one consider either purchasing a paid version or getting a different program? e.g., at the moment one can\\'t see anything before mid may.joao: i know for my company, it costs like $6/user/month.  not sure of all the payment options.yeah, i was looking into the paid version and considering it\\'s around 1,000 users here, it would be around $100k a year. lots of :moneybag:slack is still great for quick pinging of folks here and asking questions.yeah, but it doesn\\'t scale well.\\nread that gitter could be a good alternative. opinions?interesting, not aware of other options like that.what are some good practices for achieving better than random log-loss for time series data that’s bucketed by months? i can easily build a model that performs well when evaluated on the total year, but i want it to achieve good performance in each month alsonot sure it still works, but to my knowledge you can scroll infinitely far back on mobileat some point, you\\'ll hit the 10,000 messages limit, even on mobile. from this current date, that limit is around may 19th.other options are definitely being actively looked at, no updates to share as of yet though! or  may want to chime in re: h20 implementation@h2o has a really good gbm and its super comparable to xgboost. btw xgboost is implemented thru h2o too. additionally, with h2o the advantage is that you do not need to refactor any code and it runs in java. so super low level and you can directly tie it to any framework you use. you just have to write a wrapper class in java (50 lines of code) and that works wonders. h2o as a platform runs distributed - meaning you can train and test in a distributed fashion. h2o in production is an independent piece of java class not needing any external imports. this class is virtually scalable to really large sizes, we have folks in many different companies, especially ebay who run h2o on cluster sizes of 2000 - 3000 machines, and then run these production code on many machines as micro services.mattermost is a slack-clone which is open-sourced and can just be run on some aws server. its not slack, but i don’t think we actually use most of the ultra-powerful slack stuff for insight chat, especially if message backup is really the most important thing. it also supports bots.in addition to the ease and flexibility that h2o provides for taking your models to production, it has distributed implementations of other algorithms as well (glm, gbm, drf, glrm). i particularly like h2o’s elastic net with coordinate descent for finding full regularization path. i would stay away from mllib unless you need svm. please feel free to pm if you have more specific questionswhat types of data exploration plots did you include in your demos for nlp based projects? shared this great wisdom with me this eveningspeed up your sshtotally unrelated, but i’ve greatly enjoyed adding plugins to my jupyter setup (up to and including vim-mode…which is godlike)\\n\\n- plugin manager:\\n- vim-mode: awlong: word frequency analysis for n-grams, pos tagging, sentence semantic visualization (check out spacy)nuggetbot: what\\'s the best heater?nuggetbot: weather nycweather for new york, ny, usa\\ncurrently: snow -3.3°c/26°f\\ntoday: snow (3–8 cm.) throughout the day.\\ncoming week: mixed precipitation today through thursday, with temperatures rising to 7°c/45°f on sunday.eek - that sounds coldbest heater = soup:  does anyone have experience running apache airflow for etl?  ^here’s an intro to doing etl w/ spark in case that helps.\\n\\n\\nhappy to answer questions you might have  .just found this post from a 2015 insight fellow:  fyi as far as i\\'m aware, the lowest latency for this type of thing utilizing spark streaming coupled to ml is in the neighborhood of 100-150ms in case you are latency bound.  this definitely helpsi\\'ll dm you for some more specific qs, thanks a lot, lei was actually a fellow in my cohort (though left the program early to start at thumbtack), and i reached out to him after seeing that blog. unfortunately, his example isn’t so fully fleshed out, and his experience with this stack is mainly exclusive to thumbtack, so code examples are more limited! looking to have lunch with him soon, so hopefully i can provide any other interested parties an update one way or the other.anyone have recommendations for using influxdb and running analytics on top of it? that might be nuggetbot’s most terrifying “soon” yet not too concerned with latency now, looking to give a poc during a talk i’m giving next week, but the play server is more of a “nice to have”, though it’d be sweet. considering just exporting it as pmml and going that route if it’s easier, though it’d be annoying i couldn’t figure it out.for anyone else interested, i posted on stackoverflow: feel free to comment or upvote :simple_smile:also check out  and pfa.  much better than pmml, but implementations are still slow to eek out.we are also looking into using it. i don’t know much about it yet though…i’ve got to build out an efficient data pipeline at work, but i have no idea what i’m doing. we have a dgx1 which i have access and about 200 gigs of data. what’s the proper way to set up a sql environment and parallelized analysis environment? i’m seeing a bunch of docker stuff floating around, but i kind of need a list of ‘best-practices’ or anything to kind of boot-strap myselfany advice?if ur files on s3, use athena (which is aws version of presto)?we have a local machine that’s pretty powerful, its kind of specific to deep learning, but it also has a ton of cpu power, ram, fast hard drivethe whole data set can fit into ram, and there are 32 processor cores to work withhave u looked into mapd?no, but i will, thanks for the suggestion!how many gpu cores?8 tesla p100 gpusyeah mapd sounds like it\\'d work really wellkeep me updated if u go that route cause i am looking into set it up tooalrightythanks!the ciommunity version is free and open sourcehey  so i m in the baby steps of setting it up firsti m not running spark etl yet, more like simple s3 -&gt; csv -&gt; mysqlbut i am struggling on two fronts: (1) the webserver ui is not working as expected and (2) understanding the tech stack side of the server/workers configurationright now i just installed it on a t2.micro instance on aws and didn\\'t make any changes to the config fileit says it has 4 workers, what are those 4 workers? threads on the cpu since it\\'s a single instance?did you see this link: i am not using docker containers at the momentah, okaydo you plan to use docker containers?i’m considering that route, since the dgx-1 is dockerized out of the boxget ityeah, i’d prefer that, just for support reasons/replicabilitywell good to know that it can be donebut, i also have no idea how to use docker, so i’ve got to learn that as welllolyou got the reason i m not using docker containers yetnot that i don\\'t like them just that i don\\'t know how to properly set them upyeah…growing pains. moved from an ultra-small data regime to a slightly large data regimemine is more like moved from a super big data regime with engineers doing lots of that heavy lifting work for me to a large data regime with no engineer to help me :crying_cat_face: more fun though, lots to learnnot sure if worker is a process or a thread in airflow, though should not refer to number of instances.you can also do sql queries in airflow as sqloperators (instead of python or bash operators)thanks! i will come back to some more general qs, but for now the specific issue i am running into is thisso basically i do:\\nairflow webserver, and then i manually start a dag by unpausing it and then pressing the play buttoni m just starting a basic dag from an example... but instead of actually running i see a few odd patterns:\\n(1) does not create logs - if i do view log in the ui\\n*** log file isn\\'t local.\\n*** fetching here: http://:8793/log/hello_world/dummy_task/2017-08-16t18:19:04.412977\\n*** failed to fetch log file from worker.\\n\\n*** reading remote logs...\\n*** unsupported remote log location.\\n(2) it\\'s in running state in the webui forever though doesn\\'t really seem to actually be running\\n(3) task instance details on the ui shows the following message\\ntask instance details\\n\\ndependencies blocking task from getting scheduled\\n\\ndependency\\treason\\nunknown\\tall dependencies are met but the task instance is not running. in most cases this just means that the task will probably be scheduled soon unless:\\n- the scheduler is down or under heavy load\\n- this task instance already ran and had it\\'s state changed manually (e.g. cleared in the ui)\\n\\nif this task instance does not start soon please contact your airflow administrator for assistance.i am looking for ways to debug this babushka basicallyhave you gotten any “hello world” example in airflow working with two simple tasks executed via `airflow backfill &lt;dag&gt; -s &lt;start_date&gt; -e &lt;end_date&gt;?  if not, start simple and build your way up.it actually worked only one timethen stopped workinglet me try this backfillcheck out the goai initiative: \\nwe have been working on developing gpu enabled set of ml algos for use on dgx1. the good thing is that all mentioned products will have an open source/community version.can someone remind me where to find the electronic ds library? is it on google drive?dropboxnot seeing it. would you mind sharing it out? i tried. but it won’t let me share the link since i’m not hte owner. is there a specific book you want?whoever created aliases within yaml without an explicit option to turn them off should die! :headwall:hey boston fellows and alums! just plugging this one more time. come to our data challenge kickoff tomorrow at the cic!\\n\\nthere are prizes for the winners!\\n cfp for this year’s nyc dataengconf is open! please submit your talk proposals here:  looking forward to seeing what you come up with!is anyone going to jupytercon in ny?you can use domino’s discount code when you register: domino20come to  and get some advice from those of us who are there :flag-eu:what kind of problem are you working on, supervised or unsupervised?does anyone have some experience setting up a model versioning system? something for tracking production models and their performance as you role out new and retire old models?we keep all our models in an s3 bucket and have a simple folder (bucket) structure of `models/&lt;model-name&gt;/v1.0/&lt;weights files&gt;`.  you can add readme and other documentations as well.  think how open source projects maintain different releases.  i would suggest that you s=start with a simple structure to fit your needs now and modify it as you need it.s3 buckets also have auto-versioning that can be turned on. i typically version by `model/model-name/date=xxxx-xx-xx/time=xxxxxx/model` and have my code use the latest one.(i find this useful for models that are auto-updated regularly)not sure how stable it is but looks like s3 is promising, thanks alldid anyone join spotify as their first ds job after insight? i would love to connect with you!no one i’m aware of  uploaded a file:  and commented: i was given consent by the boston team to post this. sorry .can anyone suggest some dashboards solutions they’ve seen used? this would be for e.g. product teams to track day by day user behavior trends, not real time streaming or anything. \\n\\ni work for a small startup and am trying to identify a good solution — it would be great to talk to some users of various dashboards if you’d be willing to put me in touch! thanks! i have used plotly before, and it is good for dashboards if you want to put something together quickly (but scaling it to larger and more complex data can be tricky). there are some tutorials here: redashmetabasegoogle wave is apache wave.  wrong link  ? it was an earlier re: ok, still has nothing to do with google/apache wavethat article was written 3 years ago, but so many companies have switched to ipv6 for their networks that it seems odd that nearly everyone\\'s cell phone ip is still stuck in an address range used merely for convenience. that being said, i doubt the government even needs your ip to be in their range, with  stingray systems and programs like prism, xkeyscore, muscular,  mystic, and carnivore running i don\\'t think they need much help.brew updatewhoops, sorry - wrong window!anybody have experience building up an ab testing infrastructure and/or using one of the many services out there (i.e. optimizely?)i’m interested in hearing about details of allocation and analysis pitfalls in the wildme too!i saw daniel tunkelang give a talk at an event hosted by ffl a couple years back on this, you can see his slides here: and more context here: none of this really answers the question of which is better, but it might be a helpful place to start?more here: depends on what data you want to use to make recommendations. i just used collaborative filtering with implicit feedback for my project and relied pretty heavily on working with alum ethan and his blog posts thanks so much! super helpful tips. dan, thanks for the recommendation! i\\'ll definitely take a lookthis is great, thanks for sharing these!jupyterlab entered beta today: fyi for those in the bay area: as someone who only uses jupyter occasionally, is this trying to make t more like pycharm?hmm… i doubt that’s how the developers would see it. i think jupyter is more focused on creating and sharing interactive documents. here’s the overview: thanks very much ! uploaded a file: eszti: i haven\\'t performed mapreduce, but we use gcp quite a bit and have been utilizing the google cloud community slack to ask / answer questions. you can join here: \\nthere\\'s a #bigquery channel on the slack, so you might have better luck over there thanks !does anyone know of cool public ai companies? i\\'m thinking about investing, but i\\'m concerned about the multiples on the p/es of google, apple, and amazon.nvidiai don\\'t know of any public ones other (other than those large ones)but i work for a vc firm, so my exposure is biased towards start ups :wink:unless you have some new/contrarian info, the upside of ai might already be priced into the stock?yeah, i\\'m not sure nvidia has a lot of upside. the p/e is already pretty high and it\\'s kind of hot right now.only one way to find out!but yeah, probably too late to jump on the train.go the other way and short the ones where you smell bs the consensus could still be incorrect…  people were wrong on fb, netflix, google, amazon at some point in their history. uploaded a file: at least 2016 was a good year for somebody... has a bunch of these...amd is at 10-11 right now… in january it was at 2. not sure what the ceiling is, but it keeps going up and up  and others - i’d like to collect coverage of nips 2016 all together - has anyone got some good blog posts, etc, summarizing what everyone learned? especially want to key into the parallel sessions.i found a couple of posts: , my co-worker wrote these summaries of nips 2016:\\n\\n- day 0/1: \\n- day 2: \\n- day 3: \\n\\ni think they were essentially him turning his stream-of-consciousness notes into a post, so the amount of detail varies depending on how interested he appears to have been in the talk.thanks! not sure if you already saw them but ross and jeremy blogged about nips on medium:         thanks!does anyone use specialized software for model management/documentation?  something along the lines of  ?hey guys if any of you are going to ibm\\'s world of watson i\\'ll be there tuesday and wednesday at the datascience, inc booth.  would be cool to meet insight fellows if anyone is planning to go.true in physics and probably everything else:i recall converting videos to gifs and that working well in flask! the only downside is that they are autoplay iircfor a quick-and-dirty way, you could also upload the video to youtube and embed the youtube html link on your pagethanks erik and ben! i\\'m going to try a couple more things, and if they don\\'t work out, i\\'ll go with the youtube solutionone related question: does anyone know if there needs to be any changes made to nginx to work with mp4 files or other media? here, it says configurations need to be changed but it\\'s a bit unclear how: p.s. i got the .gifs working by uploading them to giphy... thanks :slightly_smiling_face:: your daughter has life figured out already!\\namazon kinesis firehose — this is pretty useful.from greenhouse’s ceo this morning: \\n&gt; for those of you looking for any kind of ray of light, some hope and some steel in your spine from my brother, the wisest and kindest person i know --&gt; :trump_hair::scream_cat::crying_cat_face:stay positive joyceschanit’s hard, nuggetbot, it’s hardif anyone feels in need of a sugar pick me up this morning - insight in palo alto has lots of donuts!! come by!hey all! i have no idea what i\\'m doing picking data science meetups in nyc. any suggestions?if any one is interested in attending the dataengconf in sf next week, the organizers have offered fellows a discount with code insightvip.  \\n\\nalso, if any one would like to attend free, they’re still looking for volunteers. i believe the deal is volunteer 1/2 the time, and you get the other 1/2 to attend talks. if you’re interested in attending, dm me and i’ll connect you to the organizer.hi all! if anyone was thinking of hiring undergraduate summer interns i work at a university with over 100 students who would love to work in data science roles (and have the skills). how is this channel different from  ?yeah i guess there isn’t a difference - i missed that ch when i was looking through. mostly just called it  to match insight’s channel for the ai sessionshas anyone worked with the amazon ml service? please dm me if you\\'re able to answer a few questions about it.thanks! are some of those internships in your specific team?not sure actually--the early career programs have their own internal recruiting avenues that match candidates to one of the many teams in each cityfor anyone how is interested in interview prep on a holiday: just come to the office, we are now 7 people here ...nuggetbot animate me old news alreadyhaha. oddly appropriatewell, i’m still a fan of the product, and joe would be a great person to work with. that’s my official responseindeed...i liked the team and it would be great to work with them and there is room for things to be done.hey all!  i\\'ve set up a channel for alumni/fellows that are attending kdd in sf this week.\\njoin  to connect with people that are attending!any text analytics consultants in the dc area interested in a nasa project to predict scientific advances?  pm me.  “predict scientific advances” for nasa sounds awesome and hilarious all at the same time!sounds like a really cool project... side comment, best way to predict sci advances is to make em...also, based on my astrophysics and data science backgrounds, the results of the text analysis for nasa is going to be “nanotech ai cubesat tardigrades on mars and titan from black holes”something about tesla too at this pointthere will also be fleeting dark matter spikesi heartily agree:  awesome and hilarious.  it has been a challenge educating the customer.  hi jon! super long time! one of the ways we handle this is a bit of a manual process. we look at how the data is spread. if the data seems to be running all around the space then higher probability for an outlier. in that case we prefer to bucket the data, and run models on data buckets than actual data itself. or sometimes we use data buckets with higher weights and less weights on the data itself. now this is no book, or a source, but it is something that i follow in practice, and kinda as folks that work with me to follow too, i understand, but it is something that i do to any cybersec and fraud algo. so mabbe this helps? thanks  !thanks  !   i will probably want to ask some follow up questions.   have you tried to run your data thru h2o? ( apologies, if it sounds like a shameless plug) h2o does have standard ways of imputing. mean, median, and mode, but it also has adjecent value fills - back fills or forward fills. this way data locality is preserved. nothing replaces manual inspection, though. no i have not but sounds intriguing.   i don\\'t think we have h2o on prem and visa is very picky about exporting data.  :slightly_smiling_face: &gt;&gt;&gt; even if you go as far as building a model to impute the values - you are not adding any real information. you only use what was available in the other features. so you fixed a technical problem, but not the fundamental modeling issue....\\n\\nthe simplest case is missing nominal/categorical values: just make it a different value and you are done. so literally, if you feature is eye color with current values ‘brown’ and ‘green’, make the value ‘noclue’ or something and you are set. thanks.  yup dealing with categorical features is the easy part...there\\'s also advice on continuous features :slightly_smiling_face::  rubicon project is hiring for a data engineer position at the toronto office, and for a data science position at the playa vista office . shoot me an email () if you\\'re interested.re-post from  on the upcoming bold/kafka meetup at intuit - all are welcome! how ai should be used to augment people rather than replace them:\\nanyone going to this panel about hiring data teams at ga tomorrow? inquiring minds in sv-2016b “wanna be in the room where it happens\"  :thinking_face: soon/giphy soon: it’s a bit of a trip from nyc, but if you go i’d love to take a look at your notes! :slightly_smiling_face: i second  - would love to have a peak and some knowledge!: and   will do my best to \"give us a verse, drop some knowledge” as well as include hamilton lyrics in every slack response :microphone:if anyone is familiar with bayesian inference using pymc3 or multi-level modeling in general, i have a question posted on stackoverflow that i’d love to get some insight into ()\\n\\nthanks! ^haha, already moved as off topic, don\\'t ever change se! :wink: (at least they didn\\'t close it!)why are they so terrible about this kind of thing? i left my comment on the question when it was still under se.hey all you people working at various companies, a request:\\n\\nis your company (or one you know of) *\"friendly\"* towards non-engineering majors for internships? my old department (physics) is putting together a resource for their undergrads and reached out to me for a list of such companies. i of course have no idea outside my own company! :confused: data science probably preferred, but anything you can tell me would be helpful! pm if you\\'d like, or respond here, whichever!\\n\\nthanks! :+1:square seems pretty open, and tends to have lots of internswait  and  ... are you saying there\\'s more to data science than being really, really ridiculously good-looking?!?: i\\'m at capital one and we have a large intern program here - i\\'m sure there is a lot of public info on this, but if you need anything else, let me know and i\\'ll try to help...: whats your exact issue? i would imagine the operation is doable with any of the lib players, opencv and pillowanyone update pip on a redhat aws instance and then get distributionnotfound errors calling for pip 6.1.1?downgrading pip seems hard (i have 8.0.2)  carlos is right. opencv had an imwrite command. make sure you use an mxnx4 nd array of uint8s. oh and open cv is bgra, not rgba. and , thanks! i was under the impression that the transparency only applies to color images (i imagine a transparent greyscale image would be a mxnx2 array), so it is necessary to convert from greyscale to color first then?human strikes back:boo humans boo! :wink:agreed, `kill all humans`stay positive, i find nuggetbot’s silence there… disturbing.you see, this is why the singularity is a bad thingstay positive dstoneproofhumans are still a valuable asset to…humans?victory song of the future nuggetbot kingdomsee now, this is what i’m talking about, nuggetboti use the free versionooops restricting access may be a tricky one... i literally just share the link but we are a small company so everyone see everything.. i found this but haven\\'t used it...gotcha. i’d have to check it out.  with no access control we likely couldn’t use it does anyone have experience using praw? for some reason i can\\'t figure out searching posts.i’ve got a little experience with it, but off the bat i should warn you that reddit’s api is rate-limited to like 2 queries/sec and can only return 1000 upstream hits per query, so using it to build an extensive dataset is nontrivialfortunately, the scraping has already been done by a helpful redditor so that might be more useful: he maintains the scraper so the dump updates monthly, but building the initial dataset in like mid-2015 apparently took 10 months of the scraper runningcan someone explain how much capability and longevity the free, open source side of plotly has? interactive plotting in it seems like a game changer, even just for eda, but i don\\'t want to get locked in somehowthe paid version is no different than the free version except for 24/7 customer support and  portal (so called plotly cloud)... what do  u mean by interactive plotting?paid version also allows you to have shared online plots that are also private to your org etc.  \\n\\nin terms of regular plotting, online plots can be useful since you can embed them in places.  with free version you either have to set up your own plotly server, or embed the plots in things that will automatically run the javascript using plotly’s offline mode (e.g. if you save a jupyter notebook with an interactive plot, in offline you may have to re-run the code that generated it if you open the notebook at a later time - there’s a bunch of options on how to deal with that depending on yoru use case).\"paid version also allows you to have shared online plots that are also private to your org etc.\" - if i have my own private server/portal running jupyter/dash those plotly plots are also private (until one clicks on the save and edit in cloud button)... but one can also hide that...oh cooli was just scraping the subset of a not too big subreddit so it wasn\\'t taking too long for me. that\\'s pretty awesome.hi all, \\ni am a new fellow, and about to start my project. i have a few questions: \\n1- can you scrape the web if the website is loading more info as you scroll down?\\n2- i am trying to figured out a way to remove text from an image, analyze its font (y-label) and train the model with those. later, when an image is loaded, i will recommend a font type. is there anyone who is experienced or familiar with these, and can mentor me through the project?1 is definitely possible, but probably depends on the website and what you\\'re using to scrape it. i don\\'t know if there are libraries with built-in support, but what i would do is go to the network tab of my browser\\'s developer tools, see what urls are being requested when i scroll down, try to reverse engineer a pattern, and then programmatically pull those urls (which will often be json).for 1, typically that’s done using ajax requests via some sort of javascript.  if you can figure out how the page is making the ajax request, you can make it yourself or send the right arguments to it to get more resultsanother approach is to use selenium, which lets you automate web browsersif you get really good, i\\'m going to start calling you selinium i used beatifulsoup in the past on ebay. i did not use ajax.i am not sure how to download it as a json.back in the day i used  to simulate a browsercan make page clicks, enter passwords, etcthis blog post seems to be offering a few more details on the first approach i outlined (but i didn\\'t read it that carefully):  did this project when we were fellowscool. i will look into this. i got another recommendation for selenium, but selenium tutorials are apparently requiring java knowledge which i don\\'t.i don\\'t think you should need java for selenium. i\\'ve only played with it once, but i used python.this is correct. selenium and what not are fun, but you shouldn’t need them to solve this problem. just figure out the ajax and then you can procedurally generate the urls you need and scan the results with beautifulsoup.python is what i know too can you give me more information?to clarify, you might need java installed on your machine, but you shouldn\\'t need to write any java yourselfoh, ok. thank you. :slightly_smiling_face:ddto think i’ve been avoiding cobol all this time...what is cobol? =pno punchcard are best :ppunch cards are great because then you can keep your code in a filing cabinet, without risk of code loss due to hard drive failure!stay positive alex_gudestatements about things!!!awww, i was hoping nuggetbot would respond to exclamation marks. :(fail!!! :wink:damn... too positive for nuggetbot.nuggetbot what do you have against punchcards? :(risk code loss failure !!!stay positive jeremykarnowskihaha!risk code !!!loss failure !!!stay positive jeremykarnowskifailure !!!loss !!!stay positive jeremykarnowskilossstay positive jeremykarnowskihahahanegativenope...calamity ok i\\'m done imagine dropping a stack of a few hundred punch cards on the way to the compiler...yes but no risk of code loss due to hard drive failure!!! :troll: stay positive scstarktrue that: also that\\'s why you draw a diagonal line on the side of themdiagonal line? ah yes, old school hacks! learned about that one from an aged professor i worked for. : yeah!\\n didn\\'t prof st. john tell you about how her mom used to program with punch cards?the line is to be able to preserve order; if it was vertical or zig zagged you couldn\\'t tell! no, i don\\'t remember audrey ever mentioned anything like that...how many different diagonal lines could there be? would they be still helpful in reordering a stack of a few hundred cards scattered on the floor?i don\\'t know how many there could be... i guess enough to keep them legible. plus there are four whole sides of a punch chard. you reassemble them by ordering them in the way the diagonal line mark appears on the card. since it\\'d be at a different height on each cardi will notify the appropriate persons about this.i chatted with someone that said they used to number the cards. is that right? there was also this really great chapter in a book by feynman about making computation with punch cards more robust to errors. it was pretty neat if you\\'ve never read it.cool! i heard that the problem with numbering them was that there would be issues if you wanted to insert new cards in between great advice, thanks austin! i am coordinating dataengconf nyc happening in october. this year, dataengconf will have 4 tracks: data engineering, data science, data products, and ai. please be sure to checkout our videos on youtube () if you haven\\'t heard of us! of course there will be discounts for insight students and alum :wink:\\n\\ni am curious in learning more about what you want to learn in your career. if you’d weigh in on our super short survey re: data eng/sci tools and topics it will help us plan our next event!\\ngentle reminder: rule number 1 of : do not ping @ channel except in case of imminent nuclear holocaust that we could otherwise not escapeor, of course, a reminder to clean up after yourselves in the officeahh  beating me to it!!!yes.  you pinged 1000 people across four timezones. the probability of making someone cranky is high., your are forever immortalizedthere\\'s also the option to mute notifications in using the channel setting cog in the upper right, if the alerts are fucking up your zen too muchsorry for another redshift question, tis for all who have wlm tuning experience.  so, i am digging into the types of queries that are running on redshift, and noticed that a lot of `volt_tt` type queries are run. the concern is that they are running in the `default` queue, which right now i have as concurrency 1, with 1% memory.  do you think this is the reason why querying can be slow? :ty:those should finish really fast so probably noti\\'ve worked with wlm management stuff i\\'ve found the concurency of the queue has almost no effect on performanceassuming your bottleneck is cpuno cpu on the metrics chart is only like 15%, which is way lowi have a person asking about the early deadline (august 14th) for jan 2018 class -- asking if there is a regular deadline? or how should they think of the deadline -- apply as soon as possible after that? or the class is usually full by nov? or interviews start in sept?i’ve changed our cluster type from a bunch of ds2.xlarges to fewer but same amt of storage ds2.8xlarge, and how there are some weird slow drops &amp; create table problem.wel if you main queue is 1%then you might be ram constrainedhmmm… that could be, but why do you think concurrency makes no real difference?hi all, handshake is hiring for a pretty special role in sf, our first (senior, full-time) machine learning engineer (). we’re looking for someone with experience shipping (production-level) machine learning code. i really love working here and would like to get another insight alum on our data team. if you or anyone you know might be interested, feel free to dm me!well we were cpu boundso having more or less concurrency didn\\'t change how much cpu we hadand the % you give to queues only affect ramso we had a dashboard that had a lots of queries that took a few minutes, and changing that queue concurrency to 1 or 5 or 10 didn\\'t change how long it took the entire dashboard to load:nerd_face: man, redshift is not easy to tune:ty: markoh, in the end what did you implement to make your dashboard load faster?  is it giving it more ram?nah nothingit was cpu bound, nothing we could door at least nothing i tried workedif anyone is in the dc area check this out. trevor hastie and rob tibshirani are doing a 2 day course on ml. in case anyone is wondering why we bailed on academia...does anyone know whether it\\'s possible to add a time slider to a map created in folium?anyone use the sephora mobile app and have strong opinions on new desired features? so sorry for using the \"here\". was just informed that\\'s a no no. my bad.yeah but you redeem yourself a bit for doing it to advertise women in tech. :slightly_smiling_face:  also, anyone else notice that there are only 8 women shown from netflix. does this mean there are only 8 there in these related fields?  also, none is a senior data scientist (some senior, just not with that job title).  good on them for hosting this, but also feels a little sparse given the size of the company.yeah, that\\'s bad advertising. we have lots of women in big data here. i think those are just the organizers.suit, no tie is hard to go wrong with.  maybe nice jeans with buttonup and suit jacket for the startup.thanks for the responses! i feel like menswear for interviews can be even more fraught than womenswear…title should probably read *deep learning* alchemyhaha, “here’s my lead, now where’s my gold!?”lecun was mentioned in this article but i still think his response the original talk is worth reading: damn, great response!poll: how much paid paternity leave does your company offer?someone collected information on this recently you have all the answers.#disagree.  suit without tie is always appropriate for men.also: can anyone suggest good resources for learning about supply chain management / analysis?here is the sheet we crowd sourced recently: curious what happens if someone has a second baby during maternity leave at some of these 52 week placesi was introduced to an eth-zurich phd recently who’s looking for a start in data science in europe — would any europe-based alumni be up for talking with him?my team at ableto, a behavioral healthcare provider is looking to add a data scientist at our nyc office! let me know if you\\'re interested in learning more or know someone who might be. this was hilarious: has anyone created visualizations of workflow status? i\\'m trying to make a diagram of the status of the team\\'s automated jobs and their dependencies, where e.g. any jobs that fail would show up as red, and all downstream jobs would also be red. i\\'m guessing d3 is a good way to go, but could use suggestions are there existing packages that do this ,but in a way that doesn’t quite fit your need? i think d3 would work, but didn\\'t know if there were other solutions people had used, since javascript is not my strong suiti’m imagining there have to be industry solutions for this already, in some format. what is the data source for the job tracking?it\\'s mostly managed in azkabangotcha. no experience there, but in general i’d try not-d3 first (something python/r-based), and then higher-level d3 libraries that don’t require as much js if possible. there are a few libraries around that also let you create d3 visualizations via python, which could be a good route if your team is more comfortable therethanks!best page from wikileaks cia dump\\n{◕ ◡ ◕}( •_•)&gt;⌐■-■hi all,\\n\\ni need to create a report that shows customers their individual results. i\\'ve written python scripts for the analytics and the text to go along with them, but i need a way to put them together so that they look nice and are fairly automated. can anyone suggest page design software that i can embed the scripts into? thanks!hey everyone, are any women going to grace hopper this fall and interested in being on a panel about career options? my co-worker had her data scientist panel member drop out at the last minute and the deadline is tomorrow at the end of the day. pm me if you are interested! i used their api to pull flight schedule information for my app. it was super useful!i also used it to disambiguate codeshare flightsthoughts from a phd who became a se:  uploaded a file:  and commented: i set up my phone to automatically record my door-to-door commute times. this plot shows how my morning commute has decreased after i moved from brooklyn to manhattan. uploaded a file:  and commented: my morning commutes usually happen before rush hour while my evening commutes are during rush hour. in addition, i have to pick up my kids from school/summer camp. this makes everything in the evening take longer, but there is still a reduction in commute time after i moved.tasker to record data, r and ggplot2 to make the plotsnice analysis josh! on sight it looks like there\\'s more variance for the manhattan points. why do you think that is? or is it just noise?looking back, it mostly has to do with whether or not i had to drop off or get my kids. the first three days after moving to manhattan,  was out of town and i had to take my kids to summer camp. so, the commute was longer.google is now a subsidiary of alphabet, inc.\\n\\nalso, if you watch silicon valley, click on the period after \"drone delivery effort” :smiley:\\ngood easter egg !for anyone interested in multiple pairwise comparisons ()teait\\'s old but i only got the internet yesterday: aws in plain english : try nature machine intelligence cometh... i was about to comment \"so they can lock all the content behind a pay-wall, goes completely against how the fields progressed so far\", until i clicked in the link. i\\'m glad there\\'s a signature campaign but i also don\\'t know how effective they are. i worry that the lure of a nature publication will be irresistible for academicians (like in the rest of stem). combine that with all the research that happens behind the walls of proprietary technology companies, i think the field may be worse off in the end.thanks all! greatly appreciate the help :)to decrease the weight that these closed access journals carry government agencies that are funded by tax paper money should mandate that any and all research results supported by them should be published in open access journals. what five words would you use to describe your job? i got a kick out of this: yes, you need to enable x11forwarding in your ssh connectionhey , i did x11 trusted forwarding (ssh -y), i’ve also confirmed my vim has +keyboard and +xterm_keyboard, but still no dice. did you have to change any other settings?me too!does anyone have experience using  for event data tracking : so many crappy living spaces :disappointed: tbh i had always wondered why the bulidings looked so similar hello again smart data scientists! does anyone want to explain parameter sharing with me?in terms of? neural networks?yeahconvolution nn to be specificalright, sorry for the delay, had to reread something to make sure i remembered it correctlyso cnns have \"layers\", but they\\'re really volumes. a conv layer might have say 55x55 \"pixels\" and then something like 25 filtersso it\\'s a 55x55x25 layerso that\\'s a ton of neurons, and worse, each other is connected to a bunch of neurons in the previous layerstay positive alex_gudeso you\\'re going to have something like 55x55x25x(3x3x3 neurons in the previous layer + 1 bias) weightsnow that\\'s a lotparameter sharing reduces this by remove the 55x55 partthe assumption you\\'re making their is that the location on the imagine doesn\\'t determine how important a filter isa filter is either important, or notso we can reduce the parameters from the 55x55 \"pixels\" down to a single oneso, that\\'s the basicswas there anything more specific? :slightly_smiling_face: there isn\\'t too much more to know, but we\\'re also near the edge of my knowledgethat’s right, alex, weight sharing is important because it constrains all nodes in a convolutional layer to detect the _same_ feature across the entire image. building out many conv layers to detect different “building block” shapes is often called feature mapping. \\n this article explains the concept pretty well\\n: how exactly does it \"reduce it\"?to compute your convolution over the original data set you still have to scan the filters over the original dataset right?right, it doesn\\'t reduce the computation, just the number of parameters you are using to define your model.ohbasically, if you wanted you could have every single neuron have it\\'s own value, but instead you say \"all the ones in this filter are going to share\"so why is the 55x55 ever considered a parameter?if it\\'s part of the original datasetisn\\'t it really something you can\\'t \"tune\"?55x55 is the number of \"pixels\" you have at that point (although they aren\\'t really pixels, they\\'re ... i don\\'t know what we call them, probably just neurons)i think block is the world i\\'d use in my head, hahright, they\\'re 3x3x3 blocks in your examplewell, not exactly :slightly_smiling_face:let me see if i can find a pictureso you have a layer that feeds into your current onbut you don\\'t link a neuron to every part of the previous layerjust a tiny partright, the 3x3x3 blockwell, all the possible 3x3x3 blocksof which there are 55x55 of in your example, right?let\\'s take this to pm so as not to spam general :wink:eric, that link is fantastic!yeah that\\'s a sweet book. also has a good explanation of my question, hah.i think block is appropriate. in video/image processing there\\'s a concept of a macroblockawesome discussion! so the dude who wrote that book is an aussie #justsayinhe was a mate\\'s phd supervisor #coolbyassociation?hi all, i”m a current insight fellow trying to use scrapy to get some text and image info from a website. i’m running into a problem with scrapy not being able to deal with a “#” in the url. specifically, .any suggests are greatly welcome! see if get an opportunity to talk to  or  they both worked on something similar. or mabbe even the same library.for the incoming de fellows:\\n\"we’re in the middle of a data engineering talent shortage\" i’d consider pinging  about data needs of any kind, particularly financialthank you! very helpful. :blush:has anyone ever seen a spreadsheet integrated into a trello card? i know there is a way to attach, for example, a google spreadsheet, but this is just a link to the google spreadsheet. i was wondering if there was a way to have an embedded spreadsheet that you can alter within trello.are there any blockchain experts out there? knows a thing or two igor barinov ( ) works with blockchaindoes anyone here know of any python packages that can be used to do type ii regression? (a.k.a. model ii regression, or reduced major axis regression, and other such names)\\n\\ni’d rather not switch to r just for this but i can’t find a package that does it for python.blogpost about salary negotiations for engineers written by an engineer. kudos to  for bringing it up. those numbers aren’t salary, they’re median pay package (salary + bonus + equity)…wsj had an article about this a while back if you want more info.what happens when you make a neural network hallucinate? : just in case you hadn’t seen this paper:\\n\\n: good find, thanks for sharingi am trying to look for the panama papers data. this data was on github for a brief moment and then removed.anyone knows where else we might find it?that isn’t the whole data set. just some summaries i believe.: thanks!certainly not all of it, ashrith, it\\'s like 2tb: i thought so too. i would like to think that github data had atleast a part of the data.thank you all for your suggestions for web scraping!i really liked bayesian methods for hackers when i needed a crash course for an interview: : save that for june 1st. we\\'ll have about 80-90 new fellows joining this slack channel!nuggetbot has plenty more where that came from:simple_smile:that bot can really dig deep for those nuggets.nuggetbot: animate me welcome unicornhahahaha wtfstay positive laurenbagreed: you heard the bot. stay positive! :smile:lolol hahahwtf now tells nuggetbot to tell me to stay positive?or apparently just wtf nuggetbot!stay positive laurenbhahahahahahilarious:smile:i just ordered the book.thanks ! i like that it has a lot of information and examplesanybody has experience using highcharts on  nyc map?what do you mean by \"highcharts\"?  i\\'ve been doing some stuff with cartodb:  highcharts is a thingcurses, slack, you ruin itfor anyone interested in the upcoming dataengconf in sf on april 7-8, the organizers have given us a 20% off promo code: hakka20xhey everyone! we’re out of storage space. everyone should please go and review the files they have uploaded and delete the ones that are not needed.\\n\\ni think we should also think about making this premium though.amassari: cleaned out mine! how do i check what i ahve uploaded?nm, found it: it was from teh 3 dots menu on the top right for anyone trying to find iti went ahead and deleted all the game of thrones episodes i\\'ve been hosting hereare new files blocked? or are the older files simply falling off?it’s not obvious to me (i got an ambiguous message) but it seems like the new ones are threatenedanyone have any favorite resources for principles of good data visualization? looking for both books and good articles.i\\'ve run into \"there is no space, delete old files\" when trying to upload new ones.exactly but then it let me upload it anyway…confusinglast time i ran into it (months and months ago) it refused to let me upload  one of the best that i still use.  and also there\\'s an upper limit on the number of messages that is stored, around 10,000 i think, which translates into roughly 3 weeks of data (ever decreasing due to later cohorts).premium is not feasible, considering the costs of ~$8 per user per month.unless you can score a discount ;)the solution is to either set up a tool to archive past messages and then delete them, or an alternative client more appropriate for the number of users we currently have like hipchatoh no, i’ve linked the above article, not to showcase snapshot, but just how people would do kind of test their datayou can also try to query the data vol right on mysql &amp; s3, and compare to what’s loaded to redshifta simple count on all the table could work (and be done programatically)hey folks any of y\\'all at the office?  i\\'m downstairs without a key cardhey guys. what is your favorite methods for clustering word vectors (ala word2vec)?and why?clustering for what purpose? with out getting into to much detail, i have short phrase representing expenses from our customers (eg. ‘iphone subscription’, ‘magazine subscriptions’, ‘car maintenance’). using this, information  i want to identify the key categories of expenses. so given a largish list of expenses phrases, cluster them into a smaller more representative list. i’m using affinity propagation, but i wanted advice on algorithmically assessing cluster performance. it seems like the word vectors wouldn’t cluster particularly well since the vectorization would be so sparse — ie, the point similarity would either be very close (for nearly identical phrases) or basically zero for everything else for having no common tokens two quick thoughts - you could try using word movers distance to get phrase similarity and then cluster the resulting similarity matrix - gensim has a wmd implementationyou could also use tsne if your word vectors are not too much more than 100dims - though tsne is more for visualization.if the phrases are that short, i would think a simple bag of words approach might work pretty well. how many times do you see car in your corpus? how many times do you see subscription? use the top counts to set your categories. is there a particular reason you need a more complicated approach? interesting, i wasn’t familiar with wmd… that may well solve the issue with word-vector orthogonalityquestion from a current fellow here: i’m working on my web app. i have a text file of web addresses that i would like to read in and turn into clickable urls. however, my knowledge of flask/bootstrap/jquery is rudimentry and the other fellows here don’t know how to do it either. could anyone here help me read a text document using jquery?   thanks for the recommendations. for some our problem, we’ve found that a word2vec model works reasonably well for word vector generation and have decent performance (qualitatively) with clustering using affinity propagation. i’ve thought about using tsne, however i was under the impression, as well that it’s good for visualizing the clusters in 2d. (maybe 3d). i’ll look into wmd. thanks. i did not even consider using simple bag of words… i’ll think about that. thanks! is there any reason you can\\'t dump it back into python and parse it on that side then return a list of strings to the web side?: have you tried this  ?i\\'m not sure if i understand the question, but missing not at random is a common problem and i\\'m sure there is at least some info out there on how it\\'s typically dealt with. the last chapter of paul allison\\'s book \"missing data\" deals with data not missing at random.  it might be a place to start.  someone on insight slack recommended it to me a few weeks back.   that’s pretty awesome can you give a bit more details on the process you ran through nips ?kevinmercurio: i’ve not looked in to anything myself, but i wonder if multi-user jupyter notebooks exist or you could gerry-rig one?\\n\\ni’ve heard of jupyter hub:  . not sure if that acutally allows 2 users viewing the same notebook at the same time though. \\n\\ni could imagine hacking together some scripting along the lines of:\\n1) candidate logs in to a notebook on your server\\n2) the notebook is configured to repetitively save\\n3) a script running on the server watches for file updates and refreshes a backup copy of the file\\n4) you view the backup notebook and figure out how to automatically reload any changes to disk\\n\\nnot sure if that would actually work or how to do all the steps. but surely something hacky is possible if a real product along those lines doesn’t exist alreadydid some quick googing. the following thread describes that jupyterlab is working on such a feature: \\n\\nand it’s possible a first version exists build on top of google drive.  i found this website:\\n\\n\\n\\nwhich links to this depreciated repo:\\n\\n\\nwhich says they are now working with jupyter in this repo:\\n\\n\\nwhich hasn’t been updated in a year. but says in a issue comment that they are working with jupyterlab in this repo:\\n\\n\\nso the take home is:\\na) one of the above links might have a working shared jupyter notebook on top of drive solution\\nb) follow jupyterlab as it develops because it looks like they’re building this functionality some possible options here? let me know if you find anything that works for you. i’d be interested in using similar for pair-programming type work as welli wasn\\'t really involved in that, but as far as i know you just submit a proposal for a competition, and they review it ...sorry, that is not very helpful :stuck_out_tongue:hey - suddenly upon re-reading your question i see that you aren’t actually looking for collaborative. just a different content test. sorry…i guess you can disregard my comments. i was thinking you wanted an environment for remote pair programming tests with pandas/plotting \\n\\nyou could always make a jupyter notebook with a set of tests/questions. i think they offer authentication well these days, so you could open a server and just give them an access account. but i imagine part of your motivation might be to save yourself the trouble of putting together content.for  me mathematician / statistician statisticiananyone in sf want to come drink wine and listen to a qa with founder of angellist today?when? definitely interested.does any one have some good helpful resources for making recommendations systems?starts at 5. at 405 howardnbc did a data dump of 200,000 tweets from russian bots. it\\'s a fun nlp project to play around with. : for the january session(s) fellows out there interviewing and considering offers. if you want to talk to someone that is the sole data scientist at their company, my slack-door is always open!hey all,  a friend of mine, who is a data scientist at jp morgan and a columbia physics phd,  is participating in  . their front-end guy dropped out and they need someone. if you\\'re interested, let me know. the ticket would be free too! this would be a great networking opportunity, and more importantly, a lot of fun.anyone else attending aws summit today in sf?so amazon is launching a “amazon machine learning” as a service ...interesting... backed by spark+mllib or another stack?\"amazon machine learning is currently available in the us-east-1 region.\"no details on the engines behind it — pretty normal for awsha ha ha — they charge by the data point and predictionindependent of which model you use?\\nit doesn’t specify price schedules by model … they price based on the amount of time consumed and other related compute resources\\n\\n\\nhere are details from the developers guide on their “types of models” availablewow, this stuff is expensiveanyone else at the bay area deep learning school today?i tried to get in, bit didn\\'t make the cutoffit is available for live streaming as well, fyialso  channel existsdefinitely seeing more companies looking for that machine learning engineer or hybrid ds/dewhere can we add our data? though sadly n=1 for the midwest may not be terribly useful :(every company looking for the #unicornanyone else at the women in analytics conference at facebook today?? (also, plug for the  channel):plus_one: midwest! (chicago)half of my team is there!nice!!hi all, one of our favorite insight mentors derek steer (co-founder/ceo of mode analytics) is doing a quora session. he has a ton of experience and great perspective on data science and analytics throughout the tech industry. the link to ask questions is: , and he will be answering questions/chatting on 4/17 at 2pm. considering the amount of experience on this board, it would be nice if we could get more interesting questions on there than “how does mode analytics work?” :face_with_rolling_eyes:i was here and i saw you! missed you but it was an awesome conference. i hope they\\'ll do it again next year.&gt;women make up 26% of all data science positions. assume _p_ is iid generated from a prior beta distribution. what is the probability your company hires four female data scientists in a row?&gt; feel free to use `pymc`. you have 30 mins.perhaps more importantly, what fraction of them will be promoted within two years... :thinking_face:did you ever found a workaround to this? i\\'m starting a webscraping project this week and don\\'t know yet if my target sites use distil. this looks potentially helpful though: does anyone have experience with making webapps (backed by flask) with an output of a dowloadable file such as a csv?random question: who is responsible for nuggetbot? i want to make a similar one for my team so looking for pointers :slightly_smiling_face: i would start here, i believe a fellow long ago forked the repo and is running it somewhere on insight’s aws resources:\\nawesome, thanks! since the flask logic is in python, you should be able to use basic python commands to write to a file on the file server (like pandas\\' .to_csv()), and then provide a download link in the web template. does that answer your question?yes thanks! i\\'m very close! that was my approach i\\'m pretty html illiterate so just wasn\\'t sure if i could just pass the csv as {{variable}} to my html. is that basically the same as a downloadable link? we have a fellow working on a cool slackbot this session,  . _slack police: you have the right to stay on topic._ yep! there can be weird stuff with permissions and the app designated to open something, but it should work for a csv - just put it where your html and image files are, and make an http:// link to it like you would to one of them. aren\\'t you responsible for nuggetbot?yep! it is just an instance of  running on herokufriederike: thanks!thanks!can anyone recommend a good interactive python tutorial?  i\\'m looking for something like leetcode where they give you exercises to complete on an  editor/interpreter that gives immediate feedback on syntax and correctness, but is structured as an overview of the language.  and i don\\'t want something for non-programmers that teaches what variables and for loops are.\\n\\ni\\'ve found the codecademy course  but it\\'s too slow paced and watered down.  i\\'m hoping for something i can knock out in an afternoon.:  its not interactive but a pretty good place to start python. : i haven’t taken it, but everyone speaks highly of google’s python tutorial.  it might start out basic but i think it ramps up quicker than the codecademy one.for anyone interested in new projects, flink is the newest general processing framework competing with spark and hadoop, which is growing in popularity (especially in europe where it’s from).  unlike spark, it uses true stream processing (similar to storm) rather than micro-batches, but also handles batch, graph, and ml processing.  it’s still new, but growing in popularity quickly!  check out  to learn more\\n\\n: thanks for the python book suggestion\\n: the google python tutorial looks just about right.  i\\'ll try it out.  thanks!\\n\\ni had also found this other tutorial, but the google one looks better organized with fuller explanations:  :  not sure if this is what your looking for but it might help : thanks for the link.  i saw that earlier, but i\\'m looking for something more structured.i want to write blog posts on some interesting data science problems that i get to work on. i was wondering what avenues would you recommend? i was thinking of github, but unless someone actually comes to my github page, its unlikely i would get a lot of audience. any suggestions besides that?doesn\\'t insight like alumni blog posts?: &lt;&gt; insight :blue_heart: alumni blog posts!i assume that since the heart is blue, that means insight loves alumni blog posts in a chilly, passive-aggressive way.stay positive vmgehman(that was a joke, nuggetbot.):laughing:somebody fix we could always delete him… he is not self-aware (yet)yeah, kill nuggetbot!surprised that didn\\'t get a response...i bet this will work:nuggetbot is passive-aggressive.nuggetbot is chilly?huh...blue:thinking_face:assume?heart!man…  where are you, nuggetbot?maybe he auto-destructed and he was self aware…wouldn’t that violate the third law?nuggetbot animate self terminateit took me a really long time to process what i was watching there...yeah. for a moment, i was like, “where are you taking me, nuggetbot?\"also wasn’t the point of that scene that he couldn’t self-terminate so sarah connor had to do it for him?  get your shit together nuggetbotstay positive johnrwalkshitstay positive vmgehmanthat did it...for a moment there i though nuggetbot is say ‘talk positive\\':  use medium! they cross promote blogs throughout their platform. also insight is moving to it :smiley:: do you run the supervisor as root? if so, the root user will not see any local installations for the ec2-user (ubuntu). first try to “sudo su” (to switch the user to root, you will see # at the prompt instead of $ telling you are now the superuser) and try to run the needed command to start your server, make a note of the error. \\nmost likely you will need to install your needed items (python, etc.) as root...i’ve also seen arguments against universal income.  universal healthcare though, is pretty awesome afaict that article misses a big point against universal income, which has nothing to do with the cost or the hand-wringing concern about people getting lazy if they\\'re not forced to work (a bad thing??); the idea that the problems that communities of poor people have can be solved by throwing money at them seems like a capitalist fantasy -- imo the reality is that the existence of social safety nets in a broader sense is far more importantbut that\\'s just me`seize the means of production` or smththere have been a lot of studies lately showing that the most effective forms of charity are generally to just give people money and trust them. but of course all studies have biasespretty strong evidence that it works very well (this is one of the most efficient charities on this planet, in terms of lives saved or improved per dollar)(btw, these guys:  are doing good work on rigorously evaluating charities for effectiveness)cool, thanks for sharing the giving linkdoes anyone have any advice for choosing analyses or machine learning methods for open ended questions? it seems like the best method is rarely known a priori. so far the only advice i\\'ve received is to stick to any method i suggest and provide support, but occasionally it seems like they want a specific method or approach to some of these questions and i\\'m curious if there is some standard approach that offers more optimal choices.do you mean like in an interview? i would say always consider precisely what you want out of a model.  some models are more interpretable than others.  also always start simple -- a 1-day answer anywhere in the ballpark is better than a 1-week answer that\\'s correct.  maybe the best approach is to lay out all the options you can think of, then systematically evaluate their strengths and weaknesses (out loud in the case of an interview).yes, for interview questions. i\\'m pretty sure i had the same awful problem!  i\\'ve erased it from my memory but i can try to look up my inelegant work-aroundstay positive jaredg```import os\\nfilename = os.environ.get(\\'pythonstartup\\')\\nif filename and os.path.isfile(filename):\\n    with open(filename) as fobj:\\n       startup_file = fobj.read()\\n    exec(startup_file)```i ended up doing something like this at the top of both my __init__.py and views.py filesit executes a startup file that then finds the right directory like so:```for root, dirs, files in os.walk(other_pythondir1):\\n    sys.path.append(root)```i actually put those last 2 lines in __init__.py file as well, just for good measure.  good luck!hahaa colleague of mine has an open position for data sciences manager at proteus.  let me know if you\\'re interested and i\\'ll put you in touch!  finally. i remember someone telling me  2 years ago they were about to ipo. :stuck_out_tongue:amazon aws selected insight as one of their hot startups of july  probably because insight forces fellows to open aws accounts and then the fellows don\\'t cancel them and so aws gets that 51 cents a month for years.hey all, if anyone in the sf bay area is looking for an experienced data scientist (&amp; phd astrophysicist) who was recently chief data scientist at a cybersecurity startup, let me know cuz i might have just who you’re lookin for! thanks!!anyone have experience with retrieving historical weather data? i want simple information such as temp and precipitation but a lot of sites limit the number of api calls and don\\'t let you batch retrieve. thanks!!!!! by batch i meant enter a range of dates for a single call. merci!!: i do. how far back? what areas?hi josh! thanks so much for responding!! my general hope is to predict aspects of the coming growing season in locations people enter where they are starting or interested in starting a farm. so i\\'m not sure how far back i would have to go to make a good prediction, but for now i\\'m thinking of just focusing on hudson, ny and 10-20 years back.noaa weather historyi wrote a short script to scrape it if you’d like that.: feel free to dm me as well — i used weather data in my project if  suggestion didn’t get at what you need.you’ll need to select the weather stations you want.do any of the current fellows own or have owned bitcoins?i’m fairly certain fcc complaints can be accessed electronically, but all i’m finding at the moment is a weekly csv summary file.  any leads on where to look?(looking for full text of complaints + date/time, etc, etc.): ask :  yes that sounds a bit like  \\'s project. you might ask her tooi used noaa weather histories as  recommends.  from what i could tell it is the richest free source of data.  kinda a pain to transform the flat fixed width files, sometimes sparse with precipitation, but not bad.i\\'ve heard noaa also provides spline interpolated data down to 5mi x 5mi at 1 hour intervals if you want to avoid figuring out attribution to a particular weather station.  never used it though.there is also a commercial airport weather data set that will give you basically global coverage but since it\\'s based on airports (which move around and don\\'t conform to international standards) it can be messy to work with.here is 30yr averaged data from noaa if that is useful: noaa density of weather stations mapped on pretty well to population density from what i remember.deep learning + stealing everyones pinshi everyone! who is the expert on working with pdf files in python? i would love to talk to you for about 10 minutes.: has anyone out there successfully gained access to instagram\\'s api? it\\'s a complicated process and i keep getting errors trying to get an access token. thanks!: how did you access your instagram data?i think i actually just got one. but i haven\\'t tried using it yet...does anybody know of a place where i can get political party affiliation data? the 2016ny channel doesn\\'t seem to have gained momentum yet so i write here: you mean of lawmakers?  and  have gotten such dataalso looking for possible data: does anyone know if there are public data sources of deleted twitter accounts / tweets, blocked twitter accounts, or other similar info?  (besides monitoring twitter for a week to see what gets deleted, of course.): no.  all i need is the percent (registered) democrats and republicans at the county level.hi everyone! does anybody have experience with text extracting from pdf? : try \\'pdftotext\\'     see:  : i\\'ve only tried unsuccessfully so far (a few years ago), but i\\'ll probably need to figure this out myself. there are at least a few python tools out there (pypdf2, slate,...): sunlight foundation is probably what you want.  their api is pretty good and they have lots of python tools., i\\'ve done it, as far as i remember if the pdf is a \\'text\\' pdf, pdftotext (in what used to be called poppler-lib, i think) works. if not, you can use tesseract to do ocr on them, then clean it up. beware that this works ok with text, but not tables.ahh,  had already answered that, and with a link... sorry!it\\'s looking to me like pdfminer3k is the only text extraction module for python 3 (which make it worth sticking with python 2). if you want to talk to someone that worked on this at sunlight foundation (  ) suggestion, lmk, i have a friend that did this work there.   thank you guys :) the pdftotext works well.that looks awesome don\\'t it though? i\\'m thinking about volunteering.i already sent them an email :simple_smile:woah, look at you!nuggetbot: animate me enthusiastic volunteerperfectthat\\'s exactly how i felthair in my face. fighting some storm trooper looking guys.low tech storm trooperhunger games storm troopersweek 3 demo day here at insight right now! anyone have any memories/words of support to share with the current fellows?don\\'t read your slideslife event: first pizza since insightinteresting podcast on terrible interview experiences, with joel grus. would be curious to hear more about terrible interviews from other insight alums. :sweat_smile:\\n\\nhi there! \\ndemandbase is hosting the next bay area nlp meetup on wed., june 21st, and we\\'re looking for a speaker. i know the timing is short, but i\\'m hoping you or someone you know already has a presentation from another meetup or conference. the group is very laid back and typically discusses \"real world\" (or at-scale) applications of nlp, rather than overly technical/academic aspects. any data science topic will do, even if it\\'s only tangentially related to nlp. another speaker fell through -- i may give a short \"explainer\" talk on word2vec but we\\'re hoping to pad it out with another presentation. there will be food, drinks, and networking opportunities, please send a dm if you can present :wink:anyone want to see dragon boat festival next weekend (june 10-11th)?   \"if you are thinking only about communication as having the perfect message regardless of how it lands on your audience, then you\\'re likely just to be spraying information at them and not really saying something to them that sticks....that\\'s what i\\'m trying to get scientists to do — to share that excitement and passion that they have with those of us who don\\'t do that for a living.\"\\n\\ni’m guessing that this has come up before, but i am not be able to search in past messages. does insight have any plans on purchasing a slack pro account, so that we can see past messages?: please keep sharing good articles you may come across. it always awesome to read awesome stuff. thanksdoes any one know a good online course that covers time series analysis?don’t know of a fully structured course, but this online textbook is a decent introduction: : thanks.  i have worked through that website but i was wondering if there was more of a coursera type course?: at some point we could even aspire to become a newsletter!: i don’t know of any myself. material like arima and var models are usually covered in econometrics classes. you can find some video-taped lectures on youtube, but i can’t attest to the quality or whether you’d get more out of them than the oxtext uploaded a file: i love this job posting. too bad the salary is so low.: which actually brings up another question.  does anyone have any recommendations for an online course that does finance/economic focused data science?: i can recommend somethings since i\\'m going through them nowit depends on what you\\'re interested in though.  finance as a topic is pretty broad.apologies if this has been covered already, i searched and didn\\'t get any hits: do people have ways of making dashboards that they like? our top contenders right now are wagon () and looker (), but i\\'m happy to check out other things.\\n\\neventually, data will be piped into an internal ui so at the moment we don\\'t want to build anything (like a webapp), we just want an existing thing that non-technical people can look at without getting confused and technical people can use without having to learn anything new.\\n\\nthere are other tools like mode that look great, but that have to access data in our aws and that violates the security agreements we have with our customers so we want a tool that we can run internally. also, this is not for *doing* analytics--the crunching happens in hive--it\\'s just to see some charts, so the data involved is tiny.\\n\\nthanks!mio: shiny is a very good option; it\\'s become very robust and also has a nice shinydashboards package that makes it super easy to make dashboards in particularcf you can definitely plug r into hivethere is a python version of shiny called spyre made by folks at the next big sound, though can’t say i’ve used it:\\nno hadley no crystay positive mpanciai have used spyre in the past and it works pretty well for quick dashboards.sweet, i\\'ll check \\'em out!thanks    awesome, this is eally helpful! :slightly_smiling_face:and going public! :tada:let\\'s talk conferences - any clear winners for data science generalists?  also, i came across this local one, anybody heard of it? i really liked a pydata conference i went to. nice mix of everything - ml, tools, engineering, visualization, deep learning, etc… and all with an extremely applied focus also gave an amazing talk there: aw shucksyou’ve revealed my bias :slightly_smiling_face:hey, i\\'ll take it!great talk ! dia &amp; co visited my session, they seemed really coolthanks!hi friends,\\nmy company @point of care, is looking for a data scientist. the position includes exciting opportunities to explore, analyze and model patient and clinician data that is powered by numerous clinician and patient companion apps. this is a chance to work with great people at a well funded, dynamic healthcare technology company. please, message me if you are interested. \\nhey all, this is a really vague debugging question about pyspark but does anyone have any ideas why the exact same code for a spark sql query from a jupyter notebook would run 10 times faster if copy pasted into a pyspark shell?same spark context in both cases? it’s a wild guess but the spark context might be initialized to localhost in the jupyter notebookand/or just not connected to the same clusterhi all, questions around feature importance from random forest.\\nwe are building an explanatory model to find out risk drivers of employee safety. we have chosen random forest classification to fit an injury binary factor with features around work hours, training, weather, etc.. roughly speaking, i plan to report that the most important features reported from the random forest model are the drivers. in terms of libraries we have been using *randomforest*, *rfpermute* and *party* in r. i am not a statistician by training. i have read every discussion online and all library manuals, but still cannot get my head around these questions.\\n\\n1. the 2 most commonly used metrics are *gini impurity importance* and *permutation accuracy importance*.\\n\\ta. which one is a better metrics in my use case?\\n\\tb. when the ranking from these 2 metrics disagree, what should i do?\\n\\n2. rfpermute and party also report *p-value* for each feature,\\n\\ta. should p-value play any role in selecting important features? why do we want to know p-value in feature importance?\\n\\tb. what does it mean when a feature is important, but it’s p-value is high?anyone here has experience using apache arrow?pyhive doesn\\'t allow you to use passwords, do these?we\\'ve been experimenting with it mostly from the pyarrow angle.  overall i\\'ve been very happy with it but there are still some features missing that will be nice.  the team that is building it is very good.  wes mckinney is a badass.  i\\'m also excited about the plasma in-memory store but haven\\'t played with it yet.what type of server side configuration do you need to take advantage of it\\'s in mem distributed advantages? are you using it on an emr cluster?i\\'m pretty sure impyla supports kerberos.  it\\'s been a year since i looked at it though. caveat that i\\'m *not* a statistician, so if anyone else has better answers, i\\'ll happily defer to them.\\n\\n1. a quick search suggests that permutation accuracy is probably better for explaining individual feature contribution, but that gini impurity is more common and good for assessing general feature importance. unless the distinction is important for your application, i wouldn\\'t spend time worrying about the difference.\\n\\n2. i might look at how the p-value/z-score *compares* for each feature relative to others, but i wouldn\\'t assign too much meaning to it individually.nothing that complicated yet.  mostly just using it as fast serialization for pandas dataframes.  if you do some work in those directions let me know.  we\\'re mostly waiting for it to be more closely integrated with spark so we can consolidate our codebase to just pyspark.i am not a statistician either- blind leading the blind.  but p value is a measure of the strength of evidence.  so you can have a feature where there is weak evidence of a strong relationship.  or you can have strong evidence of a weak relationship. get it? feature importance measures the strength of the relationship and p-value measures the probability that the feature is significant (reject the null) under the null hypotehis.  i.e. pval = p(reject the null | h0).  i.e., how likely are you to observe a deviation from the null hypothesis under the null being true...in the lr context, one can have a beta coefficient to be very high but a pvalue to be non significantfrom what i understand, the feature importance from tree based models like random forrest is the measure of a feature predictive power. but does that mean the feature with more predictive power is more important? to me, not necessarily. \\n\\ni would go back to the team who is trying to make decisions based on the feature importance and ask them what really importance means. in order words, what are the actions going to be taken. it is sometimes cost/revenue related and some other times, correcting/optimizing some operational steps. once you narrow it down to the relevant features, i would suggest running a linear model with scaled features and look at the magnitude of coefficients. higher magnitude, higher impact.also with regards to this last suggestion by , the signs of those coeffs provides the direction of the effect. hi everyone, i\\'m a former fellow looking for a roommate, preferably female, interested in living in the south san francisco area. if interested, please message me. thanks!can anyone on this channel provide pointers for interviewing with the facebook infrastructure team?practice writing lots of sql.  case statements, rank, lag, lead, pivoting columns.  etc.  not that they would ask anything particularly, but you don’t want to be underprepared if they do.: thanks! i played around a lot with graph theory in my ph.d, what\\'s up?chiming in to say the touch bar is not a problem. i never use it and don\\'t even notice it\\'s theresorry i missed this message, as  already mentioned fuzzywuzzy is your friend. it is fantastic for exactly this purpose but you would have to make sure you also install the python-levenshtein package as well as it can get very time expensive quickly. feel free  to pm me if you need more detail or questions.hi! has anyone here worked with census data? or do you know anyone who has?i have, a little bit yup! polisci background, so i\\'m familiar. what q\\'s do you have? hi. i’m interested in finding population density and income level in sf neighborhoods. but cant find a geographical identifier or zipcode to find the stats for a neighborhoodall i have is pumas and i cant find the dictionary for them to know which is whichif you go to american fact finder, you can specify the dataset you want (decennial census, acs, etc), the specific variables, and the unit of analysis (state, county, zip code, etc. i think block-level is also available), which you can export to csv. there\\'s also a census api if you prefer. hi -- does anyone have experience converting lots of messy addresses to postal service standard addresses? for example, converting \"905 east 6th street\" to \"905 e 6th st\"? that is a trivial example but i have addresses that are messy in a variety of ways. i believe there is a plug-in for excel that can validate the address fields and by doing so it will populate the validated addresses (in a standard format) in additional rows - i believe it\\'s called yadress or something along those linesyou can probably also geocode them in qgis but i\\'m not sure if it will provide a set of standardized addresses as output apart from the long and lat resultsthis table has maps of the puma codes by county: thanks -- this looks like it will do what i need. i got a vba error though (compile error:\\n\\ncan\\'t find project or library\\n(yaddressform 41:8)census tracts and census blocks are smaller geographical units that i and others ( and ) have used in insight projects, if those buzzwords might help…carola: hmmm perhaps try googling the error to see if anyone has had the same issue? i\\'ve actually never used that plug-in (just remembered it existed). sorry!no worries, of course i googled it already. thanks for the suggestion!thanks so much! i think i’ll focus on tracts and blocks. i’ll just need to find the lat and long for the center of each tracti believe usps has a web api for this: i don\\'t know how much mess it can clean up thoughthe usps api is decent, there\\'s also the `usaddress` package on github that does essentially pos tagging and parsing on addresses.for small dataset, you can also look into using google’s geo api oh, and if you\\'re wanting the centroid of each tract/block, the census also provides shapefiles has anyone tried to evaluate cluster (e.g., k-means) density at the individual cluster level? i’m trying to find the clumpiest clumps--will probably use average inertia or silhouette coefficient.  any suggestions?seconded for google geocoder: have you looked into using em instead? k-means makes a lot of assumptions about your data set’s geometry that may not be often true for real-world stuff.this is a first pass at the problem, so i haven’t looked into alternatives in any detail.  i’m putting short text documents into a vector space and clustering them. the goal is to find some subset of the clusters that show strong internal similarity.as i understand it, other non-em clustering algorithms allow for more reasonable / different geometric assumptions (e.g., dbscan).  as the timeline stretches out, i expect to do  more serious topic modeling (e.g., lda), but this is my first stab.regarding finding some subset of the clusters that show strong internal similarity: i did this by looking at the similarity matrix and only selecting  words/vectors with strong similarity for clustering.as a preprocessing step were you selecting features or observations w/ strong similarity?i see: you picked only observations that were close to at least one other observation?i was selecting features, i.e words with strong similarity. most clustering algorithms directly output a similarity matrix so i didn\\'t have to calculate from scratch. i think in your case it would be the text document.if you are using acs, there is a great wrapper in python to access the data:\\n just to clarify - there is no spherical restriction on em - in fact it is k-means which is a special case for em, under spherical assumptionsi think maybe earlier was a typoas for cluster estimation in an embedded space - there is really no good reason to assume that the geometry for these high dimensional spaces is spherical for clusters - so you might get wonky results if you start with k-means. one has to be careful with high-dimensional clustering too, because in high dimensional spaces, there is a tendency for ‘everything’ to be close together.the census bureau also has a geocoder api: ..on geocoding, see also  and  .hi  in case you still need a solution, have you tried passing the addresses through the google geo api? the output in a search has a field that is the \"standardized\" (interpreted) input address. just a thought.hello fellows ! i have just moved to la for work and im very excited about the new opportunity ! i live in venice if you are close by lets connect and meet !google api is good but if i recall there\\'s a limit on how many you can call per day ... cheap to purchase more. openstreemtap is another one, but not sure if it has an api... you can use it through free gis software like qgis*openstreetmap thanks for the suggestion...i\\'ve been poring over their docs and it seems they want the query in standard usps format. i haven\\'t tried it though. have you used it?agreed  - my accuracy was shockingly bad in that quizhappy :turkey:anyone knows the easiest way to setup mongodb on ec2?can you install it via apt-get?looks like that works and it starts the process as well\\n\\n```sudo apt-get install mongodb```i\\'m running ec2 amazon linux, it\\'s redhat... yeah i should probably start a new instance with ubuntu, ubuntu seems more friendly to me....ah, okay. before jumping to a new instance you could try using `yum` but i\\'m not familiar with its usagethanks!another questions: does anyone know how to use ssl on ec2?my chrome extension project needs to send https request to my server on ec2or how do i get a valid ssl? glad you figured it out -- as an alternate approach, i ran into a similar issue trying to pass dicts into d3, solved it with the method here:  how does the {{var|safe}} approach work?in case anyone ever wants to share interactive plots in jupyternbs for presentation purposes without too much effort, chris said wrote a nice post recently describing a few minute pieces of javascript that’ll make your life much easier:  i wanted to pass in a list of  dictionaries with strings for keys as my variable. it gives the #39 error but if you pass in the variable in jinja2 safe mode apparently it saves the strings?!  yep that\\'s the exact issue mine had.  got around it using flask.jsonify to avoid ever explicitly handing the strings off to jinja/htmli was wondering: what sort of ds related podcasts does everyone likes to listen to? i really enjoy talking machines and o\\'riley data show, but haven\\'t heard any others that i\\'ve found to be consistently interesting / useful. hi fellas, this may help for interview prep: bit twiddling hacks : : another couple i enjoy are talk python to me (strictly about the python ecosystem but covers ds-related topics frequently) and partially derivative (more fun than useful)a friend\\'s company just released this, it a linux desktop in the cloud built for machine learning! basically allows you to access a supercomputer with a bunch of gpus from your browser with tons of ml packages pre-installed  what\\'s up? how can i help? just curious if you had any tips for getting started with itastrophysicists, i saw data showing that % of astro papers mentioning p-values increased  15 times in the last 10 years. does anybody know why such an explosion? anyone going to the rev conference in sf?has anyone gone through the pains of setting up an llc / s-corp and can provide some insight into going through the process?  i’m looking into it primarily for tax writeoffs.i’ve only had mine for ~1 year, but will be happy to help if i cani’ll be there!llc is your state level entity registration.  this is primarily done to limit liability instead of individual or partnership/\\n\\ns-corp - is a irs classification.  this  makes sense if you are running consulting business with over 200k income where you don’t want take all the money as salary and take some or most of the money as distributions that are taxed at 20% (i think).  very popular among doctor and law groups.  one caveat - you can’t raise money with s-corp.  for that you will need to become c-corp.  if you want an entity that’s going to make 50k on side business, then s-corp is too much work for it imo.:+1:anyone have any conferences or workshops that they would strongly recommend for ml and/or ai? uploaded a file:  and i met over the weekend. we were both wearing our insight shirts and for once, we didn\\'t coordinate our outfits.haha, it’s true!you heard it at insight firstmonica rogati is leaving jawbone and the rumor is she\\'s headed to twitter @mrogati\\'s tweet: : haven’t seen the talk (link?) so i’m likely missing some context, but bias is of course a major issue in data science and any quantitative endeavor involving making decisions from real-world datastay positive zkermishdepending on the industry and role, i’d think it’s often the data scientists responsibility to emphasize the effects of bias, mitigate them using best practices, and honestly report how they influence results.(in the context of ab tests...with stakeholders having preferences for results and laking the statistical knowledge to interpret them appropriately…this can be one of the biggest challenges): you clearly don’t know me at allsolution: make decisions randomlytrump 2016here it is, sorry it was from scipy: hi all! we are looking to hire an experienced data engineer. it’s a position in boston, hello. i\\'m a health fellow in boston this session. i\\'m having trouble getting gunicorn to properly access my static folder on my server. using ip-adress:5000 works perfectly, but using my url results in a site without css.\\n\\nthe path to my static folder is: ```/home/ubuntu/webapp/my_dir/static```\\n\\nmy index.html starts with:      \\n```&lt;!-- bootstrap core css --&gt;\\n    &lt;link href=\"../static/css/bootstrap.min.css\" rel=\"stylesheet\"&gt;\\n\\n    &lt;!-- custom css --&gt;\\n    &lt;link href=\"../static/css/business-casual.css\" rel=\"stylesheet\"&gt;```\\n\\nmy nginx:\\n```server {\\n    location / {\\n        proxy_pass \\n        proxy_set_header host $host;\\n        proxy_set_header x-real-ip $remote_addr;\\n    }\\n    location /static {\\n        alias /home/ubuntu/webapp/my_dir/static/;\\n    }\\n}\\n```\\n\\nthere must be something wrong with my path, but i can\\'t seem to find it.  please let me know if you have any troubleshooting ideas. thanks in advance.just thought my folder structure may help too:\\nsimplified folder structure:\\n```webapp\\n    &gt; run.py\\n    &gt; my_dir\\n        &gt; veiws.py\\n        &gt; __init__.py\\n        &gt; static\\n            &gt; css\\n                &gt; bootstrap.css\\n                &gt; bootstrap.min.css\\n        &gt; templates\\n            &gt; index.html\\n```  sqlworkbenchj and datagrip remove the `..` in the paths. they should all start with `/static` thanks for the suggestion, but the problem still persists even with that change \\n\\ni’d recommend using a different method for specifying the paths for the css into your html\\n\\nyou can use the `url_for` method combined with templating, as in this answer: \\n\\ni don’t know if it’ll solve your problem, but might be worth giving it a shot no luck with url_for. thanks for the sugestionkeziah: can you share the url and ip?thanks for the offer richwinslow just helped me solve the problem!thanks for all the help  helped me resolve the problem. while my \"/etc/nginx/sites-enabled/application\"  file was in good shape my \"/etc/nginx/sites-enabled/application.save\" file had some typos in it.:+1:volunteer opportunity: chicktech () is seeking tas for a workshop on electronics for high school girls in sf. tas need to be able to read basic schematics and translate to parts on a breadboard. time commitment is 1 day (approx 8 hours). dm me if interested.hi all, \\nneed some insights from clustering experts here !\\ni am trying to find clusters in a dataset that has latitude-longitude as two of the features - i found that dbscan works well for lat-lon data - but does it work for lat_lon + other features (mostly categorical)?  or should i try something else?\\nthanks in advance for any pointers! pinned a message to this channel.want to work at tesla? what are people\\'s preferred approaches/libraries/companies for user tables with 100 or so columns (possibly linkage to other tables, at scale)? graph? relational? redshift?live from nips:  if you don’t have time to read through hundreds of papers, or watch hours of videos, we’ve summarized key highlights of day 2, check it out here! hi all, our marketing team at uber is looking for a senior data scientist with 5 years+ industry experience. please ping me if you or anyone you know is interested. :+1: thank you!!  hey @ channel (sike!), i just made  for general discussion on nns, staying up-to-date on the field, and seeking help. come one, come allnever again will i dare to @ channel :woman-gesturing-no:my team is looking to hire an experienced data scientist/manager in our san jose office.  please let me know if you are interested:  all the apis; maybe we should start a new channel for project resources?if anyone\\'s interested, this meetup is next week on cellular automata and deep learning: is anyone else doing the coursera scala course? i\\'ve created a little  channel for chats. 2016 internet trends report is out.\\n\\nparama: i have a few things i can come up with from my experience. 1) clustering with categorical features is tricky because your dataset is already \"clustered\" with whatever the categories it has. 2) scaling greatly affects your clustering results when you mix different units (lat-long + category), so you have to scale features in a way it \"makes sense\". 3) if two or more of your categorical features are correlated, they have significant effect on your clustering. 4) in general, unsupervised learning doesn\\'t have any measure of successfulness, so you have to come up with some type of heuristics for your use case. 5) one degree latitude and longitude don\\'t correspond to real distance. one degree longitude at a pole is 0 km long.: i have been told that dbscan in 2 d works very well on géo data, but as soon as you add a categorical feature the algo will not work. i am not sure if there are any alternatives. i wonder if you could perform the géo clustering, then add a column with the cluster identifier, and perform a second clustering minus the geo data but using the new cluster column?that\\'s how i would approach it as well.what’s a clean pythonic way of providing two constructors for a class? i know i can do some logic with kwargs, but i feel that doesn’t provide the same clarity of use that having a defined signature does?have you tried using `@classmethod`?like  said, e.g.: cool, i’ll have a look, thanks. decorators to the rescue!ah yeah, just what i was looking for. thanks.i saw it on stackoverflow but i was a bit uncertain since it was the second most popular response…sounds like a potential insight project.... is working on a chrome extension that highlights the best responses for python questions on stack overflow!sounds similar to we have a new blog post out! this one is especially for all you physicists out there!   rocks :clap:hi everyone! i am looking for some tool to plot an interactive time series, hopefully more high level than plain d3, any leads on a good package? have you seen nvd3?  it’s built on d3 and reduces the initial setup time for many common chart types. there’s also c3thanks! i will play with those. so far i was trying to use rickshaw, which looks really nice but difficult to customize.i’m probably a little late to the party, but the nginx config file example contained in the widely circulated “the internet, servers, and flask” google slides deck may have a typo. the line `location /static {` should read `location /static/ {` (trailing forward slash added).bmfarley: hey brian! thanks for the info. by chance, in the path contained within `location /static {`, does that path have a trailing slash or not?my guess is that if it’s `location /static {` then the path contained within shouldn’t have a trailing slash, but if it’s `location /static/ {` then the path needs the trailing slashyeah, there’s a mismatch between the two in the example config file — no trailing slash on the location, but a trailing slash on the aliaslet me try it with both missing, too!both missing also works, so it looks like the path mapping is quite literal!good catch! so either both need the trailing slash or both need to omit the trailing slash, correct?that seems to be the case, yes.great. updated the slides. i’ll blast the webdev channelawesome, thanks!guilherme: thanks - i am probably gonna try this out then!hey yusuke - thanks so much for the tips! you are right i am already facing trouble with categorical data as the clusters tend to pick up the feature with most categories and just use that!could you please elaborate on what you mean by scale features to make sense?i mean i understand normalizing for number databut how to scale for say gender and age categories?hey all, i\\'m helping organize an event on the state of data science education. our very own jake klamka will be on a panel and we have dj patil keynoting. if you\\'re in the dc area, you should check it out and please send to anyone you think might be interested. thanks! \\n commented on ’s file : : now that you have a lot more data how do these plots look? uploaded a file:  uploaded a file: : i was on vacation for a bit in august, have worked from home a few days, and gone on business trips, so the data is actually a bit more sparse than previous weeks. the biggest change is in the evening commute. i no longer have to get my kids from their summer camp in the evenings, so i think the shorter commute trend is here to stay.roughly 24 hours after the previous post, you knew it was coming, day 3 at nips! if you like rl and gans, you will like this one! hi all, particularly those with experience working with/at spotify. i’m looking to get some general information about their hiring process for a friend. if you’d have some time for a quick, high-level chat, i’d really appreciate it. please message me or email at my new blog post is out!\\nthis article is awesome ! thanks for sharing. very cool to see gans used for realistic use cases. would you be able to share the related code?just need to clean it up a bit firstthen can share the notebookhey all, i am working with a civic-minded early stage startup that is looking to hire a junior data analyst.  if you have worked with any promising interns that may be interested please encourage them to check out the listing and apply ()oh, lots of new people! is it that time!? :smile:fresh blood!welcome party this friday....welcome party, aka the last time you have fun in 3 months!(i’m trying to scare them)really till? that\\'s how you want to start out? by lying to them?it\\'s more like 4! :wink:@jessica there is enough information here. i haven’t perused through, but it looks like the tutorials and examples are good for beginners as well as advanced. i can help too. i’ve had a sole proprietor llc and now a multi-member llc with s-corp tax designation. llc with s-corp tax election seems to best of both worlds.patrick phelps will be there, i think he\\'s mc  lumiata, the company i work for, is hiring a data engineer. pm me if interested: anyone read judea pearl’s latest on causal inference, _the book of why_? worth it or too watered down?  i’m about a third of the way through it. so far it’s definitely not at all technical compared to e.g. causality, but provides a nice historical (and opinionated) perspective on how we ended up making ‘causality’ a taboo word in statistics/data analysis.oh, and how causal reasoning can help solve some of the statistical ‘paradoxes’ (like simpson’s)any des from airbnb?i\\'m completely new to this whole field but i enjoyed reading this blog post: anyone familiar with the matrix profile methods out of ucr? one of our team members saw a presentation and is now convinced it\\'s the only way to do time series analysis.not that particular technique, but i have done some dynamic time warping, which came out of the same lab. they have done a lot of good work when it comes to times series analysis. i’ll be thereme too!mini insight meetup at the evening mixer tomorrow?cool article about the data science origins of the warriors: icml, nips, kdd?@nothere we’re planning to hire a large new team of ds and de in london over the next year to work on new premium products. if you’re in europe and interested in spotify, let’s talk!what are those? they sound fancy…just a bunch of letters. almost as good of conferences as jkl and ibmright onbetter answer:\\n\\n\\n:+1:so were you asking people’s relative opinions about the three? or seeing if anyone was going?ah -- i was intending to respond to ’s question about recommended conferences for ml and aihaha, yes indeed, it all makes sense nowi underutilize threads, evidentlythanks guys. i was bummed to miss nips this year.a quick question: is there any advanced approach of finding the best parameter values better than the grid search?random search usually works better. bayesian search is the most efficient (you can use packages like spearmint)an randomized adaptive search using some variation of the metropolis algorithm would be ideal.thanks guys!meetup is hiring for senior ml and de roles:  ai in action\\ndoes anyone know a good place for sensor data? nasa has a few interesting datasets of sensor data:  awesome, i will take a closer look.no problem - kaggle also has a few interesting ones: sigopt works pretty well. it’s a commercial product but has a free tier uc irvine also has a decent collection of data sets (sensor and otherwise).  i don’t want to start an editor war, but check out the new post on using emacs for data science:\\nit’s even on the front page of hacker news!\\nsounds like high variance?  what do you mean by “random state” in this case?  the initial seed for the choice of bagging / features?would definitely recommend doing a k-fold validation! how big is the dataset / how many estimators are you running?thanks ! \"is there a paper?\" literally just came up an hour ago!hello, any canadians moving to us with tn visa? :flag-ca:: i did last summer, as did : good paperyeah, he’s a smart guy:  too moved from canada for work.: yep, made the move this year`eh`: i don’t think anyone plays with the random state other than for deterministic reproducibility. it would be easy to convince yourself that the model performs better by validating on your holdout with better performance given a different random state but that’s just due to model variance so that logic doesn’t hold true in the real world where the model will have to perform on different data than the holdout set. bagging in ensemble methods tries to reduce variance at the cost of increased bias, but (as you’re seeing) it won’t be reduced to zero.take a look as the relevant bias-variance tradeoff section in hastie and tibshirani. or this blog post does a decent job of explaining as well: thanks, it logically seemed like the likelihood of this on a larger holdout set would be small but given the inherent randomness of the models it seemed like it is theoretically possible that one set of trees could fit the underlying generating model better than another. i was mainly curious if anyone had also observed this and if they\\'ve ever toyed with it, but the sample here was rather small and could definitely be an artifact of that.it would be easy enough to decide that with cross-validation.in regards to bias, have you seen the pycon talk suggesting that bias could be a major issue in data science? i thought it brought up some interesting points, though given project lengths it seems unlikely it could be taken into consideration outside of large projects essential to the company that could afford an extended development period.but i’ve never toyed with random state as a parameter beforeturns movies into van gogh-esque moviesif you want to learn something, this may help you find free material:\\n\\n thanks for that, i’ll check it out (though i did figure out the superset issue, was actually just some browser settings funkiness :stuck_out_tongue: )i forgot to mention, thanks for the great advice!waadja: the remote program will be convening in la in a couple of weeks!has anyone worked with survival prediction models before and could suggest metrics that best evaluate their accuracies?csyhuang: concordance is one metric ppl use. i heard actuary folks also use ks testi am intrigued by this openstreamtap, however. :beer: friendly poke: please try not to leave your drinking/eating vessels lying around the office - try making it a habit to tidy up at the end of every day. haha you just pinged all past and present fellowsi am no longer in the office :slightly_smiling_face:omggggggno, she\\'s right. cleaning up my desk right now.the message still stands. wherever you are, clean that mess up!the hero we didn’t know we neededi mean, i could probably use a little tidiness guidancecleaning up the sv office now!i am mortified, but also, do it y\\'all this made my dayinstructions unclear, just shipped 2 cases of empty la croix cans to the officethat\\'s right fellows. clean up all your shit!thanks  :slightly_smiling_face: i also find concordance coming along with some survival analysis packages. however, i find it hard to interpret (0.5: random, but literature also says survival models are also in a range of 0.6-0.7. if my model has c = 0.63, is that good enough?).\\ni’ll try the ks test and see if the p-value may help :slightly_smiling_face::smiley: good enough always depends on the problem you are trying to solve. for example, there is no value for a “good enough” aucas former om, i approve this message! i just cleaned up my dinner table. our office manager is thanking you for saying this to me for the 1001th timetime to take down the tiki bar.  *sigh**you\\'re not my supervisor *yes, here: \\n\\ntry misspelled variations of a nyc address in the entry field. the adress reported at the top of the results page is taken from data returned from google. it should be automatically correcting the variations. eg try west street 45 nyc, the next page should return the street number at the \\'right\\' order. is this what you need to do?that became my personal motto after you demanded that from all of us the other day. i left a clean desk! i am not sure what happened to it!  that income on the side has got to go! :smile:: stay positive alvingo eat a pie nuggetbotcomposite indexing on mysql table:does anyone know how to set up gunicorn to repond to port 443?do i need to combine gunicorn with nginx?re: gender github articleseems to be a bit unclear… scientifically.a snippet:\\n\\nthis is normally the part at which i would question how a study got through peer review, but luckily this time there is a very simple answer: it didn’t. if you read the study, you may notice the giant red “not peer-reviewed” sign on the top of every page. the paper was uploaded to a pre-peer-review site asking for comments. the authors appear to be undergraduate students.but…\\n\\n“i don’t blame the authors for doing a neat study and uploading it to a website. i do blame the entire world media up to and including the bbc for swallowing it uncritically. note that two of the three news sources above failed to report that it is not peer-reviewed.\"anyone ever done anything with  before?maybe   or  could share some thoughts? has worked with their gym before, maybe he can helpsam altman seems to think it\\'s a big deal, and he seems pretty knowledgeable, hah.this is pretty sweet   check out the video at around 2:20 alsocheck out @insightdatasci\\'s tweet: has anyone played around with fb\\'s prophet forecasting library? i\\'m considering it using in python for time series sales forecasting and i\\'m curious to know how it stacks up against other libraries out there and where it falls short (i guess whatever sklearn and stats_models offer). any feedback would be appreciated! :slightly_smiling_face:  diego: i have used it a little bit, i think it is a great starting point for time series modeling particularly if you are interested in breaking out seasonal trends. you can impact the forecast some but it is fairly black-box as far as i can tell so far.we have used it a little for low-risk stuff. for instance we currently have a big monitor in the office forecasting a key metric and comparing to the quarterly goal. i did essentially no tweaks other than setting up a custom list of holidays. i can’t compare much to other packages because forecasting isn’t a substantial part of my job. but, the one data point is that it does come pretty damn close to a pro forecast using basf (big-ass spreadsheet forecasting) methods.hey diego.  i played around with `prophet` (both the r and python versions) in the first few weeks after it was released.  at that time it wasn\\'t stable enough for my purposes--i wanted to forecast ~30k separate time series, but couldn\\'t find a way to script it without the kernel crashing.  so i never got far enough to characterize prophet\\'s accuracy.  but facebook just published an update that may have fixed the stability.  \\n\\ni\\'ve also been using the `forecast` r package, sometimes in conjunction with `hts`.  `forecast` is great--it\\'s stable, mature, and includes a comprehensive set of forecasting algorithms.  it\\'s more flexible but less automated than `prophet`.  i\\'ve also tried `statsmodels` in python but found `forecast` to be far superior.as i understand it, one of its selling points is that it exposes the knobs in a more user friendly way than a raw arima model such as in r forecast.thanks  ,  , and   ! i\\'m interested in forecasting about 300 different time series so it\\'s good to know that prophet had some trouble with scalability in the past (and good to know that r\\'s forecast package was able to handle it), but that it seemed to do a decent job straight out of the box. i\\'ll give it a shot with my data and if it struggles with it i may end up moving to r forecast . thanks a lot! :smile:my understanding is that under the hood it is running stan, so it is something more akin to bayesian structural time series or gaussian processes rather than arimadoes anyone have experience using airbnb knowledge repo within a docker container? we are exploring this option for a company-wide repo and are unclear right now on:\\n1) if we can push to our git server from within the container and \\n2) if we can make a single repo for all of of our teams (organized in some way …) have you watched the video tutorials on recommendation sys from stanford uni in you tube?  you will find there a very good one on cosine sim but the way to go is matrix factorization i wrote a tutorial about this, if you wanna check it out - though i agree with  that matrix factorization is the way to go:+1: for ’s tutorial, i’ve used it as welldataengconf nyc is coming up october 30 &amp; 31st. come see  speak along with a number of other leaders within the nyc data community! the last day for early bird tickets is this friday and we also have a special promo code for insight alums: insight100 for an additional $100 off the early bird price. see you there! woodoes any one have experience with using pyhive to access hive? hi thanks for response. which video would you pls point to me ? like a link. thanksi’ve used \\nit works through thrift-server.\\npackage actually is not maintained anymore, but for me it’s worked fine.thanks!hi all, does anyone have experience with optimizing the phone pick up rate? try impyla it works with hive alsohi! does anyone have any opinions on  ?i\\'m looking for tools to help scale up a ds orgspecifically streamlining workflows and disseminating knowledgei have a question about python tools for hand labeling high resolution time series data with a few hundred positive events.  i know approximately where each positive event occurs, but i need to know precisely.  my current solution is to use bokeh to pan and zoom around the time series data, then select the example associated with the positive event.  i use a javascript callback to generate a web page that contains the data associated with the positive event, then i save that web page to disk.  the process works, but is fairly cumbersome.  i\\'m not tied to bokeh, but i definitely would prefer a solution that can be implemented in python.  thanks!!: : there is something about the “events” that requires visual validation by a human?: yes, i need to hand label a few hundred events to generate a useful training set.hmmm…and there’s no way to auto detect them via smoothing / deltas? (i don’t know how to semi-automate a viz process, unfortunately.)well… i think i have an algorithm that works, but i need to test it using a set of events that i know to be true, plus some that i know to be false.: have you tried the built-in event picker `mpl_connect`?demo code here: : thanks, this looks like it will be useful!:simple_smile:for all the current fellows out there:\\n&gt;&gt;&gt; it takes a lot of interviews to get used to the process and the format and to understand that the stuff you do in technical interviews isn’t actually the stuff you do at work every day. and it takes people in your social circle all going through the same experience, screwing up interviews here and there, and getting back on the horse to realize that poor performance in one interview isn’t predictive of whether you’ll be a good engineer.\\n &lt;-- a history of the technical interview and how to fix some of what\\'s broken.hello! are there any graph theory experts out there? if so i\\'d love to pick your brain about a problem i am trying to solve.  i made some cards in domo recently, havent tried full fledged dashboards. what are you looking to do.  i\\'ll be at odsc this week as well.: are you presenting for sillicon valley data science?not presenting, but will be attending, we have a number of tutorials and panels that we\\'re presenting atyeah, i got an email from one of your peeps about that...hope to see you there. is julia joining you?anyone else attending the odsc west conference?julija won\\'t be joining unfortunately, she has other plansso i have a friend with a db question — maybe someone could exchange some emails with him?he needs to store daily frequency data for ~100k different time series datasets with 20 years of history. each entry in the time series is not necessarily a scalar, and some might be matrices with potentially varying dimensionscontext is buy-side financebasically he’s looking for someone to suggest what storage technologies they should be considering the pros and cons ofcouchdb perhaps? interesting — what’s been your experience?great for logging, particularly when you don\\'t know the size/shape of you input unot*unitand the whole http read/write methods play nice with web based apps academic papers about marketing data science?hey kevin, i\\'d also love to see those papers! hi all, i\\'m a fellow in the remote program. does anyone have experience implementing content-based or collaborative filtering recommendation systems? it\\'s very new to me, so i have several (probably basic) questions! thanks! i used pdfminer which i think contains pdftotext? this enabled me to extract all the text in bounding boxes, along with location information. this is really cool! i may try it out if i run into trouble with pdfminer. thanks! i\\'m interested in it, too. please let me know if you find something useful. thanks! i have done some basic implementation of it and also used matrix factorisation in python during my academic projects. i can try and share some of the useful materials and code for it if needed. but i would also recommend have a look at factorisation machine which can incorporate various features of movies and users as well - \\n\\nps: i only have some idea about pythonhey. any of you all planning to sell your bike in silicon valley?.i would love to get one. let me know :slightly_smiling_face: i have one to sell, but it\\'s very big (xl)i can\\'t get an estimate from xl actually.. i guess one with 26\" should be good for mei’ve done a bit of it, nothing too sophisticated but i can help if you have any questions,   sorry for hijacking this thread. at customink we are starting to explore airflow as our main etl tool, i’m wondering if i can bother any of you to have an informal discussion on how your framework looks like with airflow? let me know if it’s ok to setup some meeting thanks!hey kenny - you might be better off meeting with an engineer, but i can tell you what i know from a user perspective  - just dm mehi everyone. i\\'m currently an insight health data science fellow and am seeking webscraping advice. my goal is to begin at the following website,  under \"license search\" -&gt; \"search by license number\"\\n\\nthe scrapping involves 3 main steps:\\n(1) search query: input a physician license number from a list (one example is: 41586). a successful match (here it is: akhtar, muhammad saleem) will show up as a list of doctors that match the search query. \\n\\n(2) name selection: if there is more than one doctor matching, i have to deal with choosing the correct one based on the doctor\\'s name. perhaps, i can use a dictionary to match the number to the name of interest?\\n\\n(3) save the data: extract the text at the bottom of the page, underneath the header: \"public record actions.\" \\n\\nif anyone has any advice/suggested tutorials/packages for any step in this scrapping process, it would be greatly appreciated. thanks!i’ve have built some recc systems in python and spark ml..would also be willing to try to answer questions :slightly_smiling_face:johnny-do: for webscraping off of interactive sites like this i have used a combination of selenium and beautifulsoup.  here is a useful tutorial i found a while back :beautifulsoup is a great resource. for the interactive part, rely on selenium via\\n```\\nfrom selenium import webdriver\\nbrowser = webdriver.chrome() #selenium for chrome\\nbrowser.get(url)\\ntime.sleep(5) # let the user actually see something!\\nhtml_source = browser.page_source\\nsoup = beautifulsoup(html_source) #fetch html source code after rendering\\nbrowser.quit()\\n```i’ll ping again about that pip error, which is specifically `pkg_resources.distributionnotfound: the \\'pip==6.1.1\\' distribution was not found and is required by the application`i can’t solve it for the life of me, and downgrading pip is ridiculously roundabout. if anyone can point me in the right direction, let me know...of course the second i post it i figure out it is a problem with the pip that sudo calls (/usr/bin/pip), and not the user pip (/usr/local/bin/pip), but sudo pip is required for any package installations.is there a way to sign out of the general channel? no, you can’t leave . you can mute it however under ‘channel notification preferences’thanksblog post on generating fonts with adversarial networks: hella coolanybody using or have used keras for regression?  i can\\'t seem to reconcile what model.evaluate gives me versus what i believe the objective is...hi guys! i have a flask app deployment question. i have a pickle file that my code needs. right now, i have stored it under a folder \\'data\\' under \\'static\\' in my \\'app\\' folder. i reference this folder in \\'static/data/\\' in my python code. but when i run the run.py (which is above app folder), the pickle file is not found by flask. on the other hand, another python function within the app folder can read this file! is there a reason this is happening?sounds like a permissions issue on the file... you can test that theory by making it readable to everyone with: `chmod a+r pickle_file.pkl` if it works then then it was a permissions issue, otherwise it\\'s something else!i just figured it out. so i need to give the global path -- \\'app/static/data\\' instead of \\'static/data\\' for run.py, although the local path worked for the main python function within app folder...does anyone know about settingwhat is setting?anybody have ds-related reading suggestions?  i\\'m looking for some like pop-culture-ish type fun stuff.  is nate silver\\'s \"the signal and the noise\" any good, for example?@jaime only if you want to up your nate groupie-ness... :wink:  the book is essentially good for journalists to understand some basic statistical concepts... you\\'re way above that...haha fair enough!  i only know about that book because my mom was asking if i\\'d read it.  maybe i\\'m not the target audience...whoa, looks super interesting, thanks greg!i love the signal and the noise - it\\'s full of powerful examples of people fooling themselves about how good their predictive models arei\\'ve read some chapters, like the one about earthquakes or poker, half a dozen times. so entertainingnice! sounds like just what my commute needs!hey i’m planning an offsite for my team in sf and was wondering if anyone had any good suggestions? preferably that doesn’t have a lot of walking or drinking :)the stuff i have done (roughly ranked)\\n1 ) go-kart racing\\n2 ) painting class at a bar\\n3 ) hiking in marin (toward muir beach)\\n4 ) distillery tour\\n5 ) escape room\\n6 ) cooking class\\n7 ) scavenger hunt\\n8 ) baseball game\\n\\nobviously 3 and 6 have lots of walking. almost all but the distillery can easily involve minimal or no alcohol.\\n1, 3, and 4 are outside the city (k1 speed is in south sf, distillery is in alameda)\\n6 is good for small groups. 7 and 8 are good for large groups.\\n\\nmy favorite was go karts, but the less competitive members were not very into it.woodworking was fun\\nthat is terrifying!i will not find it personally fulfilling unless it\\'s blue. i\\'m glad we all agree on that.purple, or i quit.meaning: important to you\\nimpact: matters to the companywe clearly all disagree. can we take this offline and form a team to standardize a pipeline for choosing bike shed colors?that\\'s an excellent idea. we need a streamlined way to make these decisions in the future. but... should the team also have power over other paint choices? or maybe we need a paint committee and form a bikeshed sub committee?hello insight alumni! i am a current insight fellow in seattle on week 2. i am attempting to make a chrome extension that would preserve informative reddit comments and delete the rest. i am having difficulty having my extension run python/communicate with flask as i don\\'t want to open up a new tab but stay on the current page. i am currently using jsonp with ajax to get around cors. my flask is receiving the request, but is unable to send the right information back(error 200). if anybody has any advice i would greatly appreciate it. i only need to send a few lines back. built a pretty awesome chrome extensioncoin toss!information about our stitch fix friends: i have used it beforefor those of you with a ds position opening, a friend of mine is looking. i have been his mentor in the last 2 months. he has been able to do a great deal of work in such a short time on our demand and forecasting problems. he is very comfortable with python, and sql and picks things up very quickly. this was an area that had not been touched by our data science team and helped us with gaining more domain knowledge, as well. our team would love to hire him right after his internship but sadly due to new leadership, we have hiring freeze. i will be happy to provide more details about his technical skills.someone with access needs to program  to do coin flips.\\n\\nhey all, there\\'s a team at netflix looking for people with at least a few years experience (but special cases would likely be made) who are really strong in applied stats (glms, mixed models, hypothesis testing, time series, etc.), is comfortable working with very large data, and who is interested in using data and modeling to improve the netflix product. if you\\'re interested slack me.that\\'s sweet news. i se them a lot so will def buy some of their sweet stock uploaded a file: amazing.i’m so happy they put that in the appstorea friend of mine at wayup is looking for a data scientist with at least a couple years experience. it\\'s a fun company, they connect college students and recent graduates to jobs. they are looking to build more sophisticated recommendation systems and there is lots of nlp work to be done. data is in good shape, one of the founders is an engineer and build the company with engineering in mind (and he is now serious about making an investment in data science). happy to put in a recommendation if anyone is interested. he found it difficult to hire, go figure! :slightly_smiling_face: here is the job description: also, very glad i won\\'t accidentally eat my shoe anymore when i mistake it for a hotdog. it was getting expensive.friederike: +1 to wayup -- i have a friend there as well and he enjoys the culture toodoes anyone have experience working with matrix partners? if so, please dm menew blog post is up.  describes how she built a deep learning model to remove eyeglasses from faces: :rocket: that\\'s a neat project / post !anyone in the nyc office?i’m downstairs and would love a key card to get me inside for mentoring :slightly_smiling_face:welcome new fellows!welcome! have you guy met  yet?woahhi all. i\\'m interested in using past tweets to train my data. i\\'ve found the following api: , which appears to allow access to tweets from 2006 with one-minute granularity. i was wondering if anyone has experience with using past twitter data, and if so, how did you access the data. thanks.this seems to be an useful list of datasets:quora open data set list: hi all, what\\'s the best python library to collect tweets? i\\'ve come across twython and tweepyhere is another bunch of data sets that are available: if you want ds sources, i dropped  a word document with links to datasets into the brainstorming folder at dropbox.  here is the link. please, feel free to add more sources.   hey 2016a fellows! jump into your individual channels (e.g. ds-sv-2016a) to collaborate virtually and in person! there also exist a wide variety of great channels on specifics topics, like python, sql, etc. enjoy!so there was no admonition to avoid twitter data in this session like we got two years ago (because it is overused)? i guess with jack back, twitter is too.does anybody have a link to the results of our insight salary survey?the one that was sent out last year is here: thanks!daaang, we rich!haha yeah we are! _looks at price per square foot of house near work_ oh… :disappointed:hahathis is super useful!working on a v2, let me know if there are requests/requirementsmy goal is more demographics data, more data about job/career transitionshi all, \\nwe are looking for a senior data scientist/engineer with a couple years experience.  \\n\\nwe’ve got a great location right near powell st bart! a quickly growing workforce (currently about 23 in this location), diverse professional backgrounds from health to backend engineering. right now i’m the only data person. for the love of god someone help me! lots of stuff to explore and/or build. you should follow through the contact in the job description to apply but i can try answer any preliminary questions. \\n\\ni’d love an inclusion of manager roles as many have transitioned to that :point_up:historical information would be great (eg salaries for people\\'s previous data science positions in addition to current), especially for those of us in the middle of the country where there\\'s not a ton of data on :slightly_smiling_face:has anyone here been involved in hiring a summer intern that could give some guidance on what a reasonable hourly rate would be?is it reasonable to take the salary of an entry-level data scientist at your company and convert it to an hourly rate?sure, i\\'m not super familiar with data science internships so wasn\\'t sure if that would be wildly off the mark, but even that is a helpful starting pointfreeor 1 linkedin testimonialwhat are people’s thoughts on the following line in someone’s resume:\\n“• hired 4 data scientists (all women), of which 3 promoted in less than 2 years.”arbitraryis their company promotion schedule that you should be promoted every 6 months, in which case the person did poorly, or every 5 years, in which case they did great?does their company have 100 levels, or 2?promoted within 2 years is not really something that you can evaluatedid they need to hire 10 and only get 4?i\\'m happy to pass judgement from afar, but likely just talking to the person can clarify :wink:i’m really just curious on people’s gut reactions to that bullet point. i think there’s a lot that could be unpacked there.also, the person was scheduled for a phone screen with me and they never picked up the phone…so that doesn’t bode well!i’d ask why the 4th one didn’t get promoted...but without any additional context, i have to admit that my gut reaction to that bullet point was neutral to negativeand i agree with all of kevin’s points, which usually never happensor hired below level and then leveled appropriately within 2 years :slightly_smiling_face:in which case nice job keeping those salaries low for the first 2 yearsfiscally responsible +1only promote when threaten to leaveyeah, it\\'s a weird line to write in a resumeinteresting that no one has touched the `all women` line`all women` rubbed me the wrong way, but i’m having trouble figuring out what exactly bugs meit\\'s a suspicious humble bragthere’s some sort of assumption embedded in there that women are harder to hire and/or promotethat’s what jumped out at me tooi think it is actually more challenging to hire diverse teamsdo you think that team is diverse?sourcing diverse talent is one thing many companies do not do a great job athard to saythe `all women` bit was absolutely the first thing jumped out at meas sort of a humble brag that rubbed me the wrong wayi *think* i’d rather them say, if they really feel the need to humble brag, something like “hired a diverse group of 4 ds”, but even that doesn’t feel/read quite rightagree, i don\\'t like the `all women` line, it feels both performative and also hyper-aware of genderi mean, what i’d really like is to see other evidence on their resume that they truly care about diversity and equalityyup. if diversity had been a problem, then they addressed that in hiring/promoting efforts, why not phrase it that way instead of that all women linecan you bring this person to us for a proper, thorough judgment? we have a lot of questions.i’m interested in whether or not this is a woman’s resume or a man’s?it\\'s 10000% a man\\'s resume^^^it\\'s tough because hiring women data scientists is obviously a good thing (tm). but the choice to call it out just feels strange.a bit like a mitt romney quotei think we pay ours something in the realm of $25 an hour“binders full of women”and yes, good call, it’s a man’s resumei\\'m sure the person would be flattered that so many phds are evaluating the resume right nowthat\\'s phd-ist, kevinwell i hired a team of dentists (all dentites) and i fired all of themwithin 3 daysi like that the guy felt compelled to add the 3/4 promotion rate. like, he put himself in his resume-reader\\'s shoes, and imagined them thinking \"hire women!? but i\\'m sure they were terrible candidates!\"anti-dentite…and one of the ds’s on my team (a woman), tongue in cheek, just suggested that people should start adding more gender and ethnicity info of their hires to their resumes…one last bit from this resume that i feel will also inspire many questions, under hackathons they list (in the top position):\\n“bathroom hack (“p value”)  - a/b tested series of urinal feature enhancements (e.g. bullseye 2. footprints on ground) to improve cleanliness of men’s bathroom.technologies: raspberry pi, pir sensors”“hired 4 data scientists (all white males)”sighi mean, i’d want to phone screen that guy just to see if he’s that ridiculous in real life. but based on that resume alone, i almost certainly wouldn’t hire himi was actually kinda looking forward to chatting with him on the phone screeni would pay real american dollars to hear him answer the question, \"so, what was it like working with all those women?\"you should just be like\\nbro:\\nbro!\\ncrushed so much code on that sick p-value hacklove how you did the wordplaypee-valuedamn i just got ithahahahahhaai\\'d rather hear about what a team accomplished than how many of them got promoted. who gets promoted depends a lot on things that are beyond the control of the manager or who the manager hires.and then just see what he doesok enough trolling from mei think we all want to hear an update on how the interview goesaka any delightful quoteswell they didn’t pick up the phone when i called for our schedule phone screen, so not sure they’ll get another chance with us…$50/hour plus housing, but we’re a bigcorp (tm) in silicon valleymaybe he\\'s not used to answering the phone, as a manyeah, denver pay rates are definitely lower than anything in cahe should hire a secretarymaybe you can mansplain over email why you\\'re not moving forward in the processyou mean over man-mail?let me interrupt the fun to say “as a manager i absolutely want to see you built a team on your resume if that’s what i’m hiring for. but that line is super awkward…”\\n\\nlikewise i’d want to get into questions of “how did you support your team? how did you develop their careers…” etc. etc. but on the phone in person.on the plus side, i now know what not to write on my resume :wink:just to add to this: $50 + housing is pretty close to (but slightly lower than) junior data scientist yearly salary -&gt; hourly for us, so that might be a good metric.awesome points of reference, thanks!wow haha, lots of (hilarious) cattiness today :smile:\\n\\nmy general rule is to give huge benefit of the doubt to everyone in this sort of situation.  it\\'s always a mistake to try to intuit what\\'s going on in someone\\'s head from a resume line.   would definitely just ask about anything that seems \"interesting\" assuming core skills are there.\\n\\ndefinitely also would be understanding of a missed call, especially if it comes with a good reason (e.g. not an excuse) as long as it\\'s not a pattern.^totally agree about the missed call. i don’t flat-out reject someone for that, but i do make a note in their hr/candidate profileso you\\'re saying it goes into their permanent record :scream:i once missed a phone screen call while waiting for my phone to ring. it was first thing in the morning, and i had forgotten to take my phone out of do not disturb. :man-facepalming:but the more common reason for missing a phone interview is that stuff always comes up at work. in my post-insight job search, i never understood why so many interviewers would begin by asking, \"is now a good time to talk?\" why would i have scheduled an interview if it were a bad time? but now that i\\'ve conducted a job search while working, i understand that there is no such thing as a good time for a phone interview. there are only degrees of badness, and one cannot know in advance how bad a given time will be. if someone misses a phone interview, it may well mean that they\\'re important at work.i absolutely start every phone interview i give with “is now still a good time to talk?”(after introducing myself…)pro tip: you can also find out if you\\'re accidentally on speakerphone at their current place of work by starting the call with, \"is now still a good time to talk about interviewing for a new job?\" pinned a message to this channel.interesting discussion on where ml engineers fall in the world of data science and engineering\\n\\nthis may be interesting for those still connected with academia:\\n\\nbrains, minds and machines summer course 2018\\na special topics course at mbl woods hole, ma\\ndirectors: gabriel kreiman, children’s hospital, harvard medical school; and tomaso poggio, massachusetts institute of technology\\ncourse dates: august 9 - august 30, 2018\\napplication deadline: april 9, 2018\\nmore information: \\n\\n…this course aims to cross-educate computer engineers and neuroscientists; it is appropriate for graduate students, postdocs, and faculty in computer science and/or neuroscience …hey all — i’m looking for a good applied ml interview question. something that gets at problem solving ability and ml understanding, preferably with a way to quantify the responses, though that’s obviously tough. anyone got a good go to?not quite an answer to the specific question, but i sometimes point people to this article when asked about advice around finding good ml/ds candidates. thanks  — i love that article but had forgotten about it. that will definitely help!hi everyone! we suspect our data science team is understaffed. we wanted to get a sense of how many data scientists other comparable companies employ. if you have 30 seconds, would you mind filling out the following survey? happy to share aggregated results! :pray: \\nyou guys should hire some idx fellows :wink:no not yet! i’m still poking around, never really got a response on here. i looked at that answer, but didn’t try it - did you?would love to find a solutionhelp , go here to see problemi\\'ve got a friend from canada looking for bay area based internship/coop positions for data analysis/science projects. this would be for 3 months. dm me if you know of anything!hi friends, i am working on an open source project with the world food program that attempts to apply kalman filters to get better estimates for noisy survey data (checkout the  channel for details).  i\\'ve got a complete and presumably correct implementation now but i\\'m looking for someone who\\'s got some applied knowledge in fitting kalman filters to real data, be it in any discipline.  anyone know anyone who might be willing to talk with me about that?\\nhello! has anyone worked with phone records? i have data with calling number, called number and dialed number (sometimes the last two aren\\'t the same) and am not sure the best way to tackle figuring out if certain people are more commonly dialing others. thanks!: isn’t this simply finding the number of occurrences using the pair of numbers within the dataset?maybe so, that\\'s what i\\'m currently doing.  just didn\\'t know if there were other ways of looking at it :)you could do a network-based approach with in and out going calls being your edges.i was thinking something like that too! i\\'ve never done network analysis. is there a python library your recommend? networkx is pretty easy to use: thanks!!!looking for someone whos good with apache storm + java?! :sleepy:possibly  has an idea?hey , what’s your problem?: a friend of mine who works at our company started this with his boyfriend. you can make donations and t shirt purchases all proceeds going to equality florida actionhi guys, i\\'m a ds fellow in the current ny session, having a strange problem making my web app visible on aws. the app uses a number of python packages, which i have installed via miniconda. the app runs locally on aws, but when i try to run it via supervisor on port 80, it does not find the miniconda python installation. i have tried to specify the miniconda path using\\n \\n\\'environment=pythonpath=\\'/home/ubuntu/miniconda2/bin\\' \\n\\nin the supervisor conf, but it does not find it. have any of you faced this issue before and, if so, what was the solution? i know that many past fellows have put their apps in the public domain using aws, so it must work. \\n\\nthanks a lot for any suggestion!stay positive lkashifshot in the dark: perhaps you need `path=` instead of `pythonpath=`? `path` is what linux uses to find programs (like python), `pythonpath` is what python uses to find libraries.tried both path and pythonpath, with same result. but thankshave you tried specifying that specific .conf using the -c option to make sure it\\'s not pulling it from some other location. see here: day in a life of a data scientist: does anybody have any good references on applying ml for supply chain mgmt and inventory forecasting, for parts in a factory/manufacturing context?i imagine the folks at stitch fix doin case you want to look very productive, go to this website are start typing\\nhit `alt` three times when your boss walks byinsight mentioned in the nyt: big mention   and  big mention yes! i just didn\\'t read the whole thing before posting :wink:hey, does anybody know if there is a good package for fixed effect regression models in python 2.7? i’m trying to run a logistic regression model with fixed effects… thanksyou can just add dummy variables for the dimension you’re sweeping outor manually demean (i’ve found doing fe to be a pain in python)i’m totally new to fe models. can you point me to an example?this is pretty good: thank you! i’ll read it and get back to you if needed :wink:hey insight community! i’m staffing up a new project i’m leading at my company (vevo), and i️ am in talks with my senior leadership about bringing on some contractors to help with some data science workloads in the short term (starting asap), with a minimum ~2 month engagement. if you are interested, or if you know of good data science freelancers, please send a pm!thanks to  &amp;  for speaking to steve lohr at the nyt and agreeing to be featured in today\\'s article  i found this guy’s blog to be quite helpful. he doesn’t always focus on supply chain data, but he does have a lot of material for methods/metrics that are applicable to supply chain/inventory forecasting. thank you  , i will take a look!sure thing, good luck! :slightly_smiling_face:just saw this on the way home... way to go carlos, anasuya, and insight!wow, what a coup,  - close to half of the article is an ode to insight, huge pr win!god that looks boringdoes anyone have any examples of how to do time series histograms w/ time interval buckets?  thank you! :bow:if you’re using pandas you might be able to use resample:  are you using r? python/pandas? what frequency of series are you looking at (subsecond? daily?) more like in sql through self joins and stuff, thanks, but wont be using pandas or rif you have postgres or something similar use window functionsit will save you a lot of headache ok, will take a look at window fnthis is interesting/important. white house report on big data and civil rights.  my experience with similar data has been that synonyms, abbreviations, and shorthands (i’m picturing this something like the feed on a credit card statement, which is half-gibberish) shoots a bow approach in the footworks for things like “car” but imagine how many providers/terms there are for say, cell phone plansit depends on how standardized the text is. freeform text is rough for sure, but i\\'ve had success using it for categorizing the notes of sales people in the past.it just took a day of figuring out the most common abbreviations and cleaning them up  thanks, both of you, i should be able to get it to work with your suggestions!hey all, my team at sovrn in boulder, co, is hiring for a data scientist in the r&amp;d group. we’re looking for someone with a strong math background and good coding chops (python and/or java) to work on some really cool optimization and ml problems. if that sounds interesting to you, please pm me. also, i don’t know who the remote program directors are, but i’d be happy to chat with you and see if any of your folks are interested.lisadh:   data engineers - let me know if you\\'d be interested in a job at tachyus i’ve not seen a giant difference in the bs’s who are hired vs phds who are hired. from my perspective, the bs folks seem to be more energetic and adaptive while the phd folks are more careful and ‘big picture’ oriented. i think there’s room for both, but maybe on a more depressing note for phds, i don’t think the degree is magic sauce, at all, and i think that a talented bs can contribute just as much as a phd and can close any significant gaps specific to ds in a year or so. to me, this is because the valuable parts of the phd that overlap with data science are generally just sort of the ‘tool-belt’ that lets you do good science - if the phd was all about getting that tool belt, they would be significantly shorter.as i contemplate these days (maybe too much) the value of my degree, i have to admit that the phd challenged me to think more deeply, creatively, and thoroughly about a subject matter than i ever would have done had i not gone to grad school. as far as a hard skillset is concerned, i don\\'t think the advantage of the \"tool belt\" that phds possess is very significant, and even the size of the tool belt would depend heavily on the field of training (e.g. biostatistics vs mechanical engineering). the question is, once hired into an industry role, is the trained ability to think scientifically valued, or will the value always be confined to the realm of an academic setting? among all the ds roles that are out there, what fraction of jobs would actually benefit from the experience that a phd scientist brings?does machine learning = deep learning now? almost all chatter and job posts on linkedin would indicate so. did i miss the announcement that non-nn techniques are now only called statistical modeling or advanced analytics?if true, that\\'s kind of sad. i may be mistaken, but isn\\'t deep learning really still statistical learning on a fundamental level, as are all ml techniques? taking a bunch of data and identifying patterns and trends that are statistically generalizable. i think if people consider ml to only be dl, it exposes their lack of understanding of what ml/dl really is.i agree, and one source of the lack of understanding might be the hr/practitioner divide. there’s so much hype about ml in general that it’s difficult to sift though to understand real trends, but i wonder if the industry (or at least hr) is settling on titles like ml engineer = dl pipeline engineer much like data scientist became a somewhat normalized title a couple years ago. also ai = deep learning from everything i seem to read…i suspect a lot of it is marketing . would you apply to a position that says “you’ll spend 80% of your time doing logistic regression and 20% of the time cleaning data that is probably unsuitable to use in your logistic regression but you have to anyway”? :smile:but if you’ll be doing nn you’ll be all over that :smile:i\\'m intrigued. tell me more about this 20%-time cleaning data unsuitable for logistic regression job... :wink:haha i’ve seen some interesting things.  none of the features are particularly predictive (because of lack of investment in collecting the right stuff), they’re all super sparse, the classes are super imbalanced, and, of course, there’s not enough of it for undersampling.\\n\\ni’m going to need that model performing well by mid week please :smile:somewhere i have an image of what’s been called the saddest looking roc curve by some of my ds colleagues.hi all, if anybody wants to be a project reviewer for udacity’s machine learning nanodegree let me know. it’s a part time thing that you can do whenever you want and there are no obligations.there isn’t much public information on it but there is this: if this data science thing doesn\\'t work out... i can always review data science students!good post:it\\'s la crow-ix.  like unix hey everyone! every program at insight (data engineering, health data and data science) is gearing up for sessions this fall. so if you have leads on teams that are growing please feel free to reach out to your former program coordinators, program directors, or you know, email jake! :wink: we’d love to hear from you and work with any teams you think are doing exciting, cutting-edge work. don’t forget we have programs in boston, new york and silicon valley. plus our latest addition: health data science in san francisco! :unicorn_face: :bar_chart: :100: :wow:a good read on hiring data scientists ... can anyone recommend a good print publication to stay on top of tech news? i\\'m rather old fashioned and like getting newspapers/magazines in the mail instead of obsessively checking websites. inc. is good if your focus is tech+startup, and startup. pertinent to silicon valley. if you would like a comprehensive coverage, then i personally recommend, bloomberg businessweek + wired (lots of ads but still good) + inc + the economist ( although there can be nothing about tech in politically hot seasons)cool thanks!mit tech review is good, i also like physical media and subscribe to it :smiley:twitterthere are a bunch of individuals if you follow, who will curate the news for youand it can go very specific - crypto, vc, startup, etc...problem is, it takes effort to cultivate ittoo right mate :slightly_smiling_face:hey all, many of you may know me as helping to lead insight’s new york data engineering program. we have an application deadline coming up for the summer session and we would like to increase the number of applications and fellows in the program. considering that alums are our best source of referral, if you could send our (or my) way anyone who may be interested and encourage them to apply at  , i’d really appreciate it. the deadline for de applications is march 29 (new york and silicon valley), and the session would start may 30. questions about the program can be sent to  or me at  many thanks!!friend of mine actively interviewing for ds jobs is looking for a good resource to learn a/b testing. suggestions?i found combination of udacity a/b testing and inferential statistics course very useful for my interviewsit’s not exactly introductory, but i think slater stich’s set of guides () is very good.\\n\\nalso, this article (“how optimizely (almost) got me fired”) talks about some of the practical pitfalls of a/b testing: big fan of this postevan miller has several good articles on a/b testing. hey all, hope your friday’s going well!\\n\\nquick question: has anyone incorporated test suites when building ml pipelines? would love any recommendations on conventions / frameworks / gotchas if people have any.i second evan miller’s site and slater’s blog post. between the two of them, you get a pretty comprehensive overview of common methods and tradeoffs/pitfalls  i still think the chainer version is cleaner than tensorflow, but cool either way :slightly_smiling_face:does anyone use databricks? did you have trouble importing keras?thanks davetensorflow (&amp; scikit-learn) workshop in mountain view this saturday:\\nyeah, the default binding are pretty terriblei’ve found it to be fairly fast and scalable. you can also fit multiple time series in parallel using something like parallel in joblib. pinned ’s pdf  to this channel.for fans of the new yorker cartoons, this will be entertaining:\\n uploaded a file:     how do you think... looking to connect with folks who are running ds teams of 10 or more, insight alum or otherwise - please pm or put suggestions in a thread, thanks!if anyone in nyc is interested in attending the upcoming dataengconf on 10/30, the founder has provided all insight fellows 25% off with the coupon code: ins25x (code expires this friday, 10/20)\\n\\n\\n\\nthe speakers will include many of our mentors and advisors (e.g. chris wiggins, claudia perlich), as well as ovo genevieve smithi\\'ll be there!: does the elasticsearch (and/or spark-xml parser) lib include s3 libs? if this is a jar, one can see the content (it can be a “fat” jar) by “jar tf the_jar_file.jar”\\nif, for example, there are 2 of the same library being included and they have different versions, you will get an error. if this is not the issue, post the error/exception and i’ll think about it a bit more...thanks  ! i think i figured it out finally. pegasus added the two aws jars to the spark-defaults.conf file under the driver-classpath — you need to do that to access the buckets. but in order to use his elasticsearch-hadoop connector, he specified that jar\\'s classpath on the pyspark command line. looks like that classpath was wiping out the classpath specified in the conf file so the solution was to specify the aws jars on the pyspark command line (or conversely, specify the elasticsearch-hadoop connector in the spark conf file) but not have two classpaths specified in two different places. thanks for taking interest!!glad it worked out! yes, spark’s config file is simply the default settings, when specifying new value with the spark commands (spark-submit, pyspark, etc.) they take priority and will override the defaults suggested in the config.for those who use h2o: what do you use it for? i\\'ve been asked to determine whether we should use it internally, and i\\'m having a hard time cutting through the hype to what it is actually good at.i drink it, usually. often i use it to bathe myself.: ask  he works there, started last december.  : h2o is a commodity software that you can unpack and start using as a ml platform. if you are advanced you can use the libraries and build it from, r, python and java. or if you are starting off, just download it, just the jar, and connect to your localhost:54321. and see yourself as to how easy it is to upload, and model data.nuggetbot check on jonneffjonneff has a happiness average of 1.1666666666666667i love life!!! everything is awesome when you\\'re part of a team.  happy happy happy.  nuggetbot check on jonneffjonneff has a happiness average of 1.3255813953488371nuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 0.9703947368421053that’s so awesome and wonderful! you are the best you amazing great awesome bot!nuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 1.039344262295082that’s a bunch of bullshit! i hate you nuggetbot. why do you destroy all that i love?stay positive jeremykarnowskinuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 1.0130718954248366nuggetbot check on kumbhanikumbhani has a happiness average of 0.696969696969697nice!a friend of mine without a traditional data science background (only bachelor\\'s in electrical engineering - currently working in a power utility company) wants to make the \\'jump\\' to ds. he\\'s learning python (numpy and scikit-learn) on his own time and just learning about the field as a whole. any advice or suggestions on what else to do to best position himself to make a career transition?  would those online data science courses be helpful to do (here\\'s a random one i found: ).100% of interviewers i\\'ve talked to have asked about what i\\'m currently working on, and/or my last project, so the advice i usually give people is projects, projects, projects. find some data. do a project. there are a couple of nice datasets/project guides on kaggle and elsewhere (e.g. ml blogs).hello .. i am just curious on what visualization tools are being used for json feed from api\\'s? tableau does not support live feed and i am still able to make the web data connector work. we can make custom visualizations from d3 but securing that needs effort (for enterprise). any obvious solution that i am missing?those of you on twitter have probably already seen this...\\nreally good readdimensions of  flavor   : we\\'re looking for a data engineer! please email me if you or someone you know is interested. we are thinking of developing a few different dashboards at work and i was wondering if anyone had suggestions for (non-tableau) tools.paid and easy to use or free and a bit harder to use?relatively cheap - doesn’t have to be free, but tableau seems very pricy.looker for just about anything, kibana(if you use elastic search), powerbi from msftor shiny if you use r and want to set it up yourselfthere\\'s a ton moreit\\'s a crowded field chartio, jaspersoft, bime, zoho, sisense, grow,  ducksboard, looker are all potential options from a survey i did about 6 months ago. i would do my due diligence in evaluating each option. also, r launched a dashboard-specific library this summer.tableau and chartio, the leaders in the commercial space, are very pricey.i’m a python person and i’m hoping to move my boss away from r.does anyone have experience/good things to say about quicksight?aww... nothing wrong with r. use both.: nothing wrong with r per se, but it would be easier if we all spoke the same language;) i tried to participate in a quicksight trial, but the permissions required were basically a showstopper for our security requirementsthanks, . good to know.we handle bank data though, so we\\'re extremely paranoid on the security  also starting to look at dashboarding, and we’re subject to bank data requirements too ;/anyone looked at anaconda mosaic, the dashboard component of their cluster management? the options you have will depend a lot on the end-users desire. will business folk be happy with static charts on a dashboard, or will they want to be able to dive into the data a bit more and filter/pivot/join to other data sources? for the former, you can get away with lots of free python options (i.e. jupyter notebooks, caravel) and not-too expensive managed options (i’ve found mode analytics quite good since they’ve added the option to work with ipython notebooks)if your end users want to play with the data themselves more and not write sql, i second looker as a great solution we want the business people to have some flexibility, so looker “looks” great! (sorry, couldn’t help myself)we are implementing looker here too. getting away from ad hoc requests is one of the big reasons.ad-hocs…blehyeah, the pm team asked us yesterday if there was a way to not bother us for everything.we’re also using looker, for those same reasons. it takes some setting up, but provides lots of flexibility for end-users.you may check the airbnb caravel. python/flask stack. open source, no setup for client. definitely curious about folks’ ramp up time with looker setup. it took us longer than i expected at my last gig, even with an analyst who was super familiar with our data model and good with sql working on writing the lookerml full time *and* a data scientist who had experience getting his previous company moved to it helping out (we found the engineering help they provided to get setup pretty useless unless your data model is quite simple)but business end-users absolutely love it and yeah…fewer ad-hocsat my last company, we used looker and it was fine. but people most likely need to know at least some sql. at my current company, we use domo which to me is more intuitive.tableau is the bane of my existence. has anyone successfully transitioned off it?  we’ve had a data engineer working near full time on it for 2 months building out looker dashboards for clients. def need to have solid sql background to build anything useful.hey everyone, we just launched a new insight ai fellowship, open for people who want to move from machine learning to more advanced applied ai work in industry: is it open to robots as well, or just people?you have to pass the turing test to get in actually i am trying to launch my web app on the ec2 instance t2.micro - my python script gets \"killed\" - out of memory. i used 5 gigs out of 8 gigs available (71%). i increased the storage volume to 12 gigs (47% used), the program is still killed. i am now looking at changing ec2 instance type but i have no  idea which one is the most suited? aws expertise is welcome.stay positive fplisson  would love to hear more thoughts on domo — what are the use cases? i think you are confusing memory and hard drive space. a micro instance only has 1 gig of memory, but 8 gigs of hard drive spacestay positive ryan.vilimyour process is getting killed because it’s running out of _ram_stay positive ryan.vilimrunning top will list all your processes and show you how much memory is being used in real timeif you are running out of memory, either change your code, or just upgrade to a bigger instance, it’s cheapyou could also add a few gigs by making a swapfile, but i recommend against that, it’ll be super slow and you don’t even have much hdd anyways. i upgraded my ec2 instance to m4.large (more memory). my program is running fine. i read about swapfile but i considered it as last resort. problem solved. thanks.i highly recommend against it.(a swap file that is)no problemstay positive ryan.vilimnuggetbot check on ryan.vilimryan.vilim has a happiness average of 1 given how a ml model i’m working on is currently performing, i think a happiness of “1” is too high by several millionlol we basically used it for all teams like finance, cs, growth, digital... data team manage it of course. similar to looker, you can make cards, and dashboards. we pull the data from redshift. you can also use d3 for visualization. does this answer you question? (curious as well since i might be deciding a bi tool to go with in the near future) what were the main differences with looker both from the model management (i.e. your) and the business user perspectives?we use looker and i think it’s a steaming pile of garbagethat\\'s slightly hyperbolic, but i’m not a fan1) it’s flexible enough to write queries to do a lot of different things, but not smart enough to do it efficiently. looker support told my boss that they haven’t bothered thinking about performance because most of their customers are on redshift or something comparably fast. we’re on mariadb and we’ve had to implement some incredibly janky hacks to work around looker’s limitations.2) too much config in ui. lookml (based on yaml) is a nice idea, but they’ve only half-heartedly embraced it. you can define a dashboard in lookml, or you can define a dashboard in the gui. you can even convert a lookml dashboard to a gui dashboard. but you can’t modify a gui dashboard by editing lookml. so  if you ever have to do a schema migration…have fun!does anyone have any experience using vowpal wabbit?data pipeline is equivalent to airflowbut airflow is easier to use and test, and cheaperglue i’m guessing is slightly similar to schema registry uploaded a file:  and commented: an early stage, very successful startup is hiring their first data scientist in nyc! this is a phenomenal opportunity to be able to define your own role under fantastic management with a pending patent defending their business model. please take a look at the attached job description and reply to the email at the bottom if you are interested!pretty cool way to think about it, especially\\n&gt;that’s not because optics is easy. it’s because the mental models in optics are organized well.+1 to plotly dashthe insight alumni group on linkedin could use some more members (kudos  for setting it up). just sayin’hey guys does anyone have experience deploying airflow on a heroku instance with authentication? i have been running into an `bcrypt error` :disappointed:here is my stackoverflow question if anyone has any idea passing along something that might be of interest to alumni: wish they had a list of the questions =\\\\so cold i use bokeh on a digital ocean server, works fine, never tried it on ec2 did you have to do anything special compared to running locally? i’m getting some errors when i try to run it on the server. the bokeh user guide describes setting up a reverse proxy server using nginx, but i am a bit clueless on how that should work or if it’s even necessary stay positive along528 give me a break if you simply want a small number of users or just yourself to be able to connect and use the server i would suggest the ssh tunnel instead. the nginx reverse proxy config would be recommended if you want to serve a general user on the internet. the bokeh server is run exactly the same way as locally afair, nothing special needs to be done on the serverwe ( and ) are throwing a data science meetup at svds (mountain view). come listen to two short (30-45 minute) talks and socialize at the silicon valley data science office, just off castro st., mountain view. we will have pizza and drinks and we’re hosting insight alumnus andrew zaldivar (google) and allison gilmore (one medical). the evening kicks off next week at 6pm on wednesday, dec. 7th and will wrap up by 8:30pm. \\n\\n\\nsee the eventbrite page () for more details! invite anyone you think would enjoy the event! please rsvp on eventbrite if you are planning on attending (so we can order enough pizza).if anybody is interested in post-meetup drinks on castro st., let us know. it’s a 5 minute walk from our office and the mountain view caltrain station (here’s a link to the caltrain timetable for the lazy: ). hope to see you all there!nice article about building usable research code in a team for fellows currently interviewing: i found this list of questions to ask your interviewer be quite helpful. you can find a lot of similar lists on the internet, but this one was particularly thoughtful. uploaded a file:  and commented: insight at night reunites!have asked this before but... anyone here with experience doing image similarity analysis? currently using color-coherence-vectors as feature vectors (they do ok) but wondering if anyone aware of better descriptors? (have tried local-binary-patterns also). unsupervised problem on this side, thus, not sure how to even go at it from the deep learning perspective (new to dl). gist descriptors do well on landscape similarity thanks ! unfortunately, feels not quite suited for in my case.  trying to rank images of objects from the same class with much finer differences. fourier-based descriptor tend to work best when much info stored in the low frequency spectrum (not my case). additionally fourier descp are not invariant to rotations/translations which matters in my case. its a struggle... :face_with_head_bandage:maybe should add that have also considered the heavyweights sift, orb, et al but not much further. issues with speed and feature extraction  using these bad boys.stay positive ccci have a flask/html question! i want to include a user input in the form of a \\'sliding bar\\'. depending on where the user clicks on this bar, i would record a value between 0 and 1. i don\\'t even know what something like this is called in html/javascript. any help ?: try jqueryui slider widget:  (feel free to ask if you have questions about how to use it): i’m not sure what kind of resources you have to scale, but if you have access to cuda-capable gpus then you could use a pre-trained neural net. there are a number of nns pre-trained on images that act as excellent feature extractors. you’d want to do something like take the output of a penultimate neuron layer for the two images you want to compare and then compute mse or cosine distance. how fast are you looking to do the comparison?, great advice! i was able to find some papers pointing in the direction you mention. it does sound very promising although i would have to ramp up my game to go down such a path. currently i have no idea what a config file looks like for those systems, never mind a clue about computational load.  i have gone as far as compiling caffe  on my ubuntu pc (cpu only). do you have any recommendations as to where to look to find info on alternative tools/fast tutorials to quickly understand the technical challenges?  my image data set won\\'t be the bottle neck (i think). they are small pics (size and resolution) and not many (~5000).if anybody wants twitter data .. there is currently 80gb compressed .. uncompressed to ~360gb.. and more coming… it’s in s3… msg me if you need it in your projectthats a lotta tweets: forgot to say that i aim at comparing an incoming reference image with the whole data set. look  to pre-compute the full feature matrix for the full data set and use knn with a given metric to find nearest neighbors avoiding the brute force search (as i do now with my current feature vec).  but will have to run the nn on the incoming vector request in real time.:  did you see lory\\'s post?: i like using chainer because it is pretty readable and fully in python (as opposed to caffe which merely has a python wrapper).  they also have caffe model support so you can simply import the model file for any of the models in the caffe model zoo and then do forward passes to calculate any layer output. : caffe is also not super hard to use with pre-trained models on its own and if you have it compiled that’s half the battle :simple_smile: . i don’t know what “real-time” means in terms of throughput, but for some perspective some of the models i have been building will run through 5000 96x96 color images in seconds on a gpu.: +1 for chainer. i am using it for my project too.: great. i\\'ll dive into the details then. your stats reinforce that idea that running my model should not be a problem even for a cpu (~20x slower order of mag vs gpu?) as i will only be doing 5000 360x360 one time and then is just 1 360x360 per comparison request. only concern left is my images are very specialized (imagine all shoes for example). think a pre-trained network on \"all sorts of pics\" would have learned enough to resolve features that only apply to shoes?: hi adam, i looked at the link and there are some css and js scripts i need to link to to use the slider. but it\\'s not clear where i can access them from. the downloads section has a few but not all, for example the style file in resources folder. i\\'m sure there is some straightforward way to do this that i\\'m missing. if you are using the existing model and not training i think cpu is fine. there is an option to fine-tune the existing nn .thanks ! next step is playing with model to develop intuition for the approach\\'s resolving power within the specific class of objects i\\'m comparing.anyone here has experience saving greyscale image stored in numpy array to  png files with transparency?: hi lory! i may have use for that tweet data…is it historical data?: you actually don’t need to download any of the css/js to use the jquery ui widgets. you can use the  links for the ones that are there. the /resources/demos/style.css is just an example or something; it refers to this file here which is practically empty: yah… not clean tho.. for cleaned.. next week.. only english tweets, user, tweet, date: now its zipped with json in each line: there’s apparently also a bootstrap slider, too, but it seems more complicated to me: : is there a way to get location information? i’m using tweets restricted to us locations, but i’ve been collecting via the stream using bounding box constraints: yes, can filter by locationjust starting with hbase.  is it correct to state that it has no native support for queries?: take a look at apache phoenixthanks  – that’s what it seemed the solution was going to have to be.another big-table style solution would be cassandra with cqli\\'ve updated the repo for jupyter notebook tips and tricks available here:  the readme has install instructions that might help if you run into trouble.nice! i was scraping reddit-wide for my project so it was way easier to have the set then filter it down... still slow thoughgood luck!hey does anyone have access/could connect me to data relating geography related disease emergence? (i.e., lots of patients coming in with the flu in chicago or lots of patients coming in with asthma in sf)that\\'s for asthma in 2012. and i know there\\'s also for other years, 2009, and 2012-2015it\\'s what i used for my insight projectother states also have itlike ct and ny you can check  for this project on pediatric asthma ed visits in california by counties for several years.\\nthe asthma rates were scrapped from the california department of public health and califonia breathingand this: lots of options, but you\\'ll have to circumvent the js query page.no on cares about your insight project \\n\\n*runs away*nobody likes me :sad_parrot:bos-2017b does!the community health status indicators dataset has some county-by-county data for some diseases (as well as a bunch of things that are not diseases): . in case it\\'s useful to you i have a couple of tools for working with it in this repo: question on confidence intervals in predictions -- anyone have a suggestion for how to create a confidence interval around a scikit-learn random forest prediction?there was a good post on this topic a few years agolooks like there\\'s also a package implementing (or something similar to) the above idea too. i have not used it, but looks easy enough to plug into a pipeline:  -- that\\'s basically exactly the intuition we approached it with, but it\\'s a bit expensive computationally for a large # of estimators + datasetestimating the variance of the predictions via jackknifing would be my first idea. here\\'s an implementation of such a method: so the idea is jackknife is less expensive than bootstrap?ok i rtfm.  hastie and efron’s method is jackknife after bootstrap and only needs bootstrap replicates used in bagging.  so yes less expensive. thanks! those tools look super useful.from what i understand it\\'s a variation of a \\'leave n out\\' method, but instead of removing them, they are down-weighted by an infinitesimal amount.hello insight friends! can anyone point me toward fellows who have worked with instagram data for their projects? i’ve got a project idea i really like, and i want to get to acquiring data as soon as possible. any help or advice would be appreciatedyou can also look into mechanicalsoup: cool, feel free to get in touch if you have questions about how to use it, though it\\'s been a while since i looked at it so i may not be any use (especially with the d3 map...i have to learn d3 from scratch every time i use it) albert lee from my session didthanks!i\\'ve done some stuff with instagram data in the past, but to my knowledge they no longer support their public python apiagreed! the only way i would use selenium here is if the urls didn\\'t follow an obvious pattern, e.g. they were generated by some horrendously unreadable minified javascriptyeah, i saw that it’s no longer updated. i was hoping to talk with someone who has used it to see if it would be a good way to go for my projectwhat\\'s the benefits of doing that rather than just leaving it out? from a computational perspective, infinitesimally small is just 0?besides giving more stable estimates of the predictions (less variation), it also accounts for bias is the jackknife estimates.hm, it\\'s not immediately obvious to me why that would be. yes larger sample size would strictly decrease calculated sample variance of a single jackknife iteration since you\\'re just increasing the denominator but not the numerator. but i\\'m not seeing why the actual variance of the jackknife estimate (across all iterations) would be smaller since the value of each estimate remains the same. i generated my own data: for a regular jackknife, the bias has a factor of n-1/n. if instead of leaving 1 out, you leave an infinitesimal part delta, the factor would reduce to n-delta/n = 1.\\nsomething along these lines.thanks ! i’ll look through your project and might hit you up if i have any questionsi’m giving a talk at pag in san diego on ds careers in industry focused on animal genomics on sunday. if you have any work at your companies that you’d like to advertise to academics that is related to genomics in plant and animal (non-human) genomes, please let me know! happy to include it.my experience is that these infinite scrolling pages make ajax requests that can be easily modified to get *all* the posts/images/columns in a single shot so you don’t even have to scrape the pages most of the time my point was that for a computer, infinitesimal delta is equivalent to 0. but yeah on a purely theoretical level, sure. or if you set your delta to something like 1e-5 i guess.does anyone have good tips or resources for mentoring new data scientists in their first role out of academia?mentoring is a very personalized thing :slightly_smiling_face:  would depend on what areas you’ve identified that you think they should improvei guess tip #1, identify ways in which they can growtip #0 : identify and help translate/build on strengthstip #1: build mvps with caveats to deliver value early rather than a better solution that takes more timethen build more mvpsand more mvps :wink: thoroughly go through the processes each company is using to achieve team coordination. but that would necessitate the team has such processes ... :stuck_out_tongue_winking_eye:haha, yes; our processes are mostly developed, but always being refined. i think the biggest difficulty i’ve seen is in getting into a business-focused mindset where there’s a balance between speed and perfection, so i’m trying to figure out how to support new people in that transition. thanks for the suggestions!i found that taking some of the pressure off of them works.  in teh past i’ve just said “i just need this model to do something by x date, give me the best thing you can, but let me worry about the outcome if it doesn’t perform well”  (also, you have to mean this for it to work, the model might not do well :stuck_out_tongue: )make the on-boarding period a sufficiently \"rich\" experience to assist in the change of mentality during this low risk period....?lol my suggestion is the *exact* opposite of  . i guess it depends on the person who is the new hire :slightly_smiling_face:time to save the internet! hey guys, does anyone have any experience porting a sas logistic regression model to sci-kit learn?if i have a s3 dir full of csv files, how long does a emr cluster take to convert them to parquet.  and secondly, what is the best way to convert csvs to parquet (besides emr)?  i’ve started looking into using aws athena / spectrum, but query speed is super slow if data is not in parquet formatideally, converting mysql extractions to parquet files periodically is great, if you know of any such tool, pls let me know! :bow: you should be able to write a pretty simple spark application that uses the csvreader into a spark dataframe and save to another bucket as parquet. i assume you can just submit this job to emr and it’d take care of it. more hosts the faster it is, unless each of your csv files are like 1kb in super small. one catch is to read all csv files at once otherwise there could be schema inconsistencies in parquet from file to file or folder to folder.any good resources on price testing methodologies for susbcription services?any one else losing their mind over gdpr out there? join morning everyone, does anybody by chance have the flask mini tutorial that insight provides to new fellows? this link may work: thanks mike! access requestedfrom ruiz.eulerthat request was probably routed to the nyc office. let me know if you don\\'t get access, and i can download and send you the files.thanks mike, looks like you\\'re right, i would appreciate if you can send them here has started doing a tutorial for boston. he can probably share something with you.there\\'s also this that we give to the de fellows: : if anyone\\'s looking for housing, the other unit in our duplex is opening up:\\n2br, $3800 in the mission (22nd &amp; folsom).\\ntwo insight alums live upstairs.: \\ni’m the first and still-only data scientist here at greenhouse. we are starting to think about the “career ladder” for me and other data scientists, analysts and engineers. i’d be very interested in seeing the structures and skills that you all have defined at your companies. dm me if you’d be willing to share your insights!: i have permission to share our “data science levels” deck at glassdoor. it’s kind of rough but just a sec...our director (actually a vp now) was planning on a blog post about it sometime anyway. i’d be interested in seeing that as well!that\\'s really great  -- thanks for sharing!nothing really about data engineering in that. that is a different org, but data science &amp; analytics has 1-2 product managers that are product owners for their scrum.at smaller scale we were all under one director.how did the transition between the single director and multiple product owners go?  (i’m in a similar boat to , first ds here and we’re growing our team)when i started there was five ds/analysts and a few engineers, maybe 4-5 who did a mixture of bi etl and sexier big data stuff all under one director who reported to the ceo.\\nthen i believe around the same time we brought in a director of data engineering and a second analytics director who has taken part of our team under sales and marketing. my supervisor is the vp (original director) who reports to the cto.i’d say the two areas in which things are a little more stressful big-company-ish is that 1) the core data scientists are now split into two big teams and sometimes we wish were were more intimately connected. we at least have an all data science meeting every two weeks. on the other hand our director was way over loaded before the split so in net it is good.and 2) now i have to go through a product manager to get some data needs implemented. which is sometimes feels slow.the nice thing though about having a dedicated data product manager is that there is less explaining of your needs to engineers and you get more solid delivery commitments. i had a few cases in the old days when i had something 90% done and then it felt like i spent 2x the time just explaining everything to someone who wasn’t really adding much to the project.haha i know the feeling on that last bit...well really just one guy and he’s leaving so idgaf :laughing:i am in a similar boat that i am the only data scientist here at booster as well, just curious if you could share your experiences in regards to having an imapct at organizational level, and dealing with perhaps people who are not used to data.thanks, alan !now it goes more like: “i want all our job descriptions in a new db that is more analytics friendly with metadata a, b, and c so i can build product x and y” and i get “cool, but no, we don’t have time” three times over three months until another product manager is finally really curious about whether we can do y and i finally convince our director that this shit could seriously be cool, yo; and then magically the data product manager comes to me one day and just says “yes”. later we have a couple easy meetings, i don’t hear much for a quarter, and then bam it is ready.making it sound nice being the only data scientist :stuck_out_tongue:although i couldn’t sell them on my title being “god-emperor of data\"oh, yes, enjoy it while it lasts.we’re picking up more out of the current insight session, but it’ll still be an integral team rather than splitting permanently in organization to different groupsthough likely still reporting to the head of business products like i do nowput that shit on a business cardstay positive johnrwalkyou’re not my supervisor nuggetboti was never in a single ds org, but i was surprised to hear later after we had some successes with some internal data products that the “ceo and other leadership never thought you could do x…” and i’m thinking what? they hired data scientists and they didn’t know we could do x? — it makes you realize how important it is for information to flow up the org selling your capabilities., yeah i thought i was good at talking to people at different levels, but someone just recently told me that i should stop thinking about this logically :stuck_out_tongue:you need to feel the answers.dealing with business people is more about personal relationships…that’s the lesson i’ve been trying to keep in heart and mind whenever  i have to talk with them.and just be as explicit as possible. there’s a saying in spanish, cada cabeza es un mundo. each head is its own world, and that has never been truer for me. i was hoping to hear more of what anyone had to say if they were the only ds working around challenging specimens :stuck_out_tongue:well, i think  is maybe being a bit too negative about business-folk.:stuck_out_tongue:that’s why i need to hear it from you the most experienced peoplei think the challenge is for us to come to their level in understanding the business problems and actionable insights that can be deliveredstay positive zkermish…and to communicate in a constructive way when the question they\\'re asking won’t lead to an actionable insightbut great discussion about teams and structure (and thanks for the slides )!. the flip side of ’s comment about information flowing up the org to sell your capabilities is higher-ups asking for things that they think are possible but aren’t. communicating the limitations of data science (often given data limitations) for a non-tech audience is also challengingrelated to that is the perspective i’ve run into now and again: “this data product you built is wrong in cases a, b, and c, when are you fixing it?”  well, actually, i have no immediate plans to fix it because it is correct in the much more common cases; and in fact a human is going to be wrong in cases d, e, and f. even though everyone has heard and mostly accepted the 80/20 rule, sometimes people fall back to absolutes and needs to get a little reminder that getting to 81% is going to require another 20%.stay positive alanoh, and my colleague mario will be on that general assembly “how to hire…” panel tonight in sf. he’s a cool guy, and supposedly our vp of data has prepped him with good advice. if anyone goes you should ask him about banana hefeweizens. apparently he likes that sort of thing.today is a good reminder of why this slack channel is great.one more resource for interview questions (with answers / hint in python) - hi all,\\n\\ni\\'m looking for a presenter for wwcode-sv data science group\\'s monthly talk (august 24 evening at cisco (north san jose)). we had some scheduling issues that\\'s why the last minute call.\\n\\nif any of you would like to give a presentation on a ds or de topic (about 60 min), please let me know. (the topics of the last three meetups were elasticsearch, careers in data science, scalable machine learning in r and python with h2o)\\n\\nplease let me know if you have any questions or if you would be interested in giving a presentation on aug 24 or at a later time.\\n\\nthanks,\\n    judit\\n\\nabout wwc:\\nwomen who code is a global nonprofit organization dedicated to inspiring women to excel in technology careers by creating a global, connected community of women in technology. the organization tripled in 2013 and has grown to be one of the largest communities of women engineers in the world.andrew ng\\'s coursera machine learning course in python: hey all - does anyone have recommendations/resources for ds wanting to work on failing fast and iterating?sprint planning + standups are good to see where people are at and what they plan on doing and how long they plan on working on a task, and can use those as a way to nudge people to refocus.deadlinesmake tickets with clear acceptance criteria, i.e. instead of saying \"build a model\" either put a deadline on it or some other way where they can iterate quicklyi have had my best results with short, non-intimidating standup meetings. if everybody shares a 1-2 minute progress report, no one feels singled out and no one gets left behind. this can be hard if you have members of your team that get kind of attacky, in which case the scope increases to reining them in too. this also starts to get unwieldy if the meetings are too big. i have found that groups of about six work pretty well for this kind of standup. if everyone has to report their progress every day, then it\\'s hard for team members to keep spinning their wheels on tasks with marginal returns.related to brianna\\'s question above about failing fast, how do your teams balance development code vs production code? example, i was given a data dump with lots of raw historical data which i (the ds) wrote python code to build features and train model. now my team wants to push some of the feature creation (e.g. historical averages) back into sql via sas. this production code will be run on new data not the training data, so i can\\'t plug it directly into my python features code.i’m going to guess the lack of replies is due to imposter syndrome. :wink: i’m certainly not a great data scientist, so what can i contribute? that said:\\n\\ni think “great” data scientist know that data science isn’t building models and wrangling numbers; data science is about using data to inform decisions, build products, and make your company money. they know the 80/20 rule, they talk to customers (internal or external) to make sure their work is having impact. they communicate effectively and often.+1 for standups and deadlines. if you\\'re a team lead you can evaluate progress continually, and always descope, descope, descope+1 for deadlines also - you can help them learn to scale back things as deadlines approach.  they should be thinking of the project in pieces.  what does the whole thing look like?  if i had to deliver it tomorrow, what could be cut?and then order those thingsi’m working on something like that right now for example.  deadline next friday - there’s a plan for the whole thing, and then a multi-stage contingency plan of things to pull back if it doesn’t look like they can be completedi would try and understand what the source is of their \"perfectionism\". sometimes, it is fear of being judged, sometimes it is an environment that is perceived to be one in which one can\\'t fail, sometimes it is habit, and sometimes it is lack of knowing how to work in a different fashion. in my experience, the best solution is an adoption of agile (standups, deadlines, pre- and post mortems) adapted to suit data science and ml tailored to the source of the problem.\"what does the whole thing look like?  if i had to deliver it tomorrow, what could be cut?\" -- that is also a good interview question :wink: if you have enough influence on production code then the dev code could go a long way, and be a part of the production code. if there is a hard line between the scientists and the production engineers then you are limited by what the engineers can put into production. also, writing back features to sql, is akin to the dutch boy with his finger in the dam. there are two things at play here, one the company has to evolve, and for the company to evolve they need to see the real value in models - education about data analysis. the fact that models are fast, features engineering is easy, tunable to variations in the data, capable of online learning, unlike a rules based approach are some of the things they need to see and value.more information: we have a small team with no production engineers. the structure is that the data scientists write python code that sits on top of the sas sql pulls created by data analysts.i might be inclined to see if there\\'s a larger problem thatcan be broken down into smaller pieces. this has the advantage of creating small, esaily achievable goals, and also serves as a foundation for figuring out where this person might be getting \"stuck\" in the process.i have also had one instance (and thankfully _only_ one) of someone using \"perfectionism\" as an excuse for never finishing anything because they didn\\'t actually know what they were doing and were generally bad at solving problems they haven\\'t encountered before. :confused: anyone want to save string theory!? all you need is some good data science!(views are my own: you may not believe string theory needs saving, or deserves it. ;-))“*goals and vision*\\n\\nwe are very pleased to be hosting the workshop on data science and string theory. this is the first meeting of its kind, where the goal is to unite big data techniques with string theory, in order to systematically understand the string landscape.”haha jeeze they are still at iti don’t really think of the landscape as being science, so hard to tell what data “science” there is to be done with itlet string theory die and good riddancebrutal gentlemen, brutal!\\n\\nand i agree! :smile:i went to grad school at penn with jim halversoni lived with a string theorist in grad school and me and the other astro grads mocked him mercilessly, he graduated in 4.5 years and became a phys prof a couple years later…theorists (a former one myself) don’t need nearly as much time to write a dissertation, as they/we aren’t waiting as much on datai’d be curious who that was, , as faculty positions in theory are frickin’ rarefractionally speaking (as a fraction of phds in theory produced)it was chris beem. he was at stonybrook and then moved to oxfordhas anyone worked with aws glue or aws data pipeline? i\\'m curious to hear about your use case and when would it be an ideal managed service to use.any scala devs here? i\\'m wondering what this syntax means:\\n\\nabstract class treenode[basetype &lt;: treenode[basetype]] extends product\\n\\nabstract class and extends i get, and treenode[something] means that treenode takes a variable of something type, but i don\\'t understand basetype &lt;: treenode[basetype]\\n\\nsimilarly, abstract class rule[treetype &lt;: treenode[_]] extends logging`a &lt;: b` means `a` has to be a subtype of `b`see is basetype an actual class, or is it just used as a generic class identifier?usually it\\'s either a class or typethe above example i pulled is from the spark source code, and i\\'m confused because i don\\'t see basetype as an actual class defined in the spark packageit seems to be a self-typeah! ok, i think this is pointing me to the right trackthanks a bunch! i really need to go learn scala i have not looked at data pipeline but my company was planning to use aws glue for etling data into redshift from s3. glue did most of the things we needed it to do. however, snowflake managed using home grown apache airflow did everything glue could do faster and for less money so we went with snowflake instead. i think glue needs to be more transparent about it\\'s \"job bookmarking\" feature (how it tracks which files it has processed and which files it has yet to process) and also provide a better view of job run history in general.worth looking into higher kinded types: \\npersonal disclaimer: i highly advise against trying to understand this in detail if you are not familiar with elements of type theory, for now just having it in the back of your mind should enable you to sleep better at night :slightly_smiling_face: when you say \\'snowflake\\' you mean snowflake data warehouse, or something else?yes the data warehouse.how did your airflow dag take into account changing schemas? assuming it did  something like the cataloging of tables via the crawler like in glue.it did not, our data is very simple.however, with json data, snowflake can handle changing schema. but not other data typeshmmm, perhaps glue would have been better if you had an evolving schema that wasn\\'t json.then again i can\\'t imagine what kind of task(s) you\\'d need in airflow to take into account changing schemas.i would not want to implement automatic schema discovery by my selffor the video files, if you install nginx-extras instead of just nginx, it will include the mp4 module. then that configuration info goes into nginx.conf, telling it to serve files from the \"video\" folder with the mp4 command. “make sure you are convincing them” words to live by!speaking of domo, looks like they re creating quite the splash at the tableau conference (involving snoop dog no less)   : for those of you working on data science teams, what metrics do you use to measure your performance? what do your performance reviews look like?my team is weird, so probably not representative, but our performance reviews are a mix of target metrics our managers set with us (blog post count, projects completed, some other fuzzier ones) as well as peer reviews from four peers (two we get to pick, and two our manager picks). note: i haven\\'t gone through one yet, but that\\'s what i\\'ve been told.this is awesome. thanks i used java heavily in my last ds role. i recommend learning the standard collections well (which i mostly did from the oracle docs). nd4j is a decent replacement for numpy. apache commons has a lot of general math and stats. feel free to ping me with more questionsexcited to announce our latest insight fellows program focused on data product management. you can read all about it in this blog post:  if you\\'re a data pm, are hiring for one, or just want to compare notes or get involved in the program be sure to reach out to me or , who\\'s developing the programjeremy here. happy to answer any questions you might have about the program here or in private messages.ah thank uhi everyone! get a chance to win 50$ amazon gift card by entering a few made up sentences, follow this link: affordable mechanical turk! enjoys eating all of the peanut m&amp;mshey guys. i’ve been acting as a data pm for the past 6 months or so. happy to get involved and/or chatya damn rightalso, i added fail2ban on my aws - if you go over authentication logs, you can see thousands of attempts to access your server. another good strategy is to enable ssh-key authentication and disable password authentication. you can also enable 2fa via pem.fyi, with respect to the account takeover on aws, i wanted to put this in general, as securing your account is important, esp for new folks. it is crucial to enable ssh-key authentication and disable password authentication to prevent your account from being brute-force attacked. strong password, disable the default user ‘ubuntu’ if you want to be extra secure. \\n\\nif you want to see how often people are trying to log into your instance, just type:\\n\\ngrep -i ‘invalid user’ /var/log/auth.log\\n\\nfood for thought.hi, i’m curious to learn about people-you-may-know models on social networks. that is, what features, models, and other components are involved in predicting who a specific user might like to connect with on a social network like facebook, twitter, or linkedin?for the people who use the latest version of macbook pro, is it possible to find a keyboard protector that will have the touchbar work?i was confused as to the equity questioni thought it meant per year, heh.also not sure how it differs from rsuthought someone was getting 350k in equity a year and thought damn! need to get that job!yeah, realized after the fact that those questions may be poorly wordedany advice for clarifying w/out messing up interpretation is welcomewas planning on just merging the two overlapsi\\'d just go with \"equity value per year\" or somethingthis was a good idea. thanks! okay, updated the questions, lets see how responses come in over the weekend. i’ll post once more on monday morning, and then i’ll stop spamming the channel.\\n\\nthe original questions/answers are preserved on the spreadsheet (here: ) but the survey now reflects the latest updated questions: has anyone performed a mapreduce in bigquery following these instructions:\\n\\n\\ni have a project and table correctly identified, can ssh into my instance via command line, but when i run the script, it seems to stall at the error: command failed: wait ${subproc} on line 326.\\n\\nthis did not fix it:\\n\\n\\nif anyone has experience with this, i would greatly appreciate any tips. please pm me so we don’t spam this channel :slightly_smiling_face: thanks!hi fellows, my company currently use swift for ios app, ruby on rails for the backend server/web app. i want to build a recommendation engine.  any suggestion on which language/stack/pathway i should go with? thanks! totally from scratch.the rec engine should probably be fully independent/separable from either frontend codebase. assuming they both write to the same backend db, i’d recommend your rec engine be written in whatever language is best for that problem (likely a mix of python and sql, perhaps other stacks if you have a particularly large data set you’re building recs from) and you can serve up results via an api (again, probably a combination of python and sql, but ruby could also work well here)the more independent and flexible your rec engine is the easier it’ll be for you to iterate on the it (leading to greater general happiness :wink: )i think there are a few possibilities:\\n\\n1) you simply create the full data by duplicating the rows. as long as you don\\'t have too many features, 500k rows won\\'t be too much for sklearn to handle.\\n\\n2) r-based packages have support for count proportions. the basic idea is that the likelihood is written as a binomial instead of as a bernoulli. e.g., see here for glm () and here for jags ()\\n\\n3) you might be able to use the sample size to adjust the \"weights\" in sklearn\\'s logistic regression. for instance, see the top answer in this thread (), where apparently you can input a weight for each data point. i\\'d try this out on a small synthetic example to see that it matches the results from #1 or #2, though.\\n\\nyou are correct that linear regression is the wrong tool for the job, though. :smile:(my sql tutorial slides)automatic text generation fun.  theoretical physicists especially encouraged to try:can you guess a real arxiv paper from a fake one?stay positive ruthtoneranyone using the most recent version of airflow (1.8.1) willing to answer a question about it?  we\\'re seeing weird behavior surrounding backfilling and can\\'t tell if it\\'s a bug or something specific to our setup.  i can start a thread to not clutter the main group, any advice is appreciated!brianna.schuyler: i assume you mean airbnb\\'s right?yes sirdo you guys use it at square?we use a fork of iti have only dabbled myself, but i know jackie has used it a lothmm, ok.  the thing is, there\\'s this new concept in the newer versions called \"catch up\", which is different than a backfill.  so there are like two use cases: \\n1. we have a dag that runs daily with a `start_date` of last week. when it turns on we want it to “catch up” and process the last 7 days. \\n2. we have dag that runs daily with a `start_date` of today. we want to manually backfill using `airflow backfill....` and have it run for the last weeks worth of days. \\nin both cases, each day should run to completion before starting the next day.  our dags are doing the correct thing in the case of a \"catch up\" but with a backfill they appear to be ignoring the state of previous days. we\\'ve tried using `depends_on_past`, `wait_for_downstream`, `max_active_runs=1`, and dedicated pools of size 1 and none of them have worked.  it\\'s weird that there doesn\\'t seem to be a lot of chatter about this on the internets.i would toss that in am email to jackie. or try pining her directly, not sure she checks slack too ofteni know she had to convert most of her etls overk, will do!hey sv / sf! you\\'ll receive an evite early next week, but our new session is starting on may 30 so the alumni / welcome party will be friday night 6/2 @ 8pm! save the date and come say hi!ooooo it\\'s coming so soon!soon...thanks guys, this is fantastic. especially, i love it when there is an actual package to do this :slightly_smiling_face:career advice question: i\\'ve gotten a couple of messages from recruiters at big companies that seem to be actually writing to me. i\\'m not interested, and don\\'t think i\\'m qualified enough yet for these to actually be high quality recruitment attempts - they feel right on the border of spammy, and are probably just casting a wide net to get a lot of interviews - but is there any downside to not responding?there is no down side to not responding if you\\'re not looking or interested. some are in fact just spamming everyone, and others are targeting more narrowly. you can generally tell based on how it\\'s written. :wink:i ignore the spam ones, and i follow up on others if interested. a recruiter message on linkedin got me an offer from ms a few months ago that i almost took, except in the end moving to seattle was a bridge too far for my family.if you reply to good ones and politely decline, saying something “thanks for this great opportunity but i’m currently not looking for a change” they often keep you in mind for future positions they have, at which time you might in fact be interested.but ignoring them doesn’t do anything badi attempt to respond in bulk maybe once per week to as many as i can get to in 5-10 minutes or so -- you can copy/paste something like papadimitriou\\'s message and move on. likely they\\'re just searching \"data scientist\" and spamming everyone, and time is valuable so i wouldn\\'t feel bad at all about not responding if you have better things to do with your time :slightly_smiling_face:anyone know of any good resources on proper data modeling? i\\'ve heard great things about this series  but i\\'m on a time crunch :confused:you can send interesting ones to insight :thinking_face: not interested but you can find more people like me at insight?hi all, can i get easily get a mysqldump into postgres?unsure, but maybe you can go mydsqldump -&gt; csv -&gt; postgres?also: this link looks very interesting apache drill claims to be able to do something like thiswe use tableau for lots of things, but people here know many open source python solutions as well thanks , do you have any thoughts on this ?mapdsuperset!what are your favorite resources for setting up a data science environment using jupyter notebooks/python/terminal on a windows pc? what about favorite blogs for doing data science on windows?\\n\\ni’m coming from a unix/mac background and i haven’t used windows in over 10 years.hi becky. i\\'m also a current fellow, in nyc. i\\'m using pyaudioanalysis. it is for audio data but i think many parallels can be drawn. the code is written in such a way that you can modify the functions to your particular needs. check out the features they extract. i work with python in linux, mac and windows.  python and pip works more or less the same in all three operating systems.  virtual environments work the same.  setting up virtualenvwrapper is a little different for windows ().  one little known feature of windows 10 is the linux subsystem - you can basically run linux within windows:  . it’s super cool and it works pretty well.  however, the filesystems are separate.is anyone familiar with spark\\'s catalyst optimizer (automatically optimizes queries for spark sql and the dataframes/datasets api) and writing custom optimizations? i\\'ve been working with the spark-cassandra-connector and discovered that write throughput (from spark to cassandra) can be increased dramatically if the dataframe is sorted by cassandra partition key prior to writing to database. i\\'m considering modifying the connector to automatically perform this sort prior to writing to database by writing a custom catalyst optimization, but i wasn\\'t sure if this is something that is actually possible have you been able to time how long it would take spark to sort the data for each partition key? this should help you decide if it’s worth building it. e.g sorting a df on spark and complete write to cassandra takes 1hr vs just a unsorted write from a spark df to cassandra 2hr. yeah, with a 26 gb and a 260 gb dataset, i\\'ve discovered that sorting the data first would be faster. results may vary of course depending on the exact distribution of the data, but it\\'s significant enough that there is an open issue that datastax (developers of the connector) has put up for it.  it\\'s something they\\'re still looking into. we think that it\\'s worth building, and creating this custom catalyst optimization is the way we\\'re thinking it should be implemented, but i\\'m just not really sure how to work with spark\\'s catalyst optimizerah that’s pretty awesome. you plan on extending the current connector? yeah - it\\'s quite unintuitive that the write speeds would jump up so much by a simple sort, which is why they\\'re thinking of just making the connector automatically sort by partition key before saving. catalyst is basically the optimizer under the hood that makes dataframes so much faster than rdd\\'s (and the reason why spark\\'s contributors highly recommend using df\\'s over rdd\\'s), as it does a bunch of pre-optimizations that aren\\'t possible with rdd transformationsit seems that the best way to implement this automatic sorting is to just add a custom rule in catalyst, but that kind of functionality is quite new in sparki have dealt with a/b testing quite a lot at glassdoor. we have our own in-house ab testing framework. the allocation part of it is mostly a java port of facebook’s planout (find it via the java link on planout’s github repo). the analysis and reporting is custom python. i can’t do a very fair comparison to services such as optimizely because i wasn’t part of the infrastructure decision process (but i think at least one group here *is* using optimizely).i’d say three of my big tips i can give is that to do good a/b testing you need to\\n1) get really really focused on what is important: i see it all the time that executives and product managers look to the simple top-level metrics because they are easy to understand, measure, and move. sometimes you have to settle for these, but at least keep reminding everyone of the limitations and what might matter more (and push to get the data in place to measure these)\\n2) make sure tracking is rock solid. when you are looking at 5% effects all kinds of quirks of the data can cause problems. when doing a/b testing on a new part of our product i don’t think we have ever had a clean test on our first try.\\n3) watch out for the long-tail of users: as an example consider scrapers/bots, we get a fair amount of that activity and do a pretty good job of identifying them, but just a few slipping through your filters and ending up in your tests may shift the results artificially. the trick is this activity can be really similar to your most active users which you definitely do want to include in your results.thanks for the info and pointers ! yeah, we were looking into planout as well since it seems to deal nicely with related experiments via namespacesand, fyi, if you use something more general like planout then expect that it is going to take a lot more work explaining the data model and test setup to product managers. it is not just one users &lt;-&gt; one treatment and some may take a while to full wrap their heads around it. of course the upside is that at scale it is a lot more powerful.on your analysis, what’s your method for controlling for false positives when running many concurrent experiments? it seems to be a common pitfall if not using something like a hierarchical bayesian method or whatever optimizely does (some variant of sequential testing?)thanks  for planout - working at the parameter instead of treatment level seems so obvious in hindsight. [programming] has anyone encountered generating a powerset with some of the elements having a parent-child relationship? trying to visualize the logic to put it into code.: example? also are there any special constraints for including a parent/child relationshipso like if my set = { a, b, c, d} and a is parent to b, is {a,b} a valid component{a} and {a,b} are valid, but {b} isn\\'tok so a child must be accompanied by a parent? ;0and neither is {b, c}, {b, c, d}... etc*;)yeahprobably the easiest way would be generate the full power set and then remove ones where parent isn\\'t preent?present*well i would assume taking the powerset is out of the questionyeahjust wondering if that is out of the questionsince it\\'s obviously exponentialthe powerset is here to staylolthe use case involves finding every combinationwell just take those unaccompanied children homeor take them outpretty inefficient if its sparse thoughtruedo u know how sparse the dataset you are dealing with is tho?how often is there a parent/childi have 100 elements, 20 of them have one or more childrenif not too often i say go for itok never mindhm well given the validity example i would still consider doing it and back-checkor you could always make sure to add the parent firstbecause order doesn\\'t matter here, correct?nopeorder doesn\\'t matterthing is i don\\'t think it\\'d make a difference for timewould it? it\\'s o(n) just to remove them all(i feel like this is more and more of an algorithm interview question)well there would be 2^n total sets to constructworst casewhere n = the number of elements in the original setbut you would have to generate the 2^n sets first and _then_ scan to removeyep, so that\\'s 2^n again isn\\'t it?or wait no, it\\'s o(m) where m is the size of the power setrightwhich is why i was assuming just doing that was out of the question beforei feel like if you create a power set of all the elements w/o children first you would have a good startdon\\'t get me wrong there\\'d still be a vast majority of valid elements ungeneratedbut yeah computing the power set = no buenothere\\'s a good one for lda that has topic on the x axis and words on the y. i found it useful for examining the output side by side.has anyone worked with airbnb’s superset dashboard?johnrwalk: we looked into it, but i think it had a weird requirement for the backend. i’m interested to hear about it if you end up working with it.i’ve been able to get it working on my own machine easily enough, but i’m running into trouble exporting the dashboardswhich seems like it should be simple…that it does…it has the functionality, but there seems to be some underlying flask issue that causes it to not actually present the dialog to get the export filei made a word cloud! :slightly_smiling_face: i looked at it but went with  instead good advise, the backfill is not working eitherdo you have some advise as of how to debug what may be happeningi’d start w/ an extremely simple dag and do a backfill on that and build up from thereyeah so that\\'s what i didit used to work but for some reason it stopped workingit\\'s probably something trivialit\\'s the hello_world exampleit just get\\'s stuck in the run partlet me try another simple examplethis is what i see in the logsi mean in the backfill outputairflow backfill hello_world -s 2017-08-16 -e 2017-08-17\\n[2017-08-17 13:33:25,860] {__init__.py:57} info - using executor celeryexecutor\\n[2017-08-17 13:33:26,244] {models.py:167} info - filling up the dagbag from /home/ubuntu/airflow/dags\\n/home/ubuntu/anaconda3/lib/python3.6/site-packages/airflow/ti_deps/deps/base_ti_dep.py:94: deprecationwarning: generator \\'_get_dep_statuses\\' raised stopiteration\\n  for dep_status in self._get_dep_statuses(ti, session, dep_context):\\n[2017-08-17 13:33:26,771] {models.py:1126} info - dependencies all met for &lt;taskinstance: hello_world.dummy_task 2017-08-16 12:00:00 [scheduled]&gt;\\n[2017-08-17 13:33:26,775] {base_executor.py:50} info - adding to queue: airflow run hello_world dummy_task 2017-08-16t12:00:00 --pickle 1 --local\\n[2017-08-17 13:33:26,785] {models.py:1120} info - dependencies not met for &lt;taskinstance: hello_world.hello_task 2017-08-16 12:00:00 [scheduled]&gt;, dependency \\'trigger rule\\' failed: task\\'s trigger rule \\'all_success\\' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={\\'successes\\': 0, \\'skipped\\': 0, \\'failed\\': 0, \\'upstream_failed\\': 0, \\'done\\': 0}, upstream_task_ids=[\\'dummy_task\\']\\n[2017-08-17 13:33:31,697] {celery_executor.py:78} info - [celery] queuing (\\'hello_world\\', \\'dummy_task\\', datetime.datetime(2017, 8, 16, 12, 0)) through celery, queue=defaultnot sure what’s up, sorry.my only advice would not be specific to the problem but general debugging techniques not specific to your error (e.g. start from scratch again)yeah i\\'d agreei will do some clean up and try it againfor debugging though do you know if one can actually see which workers are running and then get inside what the process is doing?  i could use celery or localexecutor, or sequentialexecutoruse sequential to start “local” is actually what i used to do parallel submitting of tasksthere might be a verbose output when running airflow, not suregreat thanks dan! local executor is working thanks a lot!does anyone use aws kinesis, i’m wondering if it’s a good idea to set your own parition key so that groupings of data go into a shard rather than have it be randomly generated; any suggestions welcomei’ve played around w/ it before. \\n\\ndepends what you’ll want to do with the data after it’s in the queue.  for example if you want a consumer to only read in data, e.g. form nyc, then might be a case to partition by city (possibly at the expense of complicating other consumers’ logic though!)\\n\\nand could see a slight performance gain if all the data is already co-located at the queue level.\\n\\na definite potential downside though is that you may get “hot spots”, like having some shards with a lot of data, but others with very little data.ah great explanation, kind of what i though, thanks for taking the time! pinned a message to this channel.linux to mac is easy for the most partjust use brew as a package manager to make up for the lack of one in the macthe new hardware\\'s\\u200b direction is another matterhey ethan, one more question — i’m a little confused by how lightfm treats the `user_features` matrix in the `fit` method (which is presumably where user attributes are applied).  it’s described in the documentation as “the user weights over the features”, (as opposed to defining the features themselves?) so i’m not sure i’m interpreting it correctly.  also, in supplying users to `predict` does the `user_features` matrix need to be fully populated?  i may not have the full set of attributes for new users coming inagree on linux -&gt; macosdoesn’t take longlot of unix functionality already providedxbox or play station! if we really want to get a war going we should do vi vs emacs :wink: :vim:vim &gt; vi:+1: :vim:or vim vs neovimneovim is my chosen platform! love it!(after 11 years of vim)never heard of neovim — am i missing out on something? (vim user)umm, not super missing out on anything after vim 8 added async plugins.  neovim has a nice terminal though.and it\\'s aiming for the holy grail: a vimlib.c that you can link against from anything. full vim in whatever text editing program or ide is the dream.:heavy_dollar_sign:theres a vim mode in emacs (evil-mode)yeah, it\\'s definitely a bit confusing. the way to think about it is that the row of the `user_features` matrix should match the row of the user in your `interactions` matrix. in terms of the features themselves, you can think of them like regular scikit-learn features. the one key thing is that you might want to one-hot-encode a unique dimension for each user. if you were to _only_ include a single, unique, one-hot-encoded feature with weight 1 for each user and each item, then this would be equivalent to matrix factorizationi\\'m not sure if that all made sense. at the risk of being narcissistic, i wrote a blog post on this  the \"learning to rank + side information\" section details how to add `item_features`.cool, i’ll check it out — i’ve been in meetings all morning so i haven’t gotten to dissect the sample data / examples for the package.as an alternate approach i’m considering a pure model-based approach with a bayesian network, which may handle the user features more cleanlyoh you can use xming, i\\'ve mostly used it when working from a windows laptop to a linux server:grinning:i kind of love where this mac/windows conversation has gone and that it’s gone on this long :wink:  just to add one more thought: historically, mac os has been much closer to linux systems (being bsd based) and with homebrew you basically get all the bells and whistles that devops folks like. i’ve always viewed macs as linux laptops that properly go to sleep and wake back up (at least that’s why i switched off linux 10-ish years ago). being able to shoot the shit with devops folk and understand how linux systems work in pretty great detail should not be undervalued. having devops friends at your company can really help you get things done.mobaxterm is also a good alternative, but i believe they both rely on having a connection to a linux server, not a local installation, right?\\nthat\\'s what makes the bash shell as a native application in windows so exciting.vim :+1:backed up a few posts in your blog, definitely more helpful than some of the package documentation i’ve found.  thanks again :slightly_smiling_face:haha thanks. good luck!random question: we all know about ‘lift’ (i.e., improvement of some value given a model or something like that), but what do you call the opposite of that? if your model does worse, then is it “negative lift”? “depression”? “decrease”? or do you just report “lift” as a negative value?“sink”? how well have you found `lightfm` scales to folding in new users?  presumably it needs to rebuild its matrices to integrate a new row (independent of the issue of that user not having any ratings, so it would be cold-starting just off of their `user_features`)i read in some blog that you can use xming with the windows linux subsystem haven\\'t tried it yetin terms of scaling, i\\'m not sure if this is what you\\'re getting at, but you don\\'t need to retrain anything. you just need to provide a `user_features` matrix with the new user\\'s attributes. as such, the only cost is the time it takes to build the matrix (which shouldn\\'t take long, though there\\'s fast and slow ways to do this with sparse matrices)hm.  my understanding was that predictions could only be made on users that were in the original interaction matrix, given that looking at the api docs for the `predict` method it calls for an index for the user-item pairs, as well as (optionally) the `user_features` matrixi think you can pass in a new `user_features` matrix, and you just have to make sure that your user-item pairs match up with that new `user_features` matrix. the only thing is that your columns (feature indices) must remain the same.lift can be negative imoloss?hello!  i made the  channel, please join if u use these technologies“drop\"?in this context i would it call a decrease in x, where x is whatever metric.\\n\\nin some contexts i would be comfortable presenting a negative lift number. (like a,b test results)thanks, all, i like ‘drop’ or ‘negative lift\\': we\\'re hiring for a data-journalist type position at nerdwallet that should be really fun. pm me if you or someone you know might be interested! i\\'m going on mat leave starting the 3rd week of november...does anyone know of anyone who would be interested in a 4-5 month temp position to cover me?  it\\'s at one kings lane and it\\'s awesome here!   basic requirements are sql, (r, bash would be nice but not necessary), tableau experience a plus.  we have a lot of alumni and fellows commuting from nj to nyc — hope everyone is safe and sound this morning!\\nmade it thru hoboken 15 minutes before the crashwas looking for a ‘random’ channel, but we don’t have one, so: blatant self-promotion … /giphy entertain us with whimsy!okay...hi guys, does anyone know of any easy to use python, d3/js libraries for showing event data overtime on a map  (all i would need to plug in are latitude/longitude and time)? is quite good.when i think d3.js, i don\\'t usually think easy. :stuck_out_tongue:\\ni\\'ve use bokeh. it\\'s similar to d3.js and can make interactive maps.nvd3 check out folium + vincent examples:  try looking into the mapbox apithanks all! this helps a tonnvd3 is beautiful, but the last time i used it (feb), the documentation for nvd3 was pretty sparse.anyone here do a shiny app for their demo and manage to embed their google slides into the app?industry leaders establish partnership on ai best practices:  russia?no?nah they’d sooner release trump &amp; putin’s honeymoon photos i used an iframe in the ui.r file. see here for the code:  hi alums, thank for participating in this week\\'s interview blitz. starting next week fellows will be going into real interviews and we still need 3 alums to sign up in order for each fellow to have at least 1 mentor session. if you\\'re available, please sign up here --  please dont use the channel tag !!my apologies i meant to post on the ny channel. i don\\'t mind if you use the channel tag !!people… chill.brick killed a guy.: if you don\\'t want to get notifications you can change your notification settings by going to the gear menu and selecting how you\\'d like to receive them.now is a great time to introduce marie paule to those of you who haven\\'t gotten a chance to meet her! she is our newest team member in nyc and she\\'s amazing! please give her a warm welcome :smiley_cat:can we just ask slack to disable the channel notification? someone here has to work therei love that faye is now an at channel meme, remember to keep your office space clean and tidy :d sending stuff to channel is the modern equivalent of reply-all. :smile:more time is spent arguing about the proper use of the channel tag then the amount of time it takes everyone who dislikes the notifications to dismiss them.now that everyone is awake - invitae is _finally_ ready to start doing some cool machine learning on genomic/healthcare data if anyone is interested.  i keep my office space extra un-tidy in honor of hahahah well played not everyone is on us time zones  ..slack is a tool for asynchronous communication to help us use one of our scarce resources most wisely (attention). @ mentions force synchronicity which is useful in case \"hey guys, the church next to the insight nyc office is on fire\", which happened (arguably, this should go in the nyc channel, or we should create an nyc office one). this channel has many new slack users, it takes a while to get used to slack etiquette. in the meanwhile, we create some of the best memes. also,  welcome!fwiw, now is the time to get all your announcements in since everyone will be checking this channel! welcome! if you guys are considering mock phonescreens you can count on me from svhistoric preservation has been battling this kind of shit for a while - look at what could be, if only developers valued rehab &amp; community engagement as opposed to maximum high priced, high density housing: hi andrea, we\\'d love to have you. please sign up at a time that\\'s convenient for you. thanks again! -- he should probably lay low for a whiledo people have slack actually send interruptive notifications their phones for things like pager duty? uploaded a file: yes. :)per ’s advice, today is the last day to use coupon code (ins25x) for the upcoming nyc dataengconf - \\n\\ni know a few of us from insight will be attending, so would love to meet up with othershello all, i am looking for a cloud storage provider with reasonable price. our data is around 100 tb, we need frequent upload but rarely download. any recommendations?!checkout aws glacier - backblaze? (the b2 cloud storage version, not personal) - my company doesi see -- we have them on slack as well but i figured people would use mobile numbersi have notifications set to hear about things on my phoneone of the most recent ai fellows  just posted a blog post about how to generate over 10s of millions of extra text data if you are lacking data for a classification project. read more here: i like this part: \"they were able to provide me with just a single example question for each of the seven sub-classes!\" nice article :slightly_smiling_face:thanks :blush:stay positive alex_gudelean analytic talks about how arbitron moving to more real time data on radios changed modern music, 538 covered this as well, and whether data (or bad data) killed soft jazz.stay positive alex_gude: buzz of! :wink:nuggetbot: weather palo altoweather for palo alto, ca, usa\\ncurrently: partly cloudy 19.9°c/68°f\\ntoday: mostly cloudy throughout the day.\\ncoming week: no precipitation throughout the week, with temperatures rising to 26°c/79°f on friday.nuggetbot: weather birmingham, alabamaweather for birmingham, al, usa\\ncurrently: mostly cloudy 28.3°c/83°f\\ntoday: light rain until this evening.\\ncoming week: light rain throughout the week, with temperatures falling to 24°c/75°f on friday.nuggetbot: weather new york cityweather for new york, ny, usa\\ncurrently: clear 24.4°c/76°f\\ntoday: partly cloudy starting later this evening.\\ncoming week: light rain on sunday through tuesday, with temperatures falling to 21°c/70°f on tuesday.: i found the following to be useful: mitx: 6.041x introduction to probability - the science of uncertainty  hello all. anasuya\\'s and my team at memorial sloan kettering is looking into a projects using wearables / activity trackers, such as garmin, fitbit, jawbone, etc. does anybody know of good datasets from these kind of devices?  not from one device but... maybe?and thanks! yeah, these are useful although seem a bit geared towards identifying activities such as running, or walking from accelerometer / gyroscope data. i\\'m mostly searching for datasets with more derived data, like step counts, or activity indicators ...you should check out my insight project, activeme at  ! i had a lot of trouble getting data sets, but i have at least one jaw bone data set of ~1 year if you would like thathi lois! yeah, that would be great!hey bay area fellows, a close friend of mine is looking for a product manager role. she has a few years experience in fraud product management/operations and has successfully launched her own consumer hardware startup prior. she has some programming/analytical ability, and is flexible on subject area. she works at a larger tech company right now and is looking to grow. if you know of any positions that she might be good for, please message me. thanks so much! :slightly_smiling_face:also, :pinterest: pinterest is hiring data scientists right now for ads and discovery. let me know if you\\'re interested! dm me :grimacing:airbnb releases trove of new york city home-sharing data \\n\\n\\n&gt; the new data set released on tuesday, which is made available only by making an appointment to visit airbnb’s new york city office [...]\\n\\ncan somebody in nyc pick this dataset up for the west coasters?  :simple_smile: thought this article was interesting...: we had a talk by nicholas here at li last month - it\\'s a really clever way of disrupting image classifiers with imperceptible permutations since the parameter space is huge. however, it\\'s less clear how you would apply that to text classifiers and other domainsthe nastiest application of that is modifying a stop sign with just the right jitter to make it not seem like a stop sign to a self-driving caranyone looking for a consulting gig or potential side work in the future with an advertising firm? if so, pm me. a friend of mine is looking to incorporate more data science into their work, so it wouldn’t be immediate work, but it could lead to something. my friend is a smart and reasonable person, so it should be a good conversation and upside to pitch your value and the value of your work. i guess i just never cared about the security aspects, but it makes sense since there is more dependence on ml making decisions, how to protect from the “poisoning of training sets” becomes a critical issue.yeah, there is a lot of adversarial aspects to worry about. nicolas\\' work showed that you don\\'t even have to know the model that is being used - just train your own, find the direction of maximal change in the jacobian of the cost function, and add some noise there. the disruption is likely to affect the unknown model in the desired direction.a gallery of interesting ipython notebooks: in case anyone else faces the problem of needing a strong but interpretable classification model (which is explainable to non-ml/non-stats experts) - i hacked together a scikit-learn estimator producing easily understandable decision rules, based on some existing code and some amazing work on bayesian rule lists. i\\'m going to be in the market for a coworking space in the near future. does anybody have good/bad experiences with any spaces in brooklyn/manhattan? if so, please shoot me a private message.hey if anyone\\'s interested, city of boston is having a neat open data challenge (with prizes!) job alert (for senior candidates): capital one is starting an nyc branch of our research-focused “center for machine learning” group. it’s looks like a very cool opportunity where you’d be pioneering new machine learning techniques for the company. \\n\\nsee the extended blurb in the child thread for details. i will disclaim that despite a moderate qualification list, they’re currently looking for experienced people that can hit the ground running. message me if you’re interested.sorry for bad formatting, didn’t have a public link to this:&gt;&gt;&gt;at capital one, data is at the center of everything we do. when we launched as a startup we disrupted the credit card industry by individually personalizing every credit card offer using statistical modeling and the relational database - cutting edge technology in 1988! fast-forward a few years, and this little innovation and our passion for data has skyrocketed us to a fortune 200 company and a leader in the world of data-driven decision-making.\\n\\nas a data scientist at capital one’s center for machine learning, you’ll be part of a team that’s leading the next wave of disruption using the latest in distributed computing technologies and operating across billions and billions of customer events to unlock the opportunities that help everyday people save money, time and agony in their financial lives.\\n\\n on any given day you’ll be:\\n-using big data tools (hadoop, spark, kafka, aws) to conduct the analysis of billions of customer transaction records in concert with distributed neural network frameworks and other machine learning libraries\\n-writing software to clean and investigate large, messy, structured and unstructured datasets\\n-integrating with external data sources and apis to discover interesting trends \\n-building machine learning systems from proof of concept through production on clickstream, customer, transaction, or behavioral data (to only name a few!)\\n-creating full data pipelines and novel data tiers for your projects on the road to production\\n-perform internal and external research for publication\\n-contribute to open source projects as part of your job\\n\\nthe ideal candidate will be:\\n-curious. you ask why, you explore, you\\'re not afraid to blurt out your disruptive idea. you know python, scala, bash, and at least enough java to be dangerous.  you’re constantly exploring new open source tools.\\n-wrangler. you know how to programmatically extract data from anything and model it into human-readable deployments.\\n-creative. big, undefined problems and petabytes of data don\\'t frighten you. you’re used to working with abstract data, and you love discovering new narratives in unmined territories.\\n-forward thinking.  you are always reading about the newest research on methods and techniques.  you actively try and apply these methods in your own projects, kaggle competitions, or in research projects.  you actively follow or contribute to open source software implementing these techniques.\\n\\ntwenty-five years after capital one was started it’s still led by its founder. be ready to join a community of the smartest people you’ve ever met, who see the customer first, and want to use their data skills to make a difference.\\n\\nbasic qualifications:\\n-bachelor’s degree or military experience \\n-at least 2 years’ experience in open source programming languages for large scale data analysis or simulation\\n-at least 2 years’ experience with machine learning \\n-at least 2 years’ experience with sql and database structures \\n \\npreferred qualifications:\\n-master’s degree or phd\\n-at least 1 year experience working with aws\\n-at least 4 years’ experience in python, scala, or java for large scale data analysis or simulation\\n-at least 4 years’ experience with machine learning\\n-at least 4 years’ experience with sql and other database query paradigms\\n \\n \\ncapital one will consider sponsoring a new qualified applicant for employment authorization for this position. : interesting. what kind of “limits” are you reaching? rds is fully managed; so scalability shouldn’t be problem. if partitioning, that amount of data isn;t that lot. haven’t used spectrum; but another approach is using redshift(which is olap based postgres):thumbsup:hey guys,\\n\\ni\\'m a de fellow and i\\'m working on the visualization part of my project.\\ni have data representing people on the  the us. in my speed layer i have\\ndetected clusters of people  and i\\'m planning to show them on the map.\\ndoes anyone have used maps for their project and has feedback on how \\nto go about this? thanks.just spoke to nathan marz about the book he used to learn clojure, he recommended this one, fwiw:\\nexcellent… the 4clojure website got me demotivated pretty quickly. gotta learn it the right way@mpancia: what do you think? (i really just want to turn my coffee cup into a donut). \\n unclear what’s going on with ayasdi at all, i am happy to talk about it at length in privatequestion for the channel -- anyone using jupyterhub  in their data science teams for collaborative jupyter notebook work?separate but another python-related question -- are people who use python using version 2.x or 3.x?insight ai fellow  shows how to deploy a wavenet model on android using tensorflow hi, guys, anyone has experience leveraging azure luis service for text information extraction? how is it compared with self trained nlp models doing similar tasks? is it really usable or just a toy service? thanks a lot!his nickname is jj yet he has no j in him name :thinking_face:does anyone know which is a good library for generating fake data apart from faker in pythonhi guys, does anyone have any insights/links/experience that may be useful in dealing with extracting main topics from text? i know  would recommend eventsim. david, did you ever put together a write-up on how to use it? thanks!  i found the eventsim write-up:  lda and other topic modeling, tf-idf... you might check out this approach:  check out  data generator. it isn’t for big data, but the data types are diverse and it may sufficient for prototyping.thanks  !sure thing! :slightly_smiling_face:some data activism. there\\'s a chance that public climate datasets might not, shall we say, make it through the next few years (something which also happened with the harper administration).folks are working on both trying to mirror the datasets, and also pay for independent dedicated server space.  if either the mirroring or donating interest you, check out this resource:\\nsent that to my friend lucky enough to have a large server and fiber internet :-)if you make your instance with a gpu it will have access  thank you!! that worked! i\\'m new to sql and spent so much time trying to figure this out yesterday to no avail. thank you!!interesting discussion about startup compensation:  second what till_be said. bq can be a butt about storing too much in memory. also, bq bills by gb processed, so be careful with running big queriesyou should get $300 for a free trial, which is plenty for an insight project!oh, got it. i work with bq a lot at my job now, so i have to watch how much i’m processingit’s still really good advice, you can speed up your queries a lot if you design them more carefully and only extract the data/rows you really needi had that problem for sure when  i did my projectthe guy who generated the reddit data for bigquery also makes the datasets available for direct download if you’d prefer to work with them locallyoh really? do you know where? i couldn’t find itbut please make a donation if you use it, the guy’s an absolute saint for generating and maintaining that set (scraper was running continuously for 8 months to generate the initial dump) and helping people with it, and downloads have to be slamming his resourceson a more general note, have folks compared bigquery to aws redshift? i\\'m not terribly familiar with google cloud platform offerings but would love to hear more. : thanks! and you’re right - i played with the scraper for a bit myself, and i can’t imagine how long it must have taken him to gather all that data.yeah, one call per second x something like 20 million api calls to build out the dataset from reddit founding up through fall 2014, then monthly dumps after thati really don’t love bq. it’s often quite a bit slower than redshift, and for similar storage and processing requirements, it’s substantially more expensive. we’re a bit stuck with it though, since some of our apps are built off the google app engine fundamentally do you want to deal with managing a database system?of course not :predshift is a managed service, but it still requires that you do a number of things to ingest and manage that data warehousegenerally if your use case is comprised of occasional ad-hoc queries on data living in an object store (s3/google cloud storage), bigquery might work really wellthose use cases might be similar to some use cases for hiveif you wanted a persistent resource/system, redshift will do that, but you’re going to take on a number of management considerationshi all, what tools/processes do you use to plan and manage your various projects?management? jira. plan? pen and paperkanban or scrum boards?plus a side `rst` file since my handwriting is terrible.i use onenote (quip is nice for this too) and i organize it as follows every quarter: \\n(1) table stating project/ collaborators / next milestone (specific deliverables to that project) / eta\\n(2) a calendar table for each end of week with the dates and on each cell the deliverable for the end of week (high level summary)\\nif i don\\'t complete i just roll them over in terms of the eta and highlight them in yellow, if i complete on time i highlight them in green, and if scope changed/not_relevant i highlight them in red or just erased them sometime if no need to be there.\\n\\ni also sometimes add a detailed project description\\n\\ni used jira, trello, etc. and all the fancy tools ended up being worse for me than just a document with tables and colors that i share with everyone and that collaborators can edit\\n\\nmind that we are a small team, if the team was much larger maybe other tools would come in handyquick plug for the  channel, for those of us who deal with the challenges of working remotely or are looking for remote-friendly gigs. i know there are at least a handful of you out there.our team uses jira with scrum boards (or atleast we\\'re moving to that) thank you so much for the detailed answer. scrum boards in jira are the pain of my existence. (hope it works well for you, michelle!) a kanban-like board or table is the closest to a useful tool i have found so far.andrew ng’s coursera course of course.  tibshirani ‘s introduction to statistical learning is a great beginner book along with the free online course at stanford lagunita.   udacity’s intro to ds course.  and of course projects projects projects.  yeah i’m definitely going to have no problem getting the vp to fund a couple more months of testing after i have gotten something to work.  why didn’t i think of that before?and, of course, there is the obvious play of using this system to answer your phone for a small business…but that’s a bit out of google’s market, maybe a good play for squarei\\'m actually facing a very similar issue like this now. i would approach this two ways (ideally both). the first would be to train the model using different sizes of the corpus (e.g. randomly take a sample of 100, 1000, 10000 etc.) and train the model and see if/when the results converge.the second way would be: train the model x number of times, each time using a different bootstrap of the data, and see if the results are same for an independent testing set. you can do this quickly by using x=2 for different corpus sizes and just use the pearson correlation between the predictions.hi all, i think i need some of your brain here. i am trying to run a regression on 5gb of data (1million rows and 5400 features). this data is very sparse where most of the features are one hot-encoding from categorical features. i want to start with lasso or elastic net. but it seems like that my laptop (16gb memory) can not handle it. do you have any suggestions?look for a different feature selection method that doesn\\'t rely on fitting a regression on 5400 variables with 1 million rows :slightly_smiling_face:  or provision a really really large ec2 instancethen you can go back to the regression model once you\\'ve eliminated some features have you tried dimensionality reduction methods?yeah, that is the one that just come into my head. i’ll start with svd. or i can even just look at the correlation matrix.not sure as i haven\\'t used it before, but have you looked into the `dask` and `dask-ml` packages?now i thought more about dimension reduction. all the sparse features are domain_1, domain_2, domain_3,….., ip_1, ip_2, ip_3, ….. id_1, id_2, id_3, …… the dependent variable is `open_rate`. with dimension reduction,  if i remove domain_2 and ip_2, how can i get a predict value with domain_2 and ip_2? and with svd, it is very hard to implement the model.looks promising! :+1: i’ll take a look.what do you mean by \"how can i get a predict value with domain_2 and ip_2\" ?eventually, we would like to know  for domain_2 and ip_2, what is the predicted open_rate at this time of the day.another feature is time.the features are \\n```time, emails_delivered,  domain_1, domain_2, domain_3,….., ip_1, ip_2, ip_3, ….. id_1, id_2, id_3, …… ```\\nthe dependent variable is `open_rate`you can\\'t do both dimensionality reduction that combines multiple features into one and have the same amount of interpretability in the form of fitted coefficients.  you can do some interesting stuff like put in the ip_2 and domain_2 to your model and then sample different values for other features and see what it predicts on average - but all fo these types of methods will make various assumptions and have various issuesyou can try to use a feature selection method that just drops features it doesn\\'t like without re-combining them and then you\\'ll get coefficients for the non-dropped features, with the assumption that the dropped ones are unimportant so you won\\'t care about the predicted open_rate as a function of those.so if i drop domain_2 and ip_2, when i predict the `open_rate` for them, i just assume it does not dependent on these two features.yes, that\\'s what it means to drop those features.  the model won\\'t see them so how can it predict based on them?would you recommend using a forward/backward feature selection in this case?can\\'t really make a recommendation blind :smile:  use the one that works best for your model:+1: thank you so much! what about the old school stepwise regression? that will go feature by feature and select/remove as it grows... it\\'s not guaranteed to be optimal in anyway but i assume that you can run local in 16gb ram computer... do you mean forward/backward feature selection?yes great idea! i’ll definitely look into that. :blush:i\\'d load the data in chunks, use that data to train the model with batch gradient descent, rinse and repeat. if you\\'re using python, check out `sgdclassifier` in scikit-learn, which also has a parameter called `warm_start` that may be helpful for this.it\\'s  :)and we\\'ll have  for toronto :))haha love it!use domino or aws. there’s always a smarter way to do things, but i’ve found that if you just up the machine size, you can solve a lot of problems without having to spend much time on it, and your time is ususally more expensive than an aws machinieone other cool application they mentioned is that google could call \"all businesses\" and ask them if they are open on a given holiday and posting this info in their apps (instead of business hours might be affected...) - stepwise options exist for a reason, but they can be problematic. what do you know about the 5400 variables that you could use to reduce, outside of a statistical reduction technique (e.g., pca)? for example, are \\'domain_2\\' and \\'ip_2\\' related to each other? or do you expect them to have unique predictive value on your dv? seems there must be collinearity among that many variables so i would recommend doing some visualizations of the data to see what kinds of trends you see prior to putting them into a model. also, make sure you are using the right kind of model for your data, depending on the characteristics of the data (e.g., are the predictors mostly binary, are they normally-distributed, etc.)some reading on stepwise regression yeah, that was a genius use case!if you write your model in an sklearn pipeline (`input -&gt; truncatedsvd -&gt; linearregressor -&gt; output`), it is easy to use the same type of data for training and prediction. the pipeline model will handle performing the truncatedsvd the same way on both the training data and the data to be predicted. cross-validation conveniently works out of the box too.worst comes to worst, you can always email the recruiter/poc and ask what normal attire is and dress at or 1 level above thatdoes anyone use something like a \"model manager\"  that you are happy with (in your experience/at\\xa0your company)?\\n\\nby model manager i mean something that tracks the model and parameters being used in production, so that any results are saved and audit-able.this can be particularly important when the parameters are being updated with continuously with new data/training.might not be exactly what you are asking for but we use luigi around hereit is helpful when data is continuously updated and your model contains many dependenciesi guess nothing could stop you from adding your own lightweight manager tracker on top of that: could you add this link and a few thoughts to  ?thanks  i\\'ll take a look and pass that alongwe currently use luigi as task scheduler as well, but it is mainly for resolving dependencies among various tasks you run. (with very limited monitoring to see which tasks have been started/finished) \\nit seems like you\\'re looking for a good logging framework that logs the model (version), parameter values and results?current silicon valley data science fellows - check out the  channel, and see the link there to the contact info googledoc that colin has startedhello all. i am trying to get my web-app running on aws and i get the following error in gunicorn: connection in use: (\\'0.0.0.0\\', 80)i have check to make sure that port 80 is not in use, and it is not.this is my netstat -tulpn:tcp        0      0 0.0.0.0:22              0.0.0.0:*               listen      925/sshd        \\ntcp        0      0 127.0.0.1:5432          0.0.0.0:*               listen      1019/postgres   \\ntcp6       0      0 :::22                   :::*                    listen      925/sshd        \\nudp        0      0 0.0.0.0:68              0.0.0.0:*                           603/dhclient    \\nudp        0      0 0.0.0.0:55074           0.0.0.0:*                           603/dhclient    \\nudp6       0      0 :::31372                :::*                                603/dhclientand when it is running this is my netstat -tulpn:tcp        0      0 0.0.0.0:80              0.0.0.0:*               listen      -               \\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               listen      -               \\ntcp        0      0 127.0.0.1:5432          0.0.0.0:*               listen      -               \\ntcp        0      0 0.0.0.0:5000            0.0.0.0:*               listen      -               \\ntcp6       0      0 :::22                   :::*                    listen      -               \\nudp        0      0 0.0.0.0:68              0.0.0.0:*                           -               \\nudp        0      0 0.0.0.0:55074           0.0.0.0:*                           -               \\nudp6       0      0 :::31372                :::*                                -any advice would really help.any port &lt; 1024 needs superuser to run. so \"sudo command” may help things alongi am already running as root. thanks though.unless i need to sudo when already root?when i just run the web-app without gunicorn on port 80, everything works. so i guess for now i will forego gunicorn for my less robust solution.nope, missed that you were running it as root.that\\'s fine. i should have mentioned it in my original post. for now i have a workaround, so this is not an emergency.hi all - my wife is preparing to move down to the bay area and is looking for a position, preferably in the energy space but she is open to other areas. she recently got her phd in materials engineering and studied electrolyte blends for application in stretchable and flexible batteries, and essentially prototyped a working stretchable battery by the end of her 4th year. she has been trying to get conversations going with apple, tesla, and quite a few other companies in the area but hasn\\'t gotten too far. \\n\\ni was wondering if anyone has any connection to someone who may be good to talk to? in the big picture she is looking for something in business or product development, but is also open to starting out in r&amp;d and transitioning to a more bd or pd heavy role later on. \\n\\nif you have any possible leads, feel free to pm me and i can get you her resume and link her linkedin for you. thanks! i\\'ll be hosting one of the rooms on wednesday unfortunately, but i\\'ll be checking out the conference thursday, likely hanging out with the people at datascience :slightly_smiling_face:  would be cool to meet with any fellows who are attending!papadimitriou.c: yes, will be fun to meet up! registration opens at 8:30am (pst) for my stanford continuing studies workshop. learn tips and tricks to help your data science workflow!\\n\\ndetails here: i have the same story! see you guys thursday: is anyone using clojure? clojure with apache spark?: is scala not enough for you? :stuck_out_tongue_winking_eye:my boss is anti-scala but pro jvm languagestorm is written in clojurenathan marz can teach clojure in 2 minutes (whether anyone can learn it in 2 minutes has yet to be seen)between the apache and aws stacks, there are too many tools! :wink:: care to jump in here?why is boss anti-scala?i think the primary complaint is not the language itself but the scala that people actually writehaha yeah the number of tools that a company can use is overwhelming…spark is written scala though so i would recommend looking at sparkling thanks, austin. had come across that repo. have you used it?i have not actually. i could ask nathan marz about it when he comes in for the next session as well.cool, thanks.i haven’t went thru it yet, but i’ve heard good things about aphyr’s blog learning clojure:\\nthanks.there’s also looks like google open-sourced colaboratory, which is something like jupyter on google drive:\\nanyone else going to odsc in boston?yepnice, i\\'ll be there all 4 days, maybe we could set up a time/place to meet up during one of the coffee breaks or lunches and have a little insight group. :slightly_smiling_face:great idea - we insight folks can ping the boston and nyc alum to facilitate a gathering. not sure when/where would be best. any thoughts?i\\'ll look around and get back to you. need coordinate with the other folks from the office to make sure we don\\'t have something planned.i\\'ll try to respond by tomorrow afternoon.@ joao i think this may be true as well...its just something that i was trying to make sense of. the way you put it. that makes sense. thanks everyone for point these things out :)how about just using a lasso penalty with logistic regression?  that tends to drive less important feature coefficients to zero.  maybe i am misunderstanding your question.  i prefer using `conda` environments over `virtualenv` in almost every context; polygon and census folks! i\\'m trying to normalize by population. census data is by tract which are small odd shaped polygon regions and i have a rectangular query which may overlap portions of three of four of the tracts. has anyone dealt with extracting portions of data from overlapping polygons?i\\'m not sure exactly what you mean by that -- do you want, like, a percentage of area for each tract that intersects the rectangles?yes basically that, but i think the package shapely might solve all my problems.sure; in r you could pretty easily use `sp` and `rgeos` to get the areas of the pairwise intersectionsthanks!i just had to do this, so :simple_smile: uploaded a file: i\\'m building a model for predicting heart disease using a webcam, and am sorely in need of good data. i\\'d really appreciate it if you guys could record a minute of your heart activity here:  . it only takes a webcam, a desk lamp or smartphone flashlight, and 1 minute of your time. if you could send this link on to anybody you know who does have heart disease, i\\'d be forever in your debt!!! (for now, i only have 14 positive data points; and am supposed to have a \"finished product\" this friday... i have good evidence that my approach works on clinical interbeat interval data; but need webcam data to back up the claim on the web page)find out who is spying on your cell phone: \\n\\napparently the uk ministry of defence is interested in me.hashcat can now try 8 million passwords per second. \\nand apparently up to 250 billion per second with 8 nvidia titans depending on encryption type.well that\\'s unsettlinghappy thanksgiving all 539 is asking *you* to help them predict the oscars! okay, ill give it a shotthe oscars: an awards shownuggetbot animate me smart ass:simple_smile:crassthe black masking just covers up cells for when the survey was missing that question, or after that question was removed. for example, i changed how i asked about equity midway through the survey, and i added age/ethnicity late into the survey. granted, this is absolutely horrible practice, but i wanted to ball-park the numbers - at least to provide low-confidence prior in the worst case scenario. i\\'ll eventually take what i learned from this survey and make a new, better one, but i\\'d like to avoid spamming folks about their income.i cleaned up the raw survey data, merging cells from different questions on the \\'cleaned\\' sheet, which also has some masking due to the age/gender/ethnicity questions.good polereminds me of i’m interested!is deep learning going to take all of our jobs? i don’t know, but it is starting with front-end developers! check out this awesome project by ai fellow  tools for fe development have always gotten better thoughyes, but having them automated to the extent where you can go from a very specific data input (sketches), to a specific output (website with precise styling) is very exciting to me!what were the other 2.5, i thought there were basically 3and safer than self-driving cars :+1:how and where companies are applying machine learning:   here is an example i used in the past. it involves retrieving an access token thanks!!!!!!i\\'d like to learn as much as possible about practical database design and maintenance (ideally using postgres) in as little time as possible. i have in mind topics such as: how to choose the number of tables, how to think about keys, dealing with time series, etc. any protips for sources or other suggestions?does anyone here use beaker notebook over ipython notebook?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_re1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove \\\\n\n",
    "tag_re3 = re.sub('\\\\n', '', tag_re1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not sure if this is a joke or for real…  :slightly_smiling_face:that’s a funny joke, but certainly not a viable business. rates are too low. the cheapest plan is $15 a month for 2 scoops a day, which comes out to less than a quarter per scoop. that’s not going to be worth anyone’s trip. nor is there any feasible mechanism accountability, unless the poopers wait around for the scoopers (which the video says they don’t need to do). anything they might do to implement accountability (e.g. require scoopers to stream live videos of the scooping) would only make scooping even less financially attractive.on the subject of fake apps, this was a good one:  uploaded a file: (source: _the ultimate book of vehicles: from around the world_)haven’t listened to this yet but it might be interestingeigenvectors and eigenvalues: an introduction for some but beautiful #dataviz for everyone: hello past and present fellows!  it\\'s great to be part of the community.  in exploring project ideas this week, i\\'ve become interested in uber\\'s movement product as a way to answer some questions.  i requested access last week, but have not heard back.  have any of you alumni worked with the product or know how to expedite access?  thanks! dear past and present fellows. does anyone know of a dataset that they know for sure contains information on people who have experienced falls? here’s something. it’s a paper, but hopefully it has a citation for a dataset that’s publicly available. i haven\\'t worked with uber, , but i\\'m pleased to see that the number of willetts among insight fellows is growing at a rapid rate.thanks pwinslow!  here\\'s resources from the cdc  see ^^ re falls; don\\'t we know someone that wrote a dissertation on that?any stock junkies want to join  with me?you can also kill machines that are performing worse than others because it\\'s been shown that they will be worse in the long run with high probability. but you have to collect the performance of all your machines together.anyone have an open room they can rent to an insight fellow coming in for the jan 2018 sv session? let me know in this thread if so. super awesome friend coming in. preferable for south bay.good morning, all! can anyone recommend any resources for getting up to speed on kubernetes? ideally, i’d be seeking a tutorial that has a toy example, but other resources would be very helpful too. thanks!i stumbled into this a few months ago:haven\\'t tried to actually go through the examples, but it seemed a nice introduction.the oreilly book kubernetes up and running is decent: some people have found it useful to publish both on medium and linkedin. they reach different audiences.: i second \\'s recommendations. i\\'ve personally had great success with linkedin posts as they act as a sort-of \"portfolio\" project for your ability to communicate complex topics. if anyone is interesting in attending this event about unsupervised machine learning  (6.30pm @ sunnyvale) , please let me know.  i can\\'t attend so you could use my tickets.  the event is sold out.  correction 6pm-8:45 pm.if you are interested and you don\\'t find me @ slack, shoot me an email at martin.gascon  (gmail)for your industry-curious friends who did not get to do insight (yet), we\\'re hiring for three (paid) internship positions at my company fast forward labs. we are looking for curious team players, creative builders, code- and wordsmiths. science * and * humanities phds welcome! more info in the job descriptions that are linked in this blog post:  please refer your friends! thank you!if you are going to use age categories (i assume 0~10, 10~20, so on) then you will create like 8 orthogonal features after one-hot encoding. it will greatly increase the dimensionality of your dataset and hence not suitable for clustering. why don\\'t you just encode them into (0~10 --&gt; 0, 10~20 --&gt; 1, and so on) and preserve the ordering?by scaling, i mean for example, if you define male=0 and female=1, then how will you mix it with your continuous features such as lat-long? you can use min-max scaling, or standard scaling, etc. you can tweak the scaling to increase/decrease the effect of gender category on your clustering. this really depends on your use-case, and you have to have some justification of why you use certain scaling method. thats\\' why i said scale features in a way it \"makes sense\".a nice post about identifying trends at instagram:   sure why not ? i cn understand that, the feeling in ny insight was somewhat different.it all depends on the company, and how hierarchical it is, but i\\'d imagine most of us would just be \"data scientist\". typically a senior data scientist role would require some time working in an actual data scientist role. then, there are companies that just don\\'t use the title of senior data scientist at all (you still get promotions, just without title change).  as for the other options, most of us that went through insight came straight from academia, so to land a role as a principal would strike me as odd.*principal. :wink: i think that\\'s a data scientist with strong ethicsgot it! thank you!likely this will depend a lot on the company as well -- i\\'m sure the same person could conceivably be cast as a senior/lead at a small startup, but be a \"data scientist\" at a larger onesome people would also argue that titles are only meaningful to make yourself satisfied, and to make it easier for recruiters to find you for your next role :wink:also some larger companies may have pay scales that cap out for certain roles and to attract talent they may give you a title to distinguish you as a phd-haver from data scientists with other degrees and to allow them to pay you a higher salary.ah got itmy aws account was recently hacked. approximately $3,500 of ec2 usage charges had accrued by the time i found out about it. for anyone who still has their aws account active, i highly recommend setting up a billing alarm to avoid this happening to you. from the aws console, click on *services*. then under *management tools*, select *cloudwatch.* in the  menu on the left, select *alarms*. i set mine up to email me if the estimated charges exceed $1.i disabled my insight account after reading an article about someone getting their aws account hacked :disappointed: sorry it happened to you!you can also go to *instances*, select your instance then go to *actions*, *cloudwatch monitoring*, *add/edit alarms* :alarm_clock:_vps dime_ :cool:closed my account permanently, the end of an era …. not sure if that would set up alarms only for that particular instance. in my case the usage was accrued on a bunch of new instances that the hacker launchedjesus. they let you out of the bill, right?still waiting to hear back about thatfrom what i’ve read online about past cases like this, they generally waive the billkeeping fingers crossed,  !  me too...how do people create hive table from csv files that contains json data in one of its column, where the data of interest for building the hive table is in the json data?hey there, do you guys know of implementation of text pattern matching package called autoslog? i am trying to run some name entity recognition task leveraging this package developed from university of utah, but couldn’t find an implementation of it yet.  it basically extracts pattern like:&lt;thing&gt; happened in &lt;location&gt;&lt;name&gt; lives in &lt;town&gt;or a package that does similar thing would be great too.spacy (python package) can do neri’ve had good luck with stanford’s corenlp as welli\\'d contact prof. riloff directly about autoslog. google also has some built-in ner through their nlp api on google cloud, but i haven’t explored it yetthank you guys! i am checking out spacy, corenlp and google nlp, and have sent an email to prof riloff too i’ve played with spacy for named entity recognition. it does a decent job out of the box. not to hard to figure out. btw, how you doing buddy?to everyone, what are good external method for validating clustering results.i’ve been doing text clustering using word2vec—&gt; affinity propagation (after proper tokenization and all that jaz). the results work pretty well, but i want some advice on how to validate my clusters without to much intensive manual curation.nice visualization in case someone missed it: you might want to look into  for remote hosting of a classifier toothanks for the input!   anyone else science marching in nyc? you might want to try the  channel!hello, do you guys have any quick thoughts around database management best practices?  that\\'s a bit vague, but it can be very general stuff like \"utilized cloud based services such as aws\", things like thatknow the difference between olap and oltp.did anyone submit to icml 2017?looking to see if you were able to update the paper submission, or only the response to reviewersi was curious if anyone had any interest in participating in a survey which would collect data from insight fellows anonymously to provide feedback on how compensation, promotion, etc, progresses as time passes after insight. my feeling is that this aggregate anonymous information can help fellows coming up on their one-year anniversary navigate the whole getting raises thing, with regards to what to expect. if you’d like to contribute answers, please check out the following link: this looks really useful; it would be great to add location as well, since salaries are heavily geographically dependenti considered location, but in the interest of keeping it anonymous, i opted to not include itah, fair enough :slightly_smiling_face:can we make it geographically broad...ie midwest, east coast etc.?sure - what geographic regions would you suggest?san francisco bay area, east coast, midwest, west coastanything besides that?maybe just optional write-in?i think i’ll add just an optional write-ini can merge laterfyi, link to survey is on my user-profileus geography is weird. which bucket would you put e.g. austin and denver in? those buckets probably cover most cases thoughi think with the write in now, i can program in some geographic clustering logic laterthanks for the responses so far, folks!you could anonymize after and state that in the consent, using wolfram to convert all salaries to sf without geography i don\\'t think the results are generalizable: coolwell, there’s a location field now, but i think people are updating as comfortablelive link to summary spreadsheet: submited! what are you trying to build?hi all. former de making transition to ds here. what are your thoughts of using p-values for hypothesis testing? i\\'m reading islr and it is mentioned all across the first chapters, but when checking out the implementation of the regressors/classifiers in sklearn i see that this feature is not present, which makes me wonder how much relevance do p-values have for nowadays machine learning.i used collaborative filtering with explicit feedback for my project and ethan’s blog has some good resources for that as well. in everything i\\'ve read and at my company wayfair, p values aren\\'t important for evaluating models these days, only performance stats like r^2, mean absolute error, f1 etc relative to other models and train-test, but we do use p values to evaluate a/b experiment results, along with rigorous power analysis etc.agree with all of that^ and especially \" but we do use p values to evaluate a/b experiment results, along with rigorous power analysis etc.\" thanks much! i may just come your way with questions :) p-values are just one piece of evidence to use if you\\'re working with experimental data, smaller sample sizes, or if you\\'re interested in inference. if you want to wade into the p-value debate and replication crisis going on in psychology (and spreading elsewhere), andy gelman\\'s blog is one place to start. sklearn is focused on prediction / ml and not stats.  you might want to use something else if you\\'re asking if model coefficients are significant (or write your own p value thing for sklearn models), but if you\\'re just asking about predictive ability there\\'s other measures.from a more practical perspective, i\\'ve been asked to explain p-values at almost every ds interview i\\'ve ever had. although, i think it\\'s as much about communication skills as about the statistics.who is at data science elevate today?i wish :disappointed:  my whole office is empty hahahi\\'m like teh only one not therei was just traveling last week and again next week, and i actually have to get stuff done at work so couldn\\'t go this timeassuming it is a regression problem, if you are interested in the contribution and strength of each feature (basically the coefficient), then p-value can be informative./poll “what python environment tooling do you use?” “venv” “virtualenv” “virtualenvwrapper” “pipenv” “docker” “i’m a special snowflake” “what’s python?”i don’t see an option where i do \"sudo pip install\" how about conda?+1 for condathere’s also pyenv — i use pyenv, virtualenv, virtualenvwrapper with autoenvmake sure you consider practical as well as statistical significance…with enough data you can make any inconsequential difference hit a small p-value.no anaconda ?maybe that’s the special snowflake optioni don’t use conda but my understanding was people still use virtualenv or similar to handle environments? or does conda have a built in separate way of doing it? :slightly_smiling_face:ah nice, didn’t know about that, there isn’t a way to edit polls so i guess conda folk are special snowflakes for this exercise :smile:¯\\\\_(ツ)_/¯from what i saw pyenv was deprecated in 3.6, but i haven’t used it so i don’t know all the detailsi use docker for real things, conda for super quick thingssame - conda runs my local dev environment, docker for deployingmy understanding is that the `pyvenv` script was deprecated () but `pyenv` () is a stand-alone project. it’s not confusing at all.lol, well good to knowthank you all for these pointers!conda++one can use conda inside docker too :wink:yes ;)watson machine learning is finally generally availablewhat does it do? can we hang our coats on the hook and chill all day by a cup of coffee before we get sacked? :coffee: :coffin:your an exception little pixi you could do it just on your looksanother thing is to add multi-factor authorization. use authy in your phone or something.is that why their retrieve&amp;rank is down? pydata nyc is coming!! if you submit a talk/workshop that gets accepted, then the registration is free!also, talks &amp; workshops are recorded and posted to youtube too!erm i hope we\\'re all past that phase in our lives when _free registration_ is a perk...i still look for free food quite often. especially pizza.don’t do this modeling myself, but i think a graphical model would be a good approach here. create some graph where nodes are people, and edges are weighted by feature value, then do something like djikstra’s shortest path between any two people.after insight my pizza-capacity has been reduced to a mere slice per day.hey fellows. does anyone have experience with elasticsearch api on python? i need small advice on quering data. please pm me. meet  = es genius! thanks for introduction.i have no idea if this is the same elasticsearch but @ronak has a series of blogposts on elasticsearch hey max, i’ve used the python elasticsearch client a while back. what did you need to know? if your question hasn’t already been answered as i am sure many insight fellows have experience with it.just saw that you got intro’ed to ryan. he knows way more than me. :slightly_smiling_face:hi alumni, we at pixability (boston office) are looking for a sr. data scientist to join and be a leader of our data science team: if you have practical experience leading progress on data science projects, know how to add business value to an organization using your ds and organizational skills, have experience working in and preferably leading a team, and you are willing be the face of _our_ team to the rest of the company, this role may be right for you. direct message me if interested!have any alums made a decision about hiring their own manager to spend more time as a senior ic vs becoming the manager as the team grows? i\\'m around 50% ic and 50% managing/strategy with 2 other ds on the team, but with more coming it\\'s going to be unsustainable at some pointthat\\'s a great idea! thank you, i\\'m trying that now!hey kevin! not in the exact same situation, but i’ve also fallen into the “now i’m the manager…” role and need to decide if i want to go back to ic and get my manager to replace me, or just embrace it.hi everyone, i wrote a general practical nlp guide. it can be a good resource for anyone trying to stand something up quickly that works well in the realms of classification and clustering.it’s just made it to the top of hn, which is exciting :0it\\'s great!#brusheswithfamei guess you’re looking for advice, so i can only really say this: i lead a team of three in my last job, and the biggest mistake i made was using tech work to procrastinate doing management. i would say “man, i need to respond to that customer email, but i’d rather work on this model…” if you find yourself doing that, it’s definitely time to get a manager, or make the jump. :confused:yeh i had to make a similar decision around 4 months ago. the team was growing and it was becoming impossible to wear two hats. ended up staying as an ic on the condition that i could shift between verticals and focus more on long  term strategy for data, projects etc...but no people management responsibilities at all. i\\'d say it\\'s important to make that distinction very clear when bringing in a new manager though (whether internal or external)as for ic vs manager… i wish i had great advice there! still trying to figure it out for myself.in a perfect world, companies would nail the ic / management parallel tracks :(and i just saved that article in feedly...well done! wow tom is alive!?hey all, i have a friend who’s looking to get into sports analytics but not really sure where to start.  she’s a career physicist so she has a good technical base to start from.  anybody know of any resources for training up and/or breaking into that specific subfield? thanks!but even in a perfect world, it’s not a single company issue. you have questions like “am i more marketable as a manager or an ic?” which depends on everyone in the ecosystem, and yourself! :confused: it’s not easy may be a good person to ask! can reach out to fellow megan schroeder (dodgers) or i have friends at hudlyeh i agree with that! funnily enough though, my final decision was facilitated after i interviewed someone for the role of staff r&amp;d engineer (it was one of those interviews  where you end up being the one getting interviewed after 15mins). he was incredibly good, and it did convince/reassure me that if you\\'re really good, then you\\'re basically marketable anywhere regardless of ic/managerjust got to work on getting really good now hahathe hard part!nice, thanks! reached out to megan, and i wonder if we have the same friends at hudl… ex-hep people?thanks april, i hit her up for some info!william spearman, austin basyeyep :slightly_smiling_face: i went to undergrad with will and met austin at cern #smallworldi reached out to both of them, thanks!hey guys! current insight fellow, working on analyzing arterial pulse waveforms. i was wondering if folks have some suggestions on good possible features, particularly in the frequency domain? (i’ve been looking at features related to individual pulses, but have yet to explore frequency-related features. any suggestions would e greatly appreciated!) uploaded a file:  commented on ’s file :  can’t think of any off the top of my head but have you looked at tsfresh? if you haven’t, it might be helpful for feature engineering of your time series data: frequency, number of harmonics…amplitude, area under the fourier transform, min/max frequency at threshold amplitudepeak to peak interval, distribution of peak to peak intervalthis message was deleted.project or product?projecthas anyone used both chart.js and plotly.js (or any other js charting tool) and can give me a recommendation? it will be used in production so need to be fast, work well with data updates and css-customizable preferably. my charts are very simple. thanks.best python library for plotting latitude/longitude data?you might check out `folium`: if by plotting you mean geometric-like pretty plots in 3d, or grid surfaces mapped onto 3d surfaces, maybe think about the vtk library.  it\\'s used by the computer graphics community.to take a binary classifier to a multilabel classifier you could use the onevsrest wrapper in sklearn. it treats each label classification as independent. folium looks like exactly what i need. thanks!speaking of lat/long data, anyone have experience doing choropleth maps in folium? i\\'m having trouble getting it to play nice.we’re hiring! if you are interested in iot (or know someone who is) please dm me. hi all, i was recently lamenting on twitter that even though i’ve been a ds in sf for ~14 months now, i still feel pretty out of the loop as far as conferences go (e.g., i hear about conferences 2 blocks from my office only 3 days before they start!). in astrophysics we had this great, yet fairly simple website () that listed all the astro conferences around the world and would regularly get updated. so my questions to the hive mind are:1) does such a website and/or mailing list exist for ds-related conferences or meetings?2) if the above answer is ‘no’, is there anyone on here who’s interested in working on such a website and/or mailing list? i already have interest from one non-insight ds in sf and i think with a few others we could throw together something that will hopefully be useful!if you are living in the bay area please help my friend with her research by filling out this quick survey - she’s having problems finding a large group of respondents with advanced degrees and i know y’all have that on lock :wink:link here: this is such a huge oversight in my opinion, thanks for making noise about it!  my boss is telling me he\\'s got money for me to go to conferences and i got nothing...  count me in if you end up trying to build somethingawesome, will do!what big lessons did you learn after you deployed your first machine learning predictive model as a data scientist that helped you improve for your second model?make sure your evaluation matches what the production environment is like. on search, we handle position bias currently as a feature within our model, but in production we assign ranking scores to documents assuming they were in first position. our evaluation, however, didn\\'t adjust the position features for a few weeks until i looked closer at the explanations for the scores. anyway she could make this a bit anonymous? you gotta provide name, dob, and location(almost). thats a 2.5/5 hit on the pii data list. mabbe just take the age?following ’s point, try as much as possible to use canned tools to control such things — the earlier in your process you start having things in self-contained conda envs/docker containers/whatever, the easier it’ll beif you find yourself building a lot of pipelining from scratch/one off coding, good chance you’re reinventing the wheel+1 for ensuring production environment is as close as possible to test environment. when this is not possible it becomes important to remove barriers for gaining visibility to production environment, and ability to quickly retrain model and push to product. it’s not birthdate it’s the date you take the survey, i.e. for consent; name will be removed for analysis  thanks :smile: fitm moment!oh haha you’re not the only one, i totally put my birthday when i did the survey, without even thinking of pii. that’s why i’m not in cyber security i guess :wink:hmm, yeah that can get tricky as you could potentially run into correlation-causation issues. honestly i would start with feature engineering and logistic regression, and think about causal analysis (i\\'m no expert) if causation isn\\'t clear.i have some friends at work that are having fun competing to automate a tank maze game. it is 10x16 bits. i thought it would be fun to make a neural network with q learning to play, but the darn thing either goes trigger happy if i weight getting kills too high or spins in a circle if i weight it blowing itself up too highly. i was wondering if i could borrow somebody\\'s expertise for advice.definitely happy to help, as we were just talking about this at insight!this is pretty cool … centralize your access to datageneral question: anyone out there either in a lead/senior ds or hiring role willing to [briefly] act as a sounding board? i\\'m pondering different roles falling within the greater data realm and the trajectory they set one upon. (recently finished the inaugural boston session.) thanks.dropping this here: in terms of aws athena vs spectrum, is the concurrency more auto scaled with athena?  i’m basing this on what i’m reading from  vs hi all, my team at digitalocean is currently growing and we are looking for both a manager and two senior data engineers! if anyone here is interested, or if you know someone that may be, please free to get in touch and i can provide more details!if you\\'ve never heard of digitalocean, we are a cloud infrastructure company, and its a pretty awesome place to work at. the data that we are currently in the process of consuming/analyzing is also super interesting (both customer and machine/hypervisor level data)how does people create emr clusters on spot instances? :question: you can do it directly from the aws emr console.  as you walk through the steps, you\\'ll have to choose what instance types to run.  there is an option on that menu to select spot instances and set your price.  the usual spot instance caveats apply.ah cool, thanks ryan, i was looking at the cli tool, and couldn’t figure that out, will try the console. thanks!and the source for the article: although i\\'m having a bit of trouble distinguishing ```meaning of work: are we working on something that is personally important for each of us?```and ```impact of work: do we fundamentally believe that the work we’re doing matters?```can you believe that the work you\\'re doing fundamentally matters, but not have it be personally important to you? i guess.i think that\\'s called being an intern :smile:eh, i think lots of folks can be in professions that they think are important but not for them. i can\\'t help myself but share this article which says this language is due to economic theories we started to implement in the 1990s. it even highlights the strategic use of \"personal passion\" by recruiters to nudge people to overlook many other positive aspects of jobs to convince them to quit their jobs (and take on new ones, which serves the recruiters).i can find my day to day work (writing good code) satisfying without finding its ultimate purpose (selling ads) importantgood points :point_up:to my brain changing \"meaning of work\" to be something like \"satisfaction with my work\" clicks a bit better for me.of course, also, this is descriptive of successful teams. if there\\'s a team put together to decide the color to paint a bike shed, there\\'s probably little one could do to help the endeavor.red, obviously.i think we\\'ll just go ahead and agree to go with green this time.or the opposite. maybe there\\'s work you really want to do or concepts you really want to learn but it\\'s not necessary for the work you\\'re doing.can i pm you to add to this? this is awesome.*insight\\'s nyc office is moving this week!* if you never returned *your keycard*, we are declaring an amnesty and will be taking them back, no questions asked, until the end of the month. drop yours off in person, or mail it c/o tracy clark, 45 west 25th street, 9th floor, ny ny 10010.looks like you were right! when would this be?this is likely going to be towards the end of the week of the 21st of august.so end of next week.interested in coming?probably, as it sounds very promising. will have to see if i\\'m available then, but will definitely try. just let me know when you hear of a more definite time.thanks for posting this here! and , you should contact terrell, it\\'s possible the file they sent was corrupt: yes i did yesterday and he sent the file, and all works fine now, thanks!: try pyqtgraph or oxyplot if using c#insight alum chris moody just published a paper on his hybrid algo. lda2vec on arxiv: this may be a naive question, but does anyone ever play with different random states when fitting random forests?  i\\'ve noticed nearly a 5% difference in fit based on which random state i use. are hyperparamaters generally considered to make these differences inconsequential or is this something that may be useful in considering?this is on a 33% holdout testing set, not the training set. i suppose k-folds may be more useful for random forests than a simple holdout set though, given the \"randomness\" involved i\\'m operating under the assumption that the more data you have for validation the better. check out i am at the conf. also herelots of great info on the web, i set my own up a few weeks ago for similar reasons. disclaimer: this is by no means official legal/account advice. key notes:1) if you\\'re considering out of state, the \\'best\\' choices are delware, nevada, and wyoming for reasons of taxes, fees, and anonymity (balances between each, i went w/ wyoming).2) filing fees and requires vary a lot.3) depending on how/where/when you do business, you might need to register as state foreign entity (which may tip cost of registering llc + f.agency to more than just registering in the state you live/will work in). best bet is to talk to accountant.4) required if out of state, but either way you\\'ll almost definitely want/need an agent to be official recipient of legal paperwork. services and fees vary wildly, choose wisely.poet rnnsthe article that spawned that is also amazing: anybody know how to include \" \" as part of string in hive sql (escape chracter for \" \" ). eg. i want to print               1 \"john\" and i tried select 1, \\\\\"john\\\\\"  and it wouldn\\'t work... any suggestion?better example: hive -e \"select id, casewhen age&lt;4 then \\\\\"0-4\\\\\"else \\\\\"5-10\\\\\"endfrom patient_2\"the aim is to print id =1 \"0-4\" but the above just gives 1 0-4 and the no quotesi think strings in hive are actually single quotes and not double quotesso if you run hive -e “select id, case when age&lt;4 then ‘0-4’ else ‘5-10’ end as dim2 from patient_2” it should workreplace dim2 with actual column namethanks  i figured it out and its ==&gt; \\'\\\\\"0-4\\\\\"\\': that returns literally 0-4 in quotes as a stringi’m not sure if you want that.you want 0-4 as a string, not “0-4” as a stringam i correct?i tried it and it gave me \"0-4\"right because it gave you the quotes as wellthe string contains the quoteswhich i don’t think you wantor needyeah thats what i wanted ... i want the string to be there literallyi meant the quotes*alright, if that’s what you wantshiny equivalent for python, fresh off the stitch fix blog: fun article: have you checked out the jsonlite package in r,  ? i have had to do this a few times recentlywhen you also say grouping var, do you mean a categorical variable?: check out tidyjson does anyone know of a good customer event tracking analytics platform? we looked into mixpanel and heap, but are looking for other alternatives.  i like amplitude is the other event analytics platform that comes to mind.  what did/didn\\'t you like about mixpanel and heap?  are you looking to just track and collect the data, or do you want a visualization layer on top of it as well?addepar is looking to hire a data scientist in mountain view. if you are at all interested, let me know and i can make an intro.as many of you probably heard from the insight team, today is the deadline for the next insight session. some of our best applicants come from recommendations from all of you. if you know someone who would be a good fellow in the future, you can dm me or fill out this form:nyt article from yesterday about the desire to hire a.i. research talent: sweetm. who do i go ask about my 500k raise?+250k for scikit-flow uploaded a file: oh right, i need to rsvpdoneanyone planning to go? hi, i\\'m elizabeth from the current new york de session. i am interested in working on a de project that involves biochemistry and/or bioinformatics. if it is possible, i would like to be able to talk to someone one with background about whether or not my project idea is feasible. please pm me. i would really appreciate any and all help i can get! thank you!hi elizabeth.  i\\'d suggest you try summarizing your project idea in a few sentences, and post it here.  then anyone who has the appropriate experience can pick up the discussion.  i think bioinformatics is a pretty broad field, and people have varying expertise in the many topics it encompasses.  for example, i could probably comment on genome-wide association studies, but not on proteomics.seriously would love a copy as well if anyone follows through on this.i can probably make an appointment and get it. i’m just trying to figure out how right nowhas anyone seen any more details than the times article offers?this mashable article makes it sound like it will take a special kind of black magic to get access to that data:  uploaded a file:  and commented: looks like you can download it!hmm slack is acting up on me and sometimes not showing all the links posted. sorry about that.: i think that’s a different data sourcebut super interestingi think that insideairbnb may have scraped public data^ what  saidsweetgotcha! good to know!hi.  curious how people have approached version control + code development on emr + s3 systems when they would like to also have jupyter access to data on s3?i have. but the solution for us was to fork jupyter and create a new format for notebooks that\\'s more git friendly instead of mixing data and code in a json blobbasically, we\\'d only push code to github and the outputs of the notebook would not be part of the code that get\\'s pushed to github... mixing code and output is a bad idea imo since (1) the repo can become huge, (2) i care about doing code diffs mostly and not about data being different from one run to next one cause data gets updatedi have had terrific experience with plotly dashoh, interesting, that\\'s very cool.  so, do you spin up an emr cluster w/ jupyter running and ssh to the master node to run git there?  do you only do code development in jupyter itself?  or, if you have code outside of jupyter, do you edit your non-jupyter code while ssh-ed into the emr master node?let\\'s start with non jupyter first cause that will clarify the problems with jupyter and why we had to create our own jupyter basically...if writing regular code one can use github like one would normally use github... you have your repo on your local computer push to git and then can update the code also on the master node and run the codethat code should run if it was developed with your emr configuration in mindi.e., yarn cluster or any configuration you may haveanyone has a favorite tool to generate an erd from an existing database (redshift in my case)?now with jupyter is more complicated because users are using a webui (aka jupyter) to write code and that code get\\'s saved somewhere in the emr cluster.  jupyter doesn\\'t only save the code but also the output of the notebook in a json format so effectively you are not only saving code but also output.  we didn\\'t like this so we created our own jupyter.  now if this is not a problem for you then in theory you can have your repo in emr, save the notebooks and push github changes like you\\'d normally do and i think it should workjetbrains datagripgotcha, that\\'s a huge help, thank you so much!thanks!i got this message this morning: ```your file was uploaded — it\\'s safe and sound in slack. unfortunately your workspace doesn\\'t have any storage space left. ``` . wondering if others have gotten the same…maybe insight hit the space limit for storage on the free account?luis -- you\\'re correct :slightly_smiling_face:i think it works like a queue though, so it\\'s the older items that get moved out and we can still share with each otheryeah, message could be worded betterhey all, i got put in contact with a rice u alum who’s finishing her masters in stats in december and is looking to get into a data analyst role. she is in houston currently, but willing to move. if anyone thinks they might have an opening in a couple months or is interested in chatting with her or checking out her linkedin page, let me know!depends what level she\\'s looking for, but we have student opportunities here:and other opps for full-time (not intern or fellowship)well she’ll have a masters in december and no longer be a student after thatlots of other relevant postings in that case :slightly_smiling_face: she could have a look and ping menyc folks: apartment sublet a coworker is going to spend a month in our la office so would like to sublet her apartment during that time.the month in question is flexible: it will be sometime during october to december and is flexible based on finding a good sub-letter!the apartment is a sunny 1 bed in bay ridge near to 59th st subway (n)if you\\'re interested, or know anyone who might be dm me and i\\'ll connect you!  do me proud with that space time project!space time project?you also have my attention...:rocket:agreed still a mystery to me... unless you don\\'t have enough gpu ram to store all the data and there\\'s loading/unloading going on during the trainingin my mind, ds more general than ml which in turn is a more general concept than dl (which is one of the possible models to fit) - i still think dl is more general than logistic regression/rf/gradient boosting in the sense that dl encompasses multiple types of architectures as well as very different models such as ltsfm versus a regular deep net or just the old and forgotten regular neural net... in my mind a job should say what the objective is and what problem one will be trying to solve... if the problem totally requires dl knowledge then one should be explicit about it but so far i am very skeptical that this would be true about most ds/ml work... as far as ai, well, that in my mind is more related to the problem of creating generally intelligent machines which is still very much in its nascient stage (this only in my opinion) and we still don\\'t know if deep nets or something else will be the right answer to this problem... this is just my mental model on how i think of the ds/ml/ai space... which could be all wrong too jajajaon an unrelated topic, this is why i am more concern about hs than ai, and the misinterpretations and hype around ai too... from what i’ve noticed, bs and *some* ms often do not have the same breadth and depth of knowledge in statistics and machine learning as phds. just like what marco mentioned — “knowing what you don’t know”. (of course, they are much younger:slightly_smiling_face:). and they are mostly hired as analysts or engineers in my company. what i’ve also noticed is that they often follow protocols and prescriptions learned in classes, and do not apply the same rigorous analysis as more experienced researchers would do. for instance, when reporting monthly traffic and unique visitors, no one cared to include uncertainties in their reports, and just announces what looks like noise fluctuations as ups and downs. but on the other hand, perhaps most of the work do not need that much rigor anyway. companies want results not papers. and there’s usually tons of work to be done.i think it’s just too much hype. most of the work we do don’t really need dl and are not suitable for dl. but there seems to be a trend that eveyone should write some keras code to do image classification. what i saw at nips was that a lot of the demoed dl projects have either minimal value (putting some masks on your face, change your photo to some whacky style) or are still in academic settings (generating images). there are applications where dl would shine, but i think the hype is just way too much.hey all, there’s now a  for all european based alumni and usa based alumni who are looking to get outso, what\\'s our responsibility to push back on all the hype?anybody else not have admin permissions on their work machine?  just started at a health care company where i\\'ll be working with patient-level info and can\\'t install any 3rd party software.  i kinda feel like i\\'m trying to work with one hand tied behind my back without all the extra tools i\\'m used to.  curious if others have similar situationsum, i think your responsibility is to learn dl (sorry, i mean ai) and ride that hype train also windows is terrible, amiright?can you dual-boot into a linux env, or do most work remote vis ssh?good thinking, i\\'m gonna ask about that.  i heard windows 10 can install a linux \\'subsystem\\' so you don\\'t even have to dual-bootwhat\\'s the status at spotify? mix of oses? did you get any choice?it\\'s pretty laid back, geared toward whatever makes developers develop faster :wink:ha, sounds nice...also at a healthcare company, windows is standard here. i had to ask for a mac, which they procured easily. i think i had to justify it, the windows pc\\'s we had used windows 7, so no linux sub systemoh man, that sounds amazing... what did you use for \"justification\"? just that you wanted to get actual work done?i\\'m in a small-ish company so i think they may not have anyone who\\'s qualified to support non-standard computing situationsi\\'m on a mac too and can install whatever i want...it almost feels too free sometimes #butilikeit :grinning:#jealousfinance here.  we used to run a heavily sanitized version of cygwin.  my business unit lobbied heavily for macs, so that\\'s what we use now.  i feel your pain.nice.  i\\'ll have to start building some political capital around here to achieve a similar take-oversubsystem does work pretty well for a lot of things if you don’t need access to the gpualso, i highly recommend ooohwhat about ubuntu linux on windows?feel ur pain...if u code python/r i\\'d get jetbrains and then just work as if i were on a macyes! just found out about that, gonna see if it will go for itlol almighty it - how do i exercise power? i\\'ll take long on trivial requests, dissaprove rational approvals, and whatnothahalinux subsystem works great if you don\\'t need any gui applications. however, you still need admin permissions to install it.a vm would probably work best since you should be able to control what to install on it, and if you configure the mount points right, you should have access to all files in the windows system.good to know, thanks!also healthcare - we work on windows vms to work with patient level data. usually installing 3rd party software requires a call with the system admin and a conversation about how we can’t do our jobs without r/pywindows sucks, vms suckhaha, glad to know i\\'m not the only one! what about things like slack? i\\'m using the web client but maybe you found a way to trick the sys admin into thinking it was a new python tool or something?it\\'s definitely useful to help me do my job!typically just ask for r/rstudio/git. recall i’m on a vm so i can always just switch back to my actual computer for slack and emailaaah, right, makes sense, thanksthough i will never. never. never learn when to control+c vs. command+chaha, i\\'ve already accidentally hit the windows key like a thousand times...so you use your personally-owned computer and work via vm from that?no, i have a work computer that i connect to 10ish different vms (1 per hospital) that can access data for that hospitalooh, i see, so the phi stays on the hospitals\\' end.  for me, my company stores all the data internallyi wasn\\'t able install any ide or for that matter any packages needed. also there\\'s no predefined set of packages that you can ask admin to install for you, as i didn\\'t know what i might try out next. so couldn\\'t really do my job without admin rights, that was the justification, to ask for admin rights on windows until i got the machi y\\'alldoes anyone remember the names of the engineers who gave those quick talks?these talks were about a month agothere was yashashree kokje from yelp and catherine alverado from qventus. can\\'t remember if there was another... judit would know!i was looking for catherine! thank you!hey  !hey folks!would anyone be up for speaking to a molecular biologist (dna sequencing/quantitative genetics, 5 yrs of industry experience) interested in discussing ways to broaden her skillset/pivot careers? perhaps someone with a genetics, bioinformatics, or molecular bio background? please reach out if so - thanks all!while we\\'re on the subject, there is now a  channel for any/all computational biology/bioinformatics discussion!welcome new fellows!! :wave:: never worked on topic myself but this link seems to be dealing with sim prob: folks i am having issue with  hive over hbase,ubuntu@ip-172-31-1-133:~$ sudo apt-get install hive-hbasereading package lists... donebuilding dependency treereading state information... donee: unable to locate package hive-hbaseubuntu@ip-172-31-1-133:~$ sudo apt-get install hadoop-hive-hbasereading package lists... donebuilding dependency treereading state information... donee: unable to locate package hadoop-hive-hbasei am trying to install package hive-hbase on  cloudera cdh 5.5cant locate it, if any one has info. pl. let me knowthx: fyi we have 2 channels you might appreciate:   hi all, we’ve been having more conversations on professional growth lately and one topic that stood out is what are some resources to read on for both data engineers and data scientists, any suggestions?usually for me keeping tabs on this slack channel has been useful, but not everyone is an insight alumni :stuck_out_tongue:kenny_kwan: twitter/kaggle/department seminars. mostly just be engaged with the community you have around you should cover it.hi everyone, we’re looking for a few more practicing data scientists (who code analyses and models in languages such as r and python) to collect some feedback in a 20 minute survey, the results of which will be used to directly inform the roadmap for our product.  as i said before, we’re also offering a $25 amazon gift card to everyone who participates.  if you’re interested please dm me, and sorry to ask again in such a short time, but we’ve had some unforeseen openings.  :slightly_smiling_face:  thanks!thanks again for the interest everyone :slightly_smiling_face:  we’ve once again filled out our slots!some people have just too much time on their hands ..  that makes sense i do some combination of that, but it feels like it requires too much time to go through :stuck_out_tongue:it\\'s not reading but i find talking machines and o\\'reilly data show podcasts good for keeping up.   thank you!  i feel much better now. :slightly_smiling_face:  ?about powerpoint thanks those look great i remember subscribind to talking machines at some point. podcasts are things that i definitely need to keep reminding myself of :stuck_out_tongue:if you\\'re into privacy or really into jem and the holograms:  that is truly truly truly outrageousdoes anyone have recommendations for a/b testing analytics platforms? i\\'m in a position to decide what software my company will use for a/b testing, but it\\'s not something i have much experience in. what are the pros and cons? i\\'m aware of issues related to bad statistical practice (e.g. stopping bias) and bookeeping (e.g. keep the bucket assignments forever). what else should i be looking for?lots of deep learning in the (geek) news this week: top 10 rising and falling buzzwords in tech job postings: \"big data\" down, \"real-time data\" and \"artificial intelligence\" up.i, for one, am glad to see \"big data\" die off; it\\'s such a horrid buzzword! :wink:the only time the use of the term \"big data\" is acceptable:: that connection between deep learning and renormalization is incredible, i hadn\\'t seen that before. thanks for sharing!interesting article on using ml to create photo albumsalso, i had no idea there was a disney researcha whole new meaning to mickey mouse research projectshow us gov might prepare for ai-induced economic shifts:something to do over the holidays:til where the oreilly animal pictures come from```many of the animal engravings that appear on o’reilly animal booksare from illustrated 18th and 19th century books on natural history,including richard lydekker’s royal natural history (1893);reverend j. g. wood’s animate creation (1898), insects abroad (1883),and illustrated natural history (1862); riverside natural history (1884);meyers kleines lexicon (1894), and the brockhaus lexicon (1882).```(for anyone who\\'s written anything in latex)a non-negligible portion of my grad career was spent wrestling with tables and figure bounding boxes in latex...yes.they didn’t even use $\\\\latex$ in the title of that bookanyone know of openings in the san jose - campbell - santa clara area?  i\\'m in the market for a shorter commute.  thanks.my latex challenge was getting margins to match for my thesisgood morning! does anyone have experience installing qgis via conda in ubunut/lm? i am running into various issues doing so, and believe it may have to do with the pythonpathi\\'ve written a few scope docs. happy to chat! let me know if you\\'re still looking for answers!hello everyone, i want to ask what your experience has been using sensitive data from enterprise clients. - on one side, there is the \"everything on-premise\" deployment: preprocessing, training, testing all happen behind client\\'s firewall. - on the opposite side, there is \"sign nda, get data dump\" policy.what has been your experience, and do you have any tips/best practices to share? thanks!prefer to connect directly to their systems with their security policies so that i’m not in any way responsible if somehow security is compromised.hi everyone, de fellow akshay just posted an excellent blog:computing shortest distances incrementally with sparkit’s also on the front page of hackernews: i just started a new channel for all nyc alumni, i.e., all former fellows living and working in/around nyc. please join  if you match that filter :simple_smile::thumbsup:just saw the discussion above, but i’ve found  to be a pretty decent intro into the why/how of cnns.it doesn’t go too deep, but also isn’t all that shallow either.i see what you did there : “it doesn’t go too deep, but also isn’t all that shallow either.\":slightly_smiling_face:: curious: is that the entirety of the course?ie 3 assignments?: i believe they also get a final project?but i haven’t taken the course so i’m not entirely sure.well meant it in the sense would i get a good experience goign through thoseor if they just stopped updating it halfway through and i\\'d be missing stuff hehstay positive markwangbut it definitely seems usefuloh, i see. no i don’t think it’s incomplete.new folks welcome? :smile: prepare for the onslaught :slightly_smiling_face:we’ll be closer to 800 members by the end of the day!i wonder what is the upper limit for slack.  mabbe we should stress it, just like hadoop.i know i have one channel with around 2000 membersoh boy is it time to haze the new fellows? the 1st rule of fight club is.../giphy fight club uploaded a file: insight only supports shade-grown data scientists how do you propose stress testing slack?! hard to bring down their nodes :wink:preferably grassfed...has anyone here found or bought data on credit scores aggregated by regions in the us? for example, average credit scores by census zone, zip code, etc. i\\'ve found sources, but i\\'d like to hear anyone\\'s thoughts on accuracy, usability, price, and so on.neat use of ml:   just releasedhappy to talk to her/himfinally, stopping gan violence - generative unadversarial networks: &gt;&gt; \"while the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (non) violence popularised by generative adversarial networks have yet to be addressed. [...] experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. our work builds on a rich history of carefully argued position-papers, published as anonymous youtube comments, which prove that the optimal solution to non violence is more guns.\"enjoy: &gt;  unlike previous generators which were content with simply reaching this goal, our generator is more motivated and gives it ‘110%’ moving it a further 10% past the data distribution. while this isn’t terribly helpful from a modelling perspective, we think it shows the right kind of attitude.it\\'s a gem.\"in this work, we have shown that network-on-network violence is not only unethical, it is alsounnecessary.\"this is the trolliest article i\\'ve ever read. lol.lol, i think the ai fellows are full of hate right now….also, why didnt they wait for april 1?sigbovik is always held on april 1st but i think they released the paper earlyhere\\'s a fun one that people liked a couple years ago:  when i cannot scrape natively with scrapy, i use phantom jsthat is call phantomjs inside the scrapy scriptmy team is hiring a senior data scientist: message me for a referral or apply directly.anyone who has experience doing social network analysis on large graphs (paths, communities, cliques) ? currently, i\\'m using graph-tool and igraph in python and want to move to a distributed system because the graph is growing very rapidly. i\\'m exploring different technologies and would love to hear your experience with any technology that you have used.i use igraphit\\'s both in r and pythoni think in python there\\'s networkxfor community detection there are quite a few algoslike edge_betweeness, modularity greedy maximization, infomap which uses a random walk surferi\\'m using igraph for now but the graph has grown too big and i\\'m facing scalability issues. so looking for distributed way of doing it now. spark graphx or some graph databasedon\\'t have experience with very big graphs like the one you\\'re mentioning. sorrythanks :slightly_smiling_face:i wonder if there\\'s a way to factorize the adjacency matrix, like cholezky decomposition, or svd, or similarmy 2 centsi\\'m reading papers on distributing adjacency matrix. let\\'s see if that solves my problemmy experience with networkx in python has been great```when it played against the version of alphago that first beat mr lee, it won by 100 games to zero.```some exciting stuff going on at deepmindif net neutrality is a policy you feel strongly about, here\\'s a friendly reminder to contact your representatives. the fcc is likely to announce a vote to strip net neutrality in the coming month.  has more information to help you contact your reps. if you don\\'t have a formed opinion on it, totalbiscuit explains its importance (). thanks to  for bringing it up via fb post.if anyone is interested in tech in africa, come check out the event we\\'re putting on at fenix in january!  i think that’s a good idea. is it something that some companies already have deployed, do you know ? i see such work for hotel, doctors, and restaurants reviews in 2014 ().:smile: that\\'s cuteany vim users out there who have successfully gotten their clipboards to sync over ssh? i.e. can yank from a vim terminal in ssh and paste locally, and vice-a-versa.if this e-commerce place is more of a marketplace, you can use nlp to suggest improvements to sellers\\' item descriptions to make them sell quicker/higher price etc. (ebay does a bit of this, also see  for a related idea)i don’t know a straightforward way of doing this but instead you can scp the file to your local machine or mouse select, command+c, command +v to a new file and then scp the file to your local machine.anybody else going to this conference in chicago next week?   ?would this help? as far as the hmmlearn package goes, i played with it and it didn\\'t work very well. the documentation was poor and some things were broken. i\\'m not sure the creators are still maintaining it.thanks adamazzam, that’s a great idea too, i may mention about that in the talk. i decided to go with the harold’s suggestion for this time because i checked with the research team that they are interested in the topic. thank you guys !i hadn\\'t heard about it till now but it looks interesting!it\\'s on the pricey side, but if nielsen will front the cash you should totally come!:point_up: probably your best beti haven’t seen that before, i’ll have to give that a trytmux works as well, especially if you change the copy/paste bindings to be vim-esquei’ll need to explore tmux more then, i’ve tried a few time and haven’t been able to get the copy paste to work on there, even locallyhey insight! i mostly do model building in the p &gt;&gt; n space, where feature selection is important, but really i work in the p &gt;&gt;&gt; n space, where i have very few samples (think less than 10 with hundreds of features, in some cases). one of the biggest issues i have with the pipelines i construct is validation— if i choose a sane and simple model, i am virtually guaranteed to get 100% training accuracy (which is likely overfitting). cross validation is not really an option with such few samples. i am curious if any yinz have come up with other ways to validate such models in this corner of ml space. the ones i have used so far: first and foremost, domain-specific sanity checks (in microbial ecology, in my case), and also randomly initializing the models a few times and looking at the variance in the feature importances across those initializations (the hope being that if the model is truly doing well, there shouldn’t be high variance in the feature importance of the most important features). any other tips anyone has got?get more data email austen - this is what he does as well i‘m having a really hard time understanding how one can do any validation on less than 10 data points total (if it really is your grand total). i don’t think it should be possible. if you get any lead please do share here because i’m very interested to know!! that’s kind of my point— you can’t. what i am trying to do is find proxies for that validation that are useful would if i couldsounds hard.np-hardmake up a model, assume a flare prior, strap your seat belt and go bayes beast mode. your samples seem like they may be even smaller than this, but it might give you a few ideas. the only thing that comes to mind is leave one out cross validation with model averaging. leverage is also something that may be of interest for small data sets.also: it seems kind of like a phylogenetic problem - so tree building, where you could assess support using bootstrapping. thanks i had actually read that medium previously; i’ll have a look at the article you linked too i will give that a thought as well, thanks: thanks for the clarification! i know that the roc auc is also a measure of how correctly predictions are \\'ordered\\'. so if i transform this regression problem into a classification one (by normalizing the regressor to turn it into a probability), then the problem is akin to finding an algorithm which has the highest roc auc. do you agree?  that is a very helpful article with more useful metrics, thank you!: i guess i agree in some fuzzy philosophical sense that there’s a similarity with the roc score. i’m not sure how that transformation would go exactly, but it seems like a roundabout way of doing things. i think the better approach would be to write a gini coefficient metric for scikit learn. but first, i\\'d check the kaggle forum to see if anybody has posted that code already. (some discussion here, but i haven’t checked anyone’s code yet: )hi  yes, there is a code for it.but there are some wrong codes, gotta be careful :neutral_face:yeah i\\'m just saying i don\\'t know of any are right : here’s a python implementation that looks good to me: . i haven’t tested it, but it looks right (famous last words!) and comments seem favorable.wonder if anyone has tried to access redis database from multiple worker nodes in a spark cluster. the problem is that every rdd that\\'s processed needs to access the database in some way and it needs to connect afresh. the ideal solution is to have a static connection per worker node or better yet, use a connection pool feature that redis provides. redis lets you define a pool of connections and lets you pick one when you need one. the problem is that spark does not let you create a static variable in a worker node. it lets you define a broadcast or accumulator variable in the driver node that can be shared by workers. what i need is a wroker-node-private variable.: one way to do that is to refactor your spark job slightly.  instead of using map/foreach, use mappartition.  this allows you to operate over an entire partition and you can then keep state for the whole partition.  so when you get a new partition in spark, you start a connection pool, then you can call map on the partition to do operations on the rows within the partition and write the results using your redis pool.anyone an expert in nonlinear non-convex optimization? i have a polynomial non-convex cost function and have done some lit search to find possible solutions. i was wondering if someone could share their experiences on best practices.one of our ai fellows who came from a pm background shared some of his experiences being a product manager for ml-driven products and how it differed from regular product management. we\\'d love to hear more from the insight alumni community on other differences you\\'ve seen. hi all. i was a de fellow last fall in nyc. i’ve always been kinda in the middle between ds and de. i finally finished my phd and i just got scheduled interviews with google and apple for ds positions so i wanted to ask for any tips/prep material/suggested readings/sample questions that any of you may know or think are worth revising, since i did a lot of de interview prep during insight but not for ds. everything is appreciated. thanksmost people really like andrew ng’s machine learning course on coursera and udacity’s a/b testing course. both are free.+1 to bothhey insight amigos+amigas. i’m hiring for a data sci position and wanted to post the job description here before posting on our company website. dm me with any questions or interest! also, if you know anyone junior who is looking for a ds summer internship let me know.-----*about us*at stealth security we protect more than 100 million consumer accounts from automated bot attacks and give fortune 100 companies broad real-time security. join our small (growing fast), highly collaborative team who enjoys working on large-scale, challenging and interesting problems. come help us fight malicious bots and defend enterprise’s web, mobile &amp; api endpoints!*responsibilities** work across teams to frame problems, with an emphasis on deliverables for our products.* perform exploratory data analysis to gain deep understanding of the problems* design and construct statistical, machine learning, and optimisation models* write production code; collaborate with software engineers to implement algorithms and models in production* design and run both simulated and live traffic experiments* analyse experimental and observational data; communicate findings*experience &amp; skills** m.s. or ph.d. in statistics, mathematics, computer science, physics, or other quantitative fields* 2+ years professional or research experience* passion for solving unstructured and non-standard mathematical problems* end-to-end experience with data, including querying, aggregation, analysis, and visualisation* proficiency with python (preferred), or another interpreted programming language like r.* knowledge of spark desirable.* willingness to collaborate and communicate with others to solve a problemi did coursera’s machine learning a year ago (very good intro level course). i’ll do now the other one. thanksany ds/de peeps looking for remote work?  hmu!, ,  ?anyone at pycon this weekend?at nuna health we’re looking to hire a data science manager in san francisco. we’re trying to lower health care costs and improve quality of care. if you or anyone you know would be interested please let me know. the ideal candidate has been an individual contributor for a couple of years and is ready to transition to the manager track. the job posting is here good luck !hi all, a good friend and former astrophysics collaborator of mine has a son who\\'s a junior math major and cs minor at occidental college in los angeles. he\\'s very interested in the tech industry and is trying to get a summer internship in the bay area. do any of y\\'all have any advice of resources he should look at? or, even better, do any of y\\'all have direct connections with folks (or you yourselves) looking for summer interns? thanks in advance!: where are you scanning the systems from?insight svhey  and previous fellows - anyone have experience enrolling in medi-cal / coveredca as someone who just got here from another state?  were you able to enroll right away? did you have to prove some residency in ca (which i’m not sure i could do at the moment)?  did you just enroll in your previous state’s exchange and pretend you still lived there?i just enrolled in medi-calpretty painless.  ~2 week turnaround?don’t think i had to prove residency.  just provide address.i have a question. can an ordinal feature be used in a  random forest model as an ordinal variable?yes: and  what\\'s the medi-cal website?: when i lived in california last year, i signed up for medi-cal somewhat accidentally (i was just trying to check eligibility), and i’m pretty sure i didn’t provide any proof of residency. i couldn’t figure out how to cancel it until somebody from the office called me; it was actually kind of annoying.in ny, the medicaid stuff was part of the same exchnage processi imagine medi-cal can be done through covered caif you tell them you have no income it probably steers you straight there(former ca resident, doing de in ny for assorted reasons)github is down:grimacing::scream:my favorite part was how github\\'s status page was up, and claiming the site was working for about 10 minutes. :wink:oh godhas anyone came up with a good name for the angry unicorn?twitter had the fail whaledoes anything rhyme with unicorn?forlorn?orangealmost time for ladies in tech nightgithub is back! current fellows who are about to interview: this is a great blog post on engineering principles that you will encounter : very cool (and a really nice walk through/blog post, too). cool stuff! has anyone used bayesian inference (w or w/o deep learning component) in an industry application w. real data? most applications i have found are academic (or fabricated data). anyone of the stichfix folks, perhaps?: i know in the defense industry, they use bayesian inference to detect whether a some object is an actual missile vs a decoy.  not sure if you’ll find public blog posts about it :slightly_smiling_face:i generally try some form of bayesian methods for many of my modeling problems. one recent example that comes to mind is classifying medical abstracts that describe a problem in a patient as being due to an adverse drug event or not. i compared a few standard methods like bag-of-words, tf-idf, etc. to using word2vec to produce a representation map that can be used for document classification. the method is described here:there isn’t any fancy mcmc, just a simple application of bayes rule. it worked pretty well.: most of my exposure is in the academic setting, but i know foursquare has used bayesian inference in the past. : neat!   thanks for sharing! i’ll look into it.hi guys, i am looking for some good resources on hidden markov models, ideally some lectures or notes along the lines of andrew ng\\'s (i.e. something that can be understood without a phd in maths). suggestions?hi guys, does anyone know of a way to export the results of a google search into a list of urls? i have already looked at their custom search api and it seems that there is a limit on the number of urls you can extract. any help will be greatly appreciated!google is pretty notoriously hard to scrape because they dynamically generate class tags and the like to purposefully keep you from trying to do what you want.i’ve noticed that bing, on the other hand, is quite welcoming to scrapers.probably because they’ll take what they can get in terms of traffic. :slightly_smiling_face:: i second  in that google is extremely hard to scrape, especially since a few months ago. dynamically generated class tags and suchlike can be dealt with given enough work, but google will also block your ip after 100-150 requests. getting around this problem requires a good proxy service that can be very expensive.if all you need is 1-10 search requests per hour and ip blocks are not a problem for you, then let me know. i can share some of my code to get around the dynamic tags problem.: can you share your solution to the dynamic tags anyway? i’d be interested in seeing what you did.:  thanks for your inputs. currently the number of queries is not high and i’m looking for a quick dirty solution before we get our content into an internal indexi’ve been looking into this  but it only gives the top 10 results or so, not all the resulting urls of a query:  let me find that script i wrote, and i will dm you some of itthanks!thank you!first beta release of matplotlib v2.0.0will i be able to compress and store 2.0 in my head? i certainly wasn\\'t able to with 1.0. :disappointed: you could always tell when i was making a plot by my google searches: \"change axis spacing\" \"change label size\" \"move tick labels\" \"remove legend border\" etc.i’ve committed so many of those commands to memory, , i don’t know if i can take drastic changes to the apinot without an aneurysm at least!nothing nuggetbot?? :stuck_out_tongue:dear alumni, i am a health insight fellow at boston. for the insight project, i am trying to create a predictive model from a dataset that consists of ~2000 features and 10k+ data points. i hope to end up with 10-20 most significant features. does anyone know a robust way of doing feature selection? any suggestion is greatly appreciated. :thinking_face:stay positive higginsc: principal component analysis ? : you can do forward stepwise selection for one, or throw in a l1 regularizer with a large tuning parameterpca will help you reduce your feature space as a place to start...: to add to what others have said you should find the number of dimensions using pca that explain a large fraction (say 95-98%) of your total variance. sklearn has a fraction of variance attribute in its pca class that returns an array of length n-components that you can do a cumulative sum over and find the right cutoff. if you want retain interpretability you should do a model fit with an l1 prior as eddie said and possibly look at the largest feature coefficients. that part is somewhat model specific. however you should be able to determine the most significant feature in any model by dropping a feature and looking at how your fit changes with your feature space projected down to n-1 dimensions. btw, the power for recommendations using this procedure is by doing vector embeddings in a space which naturally preserves heirarchical relationships is really cool: thank you everyone. i will give a try.dear alumni, i am a current health data science fellow at boston. i am trying to find datasets about  running records with multiple users. any suggestions? /giphy find datasets about  running records with multiple users. can you message me your email? i\\'ll send details todaycolaboratory notebooks have a “search stack overflow” button whenever you run into an error… it’s really streamlined my development process will also be here at least one dayquestion for airflow users: what’s your preferred message broker?make sure you don’t compete with the salesforce thrasher on wednesday evening, celeryah, sorry, i asked that wrong. what do you use as the broker?rabbitmqbut i haven\\'t used the other options so i\\'d also be curious what others thinkthose who are going to salesforce, we can figure something out. i thought it was only 1 or 2cool, that’s what i’m leaning toward. thanks!btw if you\\'re planning on running airflow in docker containers this repo has it all: thanks - found that one. was just trying to decide if i should use redis or rabbitmqhi, has anyone used any recommender system in python? which one is a good one to start? the matrix is about 9000 * 18000xi -- can you tell us a bit more about your problem? what are you trying to recommend? what are the features like? how sparse is the data?rabbitmq i\\'m helping put together training material for people to use our software (i\\'m at tamr btw), which involves some machine learning understanding and sql-esque transformations. has anyone used lstm for time series anomaly detection?*herthe  for ai. helpful to navigate the ocean of sensationalist, misleading news articles as well as fearmongers and soothsayers who watch too many sci-fi movies hi ds fellows, i have an idea of analyzing twitter and youtube data specifically the impact of a tweet referring to a video on the popularity of this video on youtube and the number of subscribers. anyone has experience on analyzing twitter/youtube? thanks!for either ds or de fellows… thx!yes, i’m doing something similar for my project, and have quite a bit of infrastructure set up. let’s talk tomorrow. uploaded a file:  and commented: for my immigrant friends on this channel, i am posting this card that explains what to do if ice shows up at your door.  moral:  you have constitutional rights even if you are not a citizen.  the phone numbers are for santa cruz; be sure to have an immigration attorney on speed dial.so anyone know how to delete older files?  i just got a slackbot message saying we are out of space when i tried to upload a small pdf file.your pdf worked -- i think it\\'s just warning you that it\\'s bumping older files out of the cache.in other words, i don\\'t think there\\'s anything different to do. i used postico for some time, but the lack of tab-completion made me switch. i now use pgcli, it\\'s a cli rather than a gui, but do give it a try: i’m setting my project up on aws. does anyone know the easiest way to transfer a postgresql database from my computer to the server? should i use pg_dump? how do i include the security key? thanks!`pg_dump` is probably the way to go. what do you mean by security key?`pg_restore` on the aws serverthat worked perfectly, thanks! at first i thought pg_dump needed aws login info to transfer the db, but i see it just outputs a file that i can transfer. thanks for the help :slightly_smiling_face:insight alum  and i wrote up a little post on our collaborative work together using webcams to measure clothing at stitch fix, with description of the algo included. no deep learning nonsense this time, just straight computer vision: does anyone have any tips for regularizing names in a database. for example, in this they\\'re listed multiple ways \"john smith\", \"smith, john\", \"smith, john a\", \"smith, john a.\".  i\\'m not sure if there\\'s a standard approach for this.: i would spilt the name into first, middle, last then recombine in a standard format. it would probably be good enough for most but not all names. does the db have location info? some countries list surname before given name even without a comma.: in addition to what  suggested, you may want to look at the openrefine library, which performs clustering and better normalization of names, including international character conversion to ascii (e.g. ö -&gt; o)google open sources syntaxnet.  nn for analyzing english textaka parsey mcparsefaceour data team stands “alone” in a weird way. our engineering team is completely independent of the engineering team, but our analysts are mostly embedded to product teams/business lines. in my opinion for a stand alone team i think that it starts with who is leading that team, the person needs to be someone who is pretty much always taking the initiative to make sure that the product and engineering and additional stakeholders are aware of the roadmap and of the data team’s capability. i agree with kevin that depending on whether you get slotted to the engineering or product team then you end up on those buckets. but then that starts limiting what each individual person in the team does, it becomes more “siloed”.thanks kevin, soumya, and kenny.for anyone getting options this is something to keep an eye oni know!!i almost posted the fred wilson article yesterday, but i think this one presents more perspectives how did you try that method in the paper? i mean code-wise..yeah, i liked that it at least mentioned possible prosmy toy model had y be a gaussian distribution whose width depended on the input featuresi can\\'t figure out how to attach a file to this thread, i\\'ll dm it to youok, technical question for you guys. i have an extremely unbalanced dataset (99:1)  for which i undersampled to balance. however, how do you go about normalizing the data? i am testing on my unbalanced dataset so using the same scale feels incorrect. also, if you guys know of any models that are better suited to this problem, let me know!an old colleague of mine put together this blog post on the topic. i recommend giving it a read anyone have experience with ranking/recommender systems in python? i do - what\\'re you working on? awesome!  so i have a set of user attributes, and a ranked list of products tied to each user.  want to be able to reproduce some of the ranking (realistically, only care about the top 5 — there’s 144 products in the db, and any given user has ~10-20 ranked) given the user inputs.  ideally i’m thinking of something like a user-user recommender system, so i don’t have to have the full attribute payload for a useralternately a modeling approach like a bayesian network could workbasically just haven’t worked with that type of outcome beforemy usual go-to is a collaborative filtering approach. if you know which products a user likes, then you can build a user-product matrix and run, say, item-based or user-based collaborative filtering or matrix factorization. this would ignore all of the user attributesfor a more complicated solution, the python package  is pretty awesome and will handle both ranking and the user attributes in a matrix factorization approachcool, i’ll check it out — i think it needs to be attribute-based, cause the reproduced ranking would be for a new user coming in without ranked products — can’t base it on pre-existing rankings, so user-product matrix wouldn’t work if i’m understanding ithave you worked with the surprise package in python at all?ah, gotcha. yeah, lightfm should be able to incorporate attributes and work for new usersspeaking of hiring and recommenders...vevo is hiring for a director of personalization and recommendation. let me know if you\\'re interested in the role.i\\'ve never heard of surprise - looks interesting!likewise for lightfm — thanks!starting my first data scientist job soon and they\\'ve given me the freedom to purchase my own work laptop (with reimbursement). any suggestions on specs? also, i\\'m most comfortable with windows, but started using ubuntu for insight. however, there seems to be a strong mac preference in the data science world. should i start familiarizing myself with mac or stick to ubuntu? thanks! it honestly depends on your role. if often you’ll find yourself having to do some excel work in addition to the more fun stuff, i’d recommend a mac over dual booting ubuntu/windowsi tend to just get the ram-maxed out macbookpro of the month, but if most of your computational/io intensive work will be done on servers / clusters, the air tends to have lots of fans (though maybe that’s changed with the newest macbook pros? i’m out of the loop)there is no inherent benefit to using a mac these days - it’s easy to do both local and remote work on windows os.  i have both windows and mac os for work and the only reason i use mac is because our it people have set things up with that in mind.  might want to find out if that’s true for your work place alsoi haven’t tried the windows bash shell thing, not sure if it’s out yet actually i shoudl update my windows :slightly_smiling_face:  potentially that could close the gap even moreuse a mac so you’ll fit in at data science conferences &gt;_&gt;i have both a macbook and native ubuntu dell for work. for bash/docker/aws there’s little difference, but i still can’t find a good sourcetree equivalent in ubuntu. i feel like your life will be easier with a mac (coming from a guy who prefers ubuntu)thanks for the feedback! looking at some of the on-boarding information, it definitely seems like mac is used by most of the employees. macbook pro of the month it is!i use a macbook pro (and have used many other platforms, though not in the last year or so) and it is great *except* i wish you could get more than 16gb of ram; i’m pretty sure you can’t even now. ram is huge limitation in my experience. just download it :troll:you need to get your boss to fax it to you get a mac  if you are a person who would like to tweak everything, and i mean everything - thinning down kernels, guis, window managers, modules, etc - all of which are possible on linux.  and the reason why i say this is because mac has limited ability to be a playground - so you get lot more *work* done. if not, windows, mac, and linux do not matter. almost all of them have amazing cross compatibility these days.  windows has great support for a bash shell, its tough to find limitations. if you go the windows and linux route consider getting a lenovo x1, you could run win/nix on a intel mac too but its just more expensive. does the lenovo x1 have the same planned obsolescence crap that mac has (soldered ram, etc)? leonvo has the same problem, especially in the gen 4. i have heard people say that you have to choose the socket option instead of the solder option, and then you would be alright ( that costs a bit more moolah, apparently. no data to back the socket version, btw)has anyone used spark-sklearn, and would you recommend it vs. other implementations of spark and sklearn?just to follow up: windows *does not* have awesome support for bash shell. their version has broken symlink support and has since release... there is something to be said for slow patch roll outs, but at some point it\\'s horriblealso, from experience, the right choice us *not* macbook with ubuntu on it... :disappointed: i\\'m running that now and they fight each other pretty badly.yeah i haven’t tried it, i mostly use mobaxterm and remote terminals, hopefully they’ll fix it so local command line stuff is bearablelong time lurker, first time poster here- more on the windows vs mac debate. i noticed that the ctrl key on the mac is in a sucky location. this usually doesn’t matter except i just started using jupyter notebooks, and a lot of the shortcut keys involve using the ctrl key instead of command key. also, it’s really bad when you have to use the ctrl key to crouch when playing counter strike go. after this mac, i intend to go for windows in the future, because they now have bash, and they just have more options, like the yoga/surface tablet convertibles that can also stylus pens for note-takingthis is a super under rated point! :+1: also... i have the keys swapped to the correct position in software... :nerd_face:haha, i swapped it with caps-lock! you can bind `ctrl` to `caps` no?there ya goonly if you don\\'t use :vim: and already have `esc` as `caps` :disappointed:that’s what i have (why would you use `ctrl` in any case?)this has been mentioned on many a forum but the new escape “key” on the mac touchbar isn’t consistently there and can be frustrating. or you could try the new windows linux subsystem, it was developed together with canonical and it might help depending on your needs .  also, you can always get say an i3 or i7 and run linux mint xfce. currently, i\\'m working with an i3 with only 4gb of ram and works very nicely with linux mint 18.1 xfce.the advantage of working with a laptop is that you can upgrade memory, hardrives, etc very easily.fwiw, you can run windows on a mac using the bootcamp partitioning utility.  i set up my old macbook air this way and am quite happy with it.  installation is easy, switching oses is easy, and you get the full windows or mac experience in your active boot session.my laptop running the bash subsystem in windows, is an i7 with 8gm ram, 4gb dedicated video memory , mind you it also does well with games :grinning:wow thanks for all the information everyone! next up: xbox or playstation? just kidding :laughing: ... was a long time linux user and recently changed to mac when my workplace offered to set up my workstation ... got to say that i\\'m enjoying the experience! the big drawback of the new bash shell on windows is that it has no gui functionality. otherwise, if you would only want some terminal action, including ssh, it works great.you could also have a system set up as a virtual machine, without the need of dual boot. for example, installing ubuntu in a virtual machine inside the windows os.nonetheless, i\\'m not sure how long it would take to learn how to use a mac effectively, if you\\'re coming from windows and linux. anyone with experience on that?does anyone know of a package for logistic regression that will allow you to enter total count and percentage of positive responses? the summary data is about 1100 rows, i could create the full data but it\\'s something like 500,000 rows. i came across someone mentioning that this was possible in sklearn but it didn\\'t explain how and i\\'ve been unable to find any documentation regarding this function.i\\'m trying to predict accuracy, and linear regression is doing pretty well. it has an intercept of 0.9 and all variables reduce this value and none are going out of range. however given that it\\'s unbounded and knowing it\\'s a suboptimal approach i\\'m trying to implement a model using logistic regression.whoa that made my day.`explain the difference between an inner join and an outer join.  let\\'s say you have two tables: one with &lt;strong&gt;n&lt;/strong&gt; rows and one with &lt;strong&gt;m&lt;/strong&gt; rows. what are the minimum and maximum numbers of rows that could be generated as results by joining the two of them (one join of any type)?    what\\'s the difference between `union` and `union all`? which one executes faster and why? (not super important)      if your query runs slowly, what will you check to try to make it run faster?        what is a subquery and when might you use one?calculate a mediansql questions from derek steer and i are going to be part of a data science career panel, just in case anyone is interested in attending:on  naming big data projects: : what language are the pokemon in? it is trying to tell me that avalugg is called crebase...submit a pull request with the json file fixedit\\'s just javascriptloading a json filefinally a way to contribute to the big data community through my love of pokemon :pikachu:also i meant natural language lolpokemon have different names in different languages...oh... hahai think the author is french or something so it\\'s in romanjii was like \"i\\'ve caught all 720 pokemon wtf is a crebase\"hahthat\\'s true, steph has a shiny charmwell technically i don\\'t have some event legendariesthose are required for the shiny charm in black &amp; white 2 thoughbut i am intimately aware of those i do not have and none of them are called \\'crebase\\'... at least in english :stuck_out_tongue_winking_eye:in x &amp; y/or &amp; as event legendaries are not required for the shiny charmwell you\\'re getting sometrue :pikachu:with your help :smile::smile: what are friends forcatching pokemon, obviously :smile:clearlythat quiz is really hard for someone who only played red...quiz?the pkmn or big data onedoes anyone have a macbook pro? how do you like it? does yours have the touch bar?it\\'s ok. touch bar is a pain - but you can always map the keys somewhere else. i like the usb-c power supply though. it takes me a while to get used to the keyboard. still not a big fan of touch bar.the touch bar is my main concern. would you buy it again?i hate it. usb-c is a downgrade, touch bar is a serious downgrade, keyboard is a downgrade, and the larger trackpad is annoying and feels like it has worse palm rejection. on the positive side, it is 10% thinner (barf)  i haven’t found much use for the touch bar yet, and it can be annoying at first (accidentally calling siri happened a lot before i removed it from the bar). i think the keyboard is great, but it did take a bit of an adjustment. i really like the large trackpad, and haven’t had any problems with it.  how much ram does your machine have?8 gbwould you rather have the macbook pro or the macbook air?8 gb. macbook pro. i\\'m happy with my decision.i just got one (3 weeks ago) and i am happy overall.  i am pretty ambivalent on the touch bar - i used it to escape and control the volume mostly, but occasionally there are fun features (like navigate among weeks in google calendar).  i like the big track pad, but was kinda bummed about the usb-c.  i bought a converter ($20) for my thumb drive and other things.same here. i have to get a usb-c converter for my mouse.the keyboard is really loud, the touchbar pretty gimmicky. i like the big trackpad actually, but i’ve dealt with daily crashes (possibly a bad video card, which is common enough … just started the process of getting it looked at). i’d trade it in for a comparable non-apple linux machine in a heartbeat, but i don’t know which brand stands behind its hardware the best anymore.: being that it is a cloud warehouse the advantage is that any data coming into domo can be automatically subject to their magic etl workflows, sliced and diced and thrown into a preconfigured view. the views i.e graphs themselves are mature , based on d3 and have some specialized functionalities like the ability to drill down into a part of the dataset . once the listening jdbc/odbc connectors are setup they permanently listen to that data source. the differential advantage comes from being able to leverage the warehousing capabilities rather than just the dashboards. also you can push and pull data and custom viz in python and r to domo.thanks  for the informative answer. i think i\\'ll sign up for the free account and try it out. thanks stephanieany one with ear phones that he/she is not using? i want to borrow for today: this is the chat for _all_ fellows and alumni  — in sv, nyc, and boston. you may have more luck asking under your specific channel (de-ny-2016a / ds-ny-2016a) :simple_smile:thankssv people! you should check out this event from pintrest about machine learning on tuesday the 9th: i was going to sign up until i saw that it isn\\'t in glorious magnificent new york city but instead is in ... uh ... well... san francisco: could you share some code snippet of how you save grayscale image with transparency? specifically, what dimension of np array do you store your grayscale image with alpha channel as? opencv\\'s `cv2.imencode` balked at saving a mxnx2 array...hi, does anyone know how to password-protect a site hosted on aws? i’d like to host my project on an instance, but it’s a consulting project so there’s proprietary data involved. do i do this at the flask level or is there some other way?: i\\'ve done that before. come chatis anyone at aws reinvent and interested in meeting up for dinner today?hi everyone! our team at viacom is desperately in need for a good candidate for a vp level position. here is the job description: please let me know if you are interested or have anyone in mind. any help would be greatly appreciated! :slightly_smiling_face: if you ever need a vp of de, i’d be happy to consider :smile:haha i wish we were that advanced  :wink:anyone out there using neural networks to take heterogeneous data about users and embed a user into some vector space?not yet, but i am actually thinking about doing this exact thing.so... i\\'m interested in it too.we meet again, dr. ainsleydr. mercurio sounds way cooler. like a nerdy super heroand i suspect we have the same types of user data (site visitation, purchases, random demographic data)yes, so say my spanish friendsyes, and i have some of that information for some users and it\\'s missing for others. so, it\\'s a bit challengingi am also thinking about doing the same thingi think we’ve just created a working group.it\\'s been in my queue too.can i get in on that working group too? that application would be super useful on some of my projects.me too :stuck_out_tongue:+1 :wink:i just thought i would check again if anyone was at aws reinvent and wanted to meet for dinner?    excellent idea. that\\'s something we\\'re starting to work on. hey dr. mercuriothis is dr. rubinme too!hi everyone! has anyone here tried their hand at spell checking book/movie titles?like input examples :  \\'the theory of everything\\', \\'theory of everything\\', \\'theory of the everything\\', \\'theory the everything\\'. \\'theori of the evrythng\\' etc.it does not have to correct but atleast identify them all as the same thing i’ve used `fuzzywuzzy` in python for word similarity, the interface is pretty simpleor go with the core functionalities in `nltk` to get edit distance / levenshtein distance etcany nyc alumns going to the office tonight? want to meet up briefly beforehand?thanks! so here\\'s my understanding of a (basic) recc system. i have a dataframe where each row is a review, and the columns are user_id, item_id, and review. in total there are 700 users, and 9000 items. i convert the data matrix into a n_users (row)  x n_items (col) matrix, where each element is the review score that user gave that item. my first question is: is there an elegant way to do this? right now i have a for loop that iterates over all the rows in the dataframe and adds them to the matrix one by one. i feel there is probably a better way to do this using pandas.i want to find items that are similar to other items. so i treat each column (1xn_users) as a vector, and for each vector pair i compute their dot product to give me the cosine similarity between  each pair of items. next, to find the most similar item i, i find it\\'s row in the similarity matrix, and find the max value in that row. the location of that value tells me the most similar item. i\\'m getting reasonable results with this, but i have a few questions. is there a more efficient way to compute the cosine similarity besides just iterating over each item? i know this is the most basic type of recc system out there. what else do you recommend? i want to add more features (like item description) into the system. thanks again!does anyone have install scripts readily available for hadoop multi-node cluster? have my hands on a respectable amount of bare metal to build out an infrastructure ... do you know how to choose which function to use in fuzzywuzzy? there seems to be a bunch of those.i’ve only used it a little, so there’s some trial and errorfor ’s example (dealing with misspellings, rather than semantic differences like “braves vs. yankees” and “ny yankees vs. atlanta braves” meaning the same thing), i think the simple `ratio()` method is best`partial_ratio()` would handle things like missing the leading “the” you can take a look at . if you don\\'t want to use the cli, the bash scripts are under the install folder and hadoop thanks austin!!! really appreciate it!!!!! for configuring the nodes you\\'ll want to go to the `config` folder and for service start stop under the `service` folder ... thanks for the directions! i\\'m going to start the process, hopefully, tomorrow when the servers are activated ... will definitely keep u updated!noice yeah feel free to ping me whenevshi all! i\\'m a fellow in the remote data science program. anyone out there have experience figuring out who\\'s speaking in an audio recording? i have zero experience with audio analysis, but my project involves finding the % time a particular person is speaking in a recording with multiple people. the voices sometimes overlap. i would really appreciate ideas.wendy.carande: hi wendy, i\\'m currently doing the data science program in nyc, and i\\'m dealing with some similar challengesto get off the ground i\\'m using the \\'pyaudioanalysis\\' package that will extract some basic audio features like mfccs and chroma vectors. there are also functions to take these features and use them for speaker diarization. have a look in here:i\\'m still trying to get the outines of my project all ironed out, and i\\'m super happy to chat with you about this. i can\\'t promise any deep insights though! thanks for that suggestion. we should definitely trade ideas.for sure. feel free to send me an email at  if you want to move the conversation off of slack. we can also set up a skype or phone call if you want. im free after 3pm est tomorrowsome 3rd party apis may do this as well-- google ml or something in microsoft azure, for example  worked on a similar sounding project in our session - i think using mfccs and hierarchical clusteringthis isn’t my project by any means, i’m not entirely sure who is responsible for it..hi all, is anyone aware of existing work related to education (school etc)? tax related? thanks!could you be more specific? data science applied to education?: yes exactly sorry it was a weird way to phrase ithi everyone, has anybody used tornado web server for python before?yeah. there’s a ton.i’ve done contract data science work for pearson.hey kenny, what are you looking to do?does anyone know a good package for multi-document summarization? i\\'m looking for something that would take a few hundred reviews and return an average/aggregate review.might be useful for you thanks josh! i looked into it but it doesn\\'t seem to do summarization.for any subsequent happy hour with pizza at insighti was wondering if someone could decode an acronym for me.  a recruiter just asked me in an e-mail \"have you thought about user insights or cro?\"  ... i\\'m not sure what she means by cro though. any ideas? thanks!conversion rate optimizationi found this weird thingah, of course. thanks  !no problem!stay positive katie.amrineneil degrasse tyson has issued a great call to action: “make america smart again”.  i believe the power of the insight hive mind can make a unique and valuable contribution to this mission.  i created a new channel -  - for sharing positive, action-oriented ideas.  the first step i will take is to contact a friend at the southern poverty law center.  they do a fantastic job of tracking hate groups in the united states ().  i’ll inquire about their needs and about opportunities for the larger tech community to volunteer services.  what ideas do you have?  please join .i think this is a fantastic idea. \"make america smart again\" sounds like a worthy call to action, and the splc does seriously good work. i would love to help coordinate with them.while i in no way consider a vote for trump to be the \"smart\" choice, to me \"make america smart again\" isn\\'t a slogan that\\'s going to make any friends. it\\'s only going to serve to create more divisiveness by perpetuating the assumption that you can\\'t be a smart person and vote trump.i understand the sentiment, but perhaps the catch phrase is a little too exclusionary. the majority of people who voted for trump are not terrible racists or bigots. they\\'re confused and scared people hoping to regain what they perceive to be their rightful place in the world. most have been pandered to and incited by demagogues hoping to capitalize on the situation for their own ends. some vocal fraction are bigots and racists, yes, but understanding how to connect with the others is not only laudable, it\\'s essential. \"smart\" doesn\\'t come into it.stay positive higginsc how do you know the majority are not “terrible racists and bigots”?stay positive varunrajansource: grew up in iowaworth reading jamelle bouie on this: i left the midwest for silicon valley precisely because i was responding to the same economic influences that are driving moderate conservatives toward trump. i saw no future there. i wanted to be a part of the optimistic technological future here, and i was privileged enough to have the education and connections to do so. most people still living in rural areas don\\'t have that opportunity. you can blame them and say it\\'s their fault, that they could always move, but really they can\\'t. people have children or other family, many never went to college. if we\\'re so \"smart\", why can\\'t we figure out how to manipulate the \"stupid\" people into seeing things our way?i don’t disagree with any of that, and i think you make good points. (and i didn’t mean to get into a heated argument on friday morning.)i simply think that, as “data scientists”, we should use data before making quantitative statements like “the majority of people xxx”. i think if you look at the political science literature, especially on “racial resentment”, you’ll find that race appears to be a much bigger factor that any economic tendencies. here\\'s an article you might find interesting: anyway, happy to take this offline if you want to talk further.i\\'ll check that out. i also recommend  which despite being a listicle on cracked, is quite goodalso, if this is what passes for heated, i\\'ll take it. i haven\\'t even gotten out of bed :)i only called it heated because i got the “disappointed” emoji from stephanie. :grinning: i was not disappointed in your statement! i’m saddened and frightened by the reality it represents. as someone from north carolina, and as someone who has just legally formed a family that many vocal trump / pence supporters would like to legislate out of existence… i see and personally feel the normalization of bigotry. this isn’t just about obsolete careers and struggling to make ends meet. some people voted for donald trump because he shouts the things that they can only whisper and dog whistle about. maybe we’ll be ok, who knows. but i have real things to fear right now.stay positive stephaniemarilol i’m working on it, nuggetbotbefore we get too lost in the existential despair of the white working class (which is a very important economic and sociological topic for discussion and action--let\\'s throw de-industrialization and the opioid epidemic in for good measure), it\\'s important to point how how jokes work. \"make america smart again\" is funny and effective because it hijacks an existing meme and turns it on its head. if you want something like this to work, it has to have a hook that stays in people\\'s brains.one last, and even more important point is that if anything i think i know about ndt is remotely correct, this will be an effort in enhancing the public understanding of science, not something to dump on conservative voters. i am guessing that there will be pushback against the denial of evolution and climate change, the fact that undocumented immigrants commit crimes in less often than most other demographics (which could be seen as having a \"liberal\" agenda), as well as pushback on the anti-vaccine and anti-gmo movement (which will be more likely to antagonize some of those same liberals cheering the first few points in this list). it\\'s possible that tyson will drop the ball on this, but i doubt it. i apologize if this message is in any way aggressive or incoherent. it\\'s still pretty early in the morning, and i am trying to drink decaffeinated coffee to see if it improves my health at all.good point. ndt is a baller.anyone else at mlconfsf!? i already saw  and  ! want to grab coffee or lunch?we’ve got openings for data scientists and a data engineer on our machine learning team at crowdstrike. (a remote option is available, but it would be really awesome if you were in the los angeles area — you would still able to work from home :slightly_smiling_face: )hi folks! international network question: i am possibly moving to paris relatively soon, and am starting to look for data science jobs there.  does anyone happen to have any connections out there, or suggestions of particularly interesting companies to check out?  (i swear this was my plan even before tuesday....)  thank you! with marine la pen you might need to move again soon  i have some french people on my team, i’ll ask around that would be great, thank you!fwiw, ndgt loves soundbites and definitely used that phrase/slogan to sound good on tv w/ colbert. i have no idea if he\\'ll actually use it more often (but if others have seen him use it in other contexts please let me know cuz i\\'m curious). it also isn\\'t clear to me that he\\'s going to be doing anything different than what he already does. he hosts star talk, tweets, gives interviews, and will almost certainly be doing a book tour for his 2 recent books, but i find it very unlikely that he\\'ll be putting together some new big public science education program.those kinds of programs already exist in many places and i personally hope that they expand and grow now more than ever. if people are interested in tutoring kids (in any subject), the boys &amp; girls clubs of america are all over the place and do great work and if you want to do more you can get matched with a kid through them. some cities have education and/or scientific organizations that match scientists with teachers/schools and you get to talk to kids about science directly. lots of places also have public science programs for adults, either free lectures at universities, or in pubs (i\\'ve been involved with these kinds!), or at libraries, etc. many of these programs are volunteer-based and free to attend and try to target underrepresented groups and lower-income groups, both in urban and rural areashe also likes talking about other sciences as if he was an expert, and makes spectacular mistakes.to be fair, if any of our voices sounded as good as his, people would instinctively believe pretty much anything we said too. he\\'s barely an expert on astrophysics these days... he still knows his stuff.  your comment could be interpreted as an attack on science educators by implying that they’re not “real scientists.”  all that does is discourage other scientists from doing their own public outreach.as a nationally funded scientist and science educator until this past summer, and currently an unpaid and volunteer science educator, i am well aware of the views held by some scientists of science educators. neil does great work getting the public excited about science. he has also done harm by passing off incorrect information as fact, about astrophysics and many other fields. as a poc in science, as a \"famous face\" of science, and as a communicator, he is doing good work for society and creating a scientific literate public that dwarfs my humble contributions. that being said, he is not perfect, as a human or a science communicator. i, and many other astrophysicists and scientists, find it infuriating that he rarely, if ever, apologizes for misinformation or corrects himself, which is the exact antithesis of the scientific method.to all my insight fellows, please join me in this petition to block myron ebell, a climate change denier, from being appointed to the head of the epa. trump\\'s major victories came from oil/fracking states and he has already started trying to lift emission/mpg limits from car companies. we need 100k signatures by the end of this month. man. did anyone know about this? stay positive markwangstay positive americaanyone have any advice or experience with event coding within a mobile app? preferably something that\\'s interpretable but also can be easily analyzed. primarily for tracking user movement throughout the app, identifying where users may be getting confused, etc.hi all! my company (unifyid, security startup in sf) is greatly expanding and hiring mostly engineers in the following specialties:devops engineerfull-stack engineerfront-end developerios developerandroid engineerwe use advanced ml &amp; physics applied to phone sensor data and similar, to identify and authenticate users into apps or websites and eventually replace passwords.please let me know if you are interested or know anyone who is. i am happy to split any referral bonus :slightly_smiling_face: i’am at your service for any further question.hi all! a colleague of mine is a founder of an early-stage ai company in the business travel industry called , and is looking for nlp, ml, ai and data engineers for the founding team. they are based in the bay area. if that sounds interesting, please let me know and i’d be happy to introduce you you should reach out to  to get connected with some of our recent ai fellows! :partyparrot: i know that  would be excited to talk to you!and jumping on this bandwagon! my company (tempalert, boston) is looking for a data engineer/data architect to build out our analytics platform and integrate it with our product. let me know if you are interested!  talk to ! :smile_cat:sending her a message as we speak :slightly_smiling_face:for all you visual learners out there...since  is too modest to promote his own work, he wrote a fun post exploring glove and different styles of english as used on twitter. preview: \"workin - working + going = goin\" :wink:that’s neatway to go nice  . this is the first time i’ve ever thought about a negative letter :confused:thanks !showing solidarity with the victims of the pulse tragedy in orlando, fl, latinotech (), lesbians who tech (), queer tech nyc (), out in tech (), ny tech alliance (), &amp; outnexus () are donating all of the proceeds of their happy hour tomorrow to “the center”, the glbt community center of central florida. the event is in nyc (), but anyone can make a direct donation here:  :rainbow:hi bay area peeps, anybody need a roommate or know of housing around menlo park / anywhere south of sf? (my male cousin is moving down from portland)can anyone suggest what is the best practice to do bulk load from file to cassandara? - i think  might have some experience.anyone currently watching lcs?league of legends?yes thanks for the tip.  i will reach out!a new blog post by  compares a/b testing and multiarmed bandits for adaptive movie recommendations take a look! if any fellow/alum wants to write a post on our blog, hit me up and let\\'s chat.i think all you have to do is emphasize the minimum requirements for the job. title is less important. also, keep in mind that anyone that might join in a junior position may want the flexibility to grow into a senior position. you could also just create a tiered system like other companies (l1, l2, l3, etc) or “junior data scientist, data scientist, senior data scientist, principal data scientist, senior principal data scientist”.many companies (like daimler) try to match seniority to other company ladders. in our case, we match roughly to google’s, but change the designation/titlei would just make the requirements (and comp range) obvious so folks know if its worth applying with their skill set.hi all!  my company, proteus digital health, is hiring a data scientist.  the role is on the platform team and uses data to optimize our core product, a pill that transmits a signal when swallowed.  please contact me if you\\'re interested.not yet - still building the main scraper, might try by the end of the weekwe really should. the survey will help us with that!anyone has experience with jupyterhub as a data science collaboration tool within their team?you might try this.  kindle version is $7.  done! looking forward to seeing the resultsyes. i like go with something involving an imbalanced data set (fraud, clicks, etc) and ask the candidate to walk me through how they’d detect the minority class. it’s a standard question, but i like it because it gives me plenty of insight into how the candidate thinks, what tools they’re comfortable with, and what types of creative solutions they can come up with if i throw a wrench in their approach (what if you discover that 90% of the clicks were bots?). i also like to go into the details of the tools they’d use, any etl (let’s say the data is in two different tables organized like this….), how models could be stored, etc, just to get a better gauge on the scope of their understanding of a tech stack.how flexible it is in order to pick up a teammate’s notebook and edit/add codes?  can you also compare it with any other platform?any recommendations for visualizing a very large amount (~300k+ records) of time series data in redshift? interested in features like panning and zooming. lots of bi tools out there and not sure where to start.i\\'d love to hear if caravel can work for this: linkedin is open-sourcing a scalable ml library for spark called \"photon\". it\\'s used widely here internally:carlos, what does “open-sourcing” mean? is it not publicly available on github already?open sourcing also implies a particular type of license.i wouldn\\'t read anything into \\'open source\\' besides \\'being able to see the source\\'but that\\'s just mesamei believe this is why richard stallman hates the term “open source”. _free software_ implies a certain kind of license.: i miswrote. you\\'re correct indeed.: if you need power you can try vispy which leverages gpus (there\\'s a pycon talk from last year about it on youtube). i\\'ve also been playing with plotly lately, it has interactivity built in.‘;n’mmn cxcvbn\\\\hey all, i’m trying to find a style guide to write quick-running, yet fairly human-readable sql. any suggestions?postgresql, in particularif anyone is interested in attending this spark streaming workshop, here is a 25% discount for insight:hi illustrious alumni, if you guys have a chance, could you try out  and let me know if it survives? (it takes a long time to load). extra appreciate authing your reddit account to it. extra extra appreciate using it like it’s a real thing for a bit :slightly_smiling_face: thanks!\"’s server dns address could not be found.\"i already broke it i don’t have a reddit account, but i ran it through my twitter account and it worked! it recommended subreddits about music, outer space and onion-esque news stories. i accept.works fine for megot a lot of space recommendations, which is somewhat but not  wholly indicative of my tweeting habitsif you need personal validation, it recommend ksp as a subreddit, which i was already subscribed tooh it’s just blocked at work…guess i shouldn’t be procrastinatingr/dataisbeautiful, r/data_warehousing, r/data_warehouse... i guess it thinks i\\'m some kind of data scientistseeing as how i follow statfacts and hadley wickham and drew conway and such on twitter, seems like it\\'s workin```internal server errorthe server encountered an internal error and was unable to complete your request. either the server is overloaded or there is an error in the application.```10,000 data subreddits... which is what i\\'d expect from my twitter(but not ones i\\'d actually follow on reddit)lots of ml stuff tooi mean it nailed my interests expressed on twitter!wow, so many data subreddits, good job!thanks for the help everybody! :slightly_smiling_face:50 free data science booksanyone has experience using flask+swagger  (flask_restplus)?any companies out there looking to hire an entry-level ux designer? one of our badass ux interns here at app annie is finishing up soon and looking for full-time work. pm me if interestedwho\\'s at odsc west?i’ll be there giving a workshop tomorrow !clean yo desks!help i\\'m locked out the office would be great to have a dockerfile in the github repothis (not @)channel just keeps on delivering :popcorn:i feel like taking glee in these interruptions is all thanks to you ! clean desks foreverpug bomb 50arrgghh how does it worknuggetbot pug meit’s onlinenuggetbot pug bomb 50:trollface:this is a good dayi thought these pugs were wrapped in towels or something. no those rolls are their bodies.:pug-bomb:renaming this channel to #adorablepugsruuuuuuuttttthhhhhh daaaaaaaaaaaaaavid!:bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching: uploaded a file: hhahahaha...sorry i know we are not suppose to laugh at this meme but i cant help it :stuck_out_tongue:   i think it was me :smiley:. i didn’t formulate my problem correctly.for me it is often safe to assume that  data is likely missing completely at random. it’s just my feature matrix is a triangle at best: number of predictors is growing.the correct way to formulate would be: i wonder if somebody get to work with the growing set of predictors, rather then growing set of samples while doing online learning.if anyone is interested in a freelance web developer job, see the message below from a friend of mine at upenn.\"my research involves virtual navigation behavior and i use a custom built website to administer the studies and collect the data online. i didn\\'t build the website and don\\'t have the knowledge and/or time to change it. the guy who built it is awol. the website runs fine, but i have no one to maintain it, update it, or make changes. we\\'re looking to continue hosting it online, but it uses a plugin for unity 3d, which, i understand, browsers are pulling support for. this poses a pretty serious issue for future online data collection. i\\'m looking for a web developer who could provide a time/cost estimate for some of this. the project uses html, ruby on rails, and is built on a mysql database. the server is a version of red hat, and is currently hosted for free on a university server.  if you would be interested in consulting on this project, or know someone who would, please contact steven weisberg by email: .\"cc:  :wink:a bio on mr. lecuna good friend of mine who’s a total badass data scientist (non-insight) and who cares deeply about equity and inclusion issues (in tech and beyond) and who’s been in the field for quite a few years is looking to switch companies in the sf bay area. so, if anyone has or knows of an open lead/director-level data scientist position, please dm me!heard uber is really big on diversity. just kidding.i must share this with the world. thanks! !anyone seeing any performance issues with tensorflow on aws today? we’re seeing a &gt;15% slowdown in training and speculating that it’s from the intel bug but honestly have no ideaindeed amazon has already applied the patch for meltdown which is expected to slowdown cpu performance by 20%unsupervised/semi-supervised learning question - i have a binary classification problem where i know if something is true positive, but not if something is a false positive (basically, some of my data is labeled with a 1, but everything else could be 0 or 1 but i don\\'t know the answer).  is there some weaker analogy to an roc curve that exists for a situation like this? i can show \"what % of true positives does the model correctly classify\", but i\\'m wondering if there\\'s something more sophisticated i can do (because of course i can tweak the thresholds to catch all of the true positives, but i\\'m probably also incorrectly classifying a bunch of other points as 1).i do have a sense that probably not more than 10% of the data set is a 1 (so 1\\'s are relatively uncommon - this is basically an anomaly detection problem)hey, in medicine this comes up a lot, where it is often easy to get a subset of the positive population via a query, but getting the complete positive dataset is very hard (manual review). i found the following 2 papers helpful:it sounds like the usual caveats apply here, i.e. wanting to make sure you minimize false negatives, and that you try to preprocess your training set to balance the data. depends on what a true positive means though, and how bad it is when you miss one.this is great, thanks! (and good guess - this is indeed a healthcare problem) has a cool blog with some others which discusses this issue. the relevant post is: hi, i am looking for *basic or simple github or tutorials* for  *user recommendations by cosine similarity*. any recommendations ? thanksall mayors should take public transportation at least once a week.i have seen rahm emanuel on the cta in chicago so some do :slightly_smiling_face:linear regression question:if i have both categorical and continuous features (&gt;0) what is the best way to preprocess the data? my continuous features have wide ranging scales - some are in [0,10], others are in [0,100000]. i can rescale the continuous ones and one-hot encode the categorical ones, but is that the right thing to do? seems this scheme would favor the categorical variables. i am just curious why “this scheme would favor the categorical variables”my approach, based on this article by gelman is to to center at 0 and then scale continuous variables by two sd, and center binary ones at the mean. here\\'s the reference to that this pretty much makes parameter estimates comparable in regression models and helps with convergence in bayesian models there is no right way of preprocessing your data. usually it’s an iterative process and you should try a few different approaches. here is a nice guide to normalization, scaling, mean removal and other preprocessing steps from sklearn: one-hot-encoding is most common way of encoding the categorical variables but there are better approaches. i really like this deck from an experienced kaggler, where he describes different approaches on how to handle categoricals: because when i rescale (and in this case i really mean normalize, not standardize) very few of my continuous variables will be close to 1. however my one-hot encoded variables will be 1 or 0. one of my categories is coming out on top in feature importance. i am just not sure if this is a \"true\" signal or a consequence of this choice of preprocessing. are you going to apply l1 or l2?right now playing with l2in that case it may be marginally important. you can always map your categorical feature to 0 &amp; 0.5 for example and see if there is any difference. it would be interesting, if there is :slightly_smiling_face:thanks for the suggestion. i\\'ll let you know if i find anything interesting :slightly_smiling_face:hi, i\\'m trying to use multinomial logistic regression in spark (to predict ~300 different outcomes) but keep seeing some strange behavior (eg. the number of jobs triggered vary even if i keep both the data and the model parameters the same, as well as when i use the trained model it just predicts the majority class for each record so my assumption is that the model is  quite messed up). do you have any suggestions how to debug this issue? thanks!thanks  and  i\\'m leaning towards luigi.  it seems easier to digest.   anyone at gtc this week? pat and i are all four days.does anyone still publish now that they\\'re out of academia?  i\\'m looking for what journals are considered good for data science/analytics articles.  thanks all!: i have a few in the pipeline, but like anything else it\\'s likely to be specific to your analytics (i.e. distributed clustering, nlp approaches, etc.)probability by peter norvig i\\'d just like to say that i love almost all of the notebooks that peter norvig writesupdate: he ended up wearing a navy suit jacket, white shirt with no tie, khakis, and dress shoes. he was interviewing with cfo and coo so this felt appropriate. for interviews with non-exec, he would have kept the top but gone with more casual pants-shoes combo. thanks for the tips, y’all!^ this is cool but it also freaks me out a little?they should make one for ratsstay out of my dreams, ! last night i had a dream that the nyc office was infested by rats! the alpha rat was named darryl! thank you for your reply. it is a great idea to visualize the data first with some domain knowledge. the  predict value `open_rate` is heavily related to human behavior - people tend to open their emails during the middle of the day and the weekdays, so there are periodicity involved in the time feature. besides that, it’s worth to look at the correlation between domains - `` vs ``. also between ips - presumably we assign emails with different qualities to different ip. but for our customer `portal_id`, i will assume they are independent of each other.thanks!i am making a list of common human cognition pitfalls when using stats to analyze data... more ideas?human cognition pitfalls in statson basic statistics1. null rejection. to think that because one proved that x does not cause y implies that x does cause y. i.e., to think that not having evidence to reject the null implies that the null is true (one cannot disprove that medicine a is effective in treatment t does not imply that actually, medicine a is effective in treatment t) correct: keep trying to find evidence to reject the null and accept it as not disproven but never as true2. correlation and causation. to think that correlation implies causation and that therefore the pattern will repeat in the future. correct: correlation does not imply causation and to infer causation one can use causal inference or experimental design.3. correlation of non-stationary time series (i.e., prices). they will always appear correlated even if they are not due to correlating trends but not the underlying stochasticity of the series.correct: first make the time series nonstationary. i.e., correlation of returns (stationary time series)on fitting models4. future leakage. to use historical data that was construed using future information, i.e., using a fundamental indicator that appears built at time t but actually used information from time t+x.correct: always use point-in-time data with correct dates.5. backtest overfitting. to fit n+ models using the same dataset until getting good results (even when doing this \"kosher\" and using the out-of-train times to test the model performance). correct: take into account the number of models that were fitted and use proper multiple testing corrections.maybe it\\'s better to start a doc and let people to contribute there?that\\'d work but we can leave it on the thread in the meantime... do we have a way for all slack members to contribute in a common doc?... sounds like we would have to create a google doc (or quip) and therefore invite people into it which creates some friction into getting everyone\\'s input...you could also make the doc accessible to anyone with the link (public)looking for significance in posthoc subgroups when the overall result is null i think i\\'d be ok with that if the next step was to run a dedicated experiment in a sub-group, rather than to make a  decision to roll-out a feature to that subgroup based on that first testi am having a problem with my reverse proxy on ubuntu (aws) for my website. i have a shiny app that runs on the apache web server, but i am getting a 502 proxy error when i try to load the site without the port number. some of it loads, but some files do not. here\\'s what i have in the default config file for apache2:stay positive alexisyelton&lt;virtualhost *:*&gt;        # the servername directive sets the request scheme, hostname and port that                                                         # the server uses to identify itself. this is used when creating                                                                   # redirection urls. in the context of virtual hosts, the servername                                                                # specifies what hostname must appear in the request\\'s host: header to                                                             # match this virtual host. for the default virtual host (this file) this                                                           # value is not decisive as it is used as a last resort host regardless.                                                            # however, you must set it for any further virtual host explicitly.                                                                servername localhost        serveradmin webmaster@localhost#       documentroot /var/www/html                                                                                                         documentroot /var/www/html        proxypreservehost on        proxypass /         proxypassreverse /         # available loglevels: trace8, ..., trace1, debug, info, notice, warn,                                                             # error, crit, alert, emerg.                                                                                                       # it is also possible to configure the loglevel for particular                                                                     # modules, e.g.                                                                                                                    #loglevel info ssl:warn                                                                                                            errorlog ${apache_log_dir}/error.log        customlog ${apache_log_dir}/access.log combined        # for most configuration files from conf-available/, which are                                                                     # enabled or disabled at a global level, it is possible to                                                                         # include a line for only one particular virtual host. for example the                                                             # following line enables the cgi configuration for this host only                                                                  # after it has been globally disabled with \"a2disconf\".                                                                            #include conf-available/serve-cgi-bin.conf                                                                                 &lt;/virtualhost&gt;any thoughts?if its not in sk learn or statsmodels, that probably means it doesn\\'t exist :confused:python tends to be lacking in more advanced linear modeling.yeah i figured.  i guess the only option is to either build it in pymc or use r hahasomeone needs to give statsmodels some more dev love i’ve had to switch to r for things that aren’t implemented in python - it’s kind of a pain but doable. if you’re happy using a notebook, then it’s pretty easy to load up an r kernel. alternatively you can call r just within a single cell.does anyone have good examples of (technical) topic summarization formats?  either interactive or not.possibly using d3 or other tools.geolocation question: has anyone figured out how to return all lat and lon coordinates within a circle of a particular radius? thanks!how do you define a circle?just wondering since a circle has infinite points and there are infinite lat lon coordinates in a circle.do you mean get all lat lon from a select list?ah, yes from a list! like you want to find all taxis near some point at a timeand i would define near as a particular radiusin python, you can do all that with a the shapely packageoh! some databases, like mongodb, have built in functionality to do that too.cool thanks! i\\'ll check out shapely:plus_one: for shapely. i use it all the time.hi all! we are looking for project reviewers at udacity for our machine learning projects. you get paid per review and capstone projects are paid at like $70 each review. let me know if you are interested.good video on how google makes their teams: ha! i had just this idea a little bit ago: haha sweet. feel like there\\'s a few title missing, got any idea to complete the set?hmmmmm i’ll think on it!facebook has open-sourced a pretty neat tool for time-series forecasting! roger.w.shaw: nice find! was looking for something just like this.i think there are lots of converts,   - vim was the fourth-ranked option in the 2016 stackoverflow survey. and the top two (notepad++ and visual studio) are windows-only products, so i\\'d guess vim\\'s use in open source systems is higher. hi all, for those of you in the la area or planning a summer visit, the date for `big data day la 2017`, a free to attend conference with over 1500 attendees and featuring many great speakers from the industry, has been set and finalized!  the conference will take place `08/05/2017` at the usc campus (yay!  we upgraded our venue from previous years as we continue to grow).  so save the date and i hope to see some of you there :slightly_smiling_face:for those who are local and want to volunteer, please reach out to me directly and i can loop you in to our volunteer meetings and opportunities.  it’s a great way to ensure that quality data and data science conferences like big data day la remain free to attendees.looks like ai is ready for insight! :+1:link to github repo: it’s worth noting jupyterlab (the evolution of jupyter notebooks) will also have collaborative notebooks. link: . i think these also use google drive on the backend. the beta milestone is due out before the new year (fingers crossed), so colaboratory is probably a better experience at the moment.colab is also really focused on using google compute backend. juypterlab seems more flexible beautifulsoup question. is it possible to use soup.find_all( ) to find two separate tags and classes? i\\'m looking for both (\"p\", class_=\"program\") and (\"div\", class_=\"center-hrs\") ideally in one final list in order. thanks!!!solved it by passing in two lists!so, dropbox got pwned. check if your accounts are compromised.   and whats your score? mine is 3 :slightly_smiling_face:1 (damn you 000webhosting!)stay positive carlosfahammy old gmail has 8 breaches and 4 pastes :confused:waoh!just one, frikkin dropbox^ dropbox for me tooshitbitching ass-bastardsstay positive johnrwalkyou’re not my supervisorno breaches ^_^no dropbox for me….but skeptical.  i had a random sign in event a few weeks ago and changed my pwstay positive rociosngdropbox, but thankfully i have a unique password and actually don\\'t use italso adobe!haha adobe yesman, using plus addressing makes searching so hardwtb regexand ..ugh myspace. yes i had one like 14 yrs agoflash player is the dirty thing for the internet ever.adobe needs people to signup to get flash player! how stupid is that? its like signing up for getting a jvm from oracle.oh wait.. you have to do that too:smile:woah, i had a myspace account. it must have really sucked because i don’t even remember having it.  i believe the system admins said, “here you go kind sir, myspace, is down and dead, take this server and do what you want with it”. it turns out it had passwords.but the most valuable thing on the server was really an index of all of our tastes in music during middle schoolhaha..i am sure half of those passwords were threedoorsdown, sum41+blink182, allstarz, etc.apparently my adobe password is somewhere to be found on the internet, but i still have to reset my password every single time i login there. well, apparently someone has access to one of the 5 backups of my thesis research. it probably would have been easier to just look on github than to hack dropbox...for all of you word2vec users: what is the method that you use to create a sentence vector from a word 2 vec model?paging , you\\'re nlp magic is needed!@alex_guide: hey alex.for short things (3-5 words) we\\'ve found that summing works finewe tried some weighted sums, but i don\\'t remember their performanceand we\\'ve used some different models like paragraph2vec and skipthoughtpat though is more expert than i, hopefully i can poke him into showing up: for the weighted sums: are you just taking the average, or do you assign different weights to the different words?: paragraph to vec is doctovec?yeah, although i think we\\'ve done with and without averaging; i don\\'t know if those experiments ever went anywhere.: i was assuming that we would use just the simple sum. example for simple bow: ‘marry had a little lamb’. assuming dropping the ‘had a’. would just use model(marry) + model(little) +model(lamb) to get the vectorthat\\'s worked well for small numbers of words for usi\\'d certainly try it as the first thingthat seems to be the way i was thinking.what data set have you had luck on training with?i know it matters for the actual problem.you could also try tf-idf weights on the word vectors before summing^--that\\'s what we did for weightingbut again, don\\'t recall the performance :confused: you would have to do the tf-idf prior to training the model as well corect?for training we generally do one of two things:1) take a pretrained model, and maybe retrain it a bit2) train from scratch on the text we\\'re interested inyeah, you\\'d have to calculate it on the full sample, you can\\'t do it on the vectors afterthanks  . it’s been a while since i’ve tried itwhat do you find has better performance. training on the data to be used. the ‘docs’ that i am trying to cluster are short. maybe ~ 2-3 words on average.train tf-idf on your dataset, train word2vec on a larger dataset that’s domain-specific if necessary (e.g. law)  thanks!!!!just to add to the noise, i\\'ve used skip thought vectors a bit... they are very interesting but the author\\'s implementation tends to kill your python process with floating point overflow errors so if just for that reason it\\'s worth pursuing slightly simpler solutions like tfidf and sum as   is suggestingstatistics without borders:   sign up for the september barug  meetup here\\'s an interesting read on earnest!   : in my company it isn’t based on paternity/maternity but on the primary care provider.  (although, looking at that sheet above, maybe that is not uncommon.)who else will be at pycon?perhaps of interest, github just released a new 3tb dataset: nuggetbot animate me happy gilmore nuggetbot animate me grateful dead: hey smart data scientists! does someone want to explain rectified linear unit and the hidden layer to me?over the chat ?  i suggest you read this : : thanks chris. let me take a look at that.okay, so ```an example neural network would instead compute s=w2max(0,w1x)s=w2max(0,w1x). here, w1w1 could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. the function max(0,−)max(0,−) is a non-linearity that is applied elementwise. there are several choices we could make for the non-linearity (which we’ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. finally, the matrix w2w2 would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores.```i get that you could do that. but why would you do that?why the hidden layer ?sure, what advantage does this have over not having ita series of non-linear transformations (each hidden layer does this) can be a very powerful non-linear predictoryou can think of the original features undergoing a series of non-linear transformations to a space where classifying them becomes easierwhat\\'s your definition of linear in this case?the rectified linear unit is piecewise linear, but not a linear function, other activation function are also non-linear like the sigmoid or tanhso the features space is mapped to a different one that cannot be described by a linear functionor better said a linear transformationwhy is non linear better than linear?so in this example, say we do w2max(0,w1x)swhy is that better than w2(w1)s?don\\'t we lose information by taking away the negative values of w1s?same reason you might want to use a svm with a non-linear kernelhi i’m a remote session 2016 b fellow with a world of problems on her project - but right now, i’m looking for a bootstrap expert. i had working flask/bootstrap code and a beautiful webpage on my mac, but once i tried to host it on centos, i lost all the images and the bootstrap nav bar. loading text files and other images outside the static folder works fine. i am very confident i have the right address - i had two other fellows check it too.anyone have any idea why mac working bootstrap code wouldn’t be working in centos? :confused:: are you serving your site with gunicorn? gunicorn doesn’t serve static files.no… haven’t even made it to gunicorn yeti’m just typing in the ip addresson my local machine - everything else loads just fine. just nothing in static or templatesand i checked permissions on those folders, it’s not thati strongly suspect this is a centos/bootstrap problemstay positive loisthanks nuggetbot but i feel like you’re taunting mecould it be a path issue? i had to change the paths in my html.i changed all the paths and have double/triple/quadruple checked themi did relative and hard pathstried with and without the leading ‘/\\'tried with .. and witouti get errors that look like this:it might be useful to know what http status codes are being returned for the bootstrap requests. you can get that info by loading the page with your browser’s developer tools open to the network panel uploaded a file: or in the log which you just postedgreat minds think alike :wink:so they’re 404si tried googling this set up too and working through all the stackoverflow pages with suggestionsme and 2 other fellows have been working at this for about 3 hours… we’ve tried a lot of different things :disappointed:this is happening with the other fellows’ apps, too? or just yours?just mineyou could use a cdn for the bootstrap files, but that wouldn’t help with your imagescdn?this is a really good suggestionbecause i can try to put the images where my output files are to load them inand see if i can get the cdn to workthanks  !!i am going to go for a run and then give it a try :slightly_smiling_face:you might also consider just moving on to gunicorn because you’re going to have to do that eventually, and there’s not much to be gained by getting it working on the flask server in the mean time.good luck, !thanks :slightly_smiling_face:: that worked perfectly!!! you’re my hero of the day!!!yay! i’m glad you’re up and running!….  !!!!  thanks for your replies the other day on the multi-label issue! i wanted to ask for some clarification... so multilabelbinarizer can be used with any sklearn classifier, and this is just a way to feed in the target…  but i believe we have to use either the onevsrestclassifier or onevsoneclasssifier meta-estimators? whereas the multi output classifier is both multi-label and multi-output, so it doesn’t seem to apply here… ? thanks!link for the lazythis might justify an @ mention for the whole channel. i’ll let you incur the wrath of the @ channel police. :stuck_out_tongue:has anyone been to datascience: elevate? what was your experience like? hello everyone. we are trying to deploy cassandra in production with very less experience with how the deployment should look like. could someone suggest few thingsdo you really need cassandra?  managed nosql databases like aws’s dynamodb or gcp’s datastore/bigtable have a much lower bar for developers to use and maintain.thanks so much for the reply aws is out of question in first release. we are deploying cassandra now some of the  questions are do we need physical machines or vm. what kind of disk. what is the max disk we can addwill your team be operating the cluster? yes we will be managing the cluster too i work at datascience so yes :smile:  this is the second elevate event we’re doing so the n is small, but the first one had some great, very high quality talks from la companies and ones that are on the national scale ones.  there were a lot of people from great companies big and small in attendance also.  the talks included i think a really nice balance of covering companies’ specific data science uses cases and technical solutions (types of models and technologies), and tying them very well to the business.  i was actually surprised that everyone brought such consistently high-quality talks, since we don’t control the specifics of any given talk and only the general themes, and i’ve seen a lot more fluff at other events i’ve attended before.  whereas the elevate talks were packed with substance.  but i think our event coordinators spend a lot of time vetting and filtering the right people.we have some great people coming again for this one, so i expect it to be very good.  let me know if you have any questions.anyone know if it\\'s possible to use the rangepartitioner for pyspark? i only see a way to access it in the native scala api....is there anything like “schema registry” for kinesis?i never found one. i wish there was!both those issues are consistent with their core offering though..they’re basically just trying to be a simple gui front end to whatever your data warehouse is for users that are not interested in editing lookml or writing sql. they’re not making any claims of being able to speed up queries beyond what your backend is capable of and i don’t know of any other companies/products (accurately) making such claimsmy issues were more on the limitations of that gui…plotting options were often limiting (though quickly growing and customizable if you’re not running a hosted solution) and i was curious if others found domo or other products less limitingour main performance issue is essentially that you can\\'t force aggregation to happen before joining. this effectively increases the run time complexity of many of our reports.  you might not mind it in red shift,  but it\\'s still poor design, and wanting better is not asking them to \"speed up queries beyond what your backend is capable of\". politics + linguistics + data science! “semantics — what does data science reveal about clinton and trump ?” by maixent chenebaux hey guys, thought this was a good cheat sheet for big o complexity of different cs and machine learning algorithms^ very good! much useful.has anyone had luck logging into facebook on python? if so what did you do? thanks!daniel lakens (of the ’no more statistically unsound science!’ revolution) has a brand new coursera course: : thanks a ton, that is super helpful!datacamp is hiring for multiple roles related to data science:curriculum director: lead our curriculum team and help to define what it means to be a data scientist in the 21st century. you will shape the future of our curriculum and expand our network of world-class instructors.data science curriculum lead: help us engage with leaders in the data science community who could become instructors, and work closely with them to design new courses.data science content developer:  work closely with instructors to build new data science courses. you will help create exercises, slides, and scripts, and most importantly be an advocate for our students.data scientist (growth): help us make sense of what our users are doing so that we can better serve their needs. you will be the first technical member of this team, so you will have lots of opportunity to make a big impact.you can see more info here:i used/battled with the instagram api for my project. happy to help!generally speaking, if i\\'m scraping multiple sources for a set of values, is it worth the computation time to use fuzzy string matching to keep it resilient to minor changes in formatting?does anyone have information on best ways to identify and store metadata on faces in videos ?the requirement is to identify human faces in the videos and then store, the related metadata - for eg., vectors of only those that contain the faces. i am wondering if anyone has experience with this and can provide some guidance.does anyone use any internal tools or services that patrol aws/gcp costs and look for efficiencies in some sort of automated way?for example identifying scheduled jobs or queries that all scan the same data every day, meaning a new rollup or aggregation might be a good idea you write this for fun with your coffee breaks, i assume :wink:lol :joy:on a train right now but can dm you later tonight also does slack mobile not let you start threads? :disappointed:yeah it’s awful about thatif you single press the message where you want to start the thread a new screen will appear with a box to start a thread! thank you you\\'re right, it is pretty much zero. but the quantity being calculated is a ratio of 2 differentials, i.e., how much the prediction changes when you change that delta. just as the slope of a curve, you can\\'t computationally have deltas, but you can extrapolate the derivative as delta approaches 0.i\\'m not privy to which algorithm they use for this extrapolation though. uploaded a file:  and commented: did a seed vs. seed analysis going back the last 30 years, enjoy! commented on ’s file : i hate you all.: . i still need to fill out mine though jejejejdoes anyone know if there is an easy way to change the distance metrics in sklearn classifiers?: which classifier(s) were you thinking of?, the easiest way is to put in a distance matrix.  it will take sparse matrices as well.that question doesn\\'t necessarily make sense, depending on the classifierdistance metrics... euclidean or bust!: no manhattan :trollface:actually better yet: chebyshev distance :troll:hmmhi, guys, i am wondering if any one knows a good tutorial or source site of ml in c++ ? the only machine learning stuff i\\'ve done in c++ was with open cv (computer vision). there is a ml part and a separate deep nn part.   thank you so much,  . i will take a look and would be coming back more question. i guess. please let me know if you have any source of this.  :smile:heads up: meetup is considering hiring another data scientist. nothing posted yet, but thought i’d put it out here first. dm me if you’d like to chat about it.anyone have any tips or suggestions for mobile app a/b testing across multiple devices (android and ios primarily). bayesian methods like multi armed bandit are something we\\'d like the solution to include. a third party vendor is fine, but we\\'d likely need to self host due to phi/pii. oh, i\\'ve also seen that you can deploy changes to an app without deploying a new build. does anyone have experience with that? hbo talked about doing this with their neural net for the hotdog app. thanks ! would love to get your input/help whenever you get the chance :slightly_smiling_face:hello everyone - i\\'m trying to get a large dataset from an external ftp server to s3. i\\'m currently downloading the files by sshing into my ec2 instance, and running wget to download the files from the ftp server to my ec2 instance - i\\'ll then upload the files from my ec2 instance to s3. the download from the ftp server is slow, and is taking a while - i\\'m not sure if this is due to the bandwidth of the server, or if this is due to the fact that i\\'m downloading a large number of small files. anyone have any ideas on how i can increase the throughput? (here\\'s the ftp server: )what happens if you try to download several files from the ftp server simultaneously?the server seems to throttle me to 3 parallel connections per hostthis might help excellent! thanks, that looks like exactly what i needthanks again guys - this saved me hoursdoes anyone know how to compute the efficiency with true positives, false positives but also unknown detections that you cannot validate?: if you are only concerned with speed, you might want to consider using cython () to compile parts/all of your python code. last time i checked cython was just a tad slower than pure c. also, if you are familiar with java, take a look at  scala -- a very concise language that offers the speed of the jvm and relatively easy parallelization thanks to tons of nice java libraries that it can use natively. i want to use the dirichlet process gaussian mixture model.   do you have a good reference/doc page that you can give me?  i have been looking all over.: there\\'s also waffles, which is written in c++ :  thank you very much guys. i will study them and may come back with lots of questions :sweat_smile: thank you so much guys , keep posting me any other references!!!: i have found vw pretty good at times, its pretty fast and written in c++, unfortunately docs suck : i was thinking of clustering algs. it is very straightforward to have a custom distance matrix there.  for the model you referenced, it sounds like you want a custom covariance matrix? i\\'ve only taken a brief glance at the source, but i think you could subclass sklearn\\'s dpgmm or gmm and override _get_covars and _set_covars with whatever you want.: ok! thnx.  was hoping not to go that direction but looks like that might be the only way...: thanks for the source. at least, this try to be detail in its contents. worth to look at. so you have tried this?: yep!  code runs well once you figure out how to tell it what model/parms to use.  good luck!: thank you, i will play with it and come back to you :smile: thanks ~~does anyone have any experience with non-negative matrix factorization packages in python? i\\'m thinking about scikit-learn\\'s nmf and biolab\\'s nimfa, but perhaps i\\'m missing some others.stay positive alex7non-negative is the best i can do  :wink: i\\'m using nimfa.  if you find something better let me know.  there are a couple packages in matlab as well.  r also has an nmf package f y ithanks guys!i\\'m writing python code, so i guess i\\'ll stick with nimfa or sklearn.: any reason in particular you\\'re using nimfa over sklearn\\'s nmf?nimfa\\'s api gives finer control of the l1 regularization out of the box.  basically i want to increase sparsity in one factor but don\\'t care about the othermakes senseone last question: do you know how it deals with missing values in the utility matrix? does it impute them, or does the cost function just skip over them?i don\\'t know offhand but i assume you could control for that by preprocessing the utility matrix.  thanks  ! i\\'ll investigate furtherif anyone is interested in running 5k/10k/half sometime in the spring, a few of us are starting to look for races in  - maybe a fellows reunion run where we get insight jerseys. join in if you are interested!to cover letter or not cover letter, that is the question.i don’t find prose format particularly useful at the resume stage of applicants.  portfolios + key skills sections are a much better alternativeagreed with , i will read a cover letter out of respect if it is attached, but it will more often sway me to reject than to accepti’m also put off by cover letter requirements on teh applicant side of things.  when i was applying for positions i always skipped that part even if it was required, opting to miss out on the opportunitytoo much fluff needed in that format for my liking :slightly_smiling_face:hi all, i\\'m a current fellow that also  has a 1.5 year old son. i just started the channel  to commiserate about how our kids are ruining our careers (joking). more seriously, i\\'m really curious as to how everyone has navigated their careers with a family.i’m taking a look at this hortonworks schema registry, supposedly it doesn’t require kafka wow, cool. i’ll give that a look.welcome fellow fellows!can you invite others? i\\'ve invited everyone from our class, but it\\'d be great to have all fellows on this same slack, don\\'t you think?hello world!tomv: how\\'s dow jones / news corp. treating you so far?hi all!hello!: fabulous thank youi\\'ve only deeply offended 4 people so farso it\\'s a major winalthough i\\'ve spent the last week doing nothing but flask -- the irony has not escaped me....was one of them rupert? :simple_smile:unfortunately not, but my next target will be one of the fox news talk show host/giphy fox/giphy hannitylamehahagreenhouse all good?very good. lots of interesting data and good nice co-workers. i don\\'t feel like i\\'ve been doing too too much yet, but i can see some meaty data science on the one-month horizonshould i start an #impostersyndrome channel? :wink:we are hiring a senior data scientist on my team at square. a lot of happy insight alums here! happy to answer any questions. question about evaluating offers that i’ve had a hard time answering via internet searches: what is the value of stock options in the case where the company doesn’t ipo or get acquired?: have you taken a look at this? hadn’t, will now. thankstl;dr: if it’s private, you can only estimate to the best of your abilitiesshort answer:  absolutely nothing.(to the question of what is the value if no ipo or acquisition). this is on top of the usual estimating you can do about the potential value in an ipo and all the the other caveats such as vesting, dilution, etc.statistic i got from a vc back in 2008:  1 in 10 of funded startups result in an ipo, 2 of 10 in an acquisition.and absent those exits, shares (or unexercised options) are essentially a worthless, illiquid asset, right?yes. though there can be exceptions such as the recent twilio buy-back: …but they may be preparing for an ipo so this may not be much of an exception. a colleague who has exercised some twilio options said the repurchase offer came with a very strict nda - it wasn’t officially supposed to be public.preferred stock is usually investor stock.  employees have common stock.  although sometimes some founders elect to participate in a funding round and purchase some preferred stock too.hey co-fellows! does anyone know of a ready-to-use method to get feature importances from a multioutput randomforest classifier in scikit-learn? thanks!, hadn’t seen it, thanks!: i started a new channel  as a place to discuss tools that improve the quality of life of data scientists in the wild. there is a rapidly expanding ecosystem of ds technology out there, and hopefully we can crowd-source insights about the best solutions.tcmoran: excellent idea!hi all. i\\'m working in a poc for a recommender system based on cf. i recently came across variational autoencoders and though it might be a good idea to use them to generate new samples in the input domain in order to reduce sparsity. any thoughts on this?anybody going to google i/o?\"you\\'re going to overfit!\"i have hopefully one last question - the webpage works great when i run using ./run.py from the command line, however the output page doesn’t work when i use gunicorn/supervisor. it gives a syntax error:     cur.execute(query2)programmingerror: syntax error at or near \"(\"line 1:  create table  (date timestamp, steps real, distance real, c…and i’m not sure why this happens with gunicorn/supervisord but not with running the python 2.7 code directly (./run.py). any thoughts?we figured it out :slightly_smiling_face: it was an issue with time in gunicornsays it runs on vm. i wonder if that vm has access to gpu.  linkedin is open-sourcing a very useful tool called \"dr. elephant\": seems like a very useful site especially for people getting their bearings in python:thoughts on the best unit testing package to use in python?  specific use case is to unit test on functions heavily dependent on luigi.we use unittest, it works, i like it, i don\\'t have enough experience with the other frameworks to give a pros/cons list though. :confused:anyone did kaggle competetion: allstate purchase prediction chanllage?hi all, does anyone have experience working with large scale geo-location data? in particular, i am looking to find a way around using the haversine function.using the mgrs grid system is a good way to organize geospatial datai\\'ve used mongodb geospatial indexing in the past. easy to configure and fasti understand elastic search has similar capabilitieswhy do you want to find a way around haversine? is that a bottleneck for you?analyzing the data for feature-selection before making the train/test split. i was guilty of this during some data challenges i did at insight :stuck_out_tongue:. there\\'s a good discussion about it in elements of stat. learning, 7.10.2.i do, long story short is kinda of a pain in the butt so far for me but i am managing to do it... my tech stack currently looks more like a babushka than a clean barbie dollfor fast processing i use mapd but it\\'s geospatial capabilites are still nascient and it\\'s scale still limited... so i had to combine this with postgis (not scalable)... i still haven\\'t found a full scale solution... i am looking into geomesa now (spark-based geospatial processing engine)as of haversine, what are u specifically doing there? thanks.  thank you. the data is already in a sql db and cannot be moved to another db. i am trying to find locations that are in a certain radius of another list of locations. the large scale of data makes it very expensive to compute haversine for all of the data.i am looking into mgrs right now.+1 to  if you organize points on a grid that could help and then you\\'d approximate the distance from point a to point b as distance from grid centroid that point a belongs to, to grid centroid that point b belongs to... you can easily convert points to x miles by x miles grid using any db, i did that in aws redshift using python udfs... and then distance from grid a to grid b is known since you built the grid.. i personally built my own grid since mgrs cells are too large.. look here: as of the data being in sql it sounds terrible to have large scale data in a sql db... but well, it shenanigans are always hard to fight...look into vowpal wabbit — lightning fast linear learner seems like your data is in dense format, even though it’s sparse, if that’s true, i would recommend converting it into sparse format, which will drastically reduce the size of the data and allow you to try wide range of models. one more suggestion, try other encoding schemas other than one-hot, this will help with size of the data. some that come to mind are weight of evidence, label, frequency and mean encodings. in my experience they work better for categorical features with higher cardinality,additionally, some algos allow checkpointing. this will allow you to train in batches, but personally not a fan.engineer humor uploaded a file: watch out! if you flat map too much, the beer is going to get dumped in the cooler! :wink:i’ve found postgis very useful. it handles a lot of these grid computations very wellgreat idea! just learned pipeline in the odsc!these are all great ideas! thank you! another thing i am thinking is cat2vec with keras.facebook median salary at 240k... that seems highanybody find a clean way to insert github, email, linkedin symbols in word for resumes?: fontawesome font, install it on your computer and then save the resume as pdf. you can find the cheat sheet here and just copy and paste the icons into the word doc. thank you this is currently my favorite data science application:   so, so usefuldoes anyone know of any interesting papers/tools for conducting audience segmentation in news media -- e.g. identifying engaged vs. experimenting vs. unengaged readers? i\\'m currently reading this paper  but am looking for something for recent.for the jupyter fans out there: has anyone played with declarativewidgets and jupyter-dashboards?for those of you interested in attended pydata in sf on aug 12-14, use the code insight for 15% off early bird admission and 20% off regular admission. hi all! i posted a couple months ago about an open position at coach, inc. for a data scientist. right after this, we acquired kate spade and hiring was put on hold. the position is open again, and now we\\'re a bigger company with even more data. data science at coach is a green field right now and we\\'re only limited by having enough people to get the work done. our work touches all levels of the company from understanding the customer, optimizing marketing and pricing, dealing with fraud, forecasting sales, product design and advertising, hr analytics, etc. here\\'s a link to the job description, and please let me know if you are interested:hey guys, got wind of this awesome plugin for jupyter notebooks on atomhello fellow insighters (hah, see what i did there....i\\'m here all week):has any worked with twitter data where they are trying to find the number of hashtags mentioned in a certain time period? how easy/difficult it is to do this?i work for boston, and the mayor\\'s office wanted to get some data on how many tweets are made using the #mbta hashtag for all you boston alumni. the mayor mentioned to the press that the mtba was generally reliable, so we\\'re trying to get him the more correct information so he\\'s aware of mtba\\'s performance. :wink:\"once again, the train is on time. #mbta\"i guess all of you could do some mad social engineering to the mayor if everyone hashtagged positive comments. the idea is to have a dashboard in his office so he can look up at a screen and see how the mbta is doingwho is running mbta, is this information accurate and can we get them to come fix the ny mta?haha i\\'m not sure you want the folks running the mbta to fix up the mta...boston\\'s transit is notoriously terrible as welli mean, for a system that\\'s as flammable as it is, the mta works pretty ok, until it catches fire.maybe we could do a mayor swapsomeone created a website that tells you if the dc metro is on fire  . i\\'m sure they could use some help tooas someone who has lived in both boston and nyc, i\\'ll take the mta over the mbta anyday. except i\\'ll trade the card taps for the card swipes.are they trying to measure the sentiment of all tweets with the hashtag #mbta? or just the number of them? because if it\\'s the former, it\\'s not going to look pretty. i\\'ve met a lot of people that only joined twitter to complain about the t to the t.no sentiment. just raw number.the point is actually to get the mayor to know that people complain about the mbta, alot, and to get him to noticesomeone from the policy office was frustrated at his comments that the mbta \"generally runs pretty smoothly\"yeah, right. i especially liked on the t scheduled daily interruptions of the d line during morning rush hour (from 8:30 til 3pm) for scheduled tree removals. couldn\\'t they have done this not at rush hour???sometimes marty says some pretty weird sh*ttoday i learned that the mayor of boston does not have a charlie card.he gets escorted by a security detail from his dorchester home so...yeahditto to what kevin said. it is great if you may have multiple different clusters of outliers. but if you are looking for a \"normal\" cluster and an \"outlier\" cluster perhaps something as simple as k means may work. i would personally run dbscan and set a frequency threshold like if the cluster didn\\'t have at least  1% ( for example) of the samples it would be an outlier. fix that threshold based on the parameters of your study. thanks to all the replies! you guys rock. ```in 2016 google\\'s ai had an iq of 47.28. it came out ahead of chinese search engine baidu (32.92) and microsoft\\'s bing (31.98) and had almost double the iq of siri (23.94)... in 2014 google\\'s iq score was 26.5,```pretty impressive, almost 2x improvement in 2 years. wonder if it plateus now    thanks so much for all the input!  i may ping you guys individually.  i’m seriously exploring this because i may potentially need to get this set up!alecalthat’s what i do. i communicate clearly to both sides “person x is looking for a job at your company”; the point of the chat (with resume) is for person x to learn what the company is really like and the position actually is, and for the person at the company to meet person x so they can make a more informed decision if they want to recommend them. you can often hear about job openings before they get posted online if you ask around. also a personal reference carries far more weight than a random online application!oh that sounds perfect then! :smiley_cat: uploaded a file:  uploaded a file: to fellows in the nyc office. i lost a pair of blue bose headphones. please let me know if you see them around.  thanksluigi vs airflow wondering if anyone has experience or opinion.  ps relatively unsophisticated data team.   we use luigi pretty extensively at work. so far, so good. relatively quick learning curve to get people writing tasks and we haven’t had many issue. happy to dm if you’ve got more directed q’s about it. can’t say anything on airflow. i’d recommend talking to  about airflow. he’s used it quite extensively. we looked at both a few months ago and decided on airflow.  luigi is really elegant and simple but ultimately we needed the full functionality of airflow to accommodate a pretty complex workflow.  i\\'ve found airflow to be frustrating at times but it meets our needs.  if your workflow is standard and relatively simple, i\\'d pick luigi.  it will be easier for your data team.thanks  and  maybe i will try bothran across this recently:  (look for links to more extensive reviews in row 3)we use luigi on my team. the choice was made before i arrived, but i’m told that luigi won over airflow because it’s really easy to install.: this might be a good place to start: what do you use/recommend for scraping sites, beautifulsoup?!old school regular expressions!but sure beautiful soup is okay.. if you want it the easy way. heh.yes beautifulsoup is the way to go.scrapythanks  and !if you need to digest pages and interact with javascript on pages in an automated way, you might try selenium. i haven\\'t tried it myself, but i have heard rave reviews.: damien had suggested splinter instead of selenium.  from my experience, selenium isn’t the fastest.good to know, thanks!splinter is also very easy to use used selenium for his project i believe.hi channel, does anybody have experience on hyperparmeter optimization on a large dataset?  can i optimize hyperparameters on the subsets of the dataset, and use the average of these to train my model on the whole dataset? do the hyperparameters of the machine learning models depend on the size of data?: what type of model are you using? in an lda (say), one hyper-parameter controls the number of topics per doc, which can definitely depend on the size of the corpus. another controls the words per topic, which wouldn’t depend on corpus size provided your sample of the data is truly random.: i am using supervised models such as random forest, gradient boosting. so i am trying to optimize number of trees, tree depth, etc.yes, it’s totally ok to optimize on a subset (esp if dataset is large) as long as it’s representative. nice thing abt rf is that it doesn’t need much tuning except ntree. gbm, gd luck with tuning =pthanks  isn\\'t tensor flow using the gpu instead of intel? which instance are you using?^^exactly! that’s why this is so confusing. i’m using a p2 instance, which uses an nvidia gpu and an intel cpu.  the time per step does seem to show slow downs at intervals equal to my batch size, which suggests that the data loading is even more of a bottleneck. but shouldn’t the training be largely unaffected once it’s on the gpu? because if that assumption is correct, a 15-20% overall slowdown would suggest that data loading of the intel cpu is slowed far more than just 15-20%... hi all, there\\'s something i\\'ve been wondering about recently and hope there are people here who can share some perspectives. i\\'ve noticed a few of my younger acquaintances have recently been hired as data scientists straight out of college to work at established companies like facebook, apple, etc. in my mind i would like to think that data scientists with a phd background offer value to companies that ba\\'s and bs\\'s cannot offer, but i\\'ve never had the opportunity to find out for sure. so my question is, for those who work with both phd and non-phd data scientists, do you see a noticeable difference in quality?i think there\\'s a lot of ds work to be done all around. some work is more about designing metrics, coming up with hypotheses, designing experiments, visualizing data, testing, etcsome is more like de, some is designing new algorithms in deep learningphds may be more biased to enjoy certain areas and non-phds may gravitate to other areas tooi think there\\'s some truth to the stereotype phds typically do spend longer on projects and have a tendency to go for 100% perfect, while non-phds get shit doneso if you\\'re a huge company and have 100 roles to fill, you might be better off taking a super-smart person from a bs and training them to be a great ds rather than taking a great academic and training them to focus on short-term deliverablesthere\\'s a need for both, to be surei\\'m curious to hear more of others\\' experiences. here at salesforce, people hired at the bachelor level are \"data analysts\". i think they can level up to data scientists through work experience, additional classes/training, and achievement. phd and masters level are treated as work experience and specialized training and start at a higher salary/grade.i think our company is similar to what people described above. we have a range of degrees, and more of the bs/ms people are in bi positions, while the phds are all in r&amp;d. one of the big distinguishing factors for phds in my experience is the ability to work on larger, unwieldy projects. that’s certainly learnable, but i wouldn’t expect someone right out of college to be able to take on as large of projects until they were more experienced.i\\'ve worked with both types of data scientists (as well as sat on hiring panels for both), and i think there are advantages as well as disadvantages to hiring either. phds do have the added skill set of thinking scientifically, but we tend to have a tougher time transitioning to the speed of industry than someone out of college. companies typically will hire college grads into lower level positions, whereas oftentimes hr and other factors dictate that phds must have higher salaries and roles. i\\'ve also noticed that some companies prefer to train up their own data scientists from scratch (for better or for worse), which again lends itself to college grads at lower levels. ultimately it comes down to the preferences of the company and hiring manager. i agree with \\'s point about it all being conditional on the company/culture/needs. i would add that there is also a lot of value in a phd knowing what you don\\'t know, which is typically not the case with a bs. that might not matter in certain company/culture/needs, but it definitely matters in some environments&gt; value in a phd knowing what you don\\'t know:100: this is clearly from what i have seen across my industry, and i mainly work in the area of cybersecurity, fraud and malicious behaviour - state/antistate/criminal actors. there are two kinds of phds ones who have graduated with work/thesis where their pi is running a project and these students have contributed a large body of work that has earned them a phd. the second type are students who work with pis where there is just a pile of data (call it pile of junk) and their pis wants them to explore and come up with insights/hypothesis/proofs/algorithms.   the second type is purely exploratory. you fight with the data, do years of analysis, come up with ideas about projects/thesis then identify insights, create hypotheses, and eventually publish them and earn a degree. when  phds transfer to the industry, the later kinds are the ones who are the ones who get into positions of driving projects from start to finish *faster* as they have done the same thing multiple times, that does not mean that the former kinds cannot, it just means that they need to be bootstrapped a bit - work with a seasoned analyst/scientist etc, and they get the hang of it. the hang of it being owning a project, the success and the grave failures that come with any research challenges. this approach is evident across all people i have worked with. people with with bs/ms degrees are not much different and that is the reason i really do not focus on the degree one has earned while hiring them. i primarily look for ownership of the project. can you own something? can you look ahead and think of pitfalls, validation mechanisms, can you imagine pitfalls, what are your recovery plans, contingency plans etc.  few folks with bs/ms degrees too have gone thru similar training and that makes them not much different than us and they can operate autonomously - that is important. but this number is small, because bs and ms does not give one the luxury of time to explore data for extended periods of time. so, quite a few folks with bs/ms degrees require some amount of boot strapping.  *my experience comes from a field where there are no prior algorithms and you almost always have to start from exploring data.*  that might not be the case in other fields.i was going to reply with something similar from the finance perspective but i think that about sums up what i\\'ve seen.  only thing i\\'ll add is how bad a new hire is if there is a mismatch between the manager\\'s expectation and the new hire\\'s actual ability to own their own projects.  in the past, we\\'ve made the mistake of hiring people of the latter type under managers who expected them to do their data science manual labor for them.  those hires end up leaving once they realize the opportunity for career growth doesn\\'t match their expectations.we\\'ve also made bad calls in the other direction: hired people who need a lot of hand-holding when we expected them to work independently.  those people end up finding different managers.purely by experience, i have figured that you could have the mismatch in hiring the one talked about by  . in that case, i it would be great if the manager takes up the onus and is flexible to operate in both the styles, makes it harmonious.thanks for all the thoughtful responses. it was helpful to realize that there are many different types of data scientists out there, and at the end of the day it\\'s about finding a mutual fit.i do find that phd data scientists on average offer advantages.  but it\\'s also true that i\\'ve worked with some bachelor data scientists who were extremely talented in all sorts of things, from modeling to experiments.  so it\\'s not surprising to me at all to see companies hiring these people (assuming this is who they are hiring) straight out of college.  as good scientists, we should keep in mind that just because the average is some amount, doesn\\'t mean there aren\\'t a handful of exceptional outliers :slightly_smiling_face:and just as an anecdote, someone i worked with in the past started with just a bachelors and behind the phd  and masters data scientists at his organization, but within the course of a year went on to basically surpass all of them in application of modeling techniques at his organization (though not quite as strong on the experimental side as some of them).  woudl definitely hire this person out of college easily, if i could somehow identify them :slightly_smiling_face:i would be happy toanyone have experience with building a google chrome extension for the final deliverable of their project? currently, i\\'ve implemented a flask-app that takes in a url, scrapes data, generates features, and outputs a probability using an embedded model. i\\'m thinking an extension would reduce the friction of physically visiting my webapp and inputting a correctly formatted url--it\\'d ultimately still report a probability. i\\'m just not sure if this is easy to do in a few days, or if i should just stick with snazzing up the web app. thanks! ^^that article looks cool, but wow, that photo is majesticwe have microservices (maintained by de outside of juypyter) that provide access to what\\'s in s3. then jupyter notebooks can make api calls to those serviceshi, sparkcognition (my company) is desperately seeking for data scientists. this is an ai startup (founded in 2013) and grows bigger every day. it currently has 130-150 employees and 1/3 are data scientists. there are lots of cool data science projects (nlp, vision, autonomous driving, finance, security etc.). feel free to connect with me if you need a recommendation. if anyone is going to the grace hopper conference this year and wants to come to the ghc square events, shoot me a dm!thanks for posting!is referenced in the post and seems super interesting, but i’m wondering how a distributed protocol like this is not susceptible to malicious behavior. : deep learning and nlp, via the useful @pythonweekly on twitter ^^ tufte is good here too: thanks! that is very helpful, do you mean programatically within sql(redshift) or using python to query and find counts?my dev-sql skills is not up to par, but i know it’s fairly easy through python/pandas to get column names programatically and do sums and counts.the prob with that is that it requires me to load the whole data to a pandas df, which is not good (i guess i could use chunksize and it would be a generator)you don’t need to load the whole data, just do a count(*) or somesuch on your table.the alternate client is tempting, but i fear we have a network effect that doesn\\'t transfer :disappointed: my favorite:  really good for communication in generalwish you were a few months earlier! i love boulder!  i\\'m going  : moving into my new place tonight :s but would love to hear your impression of the meetup after sure.  maybe lunch next week?: with pleasure. i\\'m working somewhat near insight. calav a place where you\\'d like to have lunch?opinion piece on toxic data: sounds great.  monday at noon, you pick the place on cal ave: how tempting to get good old chinesei can taste general cho’s chicken already :simple_smile:hello channel: do you guys know of a way to retrieve query status when you connect to a database server using sqlalchemy?  waiting and hoping for the best is not the best solution, and would be great if there is a way to retrieve query status while waiting!are you using a cursor to query using sql or are you using the orm?i am using a cursor to pass sql query to the databasei mean, she wasn’t wrong thoughthis mess is a place^ a universal statementnice to know twitch keeps you slack game up to parmy company, pear therapeutics, is hiring for a sr. data scientist with potential to lead an experimentation team. you should excel at experimental design and advanced statistical modeling. the position will include the exciting opportunity to lead the development of a scalable system to test the efficacy of pear products. chance to work at a rapidly growing sf startup that’s doing a lot of good. feel free to message me if interested  question for data scientists from canada working in the us under nafta tn statuswhat tn profession did you indicate? computer systems analyst or mathematician / statistician?started by same author as  and : i\\'ve been using the xlsxwriter package with pandas, and it is pretty intuitive and has url functionality. : a somewhat random request for those that are active on this slack (alumni and current fellows). would folks mind updating their profiles with their insight program-location-session, current role &amp; company? i find it useful to know more about people than just their username, especially if i haven’t met them in persongreat suggestion aren\\'t you on a boat in greece ?alright. i no longer do \"sweet, sweet data science\". i have a boring job title in my profile. unfortunately the boat has ended :wink: in seattle today you should have negotiated for a less-boring job title like `sr. director of data science and chief slayer of dragons`if only i was a good enough data scientist to predict how bummed i would be about the situation.it\\'s a great idea! i saw someone doing that a few months ago and changed mine to matchdone!: anybody here has been working on churn prediction? i would like some pointers regarding where i can start learning it. thanks! and  i thought we were c? did it for his insight projectno more @ here ing uploaded a file: looks like someone woke up the ruth monster i had the same q earlier and had a chance to talk to some of the fellowsi can give you a transcript of the conversations if you’d likeabout not using @ here, duly noted :slightly_smiling_face:, would like to see the transcript.there’s a channel we setup last session i believe for fellows to chat about churnno reason that alums can’t use it too :slightly_smiling_face:thanks! looks pretty empty, not sure if slack has deleted old stuff or fellows didn’t end up using itprobably got deleted by the output of i didn\\'t even know about the channel stop blaming the foreigners  stay positive till_bethere is a ltv channel, is case anyone is interested. i created it couple of weeks ago.: i just used survival analysis and bullshitted my ways through the demos  don’t share the secret sauce!i think one important thing is that your definition of churn will heavily depend on your business case i’ve seen others do survival analysis (non-insight), is there an issue with not doing cv? i.e overfitting on data?i’m not too too familiar with survival analysisme neither. i think i ran it on different samples to prevent overfitting but i can’t quite remembermight be easier to approach it as a classification problemyea, i was wondering what the pro/cons wereherelet’s move this to lolnuggetbot animate me pikachucrap wrong channel sorrystay positive scstarkdoes anyone know if there is a common statistical analysis that allows you to evaluate two outcome variables at once? for example, fitting a slope and intercept to each group in a study, and being interested in whether other variables are correlated with changes in those slopes and intercepts. it seems like evaluating one in the absence of the other may not be appropriate but i\\'m also unaware of a common technique that allows you to take both into account at once. in the case of this study, the slope and the intercept both reflect two different specific ways in which the other variables could relate to a change in these outcome variables.i\\'m not sure if i\\'m explaining this clearly enough.: they\\'re bread and butter macro-econometrics, i\\'ve never seen them in ds. so they probably suck at prediction.stay positive patrickdmy first nuggetbot :heart_eyes:: in the simplest case you could try a mancova, or you could look into structural equation modeling. if you want something more focused on prediction rather than inference, you could try multivariate tree boosting () long live net neutrality!: if both variables are dichotomous, you might want to try something like a bivariate probit (), or else some sort of seemingly unrelated regression (sur) setting if you can comfortably assume that the relation comes through the disturbances (). clearly both methods would be inferential...thanks for the suggestions everyone, i\\'ll take a look at these.there’s a mention of a y combinator experiment in this article. does anyone know anything about that?ubi (universal basic income) is all over the news recently, partially thanks to the swiss’s (rejected) referendum on it. y combinator is supposed to be running an experiment unconditionally giving out ~$1000/month to some hundreds of families in oakland (just remembering these numbers from my head - don’t quote me on that) to see how it affect people’s behaviorsthere was a couple of announcements on hacker news as well (in case you didn’t see these yet) - does anyone have experience passing variables in flask between html pages/ app.routes? thanks!figured it out using session!!hi can a de person help me out by confirming / correcting my plan to take data from our current internal oracle db, convert it to parquet and store in aws s3?(private message if interested)seems like a good plan if you’ll be analyzing it in spark in the futurewe currently have an internal spark cluster but we’re migrating over to aws. i was just curious how/when the best way to do this is.we currently use sqoopanyone used airbnb’s superset (née caravel (née panoramix)) with nosql databases? perhaps mongodb?the git issues are unpromising/”left as an exercise to the reader”-esqueyeahi\\'ve done some with ithi! i am looking for volunteers to represent insight and tell the story of your transition to data science. the catch - it’s through the lbnl postdoc association and they would like specifically people who did their phd or postdoc at lbnl. it would be in the evening of 3/7 or 3/9, please let me knowbay area peeps: my old company (palantir) is hosting this event in downtown palo alto on thursday 2/23 that looks pretty cool — i have school that night or i’d be there! check it out and rsvp at the bottom if you’re interested: “it starts with girls is a year long, five part series that investigates each part of the pipeline in engaging girls and young women in stem.” the speaker for this one is debbie sterling, \"the founder and ceo of goldieblox, an award winning company on a mission to ‘disrupt the pink aisle’ with toys, games and media for girls.” anne: i can helplooking for someone with extensive *marketing ds* experience for a few hours of *paid consulting*. dm for details or with recommendations, thanks!hi all, we’re a small startup focused on helping companies make better hiring decisions though data analysis and predictive modeling. we are looking for a *(data) engineer contractor* to support us on a project in a few ways:• access production data systems and build etl pipelines• identify, assess, and execute pdf parsing services in terms of cost, feasibility, and accuracy• work with us to make infrastructure recommendations to support data analysis and modeling• time estimate: 60-80 hoursplease dm me if you or someone you know might be interested.how do folks distinguish between data analysts and data scientists, in terms of job descriptions/recruitment/hiring? at my company we are having a tough time emphasizing that we don’t necessarily need the full qualifications for a data scientist, but instead want someone more junior that still has statistical chops. any input is appreciated!when you say you\\'re having a tough time emphasizing it, how does that manifest itself? do you have too few applicants from people of the desired skillset / salary expectations you\\'re looking for? or is it you(r team) emphasizing to higher-ups that they’re overshooting on what they’re looking for? i.e., the classic “5 years experience in a 2-year-old tech” job posting request or they’re just gung-ho for a phd/senior-level when it isn’t necessary? an excuse to skip your siesta.hi everyone! i’m posting this for a friend.  vl49, a new company from the same flagship venturelabs that started my company, is looking to hire for a principle / sr. computational scientist.  the role is focused on using ml on multiple ’omics and high-throughput datasets to drive therapeutics development.  it’s a great opportunity to lead the computational arm of a well-funded early stage startup.  dm me if you’re interested.[unrelated] i know a friend looking for a da positions(from engg backgrounds). if you’re looking, let me know. can share their resumes :slightly_smiling_face:has anyone tried out the packages for slack integration with python and r? they\\'re really easy to use and pretty useful. i have automated scripts sending daily metrics messages into some of my company\\'s internal channels.i\\'ve used the python package for slack to do some visualizations of channel activity at work.this guyhey everybody!  i just created a new channel  - a place for people to chat about founding and starting companies.  we have seen fellows and alumni go off to start their own company, or have a strong desire to do so even if they dont have a product idea of there own.  let’s plan to use  to connect!ps - insight also gets early stage companies that are looking for ctos and founding heads of data/eng.  currently have a couple interesting opportunities kicking down our door.  pop over to  to find out more.has anyone used neo4j in a production environment, and in particular within docker? we’re running into some snags getting it off the ground and i’d love to pick someone’s braini\\'d be interested in this as well! now we just need someone who knows it hahit’s a delicate dance to get right. depends on the workflow you want to have. for ex, if you have a shared directory accessible from different team members’ jupyterhub then by default notebooks created by others are executable but read only to you. how you structure your version control becomes an interesting complication. i don’t know how to compare it to other platforms. thanks jonathanwould anyone that has experience with facebook\\'s graph api be able to answer a few questions to help me get an app up and running?any recommendations for movers from sacramento to san jose? i got a quote today that estimated we had over 4 tons of stuff… that’s roughly 4 ford focuses :car: :car: :car: :car:beltmann relocation groupwould be one i can think of, thank you! i’ll ask for a quote from them.i had a good experience with  nuggetbot: animate me settinghmmm.... i know nothing about thatgood settingi am doing a price comparison between aws ec2 and another service, would people here mind saying what they spent on a monthly basis hosting their insight project on aws?~$50 /month but can\\'t remember if it was for a small or medium instancethanks chloe: : thanks. i am aware of the pricing model, but i find its still hard to accurately predict what the monthly cost would bethat is probably by design :simple_smile:eh?what is the study for?i\\'ve done some cost comparisons in the pastec2 is rather opaque because of the reserved instance pricing schemegce is rather nice because your discounts automatically kick in based on utilizationthis is a new feature thoughi am evaluating another service for my own personal use that uses a flat rate pricing model once you\\'ve reached 1 month and i am trying to figure out how much i\\'d save or how much more i\\'d spend vs using awsit really depends on your use casethere are many many many vendors that provide a vps service(virtual private service)at the lowest end that can beat the net cost of a t1/t2 micro ec2 instancebecause the lowest tier instances require that you use ebsexample: digitaloceanlinodeetcec2 provides the largest variety of instance configurations or \\'flavors\\'what is your use case?many different casesif you expect to have sustained use, the appropriate ec2 strategy would be a reserved instancein one situation, i want to experiment with building a cluster of elasticsearch nodesin another situation, i want to build a small web application and host it, perhaps with a database such as mysqland finally, i may also be interested in hosting several static or dynamic websites (not sure yet)which of the cases, if any (or all), would be running in perpetuityand -- do you want to be a sysadmin for all of the websites?that depends, as of now, i am not surefor standard web hosting, a resellers account at a webhost probably makes the most senseunless you want to also be a systems administrator and want to build everything from scratchfor your web applicationswhich i doa vps or \\'droplet\\' from linode / digital ocean probably makes a lot of senseand for a cluster/lab --- ec2 / gce / azure probably makes the most senseyou can also build your own stuff from metal :simple_smile:yeah, i already have several servers at home which i am using currently, but due to network issues in my area, they are no longer reliablehow about a colo ?haven\\'t looked into that option, but thanks for giving me another ideacolo is classicdon\\'t forget to check out rackspace too for a vendorif you wanted toyou could provision yourself a virtual private cloudand use that as a labhomelab!it typically turns out to be more expensive in the long run if your utilization is greater than 33%which is why homelab ftwthanks, i will look into ithomelab + colo would be a really cost effective lab setup if you find someone friendly -- keep in mind you\\'ll need something that fits in a smaller footprint than most homelab gear since they\\'re charging you based on rack space, power, hvac, and the network connectionif you’re already using jupyter notebooks, extensions + jupyter widgets is probably the fastest and easiest solutionyou can just spin up your notebook dashboard w/widgets, using your extensions to hide input cells/lock editinghas anyone used loftey in nyc for finding an apartment?*what separates _great_ data scientists from the _good_ ones?* if you have opinions, ideas, and/or anecdotes on this topic, i\\'d be excited to hear them! please add a comment to this thread or send me a direct message.short q, is there a way to get list of all actors, directors and movies from imdb?  how big is that? by that i mean title id, title name, title rating, title genre, title type, country, director, actors (4 main) contains info about the data they make publicly availablei don’t know if it helps but might have an starting point via the bokeh gallery, upper left. has anyone had to set up a vpn on their aws instance because a site they were (in this case scraping) blocks the ip range from aws? uploaded a file:  and commented: hi alumni interested in health!my husband (who’s a high school math teacher), wanted me to pass along this community service opportunity that’s taking place in oakland on february 17:i wanted to invite you to help support the new 9th grade interactive assessment for our integrated project about cancer. this february, 9th graders will be looking at the many faces of cancer through a deep study in english, ethnic studies, biology, bioethnics, and math classes. they have been given the task of testing the genetic material of a family that has a history of colon cancer for the hnpcc gene. they will then be given the opportunity to be either a patient receiving these results who needs to ask the right questions, or the genetic counselor who will be delivering these results to various family members. this is where you come in!we are looking for community members to assess our students’ simulations of this conversation live in the patient rooms of our on campus health clinic. following the conversations they will be writing a formal lab result of their results. when: february 12th where: life academy health clinic (best entrance is on 34th street).times: students will be engaging in 15 minute role plays from 8:30-12:25. we will be scheduling students and would be happy to have you for any portion of those times slots. you can come for one 15 minute session, or stay for 4! please see the attached flyer and we are looking forward to having you support our students in this real world learning.thank you very much.sincerely,david gardner*please rsvp to david gardner at . i think those are sample datasetslike this would be all the data which is not muchmovie 467,523i am wondering where\\'s all that availabel u may be right these may be all the titles - it seems to be missing country of origin for director, actors and titles, but the rest seems to be all present imdb sells the full data set... same with rotten tomatoes, etc... you can ping limited amount of cases using their apis, but not get their full dumpwould anyone have experience saving a checkpoint model with just the relevant trained weights, ie. the minimal needed to restore the model.  i’m working on trying to package my model while trying to save space.  it’s currently 800megs.  that’s the output of `tf.train.saver()`.  i’m looking for a way that’s more efficient on model size.  any input is appreciated. thanks! hi i am using flink complex event processing and i heard that you had issues with out of order sensors?nlp solves the voynich manuscript language: was it a d&amp;d manual all along as suspected? ;)ancient hebrew. the original sourcebook language.whoa!hello! does anyone have any experience playing videos from aws using flask (with nginx and gunicorn)? i\\'m trying to play a .mp4 video that is saved in my static directory. i tried two methods: (1) a redirect to the video filepath in flask, and (2) rendering a html template with the video filepath (\"/static/video_filename.mp4\") as the source. any help would be appreciated... thanks!the government uses evidence? :thinking_face::grimacing:in all fairness , the fed is not really \"government\" as much as a hybrid bank-regulator in the financial sector... :wink:they have the best and biggest evidence!hello wise former-fellows — current fellow here: what do you think about job titles? as far as career growth, how critical/useful is having the “data scientist” title?  some of the companies are offering positions with other titles (research analyst, quantitative analyst, etc.), and i am wondering whether its something that should be given strong consideration when examining offers. thanks!i would also love to know what the wise former-fellows think about this. and also, do the different titles correspond to different wage brackets (assuming the roles are ~equivalent)?although it should depend mostly on the actual skills shown in your resume, i would rethink taking a \"data analyst\" position, as it has a different foundation and career trajectory than a \"data scientist\".piggy backing on what  said, i will suggest talking to the team about the role and what the day to day job entails to get more clarity. lot of established organizations has their own nomenclature for data-scientists, so don\\'t be disinterested just coz job-title doesn\\'t look that fancyi have been presenting in countless meetings where i had to explain performance metrics to non-technical people. i wrote a medium blog about this topic. please let me know if you have any comments or edits. i think some of the fellows in our cohort negotiated for a title changeyes, title change is totally on the table when talking about offers.i negotiated for an external-facing title change. outside of large tech areas i found that many organizations have their own terms. sometimes places would be happy to have a data analyst turn out to be a data scientist, but don\\'t know the skill set exists until you sell your skill set to them during the interview process.on the hiring side, a job title of data scientist generally gets more applicants than jobs where the work is the same, but the title is different. we have our own internal titles, but when i hire, i never use that title for the job ad. if i want more applicants, i put the job title as data scientist. i also don\\'t care much about previous job titles that applicant have held as long as their experience is strong. so, it could be that applying to jobs that aren\\'t titled data scientist could mean less competition. this is only my anecdotal experience though.i’ll agree with josh, but reiterate joao’s point: if they say “analyst”, they might really mean it is a different job than “data science”. see what they actually do day-to-day!when i’m browsing resumes i find the titles don’t make a lot of difference but they do makes some difference. i do glance over what they say they did in those roles and i do know typical alternative titles and titling quirks of some of the large employers. in fact i recently screened a resume that was pretty good but it had all “data science” titles going back to 2004. no one was using that title back then. when you look at his linkedin profile they were all non-ds titles. i felt it was too sketchy and you definitely don’t ever want to give a hiring manager a chance to question your honesty. though a colleague disagreed and thought it was ok given the work described was fairly data sciencey.you definitely should demand that your title reflect your work. if you are using substantial statistics on large projects or doing engineering you probably should not have analyst in your title.and of course the point about getting a lot of clarification on what the current role is going to entail is the most important thing. plenty of places will happily give you a data science title and then hand you a locked-down laptop with only excel.what **data** pipelines do people use for working on small amounts of data? something under 10gb where you can\\'t justify a map reduce library.if it fits into memory on a single machine, i\\'ve used the sklearn pipeline pretty successfully. if it\\'s larger than that, python/pandas/dask is a solid option.for sql lovers:  the query:  uploaded a file: following on to  sklearn-pandas () is good for stitching transforms together - i’ve found it more frequently useful than sklearn’s in-built pipelinesif you need to encode more complex etl tasks, i like luigiminimal boilerplate to build the task pipeline, and it actually scales decently well (mainly due to good integration with pyspark)i was more curious about what does the bulk of the transforms. spark\\'s nice, but the map reduce paradigm works well for larger datasets, it\\'s not really needed for datasets under 10gb.i\\'ve looked into luigi though, and it seems cool! i\\'m using an aws stack though so i\\'m looking at aws data pipelines, but they seem...unpopular in the search results i\\'ve gotten because they\\'re configured via json.pandas is cool! i didn\\'t know much about it on the de side, but it\\'s the best answer i\\'ve gotten that close to what i want. it kind of stinks that it\\'s only single core though.some of pandas’ functionality is vectorized under the hood via numpy (and through that some compiled c libs) that should run more efficientlyand then there’s dask for running pandas(ish) at the cluster leveli like luigi (another similar option is airflow) since it can run p much arbitrary python for its tasksi\\'ve used a containerized airflow deployment on a single box running with the localexecutor for stuff of this scale. i used this repo: can\\'t wait until this is fully up and running... anyone want to come have wine and hear a q/a from cofounder of stripe on friday at 4:30? at 405 howard street in sf.this is matar’s talk at strata this winter: starting with a spectrogram (looking at the signal in fourier space over time) and clustering based on derived features is a good startinsight alum  just published a killer paper on a new approach: hey, i am trying to get hold of a data set of marriage information. the data should include spouse names, the spouse birthday (day/month/year) and the marriage date (iff possible)i did a bit of related work in grad school, and spectral features are definitely quite good for speaker identificationmarriage records are held by the state, often by a department of vital records. finding a dataset that includes names doesn\\'t seem very likely to me, because you usually have to request records on a one-by-one basis. however, you can probably find aggregate, anonymous marriage/divorce datasets (but, again, on a state level). thanks. i was thinking that might be a limitation as wellyeah once you are down to names and birth dates that\\'s highly identity theftable pii if you\\'re okay with older records, census data i believe gets you name, birth date, and marriage status, so you can get within a ten year period of the marriage date by comparing censuses (censi?).  but the most recent records publicly available are from 1940 to address privacy concerns.,  thanks for the info about matar\\'s project! that is super useful to have as a reference pointold timey big datavintage data@limventa  thanks! yea, i was aware of her project based on a recommendation from someone who was in her cohort. :smiley: pretty cool methodology. this site has marriage license application data as well as divorce reports. it is limited to texas. it might be helpful:   thanks!i wish i had been wrong.   by no means an expert, but have some experience. feel free to dm.: thanks. i am wondering if domo has any advantages over de novo dashboards created in r or python.for silicon valley and san francisco members: a good data science conference in the area this year?anyone here done hackerx before? i got an invite and can\\'t tell if it\\'s a solid program. i\\'m happy where i\\'m at but always enjoy the opportunity to network.i see rwd groups at pharma moving this direction ^any airflow experts in here?   i am trying to to understand why when a task times-out  and fails it  does not call the on_failure _callbacksto piggyback on ^^, does anyone have guidance on best practices for writing tests for airflow pipelines? we’re having an internal debate about how much to use airflow-specific features like plugins, sensors, operators vs. keeping that code in separate libraries that are testable outside airflow context and just calling them with the python operator currently we add “check” operations before and after every task to test assumptions of the data before we land it in production tables. we’re starting work to basically write a testing library outside of airflow, and even starting to think of building an entire etl framework on top of airflow. airflow is really just a scheduler, not an etl framework, so it’s not optimized for these things.also, you might want to check out this: it doesn’t quite work with our infra right now, but could be a fit for youha. funny you should mention that. we had abe in yesterday to present to our team and had a good discussion with himi guess he’s on a roadshow! he’s coming here on monday :slightly_smiling_face:i use af, but haven’t encountered that, but try af mailing list, contributors there are quite responsive.not sure if this is relevant, in my team, my teammates prefer to do everything w/ operators, like we may have stuff like mysql to redshift operator, etc.  but i don’t feel personally strong in having dags only use operators, because a lot of the dag tasks are very one off sources or sinks that aren’t reused elsewhere.hey all, ai fellow  just published a cool post at the intersection of video games and machine learning, give him some love! hi all, if you\\'re interested in grace hopper 2018, the deadline to submit talks and workshops is next week! you can read more and apply here:  a few insight team members are working on submissions, so let us know if you\\'d like to collaborate, or if you want anyone to proofread your submission. we\\'re hoping to have an even bigger insight presence this year :slightly_smiling_face: you can also join the new channel  to share conference-related updates and eventsi’d like an emoji for “i enjoyed reading this discussion”this is great !  during insight i read an article on a/b testing that had a great plot on the dangers of early stopping, which showed the p value of an a/a test over time. anyone know what i\\'m talking about and know where to find it?maybe this one? this? i don\\'t think the air bnb one is the one i remembered but is perfect for my current needs. thanks to you both!*where may i buy a gpu desktop* (for deep learning , not gaming... of course) ? it has been 10 years since i did diy myself... i do not wish to spend my time on learning diy now. :sweat_smile: thanks maybe: figure out what it costs..then, see how many hours it will buy from paperspace cloud or gcloud or aws - spot instanceshey folks, thanks so much for the responses to the alumni compensation survey. we got about 50 responses so far, which allows for some splitting of the data among region, seniority, etc. the data is available here (with some prelim descriptive statistics on one of the worksheets): if you’re looking for a reason to procrastinate a bit on a monday morning, and haven’t filled out a survey, feel free to do so here:the survey is anonymous, and may help fellows figure out based on region and experience what kinds of compensation they should expect from employers, especially helpful during performance review/compensation adjustment periods.additionally, once enough data is collected, one can compare to other online compensation aggregators to measure the collective effect of “insight alum” on compensation.cheers!mike in future iterations can you please track info like gender/race (provided you can suitably anonymize it)?that\\'s a great idea, i\\'d be happy to.gwen.gillingham: for gender and ethnicity, what do you suggest for an anonymization strategy? i think this is important to include in the data set, but i think maybe the best way to include it is to make it optional?making it optional will make it less useful. maybe the simplest option is just to use a randomized response paradigm? there\\'s other options that have been used for generating differentially private datasets in medical fields...(but they mostly involve generating data after collection, rather than mitigating identifiability risks during collection)hi all! can anyone talk about “productionalizing” their ml model? more specific i have a sklearn random forest model that i would like to put into production. tips, tricks, tutorials anything that has helped. thanks!we pickle our models .. save it to s3 .. and save associated metadata in dynamodb .. and the production pipeline loads the models from s3 if the models are labeled ‘for prod’ in their dynamodb metadataand make sure your models pass a sorta smoke test before it is labeled as `for prod` (i.e. expected scores given a static set of data)i’ll have to see if google forms supports randomized responses. i understand the basic principal, thats a good idea that still gives the survey statistical relevance. i’d be interested in making the survey even better, so if you (or anyone else) have any additional feedback, i’ll look into ways to implement, or just add y’all as collaborators on the form itself. uploaded a file:  and commented: hello, i am looking for a machine learning/software engineer for a vc backed early stage startup in the bay area to help implement image processing algorithms for personalized virtual reality experiences. if you know of some one who will be interested in this space, please connect with me at . thanks!hi everyone, i’m doing some research and need a handful of volunteers.  this will take about 45 minutes of your time to video conference with me where i’ll ask you some of your opinions on some data science topics, then send you a followup short survey that takes about 5 minutes to complete.  then i will get you a `$100 amazon gift card` for your time :slightly_smiling_face:   only need a handful of people working as data scientists at any level, but who are actually in an industry position, so first come first serve.  please dm me!  thanks!hello ny: with cmj music festival coming up next week, i\\'d love to hear your suggestions about what bands are a good pick. i personally want to go see the henry millers as they are friends of mine and i like their music.we’re trying to sound more statistically sound...the non-science parts of funding agencies like p-values?that\\'s my guess. grant or journal reviewers started asking for them thinking they were useful.any other folks in media talking about facebook\\'s metric mishaps these days? and to a broader degree, how much reporting accuracy falls on data scientists? given that a majority of us come from academics where job number one of the scientist is accuracy. i think someone before was saying they wanted to retrieve old messages off of slack. i figured out that on the iphone app, you can retrieve the old messages without a premium account. it just doesn\\'t work off of desktopgiven that a majority of us come from academics where job number one of the scientist is ~accuracy~ getting results for funding.fixed that for you: someone posted this earlier:  i have not yet listened to any, but would like to. if anyone has some hand curated suggestions my gym time could use some data science podcasts!: the insight project i just finished is actually a podcast recommendation app. i can’t link to it at the moment since i need the site to survive demos, but here’s some recommendations (note i haven’t personally listened to these): friday lunchtime lectures, software engineering daily, uie brain sparks, ctrl+click:   thanks all! and good luck with your demos, lindsaycan anyone shed some light on why hadoop streaming would map and reduce my input perfectly when the input is in a single hdfs file, but the mappers output nothing when the input is split across multiple files?are the multiple files partial zips? raw text.  if i run my job on the output of a prior mr job it works finei wonder if it\\'s because your splitsize is set too small, so the raw texts got chopped up and not being mapped properly, but if i use more than one reducer and then run on that, no mapper outpuyin case anyone is or knows a software engineer who wants to make the next generation of tools for genetic analyses, we\\'re looking for someone at the broad (cambridge, ma): pretty recent post about netflix\\' data stack: : anyone else at spark summit east?no, but you should share cool things you see on here. :simple_smile:hey guys! anyone at scala days today?another analysis tool: .  seems interesting because of the potential for flexibility.  potentially not quite as useful for people who don\\'t code, but i haven\\'t looked into it much yet.: i have a friend who is finishing his phd and is looking to go into data science. but he is mainly interested in working in the public sector. would anyone happen to know of any opportunities, companies, or government agencies that are hiring data scientists? is awesome, and dc was looking for a civic hacker for a law fellowship- the free law innovation fellowship. bayes impact is a 12 month ds fellowship for social good: have also seen some good things about the data science for social good summer fellowship: maybe?  another maybe?  lots of opportunity to work in private sector (consultant), yet support the public sectorthanks for the suggestions, very helpfuli really don\\'t like the keyboard. otherwise, i\\'m happy with mineit\\'s thinner and lighter than the older macbook pros,  that\\'s mostly the pros. neutral - the new keyboard has much less travel, takes getting used to but once you get used to it, it\\'s fine. no complaints on the size of the touchpad. i have very mixed feelings about the touchbar, i think it\\'s mostly a gimmick, but sometimes, when there\\'s a button to click like save or install, it offers a quick alternative to using the mouse. cons- 4 usb-c ports are very limiting. the dongle apple sells isn\\'t great, e.g. the usb-c ethernet adapter we got only works at the port on the mac body, not through the dongle. each dongle supports only 1 monitor. 2 monitors, 1 port for charging and 1 for ethernet and all my ports are occupied. hopefully i don\\'t need to connect anything else :confused:. battery life isn\\'t as good as my older 4 yr old mbp. limited to 16gb ram, expensive as hell, especially at the top end. hey nyc folks, i\\'m sourcing projects from companies for a class i am teaching at columbia in the fall.  the class is project based where mbas and engineers will work together on an analytics project over a semester.  this is an awesome opportunity to get some work done (reasonably scoped of course), get connected with analytics and ds folks at columbia for recruiting, and possible get quality content for a blog.  message me if interested and i can mail you more details, thanks!!channel notifications for hundreds of people, the majority of whom are probably not in nyc? gutsy move, !h2o is awesome. sometimes, not really sure why people use anything else. i feel like it’s pretty much won this spacesorry for being so mia wendy et al. i\\'m in the process of moving and will try to get the strata slides online soon. there is a good github repo for mfccs. i can\\'t remember what it\\'s called, but i think i forked it (so you can see it on my github account). sorry for not being more helpful - i am not near a computer. any people interested in security/exploits seen this? insanely simple exploit for root access on many inexpensive arm devices.reminder that ethics and anonymizing data = a really important part of data science:(re the ok cupid data dump today)working on a federal grant with human subjects will teach you how important anonymization is. :slightly_smiling_face:they have some pretty strict rules about data storage.none of my datasets have any personal information, it\\'s possible to link them eventually through subject numbers and experimental logs, but then you need access to our database of subjects.we\\'re expanding the data engineering program to boston. really excited to see the insight community in boston grow. we\\'re already at over 100 insight alums in boston and now we\\'ll add a  data engineering contingent to that - applications open for april now woo! :party_parrot_shuffle:hi, has anybody used h2o with python wrapper on mac? i can’t get it to work. it keeps telling me i have to install java but i already did it.just a guess but is it asking for a specific version of java?i think  has some experience with this.thanks. somehow i solved the problem. i switched the default browser of jupyter notebook to safari and then manually activated java for safari. it might have helped.i’m not sure if it was asking for the exact version but i ended up installing the latest version.nice advice when updating your cv!   this person likes 10 page cvs apparently:smile:\"another good example is where the candidate explains why they chose to use one algorithm over another for a specific project.\" i guess he is assuming a several page cover letter?there is no way i can fit: \"built a recommendation engine using collaborative filtering in spark. we considered using content based filtering but we needed to cover a very wide variety of item types so there was no single good way to parameterize them all. we also considered writing our own big data processing code, but you know what that\\'s crazy. during this project i mentored two engineers who were new to recommender systems....\" on a cv for the 5 major proejects i\\'ve worked on. :wink:i am always skeptical when a single person extends their personal anecdotal experience in their title to the plurality of people like them.  want to make a claim about what hiring managerssss look for?  i’d love to see your data that goes beyond what you want :slightly_smiling_face:  also i think this is generally some horrible advice `but you should never not apply for a job if you think you don’t fit all of the criteria. `listed job criteria usually not representative of actual job requirements :smile:oh woops that’s a double negative in there, uh, so it’s good advice after all in that line :smile:yup :wink:also he does specifically say he loves 5 page cvs\"and candidates should not be afraid of having a cv that is longer than a page or two. i would be happy to read even up to 5 pages of useful and relevant details, especially if it allows me to move faster in the interview process\"does the uk not do phone screens?a lot of what he\\'s advising is stuff that you would want to cover in a chat, not typed out on a cv of death\" from experience i have found that combining the hypothesis driven approach with the kanban agile framework supports the lean startup framework really well” … :skull:look, if candidates write a 10 page cv that gets them rejected _everywhere else_, then he always gets first pick. it’s really a brilliant ploy....if your filtering criteria is \"does not understand the hiring market\"zero attrition!that company they use as an example of a current gen crm on page 202 must be awesome and everyone who works there must be really handsomeanyone using tableau? would love to discuss usage with python :slightly_smiling_face: thanks!hello everyone! has anybody had experience with tensorflow. i want to figure out if use it or keras for my problem (my mentor softly isists on tesorflow)you can use both: you can use keras with a tensorflow backend. in my experience, it’s much easier to get going with keras. but i think keras may not support all of tensorflow’s functionality.an, nice! so, i can you tensorflow without knowing much about itthe question is whether your mentor is insisting on tensorflow because it’s the best tool for the job or just because it’s a buzzwordi am not sure. he said that it is a lot going there. for example google put out some new algorithm or something like that.as a matter of fact i am not sure if neural networks are the best. mentor also wants to have insights from the dataif _he_ actually uses tensorflow, that would be a good sign. if it’s just something he’s heard about, i’d be worried.i see. thank you for the advice. i\\'ll ask him what is they use iton the subject of tensorflow: it is what everyone in deep learning is really excited about now. at gtc this year they had to move the tf tutorial to the ballroom so they could fit the o(1k) people that showed up for it. i think this excitement comes from two things: 1) google is really good at deep learning, so people assume tf must be awesome2) google has some of the biggest scale problems imaginable, hence people think (and i think there is evidence that) tf can scale to whatever size problem you have.i haven\\'t used it enough to say; we don\\'t have the scale issues that others have, and i don\\'t have enough experience with tf to answer the \"is awesome\" question. but our current deep learning project (that we just started and i\\'m working on) will be mostly tf.that said, if this is for your insight project, pick the thing you can develop fastest in!! :wink:there is integration with scikit learn that is supposed to make the interface simpler.  it was mentioned briefly at a workshop i attended but i have not try to use it myself.from my limited experience, you probably fine using keras with tf backend. the tutorial linked above is a good place to start.a note about the top 10 example that niharika, mudit and i discussed this morning. if you look at the program am listing around page 62 an 63, you will notice that the mapper class declarer a map, that maps a \"reputation\" (which is derived from the input record) to the entire record. as it keeps reading more input, the map keeps growing. whenever the size gets bigger than \\'k\\', the \"first_key\" is dropper. after all records are procssed by the mapper, the map contains \\'k\\' elements. now the \"cleanup() is the critical part of the process. it\\'s this \\'cleanup() that takes the map and emits a (key, record) so the reducer can combine all these inputs and pick out the top \\'k\\' elements. if you look at the program, the mapper class has two mthods, map() and cleanup(). presumably map() gets called every time for each input, but cleanup(0 is called after call input has been processed. the cleanup is doing what a local combner does. but it is not clear where cleanup() is getting called. it\\'s clearly not called explicitly from the mapper anywhere. it must be happening behind the scenes my feeling is cleanup() is being registered as the \\'combiner\\' method. but this needs clarification.i am looking for fun and interesting applications of probabilistic programming and bayesian inference like anomaly detection, inference of missing data, or making decisions that involve risk from limited or sparse data. at my company (fast forward labs - an r&amp;d company), we\\'re working on a report on probabilistic programming. if you use these approaches, i’d love to chat. :upside_down_face::slightly_smiling_face:: these posts might be relevant for you : nuggetbot animate me parsey mcparsefacethanks  the bayesian deep learning stuff is really cool, you can imagine all the applications one could build. :slightly_smiling_face: for now, i am curious about what people have built! anyone?i built it something for a toy problem, doubt it will be useful for your work :  i haven\\'t but was looking at this paper recently... ... keep us posted on this, i am def interested hi kevin, thank you for your response. i am trying to recommend if a given domain is a spam trap domain or not, by using the known spam trap domains. the feature is the name serve of the mx records of the spam trap domains. (we find out that spam traps domains tend to have the same name server.) i am in the process of collecting data now. we have ~9000 spam trap domains. from what i know so far, each spam trap domain has at leas two name serves.so you have the raw text of the name server and the raw text of the domain, as well as a list of spam trap / not spam trap labels for other domains?i\\'m just not familiar with the domain  at all unfortunately -- if anybody couldn’t make it to nips, we wrote up our thoughts about day 1: benjamin recht just laid down the hammer in his “test of time award” talk! yes, you are correct. i am trying to lean “how the internet works (dns records)” what about the data made the recommendation system seem like a good approach?this is a long read, but it’s very relevant to all scientists turned data scientists. includes a great quote from fernando pérez, creator of the jupyter notebook: “i have seen many talented colleagues leave academia in frustration over the last decade,” he wrote, “and i can’t think of a single one who wasn’t happier years later.” there\\'s...a lot that went a bit too underexplored in the article. the title\\'s def sensational. i think a more honest conclusion after reading this would be\"scientific papers need to be published in a medium that allows interactive integrations. the pdf is too static for the types of complex ideas that can be easier interacted with than explained. the pdf also can\\'t embed data necessary for scientific analysis.\"there\\'s a couple of problems. like...evident problems that you see right after reading the article. the notebook for gravitational waves is getting the \"hug of death\" right now from all the traffic. and if there\\'s a lot of data, as you\\'d need sometimes for modeling complex systems and for training in ai, then there might be problems just storing all of it. then...there\\'s just cultural, financial, and incentive problems in academia that might need to be overhauled before a system like this can thrive.the title is total clickbait--obviously we’re not going to ignore scientific papers--but i think the article demonstrated how research workflow in academia is archaic and could be so much more innovative. of course, an alternative approach will almost certainly not catch on any time soon in academic research--yet another reason why i’m glad i’m no longer part of it.maybe i should preface anything i post with “i don’t agree with 100% of this…but…”workflow is definitely outdated but i’m not sure how well thought out this article is.  for example, i think sharing raw data in a paper is a very tricky idea.  the naive idea to give it out doesn’t hold in many situations.  a lot of what matters is how data was collected for example - and a lot of what you should be doing as a scientist is interpreting raw data responsibly.  without literally writing a book, it would be impossible to describe how data that is difficult to get is collected so as not to confuse what a user playing with it may encounter.  what i think would happen is that it would flood many fields with false objections to quality work, and bog people down with addressing them, by essentially writing ‘this is precisely how the data was collected’ book to accompany each paper.  there’s something to be said for interactive figures and such in papers, but i often find the best figures are simple and striking - and don’t require interactivity or manipulation to understand the concept - it’s sort of an art to do this well.  and in that sense, interactivity could also be a distraction.  and of course, many people still do print stuff :slightly_smiling_face:  a very important thing in academia that isn’t as useful in industry is to surround yourself with pieces of your findings that you can consume simultaneously (e.g. a giant wall of printouts ) as you try to put together the various pieces.  like those detective shows with all those pin-up figures in the wall and pieces of string connecting them :smile:all this said, should definitely integrate new tools to improve old workflows - and people are slowly beginning to do that.crawling through \"a giant wall of printouts\" should have a software equivalent though. i think \"data gathering\" and documentation could be a separate issue, but there are other places like data.world and aws that can expose data like that. i think in psych data sets aren\\'t as useful, but for things like twitter it doesn\\'t matter as much. whatever comes of this should probably integrate data with a separate data hosts so that researchers could \"see\" data in the same way that a python programmer could import libraries instead of trying to package the data with the notebook.the “software” equivalent in industry i sa wall of monitorsso far anywayit’s pretty great, but also less portable and more expensive :smile:as far as documenting data, there’s cases where it’s easy and should be done and shared - and there’s others, where the data collection, because it’s done with some newly developed thing heled together by pieces of string, or is pushing old tools and methods to the limit of their capacity, can just be endless.so it can be done but probably at a big cost to productivity, and would not add muchin terms fo valuenah it\\'s necessary. papers just show their results. having data, even badly gathered undocumented data is at least an indicator that something could be wrong. having nothing but the results means someone has to replicate the experiment without documentation which is just as bad because sometimes descriptions and methods are too vague to re-implement.that’s definitely a problem.  but having data without the right details in the wild will bog down productivity to more of a crawl than it already is.  the reason is that people will make bad assumptions about how stuff was collected, and be mislead down wrong pathsthe best thing to do would be to consult the scientist and work with him / her in getting the data, telling them what you hope to accomplish, so they can tell you any relevant things about the data as they hand it to youas far as trusting the ‘results’ that’s more of an argument for improving the review processthat’s how i recycled the papers i read during bbq.hi insight peeps.  anyone know a good beginners reference for sizing a kafka cluster?  i am looking for something that works through an example problem.  thanks in advance.  has anyone switched from jupyter to zeppelin and been happy with it? i’m trying to figure out what the advantages of zeppelin are, but i’m coming up short. it seems to be missing a huge number of features compared to even a standard jupyter installation.no, we tried them out and went back to jupyter.this was from many months ago, but we found it to be very buggysame as above, checked it out, but for python it wasn\\'t an obvious win. i\\'ve seen people use it for monitoring etl jobs or other things more successfully.i\\'ve used it and liked it, but i haven\\'t used jupyter. i think one is more collab friendly than the other.i setup zeppelin on our hadoop cluster a few years ago and gave it a solid try to run adhoc spark code.  it worked alright, not sure if jupyter could be integrated with spark back then.  i like jupyter over zeppelin though.the sv-insight office is really cold after the party last night. anyone know where the thermostat is? (so far, 15 phds can\\'t find it)hvac is under central control. hope you can find some blankets!:disappointed: thanks!apologies for the off-topic question, but my family is traveling to boston next weekend and i was wondering if any boston-based folks can recommend a good babysitter? if so please pm me. try : thanks, will do!datakind is hosting an hackathon on nov’13/14th in sf.  check it out if you are interested - does anybody know of a good book/resource that covers a/b/n + multivariate testing?i like slater stich’s series (it’s biased towards bayesian methods, admittedly) has anyone set up ds pipelines in aws? specifically a pipeline with a daily retrain/score task and then a persistent api for retrieving predictions. bonus points if you had a probabilistic model in there somewhere.i\\'ve worked with lambdas to do data collection, storage data in s3/rds, daily retraining of models in ec2/sagemaker, and serving of predictions with rds&amp;api gateway/sagemaker endpoints, happy to help with any one of those points if needed :slightly_smiling_face:thanks  will check it out!if anyone\\'s looking to sublet a room, a friend is renting in fidi:  - $525 worth books for $15!i have an onsite interview coming up on monday next week and i could really use some advice for interview preparation. it\\'s a data scientist position at travelers insurance. i\\'ll be speaking with seven people, six of who work in predictive analytics and/or dev. the other is hr. i\\'ll speak with each person for 30 mins. i really want this job! so i have to ace it. any advice on types of questions i should prepare for and/or resources would be great. thanks!:  my biggest advice is to not put so much pressure on yourself: before having your on-site, there\\'s really no way to know that you really want to work there. i know you\\'ve heard all about \"cultural fit\" from the program directors (and that it seems like a lot of b.s.), but it\\'s actually pretty true.  to be honest, you\\'ll never know if the company is right for you before your on-site.  at this point you\\'re probably ready for any question they might have, but that might not be so when you bring a lot of self-generated pressure to the interview.  so, as hard as this \"advice\" might seem, the best one i can give you is to \"relax\", and go a little easier on yourself.  :wink:`chill`stay positive  good luck  !my friends are now hard launching their funding campaign for their educational non-profit startup. if you are interested in the cause, you can definitely donate. but another way you can help is upvote online and spread the word. thanks! twdoes anyone have a good reference for the starting salary of a data scientist at a very small start up?  all the numbers that i see online are cumulative over all company sizes and i would assume that a small start up would offer less than this; but how low is unreasonable?: did you look at glassdoor? this is close to the numbers i have heard: there’s also the o’reilly data science salary survey: with glassdoor, can you sort by company size?  the o\\'reilly survey does have salary by company size but is again cumulative over location.  i know that salary for data science is quite a bit lower in other parts of the country....insight it self may have some of the best information on this  i would also keep in mind the cost of living, etc. absolute numbers mean little on their own. 100k in nyc wont take you as far as 50k in san juan, pr. just saying...:  glassdoor, gives some minimal sorting options, by popularity and more recent updates. also, glass door gives you the median pay by the nation and by city. : you might look at angellist. you can filter by location. although, there are some reasons the data might be biasedangellist is the first thing that came to mind. and i work at glassdoor!company size is one thing that glassdoor doesn’t have great data on.oh, correction: apparently mr. salary estimator at gd tells me he *is* using company size now. i can tell you this: it is the weakest indicator of base pay out of everything we look at. maybe a little surprising. that is interesting.  do you mean if you are looking at all data or is it more predictive if you are looking at specific data (e.g. data scientist in sf)?not specific to data scientists unfortunately.we do have numbers specific to data scientists… that i’m not allowed to share. but you won’t find anything revolutionary.insight should have summary statistics on this, reallyyeah, insight should have some very good data on this.it\\'d probably be to their benefit to share aggregate data with the fellowsi found it hard to negotiate coming out of academiasome random thoughts: you do have to think pretty strongly about totally pay. there are a lot of complicated issues around it. i’m sure insight gives every session some instruction in this regard.on one hand signing bonus and options are often very negotiable. people compare salaries much more often than bonus structures (in part because it can be complicated) and in fact the comp expert in hr (if the company is big enough) is doing this very carefully once or twice a year. in a reasonably fair company salaries will tend to normalize but the hiring bonus stuff is a one-time thing.on the other hand, compensation in the form of options is a huge gamble without some very solid understanding of the company, its industry, and all the annoying financial quirks of it (amt, 90 day limit on vesting after separation, dilution risks, etc).true: are there any resources for us (current fellows) to learn about options and how to assess those opportunities during the negotiation process?we had one session where a mentor talked a bit about how that stuff works, but i’d love to dig into some more information if you can point toward any good sources useful tools in the field of bigdata: quora can be have some decent info: the tldr: “… this is why i value pre-ipo stock options at $0. …” (maybe a little extreme, not much)very interesting, thanks for the reference has anyone used the python difflib module?? is there anything similar?: similar to difflib in what way? i’ve been using fuzzywuzzy which has difflib as a dependency.having issues opening ports on aws ec2.  security group is set to open ports 80, 22, 5000 but port scanning shows only 22 is open for ssh.  does anyone know how to go about fixing this?all ports are opened to anywhere: have you checked both the inbound and outbound ports?: so the security group for the instance has the desired inbound ports opened.  outbound is open to all trafficodd, are you sure that you’ve put that instance with the correct security group?me tooi was planning to start down this path myself with ch. 13 of bishop:  they are very popular in comp bio, speech recognition, etc.these papers are very good overviewnow for almost all about them this book is good but it\\'s completely focused on math/algo\\'s but no implementation examples... hey everybody!  does anyone know a good place to start to learn coding for someone that would like to eventually be a software engineer, but has zero experience coding?there an `hmmlearn` python package, but i haven\\'t played with it yet.the wikipedia page is actually really good.the examples are easily understood and there is even example python code for the verterbi algorithm thanks to  for the pointers!there\\'s an outstanding mooc on probabilistic graphical models (of which hmms are an example) by daphne koller on coursera. i think hmms are discussed in week 2.+1 for daphne koller\\'s coursera classthe textbook that was used in my stochastic processes class in college was *“introduction to probability models, tenth edition” by ross*. it discusses markov chains, hmms, the poisson process, queueing theory, etc. i would recommend it as reference material even if you don’t do well self-studying out of textbooks. the bdt (xgboost) -&gt; logistic regression approach is used quite a bit here at li (an internal library does this for us automatically), and it works really well since you get the best of both worldshere is a useful reference: thank you for the paper carlos!i\\'ve been trying out the method you mentioned since you suggested it.i\\'ve also been exploring using something called shap values for individual feature importance and that for interactions. have any of you used these?hi everyone, i created the  channel to connect with other fellows who are either interested or are already working in sports analyticspandas docs has some interesting results as well i usually suggest that people brand new to coding check out , which is cute and well scaffolded.that looks awesome , thanks!i use it with hourly granularity over a few years of data for a production task. it isn’t fast, might require 10-20 minutes of running even if you don’t want seasonal uncertainties, but it is very solid. it is really just a wrapper around  which preps the data and compiles an appropriate stan model.is anyone familiar with nlp and willing to discuss with me about good nlp related projects in e-commerce industry ? i’m going an onsite interview for a nlp research scientist position and asked to give a 20 minutes presentation on a research proposal. i’m a complete beginner of nlp and need some suggestions from experts !thanks.i have not used shap. looking through the docs, it seems interesting for interpretability, but not sure it\\'s useful for large-scale feature selectionas a massive disney fan i like this one: i\\'ve done both ubuntu and windows 10. ds work was easier in ubuntu. communication with rest of company varies as others have saidif the e-commerce has a review system, you could use nlp to detect fraudulent reviewshahaha... hehehe.... haha.... no:arrow_up: (for anyone lucky enough to have used root)nuggetbot: weather antarcticaweather for antarcticacurrently: overcast -28.5°c/-19°ftoday: light snow (under 1 cm.) tomorrow night.coming week: snow (7–15 cm.) throughout the week, with temperatures peaking at -10°c/14°f on saturday.a free webinar titled keys to data strategy: online seminar this thursday at 11am pdt hosted by svds it\\'s aimed at business leaders, product managers, marketing leaders and senior architects -- so if you know people who would be interested please pass this info along.   that’s surprising to me, but interesting!super interestingyeah, i imagine many of us had our start theredoes anyone have experience using this:  ?  or built their own data and model management system? well, not a rumor anymore! for bay area folks: a team of graduate students at berkeley are starting to organize the 5th annual cdips data science workshop (), which runs july 15-august 5 and they are looking for mentors. from one of the organizers:\"the workshop is designed for berkeley grad students and postdocs to get a taste of what it\\'s like to work on data science projects. the core of the program is the mentorship aspect - 3-4 students/postdocs are paired with an industry mentor who devises a project. if you have a project that could use a few eyes on it, or are just interested in exploring a particular topic, you might find this useful. the only fixed commitment as a mentor is joining us for a few hours for the opening and closing ceremonies; we do ask that you check in with your team twice a week, but the timing is flexible. if you\\'d like any further details, please let me (diya das &lt;diyadas@berkeley.edu&gt;) know - i\\'d love to chat more. you can also reach all of the organizers at .\": i changed my project to predict what songs/music genres are best suited for health recovery (mood or emotions change). would you know any psychological site that connects or labels emotiond with health. thxpublic service announcement to people writing their resumes: it\\'s ipython not ipython (it\\'s not an apple product :simple_smile: ) source: you make one of my favorite editing points on resumes,  !  to current fellows in both sv + nyc -- if in doubt, please look at the language/tool website to see how the name is (or _is not_) capitalized. e.g. ipython, scikit-learn, pandas, git, python, numpy, latex, etc.  feel free to direct message me if you have any questions.i once got a complement on my resume because i don’t flagrantly assume c and c++ are basically the same, a less subtle distinction than ipython vs ipython but i think common.  the facts of a resume are easy, the really tough task is instilling confidence that you are trustworthy and have sophistication in your field.can someone help me understand what a typical ai stack should look like? or any literature that can help me understand on a high level? haha, so true...welp, boss approved the cost, but now registration is closed :cry: looks like it\\'ll be back in the fall though? i\\'ll try to watch for that registration to open! pinned a message to this channel.this is too realdarn! yeah, i think it\\'s bi-annualohh common! that’s completely self-inflicted by using anaconda and easy_install :smile:i’d like to add the eleven virtual environments i’ve set up in various places with uninformative names and conflicting dependenciesjust created  for attendees later this week. we can use it to coordinate meeting upi learned about pipenv during one of my interviews. it’s become my go-to for environment stuff: i’ve been totally happy with the python built in virtual environment `python3 -m venv my_venv` …works great. i think it’s way easier to keep track of than all the anaconda path stuffam i the only one that is happy with exclusively using conda environments? i have a dozen or so, but they’re all named well, no conflicts. just...stick with one method?just out of curiosity - has anyone tried any of these? nuggetbot: animate me welcome newbiesthanks  ! what you forgot to mention is that it\\'s free!hey everyone.  i have a 2bd 2ba in mountain view and i\\'m looking for a roommate.  gimme a msg if you\\'re interested!hi fellow fellows: if you know anyone looking for a room for either short term (feb) or longer term, my room is up for rent.  place is really nice and 5 min walk from f/g: good morning all!  does anyone know of a clean way to incorporate a python model into a sql database?  basically, all our data is stored in sql databases, and we want to streamline a process that appends the model output as a new column in the database. any reason you wouldn\\'t create a new table with the predictions and be able to link by id? might avoid conflicts if you re-run the predictions.  but you could use alter table to add a column, and use update to change the values within the row. great suggestion to improve the structure, thanks!  the challenge here is really that we want an automated process to run new data through the model as it comes in - so how to incorporate the model component.  i can see it working using a cron job to run the python script outside of sql, but i was hoping there was a nicer way, especially since i\\'m the only python programmer at my company, but we have a few people who know sql. so do you basically want to generate a model prediction as each new data row comes in in real time?  i think you may be able to implement that at the layer that generates the data row, e.g. if it’s on a website, call the model from there (e.g. if you put it behind some type of api you can call).  but i think if there’s a lot of data rows coming in and its not critical to do in real time, using a cron job to do it in batches and during less busy periods is probably ideal.  we use rundeck for scheduling these types of jobs, hi everyone. does anyone have experience analyzing bed-sensors and/or acti-watch (acti-wear) sensors for sleep staging? it doesn\\'t have to be in real-time.  really the problem is the data warehouse guys do everything in sql, so they want the model in sql as well, and i\\'ve gotta convince them that hard coding all the rules for a random forest into sql is a bad idea (note: doing the math for him on how long that would take and how inflexible that would be did not work).  so if the general consensus is to run it in python with a cron job, as there\\'s no easy way to run the model in sql, i can use that to support an argument for leaving it in python.: i\\'ve done some work with sleep analysis from wearables.  feel free to pm me.ah i see.  that’s interesting, i wonder how hard it would be to store the trained rf as some sort of sql structure.  however, yes, i think it’s a bad idea in practicei’m quite surprised someone is asking you to do that hahathat’s why i didn’t even realize what you were askingyeah, it\\'s much more a business company than an analysis-heavy company at this pointyou can see how it sounds easy from an outside perspective, just switch from python to sql.  i figured i\\'d see if anyone in the community knew of something i was missingyeah, makes sense, but i think if you give in to this type of ask, it will be more difficult in the future to deal wiht your model , e.g. refitting it, and things like that, i bet even if you come up with some sort of sql solution in  a sort of ‘minecraft emulator’ sort of way it would not be easy to manage some db (redshift for sure) support python-based udf (user defined function). have you looked into those? no, that\\'s a great idea, thanks, i\\'ll check them outthat’s neat, i hadn’t seen that beforehi everyone!  i\\'d like to do some crowdsourcing. can somebody please recommend a simple tool? something besides amazon mturk (i\\'d like to avoid php if possible)does anyone have any experience with chunking in natural language processing?i don’t have personal experience, but i think `spacy` has an inbuilt chunkersee:  thanks!this might also be helpful: another nlp question from a newbie: i\\'m trying to build a corpora from reddit posts (using praw), and ultimately apply lda or lsa for classification. for example, i\\'m focusing on extracting titles from posts on reddit. to build a corpus, would it be useful to write each title out into a text file, or append each title into a single string, then write it out, or something else?here\\'s some sample code i have `# create a for loop that iterates over a subreddit, excludes posts made by the automoderator,# and appends titles to a stringfor submission in reddit.subreddit(\\'anxiety\\').hot(limit=100):    if submission.author != \\'automoderator\\':        corpus_titles += submission.title        print corpus_titles` uploaded a file:  and commented: better code snippet.really depends what you want to do with it after, but in general the more general your data storage is the betteri.e. write it out in a way that you can further process itmy understanding of praw is that each title comes in 1 by 1 so saving it out individually makes sense in case of failure is a much better way to get reddit data out -- the reddit api is capped at 1 request per second, and can only return out to 1000 results due to how their data storage is structuredso scraping a significant amount of data would be very very slowpushshift has archives of praw outputs as monthly dumpsyou can also download a reddit dump from bigquery directly, i forgot about that. that’s what i did for my project.iirc to build the initial archive up to like fall 2015 required his scraper to be running continuously for like 9 monthsthanks for the advice  and ! you can pass python data structures around via pickle over a network - in which case this is just a bytestream. i bet its possible to store this bytestream in a database, but i have no idea how that would scale, and it introduces the complexity of having to track what these collections of bytes are when they are unpickled, in addition to ensuring that all necessary libraries to access this byte-stream’s attributes are installed everywhere the pickled object needs to be used.does anyone know of any semi-supervised clustering algorithms for network data that are usable through either python or r? i have only found some packages in java for weighted-vote relational network models or unsupervised community detection algorithms. or am i stuck with importing java through some java-to-python package? uploaded a file: what dis, ?an example of how one can copy/paste a plot from a jupyter notebook (the jupyter notebook talk was tonight)by the way here is the link to the repo from tonight: if i could not copy paste plots from jupyter into slack i think my company would fall apartat razorfish we often took the sign nda, get data dump route. for us that meant we’d spin up a cloud environment for each client engagement based on need, proposing aws/gcp/azure based on the specific case, and we’d have a somewhat cookie cutter approach to each use case. part of the contract was that those providers guaranteed some security standard and the client would transfer the data to that cloud env, which was separate for each client.  they paid the cloud costs as part of the engagement, and therefore they still “owned” that storage etc, so it never really moved to something they didn’t own. pros were that we could iterate faster because we could customize our env and resources, which sometimes you can’t do entirely on premise because they’ll give you some small vm that’s insanely slow and you have to keep asking for changes. also you can develop some expertise in one env so you aren’t learning something new for each client. cons include that set up takes time, and they’ll also usually give you prototype datasets initially, and sometimes you run into walls looking for the most new/correct data or just build models on data that you later find is incomplete. probably that happens anyway though and you just need good communication w the client all the timeanyone going to be in seattle for data day?hey all,  , i have 2 tickets for red bulls vs nycfc @nj this sunday that i’m trying to offload. anyone interested?does anyone have any suggestions or tools that you like for  automating data analysis on data from  google analytics? so far, i\\'ve been using the rgoogleanalytics package from r. hi asli! sounds like it might a really useful question — what sort of analysis are you trying to automate?for those interested: : pandas has google analytics api integration:  have not used it though, but thought i\\'d give you more options!hi . i  am trying to automate user segmentation reports. thanks . i wasn\\'t aware that world bank data was supported for remote  access. hey folks, do you have any recommendations for data science/data engineering meetup groups in the silicon valley area? too many to choose from :thinking_face:that\\'s a lot of people...any one know when the east river swim initiation is for ds-nyc-2016c ? the trick is where the source is… sourcing from.  for example, lending club produces a free dump of issued loan data, including fico scores and localization data (at least to the state level) but it’s biased (naturally) to people who are applying for personal loans, which is emphatically not representative of the overall population.  most other groups will have similar issues, as they can only report on their own users.  the best source for representative populations would be the credit reporting agencies themselves, but they’ve been quite protective of that data in my experience.ibuder: integration with whatever platforms you’ll be testing on (web, native apps, email, …)? how attribution for various success measures is done (how close can you tie a success to a testing scenario)? are you only testing ‘users’ or could you potentially split on other entities? does it tie together logged-in and logged-out sessions?does anyone have a hacker-rank type tool to send out data science screens?we\\'re looking for something that\\'s more like a tool to screen for data frames + plotting rather than cs fundamentals let me see which of the internal stuff i have i can dish out. in the meanwhile do you want more technical stuff or just a general audience overview thanks! technical reading would be super appreciated!we used swrve at various points at my last job, and it was okay, but definitely built for non-experts. to be honest, it might be better to build a platform in-househi all, a good friend of mine/colleague has to move to the west coast for personal reasons, are any of your teams looking for a data engineer in the south bay (california?) thanks!this is a good point, thanks for reminding me of these privacy concerns. we only have mobile device  ip adresses, which i think is fine in the us.anyone have experience with building ml models on full-homomorphically encrypted data?  we used monetate, optimizely, and an in-house platform at various points during my time at bonobos.optimizely seemed the best of those options from my perspective, allowing the raw allocations to be written back to our tracking backend () so we could do our own analyses when their tools fell short and the team having enough statistical sophistication to not be doing things obviously wrongthanks! this helpssomething to keep in mind is that in our experience nearly half the experiments end up requiring some sort of ad hoc analysis. either to figure out what was broken or get some sort of explanation of the result. so you really need the allocations easily available alongside all your other data.  our analysis platform was mostly created in-house, the allocation system is a java port of facebook’s planout. this is for search algorithm and somewhat complex ux changes. for simpler, try different creatives in a marketing funnel type stuff, we have also used off the shelf stuff.does anyone know what kind of (emergency) us healthcare insurance options exist for a canadian who is considered resident of us for tax purposes (on j1 visa)?i know a little. j1 visa holder (excluding full time students) can buy the emergency (bare minimum coverage) of obamacare or whatever providers. but they are not required to get health insurance until they are considered resident of the us for tax purposebut, genrally, the us won’t issue you the j1 visa from the first place if you didn’t show that you’d have health care coverage at the time you apply for the visahowever, the visa allows you to enter. so it does happen that someone had health insurance, say, though employer/school when entering the us. but then lose coverage for whatever reason. at that point, you are not breaking immigration law. and if you’ve been in the us for less than…i think2 years…you’re also not breaking the law which mandates everyone to have some types of coverage or you’ll get a penalty in your taxthanks  !!hey guys, check out this kaggle competition that was put together in my team at mskcc work on cancer and win $$ : if you want to know more about the connection between deep learning and renormalisation, there is a much more technical (but also more precise) description here (with some nice references within):  insight is continuing to expand, and we\\'re looking for a great program coordinator to join the new york city office.if you know someone that would make a great fit with the insight team, and would work well in the role, have them submit their resume to . also, if there\\'s someone in particular that you\\'d like to recommend, please send me () a quick email. many thanks! :blush:: fantastic, thanks!: you\\'re welcome!does anyone have significant experience doing data science in java? i\\'m a python/r guy but will need to build some analytics functionality in java production code. are there tutorials/books/java libraries that folks would recommend?  i deployed some java models in production and a few months prior to that, i thought learning java is going to take time. it turned out moving from python to java isn\\'t that bad. this is a good tutorial: but what really got me started were:1. reading through some production code and make it some minor changes (and of course with pr)2. having someone available for asking question. does not need to be a java expert necessarily3. a good ide. this is as important the other two. i recommend intellij. there is a free community version which does the job for a ds projectthis book ain’t bad to help better appreciate the mindset, constructs and design of javadoes anyone have recommended reading around centralized data science teams vs embedded in products? thanks in advance!i would  be super interested in it as welljust to let you all know, i\\'ve sent an invite (to receive an invitation) to the current insight nyc fellowswe could have channels for different sessions to partition the conversations... yes, i said \"partition\":clap:well done andrew!haha, thank you leifur !(my coworkers were criticizing me for not knowing enough emoji ..)i would like to invite all fellows soonall. the. fellows.let\\'s allow the conversations to build up, then perform nlp to identify popular conversation topics. that sounds like a data sciencey thing to do right?or we can just start with a #beer channel and see where things go from there.dvats: you on that?,  excellent idea!! :simple_smile:we\\'ll do supervised topic modeling for now: i\\'ll be attending these workshops in nyc, may be useful for others:mining the quantified self :  does anyone know of a service that provides financial text data (letters to shareholders, investor presentations, etc)? you might want to ping  - he’s done work in the area. i can provide you with an email by pm if that’s helpful; not sure how frequently he’s on here.hi  yes an email address would be great, thanks! i\\'ll try him here too.yep  is the right person to ask, as far as i knowfrom training a model to predict gender on twitter from tweets: men talk about wrestling, women use :heart:.men love to nerd out on pro wrestling :wink: also,  new avatar is amazing  and bayesian lovers - can anyone advise a good accessible book on bayesian networks? there\\'s some material on it in koller and friedman \"probabilistic graphical models\". daphne koller also has a coursera course on pgms.there\\'s also barber \"bayesian reasoning and machine learning\"didn\\'t someone build an app for finding the right textbook? :wink: thanks   - book in dropbox. i still believe in well-calibrated human advice, this is not like some cooking where you can just find any recipe online :wink:semi-supervised learningmy project used rnns also (for video analysis). i found the keras wrapper for tensorflow to be a lifesaver. also, this site might help: but message me if there’s any questions i can (try to) answer!what\\'s everyone\\'s opinion on the best python package for live dashboard implementation?i haven\\'t had a chance to explore this, so i\\'m wondering as well. i heard some good things about superset. i\\'m curious how plotly dash stacks upsuperset was a little clunky in my experienceeasy enough to get it set up thoughi tried out metabase as wellsuperset is nice! fairly easy to get up and running.still nascent so lacks a ton of dashboard options, but it\\'s getting therebootstrap has very nice dashboard templates (). after that, plotly is very convenient for the content.i\\'m sure your imagination was better :simple_smile:we\\'ll do our best jboysen0 !has anybody (in nyc) done pyspark training they would recommend?i have permission but policy says i can only install “approved” stuff.  i tried to go through proper channels at my company to install anaconda, r and rstudio.  i was told no one ever asked for that before.  (!!!???). then i talked to other ds folks who just did it anyway.  i followed suit and approved software list eventually caught up with us.  :grinning:before that i worked at a financial company where admins refused to install impyla on server even though it satisfied all their security requirements.  so i had to make system calls to manipulate hive in python.  painful.   not in nyc but i liked spark moocs on edx developed by databricks.  they teach pyspark via jupyter notebooks in vms.nice to hear maybe the ds people have the power to affect the policies over timewe had better change the policies, otherwise we\\'re not doing our job :slightly_smiling_face:who is the best person to send a referral to for insight?an insight employee should comment on whether this is up-to-date, but i’ve used this in the past:   not answers to the question, just tagging folks who might be able to give an answer. to what program would you like to refer someone?probably health data science, and regular insight. both would be good fits(data science)i checked with our admissions manager - the link above is the correct one! thanks for the referral!what do people use to make dashboards in python?i\\'m looking at:but it\\'d be great to know if there are other good options as well.dash is awesomeplotly dash. also look into superset by airbnb i’ve used metabase and superset, plus bokeh in an in-house ember app by one of our engineerswe ended up going with the in-house for access-control ease because like 30% of my job is fighting our cto over data accessthat said bokeh is pretty easy to get up to speed with, and you can make some cool shit with iti found metabase to be a little easier to work with than supersetlooked into dash for the project as well, it looked cool but i haven’t done much of anything with itplotly dash in my opinion by far the bestthat link is pretty good to get started since it has lots of examplesand exercisesi will say dash seems way easier to work with at the “dashboard” stage rather than just visualizations themselves compared to bokeh as wellthis is what it’s looking like… i think we were more tightly bottlenecked on the i/o due to the bug, which let the gpu have more idle time, and the slowdowns started stacking on top of each other. we’re seeing decent returns on speed by tuning our i/o parameters.now we play the game. who @ channels general first. new fellow or  :smile:do you use free, or commercial version? how do you share dashboards internally where you need to limit access? yeah, you can have grayscale pngs with alpha as well. i used to use them a lot before coming to insight. i\\'m really digging this silly webapp: would\\'ve been a fun insight projecthello! i\\'m looking to overlay a kernel density estimate output, plus some basic lat, lon scatter points onto a street map. anyone with experience in this? i\\'m handling my data and generating the kde in python.: i did something similar on googlemap, but i have to use heatmap to do it, which is not very accurate representation of 2d distribution. i am not sure what you can do with openstreetmap. most of our interns at viacom tend to be undergrads and we are always looking. we just finalized our spring interns so the summer program will start recruitment events soon. the company page should have all the info , let me know if you need something specific and i would be glad to help our unit at mskcc accepts grad and undergrad interns - in the two years ive been here ive seen math, pre-med, econ and or folks come through and do some really cool stuff. its a great way to see how tech and data science can impact healthcare.let me know if you want to be connected to the person who manages the programany insight fellows playing overwatch on ps4? i just got it and am looking for friends - my psn name is avicura :slightly_smiling_face:i am also looking for friendsin general:thinking_face:stay positive alex_rheebut i am stephaniebut i am:sakamoto:: generally i think its best to start with the simplest and most interpretable model that will solve the problem.  be sure to understand how to implement and interpret any results you will get in addition to assumptions and limitations to the model.  if the interviewer wants you to provide more complex models they should make that clear in their feedback in the interview.  be ready to explain how you would choose the best model to solve the questions..as  points out there may be tradeoffs between accuracy and speed/interpretabilityhey data scientists! how does one determine the threshold for winsorization? any papers you can point me to? :smile:does anyone know how to write files while in a flask app? i am trying to create a png image from a third party api based on user input, and then displaying said png image.any body using d3.jsuber news: : ton of useful stuff on d3.js: .  the modules c3 and nvd3 listed therein are particularly nice, easier to use for simple graphs.fantastic collection of data science ipython notebooks: jure leskovec\\'s team has been advocating large memory machines over distributing over a cluster.  the snap project ought to have some benchmarks for this.hi all, does anyone have code or suggestions on how to convert a json with a grouping variable to a data frame in r? if anyone is curious, you do this by extracting the json data and using \"unnest\" from the tidyr package here is a good example of solving expectation value questions with recursion: has anyone worked with api consumers in python? i am looking for something like  which lets me define schema in sqlalchemy? any suggestions?@newbies: welcome to the world of data science!welcome, [ds,de]-[ny,sv]-2015c!hey sv guys, can\\'t wait to meet you at the party this friday!the shit list: stay positive sebastien_dery: i\\'m all good, but thankspanama papers go live tomorrow: thought i would share:so, i have an acquaintance (fellow phd in physics) who is looking for ds jobs anywhere in the us and asked me to let him know \"if i hear of anyone hiring\".  if i don\\'t know of anyone close to me actively hiring, any tips on how to respond?  it\\'s such a general request i don\\'t even know where to start...  i guess tell him about insight :slightly_smiling_face:we\\'re hiring at samba tv in sf!but, i guess, does it somehow help him if i send him your way?  i just feel like he could find plenty of postings online (i imagine you have an ad out?)the way i’ve done this in the past, and it’s been moderately successful, is:1. use the insight network to find people i know at companies.2. ask them if i can make an intro to my friend.3. use that to set up an informational interview with the goal of finding out about the position and job in detail, and getting a warm intro to it (either through the employee referral tool, manager intro, whatever)very cool, i really like that workflowand maybe i can talk to him first to narrow down what he\\'s looking for? i mean, i don\\'t want to spam all 1,549 of us here...that’s always a good start! i normally start with step 0 of “hey, i know people at x, y, and z. what companies are you most interested in and what sort of ds track are you looking for?”i’m partial to the “introduce them to insight” route… -- i get that a lot too, and i recommend reading our 6 white papers: ds / de / ai / hd / devops / pm to try to think more about what they want to do in their day to day and where to do it. doing a job search in all locations for all kinds of roles isn\\'t usually very effective.awesome, thanks kathy!there’s an ai backlash underway for sure. even henry kissinger is getting in on it in this month’s atlantic. thought this piece actually cut through the hype the best. quick question: for someone moving to the bay area, who is interested in staying in touch with other techies/broadening skills/hackathons, can anyone advise how to find those and where the best ones typically are? are they more in sf or sj? any advice about getting the most out of the area is welcome! i very much appreciate direct refs from insight alums!anyone working in city infrastructure/smart cities? this is from a friend:```i’m on the steering committee for a leadership training program called the water leadership institute. we’re planning a panel (webcast) on smart cities and topics related to how data impacts the water industry in cities. i wonder if you have anyone in your network that might be in thinking about data as it relates to city infrastructure - specifically water and wastewater but not necessarily so. a lot of engineers are really new to this topic and could really benefit from learning about the potential of big data in our industry. let me know if anyone or anything comes to mind!```?ooh, i’d hesitate at that use of informational interview…would consider informational interview more as ‘interested in learning more about this field/your career/general insights/etc.’ and would much rather be told straight up ‘friend of mine is interested in job opening at your company, do you mind talking to him/her and, if good fit, referring’.also, would def want a resume sent to me with the request!i would say it\\'s evenly split, with probably more hackathons in sf. plus sf has that density thing going on, everything is blocks away. i would hop on , type in all my specific interests, and just start attending as many meetups as possible... that would definitely open the door to hackathons and other networking opportunities.for those of you who live in the bay area, and climb and/or drink alcohol: i propose thursdays at dogpatch boulders followed by drinking nearby.  the bouldering gym is very close to the 22nd st. caltrain station and several half-decent bars.  i\\'m going to attempt to get to the gym between 6:30-7:30 typically, and to a bar by 9:30 or 10pm.  for a bar, this week, magnolia brewery (pretty sure that\\'s the name), about a hundred feet up from the bouldering gym.  perhaps i\\'ll send out an email too, given the low density of people on slack at the moment...fellow fellows! i\\'m trying to get my jupyter notebooks to render on a github enterprise account and having no luck.  the internet is giving me mixed feedback on whether gh enterprise supports notebooks.  do any of you know if/how to get notebooks to render? i’ve encountered this problem before - i believe that this is an add-on feature in gh enterprise and needs to be configured by an administrator. a quick workaround is to save your notebooks as markdown files, which will render in gh enterprise.thanks !i am wondering if anyone has thoughts on the best way to build a model in the following situation. its a regression problem but the predictors are all 0/1 categorical and the matrix is very sparse, but we want interpretable coefficients (e.g. this variable is contributing this much when others are held constant, so no deep learning fun... etc) and preferably doing some predictor selection (e.g. these predictors are contributing the most). linear model with l1 penalty is not performing well --- its not doing any predictor selection in this case --- we suspect the matrix is so sparse that the coefficients are super unstable.sounds like an interesting problem — what’s the context? site browsing data or ad exposure data?you can try some type of leave-one-out method and see how removing predictors affects the accuracy of the model.  it’s sort of different and aimed at prediction so may not be applicable.you might try an elastic net regression if your solution is unstable. when you combine l1 and l2 penalties, you gain the benefits of a more stable solution. yes, it does support it. how big is your notebook?anyone going to odsc-west this week? sadly, can\\'t say more. :disappointed:   i\\'ll look at that.  it came up in our discussions, but then the added complexity was cited as a reason to not necessarily go that way...stay positive amelia are you also using interaction terms between the features? with many binary input categorical features, i assume there must be some correlations there that may be interestingi would think so also, i\\'ll have to ask the person who is primarily working on the model about that one.  i\\'m just helping on this one on the side...i’ve also heard of people applying pca to discrete variables with some fancy math, but i haven’t ever read up on iti\\'ll be at odsc this weekend! neither a small 2k nor a larger 250k notebook are rendering, so i think its more likely a configuration issue hmm. interesting. i didn\\'t need to play with the configuration. anyway, have you tried this?does anyone have experience using play framework for scala to serve spark ml models? if so, dm me :sunglasses:or just post some info about it here :slightly_smiling_face:does anyone know how to open an .xz file on mac?: did you figure it out?? are you doing ml or probability? i am doing probability, and nothing i try is opening the file!it\\'s all part of the test!!!!oh no, i pretty much fail at the beginning :disappointed:that is *many* billionsi agreeso, curious - i\\'ve set up ssh servers in the past, and made some attempts to secure it with key-authentication, no root login, etc. i\\'ve noticed that even with password-authentication only, there is a slight delay in the response - is this a network effect generally, or a security feature of ssh? i imagine that even adding a slight delay to password validation would basically make most brute-force attacks worthless. so, curious, if someone does want to brute-force a password - how are they actually going about overcoming these kinds of delays to make it a viable method to crack an account?generally you need offline access. either a dump of the site, or hardware access to the machine. things like this are useful when $site_x has it\\'s database stolen, but you only have the hashes. then you can brute force hashes, and now you have a pair of usernames and passwords which you can attack other sites with.jollyhrothgar: just speaking of generic ssh servers (openssh) its probably your network delay or some processing (ssh -vvv should tell you). there is a feature in pam to prevent common attacks, but its negligible enough for us common users to not notice. you can check that by spinning up local machine, setting up ssh server and try logging in, it should be much faster (in ms). speaking of attackers attempts, it depends on a myriad of things, there are automated tools (even a distro: kali linux)  that are custom written for different versions of services we use. so if you use a really new pen-test tool against really old service(including ssh, never tried it but just guessing) you have a really high chance to cracking into the system just with brute-force. at this time, its just a race in versions against who finds out vulnerabilities first, attackers or developers :simple_smile:: openssh does not have built-in delay for wrong password to my knowledge. but one can always use fail2ban or google authenticator (two-factor) to make the server more secure. to actually break in ssh servers,  one has a lot better chance through bugs in the implementation of various security protocols as  pointed out . passwords were rarely the weak point.jollyhrothgar: three\\'s also a distributed brute-force attack, with a coordinated effect of thousands of machines hitting an ssh server. assuming don\\'t crash the host, you can try multiple passwords in rapid succession., , , : ah, okay - i guess it makes sense that these kinds of attacks would be used offline, or in a distributed attack (assuming a server can handle a high volume of requests). thanks for the info, folks!hello. i was wondering if anyone has any resources on the impact that social media can have on a business. e.g., from the perspective of a data scientist, how can inclusion of social media data impact the type of analyses (and business insights) of a company? ideally, i’m looking for research/articles and/or use cases, but even just examples that come to mind would be helpful. thanks!apparently slack is losing it\\'s appeal for some: slack: let\\'s kill the inbox so we can be your inbox! :wink:does anyone remember google wave? hehe.this starts off slowly but is very well-done and gets at several of your interests: is anyone familiar with using things like app usage patterns for classification of things like churn, purchases, etc. i\\'m having trouble finding the term for this type of analysis. it\\'s not really time series, as i only have two measurements, prior to app usage and after. but i\\'m curious if different usage patterns correlate to certain outcomes. is there a term for this type of analysis. or is simple feature engineering using app usage times the best approach, things like average time of day, variability in time of day, average days between app use, probability of using the app for each hour of the day, etc. interpretability of the results are extremely important, so black box or ensemble methods may not be the best approach for me.i would call this *cohort analysis*.that\\'s somewhat along the lines of what i\\'m looking for. though in this case there is a single cohort, and use was limited to about one month. think of something like a work tracking app with a one month free trial. i\\'m interested in something similar to whether certain usage patterns were more productive after the 30 days as compared to before using the app. implications could be importance of implementing daily reminders, whether reminding at certain times of day matter, etc. there is no chance for future data collection and i can only rely on the data at hand. things like forecasting obviously aren\\'t appropriate because there isn\\'t a measurement at each time point, but it seems like this is a problem that has likely been tackled before. is there a specific type of cohort analysis you\\'re referring to?this sounds pretty similar to engagement measures in e.g. mobile gamingmeasures like sessions per day and session length might be a good startthanks, sounds like the feature engineering approach and logistic regression may be a good starting place. i just wanted to make sure i didn\\'t have an obvious blindspot with regards to a standard approach.anyone know a good way to deal with changing data schemas with an implemented data pipeline? for example you have a data pipeline configured to save data in a specific schema. business/product decides to change the schema, is the only way to ensure pipeline still works is to change the way data is handled. is there away to allow pipeline to have visability of schema changes with minimal changes?why don\\'t just take the entire fft ? if that\\'s to big, you can take mfcc instead (or an equivalent mesure that does not use the mel scale) confluent has a schema registry. aws also has a strategy it’s been telling businesses, which is to use dynamodb to store metadata on data stored on s3, which potentially could be use in your case. that said, any alumni who have had direct experience with this?i’m getting a shape miss-match error from somewhere deep in a pretty complicated model (keras implementation). i was wondering if anyone had any tips-and-tricks for debugging this kind of thing? unfortunately, printing out the model summery wasn’t helpful for this particular issue.the model compiles, but during training it breaksyou should be able to call each layer, one at a time and look at the shape of the output (are you using functional or sequential api?)i’m using the sequential apiuhm, it\\'d probably be easier with functional (see this example: ), but it can work also with sequential, if you explicitly define the instance for each layer (and append with model.add)interesting, okay yeah i haven’t been exposed to the functional api yet. i will definitely check that out.yeah, it requires a bit of getting used to but it makes it easier to handle complex models, imo/poll \"straw poll - where would you be most excited to work in europe?  i\\'m wondering what city would have the most draw for good talent.\" \"amsterdam\" \"berlin\" \"paris\" \"#nevereurope #godblessamerica\"i retract my previous commit… i am using the functional api…:five:london:six:munich:seven:warsawhey all, i know an experienced executive assistant who is looking to move out of the finance industry and into tech. if anyone thinks their company might be interested, send me a pm!ha, fair point - i only included those three cities because they would make sense for us for other business reasons :grinning:i figured…and we have an office in warsawi have a similar problem, and in the middle of investigating differnet tools.  i have been looking into aws glue, and think that it can be a potential solution  cool - yeah i looked into aws glue as well. any other tools you think are worth reading up? thanks in advance.i also rolled my own tool currently, and deployed as a lambda, but that’s all i can think of for nowfor all des, what do you guys think of managed services like alooma, stichdata, segment, fivetran, xpenty, etc that offer etl and data pipelining solutions. what kind of companies would benefit from these teams better than let\\'s say hiring an entire data engineering/infrastructure resources?so happy i added this extension. :slightly_smiling_face:what techniques/strategies do you use to improve your precision without hurting your recall? this is for an imbalanced classification problem where i sub-sampled the negatives to match the number of positives. i have plenty of positive data to do this. i use some of those services, it’s cheaper than doing it from scratch and putting people to manage that when you want to centralize a bunch of data sources like hubspot, salesforce, etcdid anyone join amazon as their first ds job after insight?i\\'d love to connect with you!, , ? happy to gchat or gmail. won\\'t respond to slack unless directly pinged, appreciate the help michelle :grinning: . amazon is great! i\\'d love to help in any way i can.hi, becky. i am a current ai fellow. for my project i am working with mfccs. if you want to discuss about that, you can talk to me.i can connect you with several people in seattle if you\\'d like :slightly_smiling_face:a good way to start is to set up a chain of different methods.you can start with a good high recall algorithm that doesn\\'t have a high precision and then systematically reduce the amount of fp with other algorithms and therefore increasing the precision without affecting the recall.you guys hiring this round?yes please!a good chunk of the company isn’t hiring. but, many teams are still hiring this round. we, down in san diego, will start hiring again hopefully in the fall, except for a few teams which are hiring software eng right now. no data scientist hiring in san diego at the moment. thanks! i mightmichelle, i can\\'t get over my initial reading of the event, \"zero data\" . it makes me a very sad panda.hahaha, i just noticed that! unbeknownst to all participants, we are trolling them by continuing to dangle cool data despite not even having any. :upside_down_face:has anyone gone to grace hopper?  love/hate breakdown?based on my experience last year, i believe that every women working in a technical field should at least once go to grace hopper. i was, for the first time ever, surrounded by overwhelmingly deeply technical people who happened to be overwhelmingly women. when i got up during talks, i did not stand out just because i was part of a minority. it gave me the experience of what it feels like to belong to the majority, it allowed me to better distinguish between personality and situation, if that makes sense. also, it is an extremely friendly, very fun conference. there are many undergraduate students in the audience, many of the events are not directly relevant to data science/machine learning/ai, but you learn a lot about your field in ways that have less to do with, say, \"stochastic gradient descent\". also, you will be able to help undergrads figure out what they\\'re excited about, what it means to work in data science/ml/ai, etc.hi all -- a new seattle fellow here. does  anyone have experience with saving/loading tensorflow models? i have managed to re-load a model i pre-trained, but i am having trouble using it to make a new prediction... my goal is to re-use the pre-trained model to make a prediction on a new data example (e.g.: user uploaded data in flask! :fearful:)what kind of model are you using? i retrained the last 2 layers of inception in my project. that is an awesome visualization   thank you for sharing!!hi fellows!! i have a question. i am looking for a way i can implement a register website to download load a file. like, let\\'s say, you go a web site of data set. then you have to type your name, email, institution/company, and so on the web page, then you will receive a link to download the data set.  anyone have any idea or know any website that help me? i think you could set that up pretty easily using flask and some extensions: thanks! would you point me any tutorial site if you know any?i won’t do anything besides google it, so you might as well just do that yourself :slightly_smiling_face:ok i see, flask with extension...thanks!!if anyone is interested in the dataengconf in nyc next week, you can get 20% off by using the code insighti think you should still encourage people to post senior level positions on the board (in addition to emailing them). i know i always look at the descriptions to see what\\'s out there and figure out what skills i should focus on next. also, while i have no intention of embarking on a job search, if something interesting pops up, i will apply. hello, i was curious if anyone had any experience with enterprise-level api design. our current stack is all java but we’d like to setup a python api that will be queried 100,000+ times a day. if anyone has any expertise in the area and can answer a few (newbie) questions, please pm me! that\\'s possible.  100k requests / day can be solved in many ways.  one way it\\'s done in wsgi apps like django or flask is to have many workers behind nginx or a load balancer.  the number of workers depend on how many *concurrent* requests do you want to handle.  that also depends on how long does it take for the request to get processed by the application.  is the response in 500ms vs 1s means that you need half the workers to do the same job.i have used python tornado (which was designed to solve the c10k problem).  tornado is usually architected when you there\\'s lot of wait time (i/o or db) in processing the request on the server.   i have written a sample app that might be useful -template: `` or sample project: ``.  i have given a few talks on this as well at pydata, so ping me if this looks interesting.created a public channel called  to talk more about it.for anyone interested in attending next week’s dataengconf in sf, they’ve given us a discount code for 50% off - “insight50”. there seems to be a lot of good presentations and we’ve got several alumni presenting. looking forward to it! and i + a few friends will be starting at 11am at the justin herman plaza. i\\'m borrowing a lab coat. anyone else interested?^i’ll be at civic center helping organize some of the official march volunteers!hi everyone! i am very excited to announce that my first course is now up on datacamp :tada:.  it is geared at building an intuition for how to use clustering for exploratory data analysis. i would greatly appreciate any shoutoutouts on social media to help spread the word! when i\\'m doing personal stuff i use conda for envs. at work i\\'m a really special snowflake.hey guys! quick plug for one of my mentors: you might have seen in datascienceweekly that lending club are on the hunt for a weapons-grade data scientist. chaomei chen, chief risk officer at lending club, is widely regarded as the reason lending club achieved the second largest ipo in tech history behind facebook. she is an incredible woman, she was humblingly kind and helpful when i panic-emailled her out of the blue to get my insight project over the line, and many people owe their careers and their success to her. we would be lucky to work for her. let me know if you are looking for a new position and i would be delighted to make an introduction.does anyone have or know of some nice slides covering \"ml for pms\". i am giving an introductory-ish presentation to our product group on monday and it is always great to see how others have organized the topic.i’ve shared around  and  to our groupi think the latter has slides up on slideshare as wellyes, found those slides. thank you! thank you ! one of the things the bothered me the most in insight was the fundamental lack of flexibility in the infrastructure we usedi used digital ocean personallyyeah and i used hyde+google cloud+blumix, and the support was amazingyou can always just say “thanks for the interest, i’m not looking right now, but can i keep your contact information and reach out should that change in the future?” i was actually quite happy with the level of freedom (and everything else, generally) at insight. they provide tools/support for certain ways of doing things (e.g., aws) so it makes sense for them to encourage people to do things those ways, but in my experience they never stopped anyone from doing things different ways.hi all, for your very first job right after insight, is it normal to go in with the title “data scientist” or “senior data scientist” or else?“junior”, “principle”, “associate”, etc.slide-share woes:  anyone have any suggestions on having a nicer looking slideshare?  mine is cutting out edges of my tree on p. 13, even tried to change the color but still a few aren\\'t showing up in slideshare, and do show up on my .pdf export from keynote:don\\'t use slideshare? [hi dan]yeah, i’m just linking ot my pdfsha, hi matt!  how else would you recommend us present our demos to companies?  slideshare was recommended to us as being safer than relying dropbox.  note:  i do not have a front-end :simple_smile:well, a link to my github repo is my \"front-end\"you can stick a pdf on github, why notor make a website that serves as a slide deckflash drive?internet of thingsthings of internetdoes not show up as a presentation on github:ah, well i can click \"raw\" and download the presentationyeahpsa: github renders markdown/rmarkdown/ipynb inline nowi used google slides.yes me too: check out github pages thanks worked like a charm!if you structure your readmes well and use markdown you can turn your repo into a project website using the automatic conversion processnice: i used speaker deck for my demo  (slides are actually still up at ). no issues with the slides getting cut off, and the website automatically updates when newer versions are uploaded.i’m also using speaker deck. slideshare destroyed my vector graphics, even though i converted to pdf.stay positive brad.deutschyour daughter is a good icon designer uploaded a file:  and commented: good luck to the current batch of fellows as you embark on demos.  here\\'s a checklist my daughter (age 7) made for me when i was getting ready to demo last summer.  i think it\\'s pretty good advice, not just for insight but for presenting in general.see now i look prescient and weirdhah, sorry, couldn’t get the file uploaded…should work now.hey all, i’ve a very open ended question for the health people: i’m looking for some starting points/articles to start thinking about merging and using messy emrs. experience/tips/packages/cat gifs welcome. :cat:hey  messy emrs are the bane of my work life. not sure what you want to do  but the fhir project might be a good starting point: what kind of emr\\'s are you talking about? is it still a data integration problem or are you looking to do something more?this is the python client for it. our team (leifur\\'s and mine) is slowly moving towards adopting the fhir protocol and setting up a fhir api that allows patients to be retrieved as fhir objects. its essentially a standardized protocol/format for exchange of emr data across providers. we are using it internally because msk has a really wide variety of emr systems. so integration and lining up data from different emr systems is quite painful. ok really painful!!! :scream_cat:/giphy banecatthis looks super good thanks ! it’s good to know what your data should look like, even if it doesn’t. we (at mt sinai) have a dump of emr data from different hospitals and are looking at extracting any information. at the moment the main objective is integrating and lining up data. :upside_down_face:yeah the fhir project is really pretty awesome.  which team/dept are you with in mt.sinai?  we\\'ve seen some really good stuff come out of there...just started at the arnhold institute of global health. mainly focused on global public health but with some local and internal things too.hey all, does anyone know good sources for learning time series analysis with python?:: also,  thanks the tensorflow dev summit is happening next wed. on-site in mv has sold-out but there are still spots for the live-stream viewing party at google sf office (for now): that\\'s great, thanks for sharing! does one need a work visa for this?anyone know any data scientists in vancouver? i have a smart friend trying to get into the field who just moved there.i know someone who used to work at mobify as a data scientist in vancouver, but he is now a data scientist in ottawa at shopify. if there\\'s no better connections, he may know someone or be useful himself. thanks  - probably that’s removed enough that she should look for more direct avenues. but thanks for the offer!mike aka schnaww works for kablamo in vancouversadly, most likely yes!oh yeah! i remember that now! thanks : when you say you don\\'t want to fit a distribution to the data do you mean you\\'d like that continuous function to respect the weird shape?that\\'s correcti was thinking perhaps some kind of gaussian expansion, if that\\'s a thingmy first thought was edging towards using some spline interpolationfitting the spline to your data will give you continuous measurements over your initial range of datatoo naive perhaps ?i\\'ve used spline interpolation with this data set, and it does work, however, this distribution has to be sampled hundreds of thousands of times. spline interpolation increases the run time significantly, unless there is some way to serialize the spline interpolation?i could probably serialize that, but it would still require lookup of the proper splinebut still, it would probably be faster, since the interpolation would only be computed oncei was considering some kind of fft, using an exponential basis, since i require this distribution to converge to 0 at +/- \\\\inftyhmm guess depending on the precision required by your sampling procedure you could use some form of memoization and only compute those that are absolutely necessary and lookup from the previous computation whatever falls within a threshold distancei\\'ll mull over the fft, don\\'t have a great intuition as to how it could be applied at the moment :slightly_smiling_face:alrighty, i\\'ll mull it over as well. thanks! i\\'m here!just find out this online ai conference next month - seems cool. can\\'t argue with that price also the results are in: thanks gabriella!so, i haven’t read that article yet, but are physicists back in fashion? i thought the \"physicists are great data scientists\" thing was very 2010, and that humanities and social sciences were growing in demand. no?half my team is physicists for what it is worthi’m also a physicist, and i hope i do at least average work :wink:hi all,does anyone have experience web scraping using python, in particular scraping for images?  i’m currently trying to scrape google images, and am hitting some road blocks since the api is now deprecated. i’ve created the channel , please let me know if you have any tips/advice!i have used splinter for my scraping in the past ()i’ve used requests + beautifulsoup, works pretty wellhas anyone had experience getting data from hcup, namely the kids\\' inpatient databasetrying to get data on a particular diagnosispediatric asthmathis is more of a pm question, but can anyone refer me to a framework on a best practices approach on how to evaluate projects?i’m aware of irr/npv for larger capital allocation, but i’m wondering how often these are usedif you have projects a,b,c,d.  other than the hippo, how does one decide what to pursuewe’d used a scale in the past where we scored projects across 1) estimated impact of project 2) estimated confidence of that impact 3) ease of implementationand then added projects into a sprint in a way that seemed to balance implementation time with potential impactthis was passed along to meseems very similari think reach is really important to keep in mind as well — we run into this when prioritizing analytics projects for clients, e.g. this model will add value, but only for 10% of users so we will not pursue scaling it yethi past and present fellows. does anyone have advice for downloading medicaid/medicare outpatient claims data? ? on the free end, meps is a good choice (medical expenditure panel survey) -- . hmm, just saw you were interested in outpatient claims. not sure if meps includes those or not. thanks for the suggestion! i\\'ll check out meps and see if anything clicks.does anyone have experience writing scopes of work for contract data science gigs? if so could you please pm me? i\\'d like to pick your brain. thanks!  i\\'ve used scrapy new fellowship here. throwing this out into the ether. is anyone knowledgeable regarding multiple sclerosis? do you know what type of drugs or therapeutics are commonly used for treating it?there you godata science from images obtained by google street view though no longer a neuroscientist by occupation, i can tell you some general stuff. since ms is an autoimmune disease most drugs used to treat it those that block the immune system or mitigate the pain, tremors, and other symptoms of ms.  have you examined ? they have a section on treatments that fairly comprehensive and easy to read (bonus!)., thank you!  how about ny state hospital inpatient discharge data:congrats  !  great post: awesomethanks ! it was a fun project! data and analysis code is in a github repo (linked on the viz page)i appreciate your implicit use of the isoperimetric inequalityi didn\\'t know that name for it before, but now i do! :simple_smile:“that thing about circles\"hey there, anyone have a clever way of working around distil network\\'s scraper detection? i keep hitting up against captchas while trying to scrape streeteasy using selenium, even when using very polite sleep timers and vpn cycling.from my memory, distil embeds some javascript in the client side code that is used for detecting automated clients and esp browser automation tools like selenium. so sleep timers and vpns won’t help  it also acts as a reverse proxy for the target website’s traffic like cloudflare does. so no real way to get around it i have a couple ideas for dealing with it that are out of the box, so message me if you want to discuss further :) :+1: thanks francesco and eddie! looking through the medium post now, it is quite helpful. :slightly_smiling_face: (and i\\'ll probably take a look at the book once i delve deeper)the bay area bike share has released it\\'s year 2 data:  perhaps of interest for future projects!question for python de types: can you suggest any resources (e.g. articles) about building scalable python production servers? context: we have a ml algorithm written in python to put into production. in the past, we have rewritten algorithms into our existing java production stack. however, this one uses enough scientific python libraries that we wanted to explore the feasibility of a production python stack. info about memory issues, performance comparisons, or server and container options would be especially helpfulhi ,  we actually do a lot of this at datascience as deploying python models is a main part of our data science platform.  what i\\'ve found is the main thing for performance is python itself.  therefore using modules like `numba` or building the models in cython will have by far the greatest impact on performance.  we also find that load balancing the model between multiple instances (e.g. if you have it running on 10 different hosts or docker containers, then use a load balancer to route requests to them) can help scale python models.  as far as python-specific options, i would be weary of default web servers in python projects like django.  these can be a bottleneck, so you should look into a more sophisticated python web serving stack, e.g. :  thanks  !hi guys, question: in my company we are trying to come up with a title system for data scientists that can show different levels of seniority/experience. we span from people with undergrad degrees to people with phds and years of experience. any suggestion? all i can think about is data scientist/senior data scientist but would love something more granular...junior ds (bachelor)/ regular ds (grad school or exp) / senior ds (grad school + exp) ?what purpose do you want it to serve?  junior / regular / senior are all based on a combination of education / experience levels.  what do you want to highlight with these titles?data *analyst* for bachelor folks?we have people with bachelor degrees who are definitely extremely talented data scientists :slightly_smiling_face:  i would not base titles on things like that alone, that’s why it’s important to consider what you want the titles to convey before just adding granularity^i personally agree, i was just suggesting what i’ve seen at companies...data analyst is usually a pretty different rolecan be, at least.yeah it can be a mix, we in the past have had people who do data scientist things but don’t have the experience or education needed for higher level, as data analysts who were later promoted to data scientist, then other analysts who do entirely different things and are on different career pathsi\\'ve been chatting a lot with companies about this recently actually. the distinguishing of analyst vs scientist can vary from company to company. the general trend though is that analyst\\'s job is to surface numbers while the ds\\'s job is to understand them. you could ask an analyst to find the % of users that click on a button for android vs iphone, but you\\'d ask a ds that works on analytics to find why android users are churning out at a higher rate.so if you\\'re an analyst on paper, but actually do ds/ai work, that distinction\\'s easy to communicate to potential employers?hey - i\\'m a new fellow in the health data program in silicon valley. does anyone have experience getting plotly plots(specifically chloropleth) to run on a webpage through flask ...?? ^^ have you done this? in an interview it’s easy to explain, on paper less so. squarespace went through this angst recently hits the right points too — in my company analysts are great with sql and tremendously helpful with feature engineering, but perhaps less sure of why those features are likely to be mathematically relevant. we’re trying to train them in python to move up the stack a bitjunior, ds, senior, principal for individual contributors. above the ds level, the managerial track can add \"manager\" and \"director\", which naturally funnels up to the vp and cdoalternatively, junior has a bit of baggage, so associate works as wellyou can always do the really boring microsoft-ish way: ds1a, ds1b, ds2a, ds2b, senior dsa, ... :wink: but i guess big-ass companies need huge laddersthat’s an interesting problem, how would you find the best so page? compare code block you’re currently working on to the question or answer code? or only look up the actual line of code you’re working on?  is anyone else going to nips?,  from what i’ve read that only works for python 3 though.has anyone ever had a grumpy it department who will not install required ssms (my work chose this for sql) on their computer and instead wants you to use the interface remotely? i am not familiar with this or what it will accomplish since out database will be hosted securely on their server without full administrative rights to us.  but i am also worried how this will affect my interacting with sql from python. anyone done anything like this? or should i stand up for myself and tell it this is bonkers? thanks!i guess the question is what would you want to use ssms for? an it department is probably wise not to install software you don’t needwe are using tsql and i assumed that i would need ssms to create and modify  the databases but i\\'m really not so familiar with it so maybe there is an alternative?i haven’t worked with microsoft sql server, but i doubt you’ll need the administrative tool it to create schemas and tables. in general, once you can connect to a database server via python (using pyodbc or sqlalchemy or one of the many other libraries) if the user you’re connecting as has the appropriate permissions, you can do everything you need.yay!!!! i love this solutiondefinitely read up on it first before taking a stand with it…best not to make enemies:wink:very sound advice! thanks!anyone have suggestions on what kind of “hustle stats” they would like to see in their workplace as part of employee performance metrics? anybody else going to meet up on survival models in stan tonight in nyc  ?  : sorry for the spam but can my mentees this evening please slack me? i will be late.i had a few questions related to an app i am developing. every query to this web app (in flask + spark) is answered by a simple pre-trained machine learning model in h2o which is then served to the user.  now if too many users begin to go to the web app, what are the problems that i would face and need to be solved? how many users it too many? and how would i solve these problems? if someone can shed some light, it would be terribly helpful! p.s. someone mentioned that i should be using spark streaming + kafka to solve this problem.: normally a web server has a queue for incoming requests and one or more workers to handle those requests. in flask, there is an easy way to set the number of workers, and it is usually set to the number of available processors on the computer + 1. that means that if the average rate at which the requests come in is more than the rate at which they can be handled, you will soon run into trouble since the queue will overflow. one way to solve this problem is to get a better computer with more processors to have more workers, but that soon becomes impractical. kafka is a messaging system that has its own queue and allows you to scale to more _computers_ rather than more _processors_ on one computer. it is not at all trivial to set up and maintain, but it is a much more robust solution for systems that need to handle millions of requests per minute.thanks alex for the great explanation! millions of requests per minute is certainly not something i\\'m expecting :smile: one last question, what happens if there are more incoming requests than processors on a computer? does flask queue these requests for later computation, or are they dropped?the requests should be queued, but i don’t know the default size of the queue in flask (it’s probably 2048 or something). another point is that http connections tend to time out after 30 sec of waiting time, but you should be able to increase this timeout in flask settings if needed.great episode of data skeptic discussing a ds interview problem from the perspective of the interviewer: stay positive dima… someone really needs to take on a project to make  smarter.... i listened to that last week, it was great!i think i’ve removed that sentiment “analysis” from nuggetbotcan be summarized in a single bullet point:* give everyone access to the data.¯\\\\_(ツ)_/¯it’s a good point, though probably doesn’t need 15 minutes.  the one complication is that sometimes you need to have spent a lot of time sql monkeying around to understand it’s quirks.so  you need at least a couple sql monkeys somewhere that you can quickly find and exchange bananas for insight.also, i’m staring at a empty banana peel on my desk right now. what does this say about me?hahatrue, or, you need to have someone etl the data into a nice denormalized schema that is easy to query for non sql expertsbeen finding some inspiration from this articleyes, though such schema can make easy questions easier and harder questions secret traps. also, the only thing i like less than being a sql monkey is working with bi to get an new etl setup.it leaves you to do the hard stuff on a different data store while everyone else can pull their own data with the bi tool + data warehousewe had a client with a horrible, several-hundred table database that only one guy at their company understood. i was asked to pull some analytics out of the database. because their one expert never got back to me to answer my questions, what should have taken me about an hour took over a week. oh well... we bill hourly, so it was good for us.haha, yeah, bad for you and i don’t want to be that “one guy\"hmm, i\\'m a male scientist married to a female teacher... as predicted! data science verified!n=1 and doneit works for my parents :smile:: \"the small sample size is made up for by the sheer consistency of the sample!\" :wink:i think one lesson learned is if i want to find a spouse in the future, i should be a truck driverthey’re basically married to everyone, not simultaneously i believefor us physicists and astronomers uploaded a file: i dunno… a mobile lifestyle might lend itself to having multiple lives and marriages.if you’re into that. i\\'m also married to a teacher, n of 2!qed!new blog post: feeling out the famous mnist handwritten digit set.hey guys! two of my best friends from the uk are coming to sf for the week, and i wondered if anybody could help me with a couple of favours: 1) are there any spare spaces they might crash on monday to thursday evening? and 2) might somebody lend one of them a bike during the day on tuesday? cheers!sorry everyone, wrong slack group, ignore!stay positive mattgibbany alumni currently work with aws tools? please dm, i\\'d love to pick your braini feel like the answer to that is probably almost everyone. any specific tools you have questions about?trying to pipe simulated data through kinesis streams, dynamodb, and lambda. i\\'ve found a few tutorials on the web, but none quite seem to work (not sure if me or them)etl question:  i have an sql database with 3700 people.  each person has hundreds to thousands of associated multi-category columns events, such that when loaded into python the 3700 people become a 10+ million row database.  i want to one-hot-encode it for sklearn.  after one-hot encoding, it would become 10+ million rows and thousands of columns. by using db.groupby(\\'person\\').max() in python i would then have an accurate and useable one-hot-encoded 3700 row dataframe (one row per person) for sklearn.  i don\\'t have the compute for the large one-hot-encode step.  has anyone experienced the similar and have any suggestions that don\\'t require more compute?for big transformations like this, i often use the just-in-time compiler from numbabasically convert your dataframe into a numpy array and then write for loops to do the transformation that you wantmake sure to use the @numba.jit(nopython=true) annotationi often get huge improvements in run-time this waybtamadio: thanks brian!  i\\'ll give that a try!no problem. let me know if you run into any issues. also if it works for you, i’d be interested in hearing about that too!that’s super shitty of nyc. tho, congrats on the baby !i\\'ve worked with geospatial indexing in mongodb and it is fast! if you can get a way to index your points and/or your shapefiles geospatially you might find a good use for it.yes, it is. and thanks  !if you love scala, building data pipelines, creating key metrics, building and enabling actionable visualizations, and helping to measure a new business, please come work with me :wink:is there anyone who has experience in policy or regional collaboration? i have someone at brookings (vp of cities) looking to work with a data scientist to review a proposal. dm me if initerestedprobably depends on the subject area, but maybe try some of these folks?      i\\'m happy to talk if you\\'d like. what are you planning to do with this data?i’m looking for advice on navigating the process of taking over a team, especially for a first time manager. i’m sure a number of people here have experienced this—any willing to share publicly (or privately)? (also, please don’t share outside of insight network—nothing set in stone yet.. just looking down the road.)i\\'ve read several articles over the years, so that\\'s where i\\'d start. i\\'d say probably 80% of it is good advice, subject to the usual disclaimers about personal style and company culture.message me and let’s catch up? prodigy looks really neat! how have you liked it so far? can it handle multiclass or multilabel labelling?yay! genevieve you made it!nuggetbot: animate me welcome genevieveit would be fun to attend and hang out but might be too soon for me to give my insight spiel again :robot_face:good input, nugget!haha, i think it\\'s totally fine! do let me know if you\\'re up for it! why not talk about work then, were all data scientists :stuck_out_tongue:hi all. i\\'m thinking about a move to the bay area, but my husband and i are not sure how affordable it would be. i\\'m trying to figure out salaries for data scientists and i was wondering if i could crowd source some data here. i have looked at glassdoor, but i would like some more specific information. i\\'m wondering what the salary range would be for someone like me, a lead data scientist currently with three years of experience in the field by next year. i\\'m thinking about looking at jobs at established companies or late stage start ups (100 or more employees). i would also like to get a sense of more than base salaries, like what do bonuses and stocks/options plans look like since these can make up a large part of compensation? thanks in advance for your input!check it! alexis there was a recent crowdsourced salary survey run here (so only insight alumni) that might also be useful: sorry that link is from another source, here is the insight one: links look the same, let’s try againwow, that\\'s awesome! thanks!for those who use spark, do you prefer pyspark or scala? my team is moving to spark and considering which route to take, since we’re primarily a python shopi’ve used both and i would suggest going the pyspark route and have a few people dive deeper with scala as the api is complete (pyspark has a few limitations). the future of spark appears to be in python/r (and other higher level languages).i don’t think i’ve encountered a spark use case where i couldn’t get it done in python, and that intergrates with the rest of the workflowwe use pyspark. works well and i only moan about the lack of typing when reading pull requests.scala! :thumbsdown:if you really want to understand and leverage spark, scala is the way to goalso worth noting that we use luigi for task flow management, which plays well with pysparkthings are typically developed in scala first, and extended to pysparkanyone know if there a preferred method to advertise job postings?  we have a bunch of insight alumni at grand rounds and have a bunch of positions open.  pm me for all the juicy details.interesting. thanks for the advice!i have also found pyspark to be able to cover all my needs.  for reference we use it in standalone mode (etls, exploration, some ml) since our data is not at hadoop level scales yet.anyone have recommendations for an introductory spark text or two? preferably r-based but python would also be okay. a focus on ml applications would be a plus.i prefer scala - error messages are easier to parse (just jvm rather than python runtime wrapped in a jvm), the api is more feature-complete since spark is written in scala, and type-safety can really cut down on the rate and variety of errors you can get in the middle of a long-running job. however, scala is a steep learning curve from python so weigh it against how heavily you\\'ll be using spark.hi guys! any idea why the insight websites are down? this may have been announced but i\\'ve been offline!... and we\\'re back up! the internet overindulged last night, perhaps? :wink:internet, you\\'re drunkhey folks! i have a tuition reimbursement program at my job and was wondering what kind of external learning/moocs people have found helpful?i\\'m more on the engineering side, and found an offering for a data engineer certification from google here:udacity is pretty greatcoursera has some helpful specializationsfor all our nlp projects, we use py3 because of better text handling. it wasn’t hard to switch to py3, so ideally, the choice of 2 or 3 depends on the project.hey fellows, i have a quick question about recall optimization for imbalanced data sets. any insights appreciated :slightly_smiling_face:how imbalanced? i can\\'t quite remember the exact cutoff before it becomes worrisome, maybe 15%/85%?hi fellow fellows, does anyone now of any existing, straightforward methods to calculate error bars/confidence intervals for xgbregressor predictions as applied to time series forecasting (where you can’t do standard randomized cross-validation/bootstrapping due to time stream autocorrelation)?  i\\'ve been scouring for one and have only found the following semi-cryptic blog post.   thanks!  i wish :slightly_smiling_face: it\\'s 99.1% and 0.9%. i\\'m using smote and xgboost, which is helping the model recognized the minority class. the business situation is detecting credit card declines.  i have an intuition about the answer but i\\'m curious if there is  a convention for optimizing recall for a minority class when doing so lowers the recall for the majority class.do you know the potential cost of lowering the recall of the majority class vs minority class? if detecting the minority class has higher value for the business, it makes sense to optimize for it and just take into account for future predictions that this will mean an increase in falsely labeling majority class cases as minority class.+1 to what  said. also, xgboost has a `scale_pos_weight` parameter which may help directly balancing your optimizationsince you can’t apply regular cross-validation for time series data you should use something like it:fold 1 : training [1], test [2]fold 2 : training [1 2], test [3]fold 3 : training [1 2 3], test [4]fold 4 : training [1 2 3 4], test [5]fold 5 : training [1 2 3 4 5], test [6]sklearn provides a special class for timeseries splitting sklearn.model_selection.timeseriessplit there\\'s a minimal cost to lowering recall for the majority class and the scale_pos_weight has been super useful.  i wanted to be cautious not to make any amateur missteps for this type of case and my thinking sounds in tune with your suggestions. thanks for your input; it\\'s reassuring.hi all! a bunch of us from insight are going to grace hopper next week, if you\\'ll be there too and want to meet up, join us in . we have a panel on wednesday, event on thursday night, and we\\'ll be doing office hours with applicants if you have friends who would like career advice. :woman-woman-girl-boy:i know what you mean, . if you come up with something else, i\\'d love to hear about it.oh, and as a reminder for folks who want to try upsampling the minority set, make sure to split out a validation subset before you do it, so as not to accidentally pollute it. :smile:hi georgiy, thanks for your response.  i have implemented this walk-forward/expanding window forecasting with xgbregressor, but i\\'m wondering what the error bar/confidence interval is at each prediction step.  there  doesn\\'t appear to be one for the model output (unless i\\'ve somehow missed it).  i\\'m thinking i might bootstrap a confidence interval, but have to be careful there as autocorrelation means leaving some samples out doesn\\'t entirely remove all of their associated information from the model (and leaving \"holes\" in the data may destroy  autoregressive components the model relies on).hey folks, data for democracy is having a hackathon this weekend and i\\'m co-hosting an in-person event saturday in boston.  there\\'s also a nyc one! hi folks, a friend from indiebio is organizing a science hack day that i thought some of you might enjoy: i\\'d love to see updates on this project as it progresses!our start-up company (grapevine ai) is looking for a freelance full stack developer,  javascript, node and angularjs expert, please shoot me a note if you have contact with someone like that.. thanks!@hee you can use the 80gb file by manipulating the rdd (`rdd = sc.textfile(’filename\\')`)most probably, you’d wanna forget about the order of rows in the column, given that the computation is distributed across the cluster.please feel free to pm me.hi everyone! can anyone share best practices for code review processes that work for their teams? we are a small group, working mostly in jupyter notebook environments (python, r) as well as lots of sql queries. our team is just getting started using git/gitlab, documenting code, and generally collaborating (new analytics team within gov\\'t agency). any suggestions would be great!we have two kinds of reviews, a style review and a requirements review.  style review - we have slowly done away with it if the language has a robust set of linters (flake8 for python for example).  requirements review - done by another dev.  there are blocking and non blocking comments as welland if the comments in github review is getting long, best to  have a face to face with the reviewer.and we try to make the code for review as small and atomic as possible so it won’t be a pain to reviewfor big/lengthy code commits, a face to face at the start, usually helps to explain things to the reviewer before she/he spends hours trying to figure the code out :slightly_smiling_face: i saw this the other day: thanks    for your comments! nice! thanks for sharing i think one of the most important ingredients of successful code review process is the individual pull request (pr) themselves. modular, topical pr are much easier to understand than one gigantic pr that contains lots of changes. as you build up the code base, keep pr small (maybe one class per pr, or one set of related functions per pr). when it comes major architectural changes, share and hash out the design in a different platform (like google docs) before putting out code changes. code diffs are really bad ways to see big changes. for jupyter notebooks, try to refactor most of the code into utils .py files and library, and import those in the notebook instead of putting most of the code in the notebook. it’s fair to have different code standard for those common utils/libraries and the code in the jupyter notebooks themselves.: thanks for the detailed response! i\\'ll follow up on separate channel ...super bowl sunday!has anyone done a pid controller for discrete data?does anybody know of a compilation of common character substitutions made by ocr software (e.g. i-&gt;l, e-&gt;o, 0-&gt;o)?thanks guys!what you expectbay area -&gt; austin for data/tech people!where in south bay? and is month-to-month do-able?mountain view and no, it\\'d be for a 6/12 month lease :disappointed: my friend susan is helping to hire 10-15 data analyst/ds positions at the federal reserve based on these two descriptions if anyone knows someone looking. you can email her for more info: does anyone have experience with azure and some spare time to chat about it? thanks!: if you\\'re really into emoji nlp, check out idibon, especially tyler schnoebelen\\'s blog posts. he\\'s a stanford linguistics phd that wrote his thesis on the grammar of emoji, and cofounded idibon.: thanks! i’ll check it out. so cool.my mil wants to come visit on the ues. anyone have any good experiences with inexpensive hotels in the area?: we got a hotel room friday night for $250 with hotwirei might be missing something but i believe at least for sqlalchemy you want to use \"?\" (without the quotes) apparently the formatting is driver-specific, but will be chosen from one of the 5 param styles detailed in pep-249: pymysql driver at least uses the “pyformat” option (’s answer)thanks for all the help.  i saw that page above and have tried the ? and also found some ideas on . i can get any number of \"correct combinations\" in string and params code to not throw an error, but i still get no data...so weird.  when i figure it out, i\\'ll post here.time to make a point. if you support net-neutrality here is the link that makes it easy for you to voice your opinion to the fcc. ^jon oliver also purchased the domain  (which links right to the page to comment on the fcc site) which driver are you using?  i think sqlalchemy defaults to using `mysql-python`mysql-pythonconfirmed.as so my connection string starts with `mysql`right.  if you have the pymysql package installed you can use that as the driver instead, e.g. `mysql+pymysql://...` for the connection string.  think the pyformat style `%(param_name)s` should work thenleast that worked on a mysql instance i have locally herethis is in a prod system and so adding a package is going to be frowned upon, but i\\'ll bring up the option in a review meeting i have later today and i can test that...i\\'m not sure if anyone is looking for opportunities right now, but if you are, automotivemastermind is looking to hire data engineers and scientists in both san francisco and nyc. the (possibly outdated?) job descriptions can be found here:if you\\'re interested, feel free to reach out to me for more details that\\'s exciting to hear! if anyone is interested in connecting with automotivemastermind _directly_, then the best thing to do is to reach out to me or someone else from the insight team! :smile_cat:whoa, microsoft to buy linkedin yes it\\'s big“linkedin, powered by microsoft” sounds a lot worse than “linkedin” :disappointed:stay positive tigrani guess the question is: why would ms drop that much money for linkedin?same reason they paid so much for minecraft. to make even more money.to be fair, after failing to buy yahoo! for $45b, they have a lot of \"we need to buy someone!\" money burning a hole in their pocket. this is basically free!i meant “how” will they make money from linkedin? linkedin apparently made $3b in revenue in 2015. unless they’re looking at stellar grown, it’ll be a while until ms makes money off this deal.i suppose they’ll leverage linkedin’s network in other ways...it doesn\\'t make sense to me either... my ms spies tell me they got an email saying there will be \\'great synergy with office 360\\'microsoft user data + linkedin data = :hippo:ha!this is going to make it so much easier to recruit via xbox live*xxx skull destroyer xxx* would like to talk to you about working at abc corp!my guess is they are trying to keep a foothold in the office software game to fend off google apps and such. linked in is the biggest professional network and will probably give them a lot besides the revenue streami think msft was supposed to have a call to talk about all of this today. not sure when it was going to happen though.yeah  , that\\'s what my contacts in ms said, it\\'s a 360 play.which does make some sense, office is (to my knowledge) their biggest product, so spending $25b to try to protect it makes sensei\\'ll try real hard to not take offense from these comments :slightly_smiling_face:yeah. if they are going to continue to “give\" away os upgrades for free, they’ll need to be sure to keep the 360 revenue stream going.would love to hear from both our linkedin + microsoft alumni  :slightly_smiling_face:*addendum*: my spy has told me that it is actually *office 365*, not 360... oops. :wink: working on that branding guys!office 360 sounds more funbut 365 &gt; 360how about office 720 or even 900? tony hawk styleoffice graham’s numbersoon we can upload resumes using word!seems a lot of folks have mixed feelings about this.. i believe it\\'s an interesting movebut not as interesting as that gif...there is a lot of potential to utilize cortana intelligence using linkedin data to empower networking. let\\'s seei guess that\\'s all we can do, wait and see right?i look forward to the future of this.i think emmanuel has had too much kool aid. but i agree. as much as i don\\'t use microsoft products, i do think they have some reason smart people working for them and there is a good chance something interesting will come out of it.  maybe, this is the start of microsoft bob 2.0bing?bingedin: the best place to search for a new jobyer welcome msft:stuck_out_tongue:lol, you guys..hahhaahey all! i\\'m helping udacity to build a new nanodegree on data science. we\\'re looking for partnerships to build industry relevant projects for students to work on. let me know if you are interested in.sounds like a familiar plan :wink:yes and no. udacity is offering a paid educational program with no job placement promise.has anyone\\'s company migrated from a vertica warehouse to snowflake and been subject to the very unpleasant surprise that all queries in snowflake return column names in all caps? uploaded a file: i don\\'t think it can be changed but i have found that snowflake is not case sensitive unless the database/schema/table/column was created with double quotes around the name. commented on ’s file : this is an old tutorial but might be useful if you haven\\'t seen it yetif you use docker there are some great containers here: the appropriate containers come pre-built with spark running nicely in jupyter along with kernels in scala and python and r.pretty well maintained, when new spark versions come out they usually update the containers within a few weeks.use docker as much as you can  why docker? why not...anacondadocker makes it easy to standardise, and it’s portable. for example, i have docker images that colleagues and new hires can run with very little setup, then they have access to my entire stack.yep- you can port the environment and code anywhere in a box, no need to ask a colleague to set up their env in the same way, or to set up a server the same way etc. just build the container reduces env issues that come up inevitably just being silly. plenty of good projects to go around, and most people here may be good at coming up with well-planned ones hi question, how many of you are managed by non-technical people (what are their backgrounds) and how has that worked out for you?what task (regression/classification/ranking/etc) are you planning on using it for? i can flood you with ranking information and pitfalls if needed.i\\'d like to hear about ranking :slightly_smiling_face:@ channel, ha ha, just joking. my company is looking for a machine learning engineer, let me know if you\\'re interested:  uploaded a file: anyone at dataengconf in sf today and tomorrow?update: i actually phone screened the guy who’s great at hiring women today. he was not great, especially considering he’s been an analyst since 2004 and a ds since 2012 (including being a sr ds for the last couple years).from my interview summary:-candidate did not know quite a few basic ds ideas and was overconfident to the point of condescension, even arguing with me on a couple of responses-what got you into ds: “this is how i’ve always thought about the world”-when describing a logreg model: “throw it into logistic regression then things happen” and “the model will spit out a thing”that last one is amazingwould it be unethical to give him an on-site just for the additional slack gems it would generate?i\\'ve had interesting experience with the first one, where the interviewer was clearly wrong.  instead of arguing i used my standard strategy of immediatly accepting the statements as true, then qualifying them into oblivion (\"yes, but it\\'s only true in this impossible scenario, otherwise it\\'s generally not\")i\\'d definitely prefer someone who  challenged me if they thought i was wrong though :slightly_smiling_face: it seriously crossed my mind…nothing to lose if he\\'s local :smile:it\\'s win win!hahaand i totally agree that i appreciate someone standing up for something if they can make a cogent argument in their favor…this was not such a case howeverright, it doesn\\'t sound like it\"throw it into the model\", \"the model will spit thing sout\" is enough to convince me of that :smile:though... that\\'s where data science is going, this guy is going to be a star soonoh god nuggetbothahaq: how do you build model x?a:  buy data robot and throw data at itgreat!  welcome to chief ds positionit\\'s the future!did not expect a follow up on this sagathis guy has been a sr ds manager at a well-known company for a couple years so he’s already a star!did you consider that if you don\\'t hire him, you guys won\\'t be as good at hiring women as you could be?definitely a considerationsure, he may not know logistic regression, but he brings women hiring to the table“sr ds manager at a well-know company” … makes me nervous!well, alex, did you interview recently and forget the details about logistic regression?i mean, what even is a logistic?! i think you throw data into it and it spits out a thing!i can shed some light into thatit does logistic things to the data before spitting out logistified outputspsh, i have so many logistics just laying around the office, we just shovel data into one of them and it spits stuff outlogistic oven is the best for neapolitan pizzai think it has to do with shipping and storing things.  whatever.there’s a quora for that: i argued with an interviewer when he said that a random forest was an interpretable model, and i can\\'t say i managed to stay polite. they did invite me for an on-site including an hour long talk. i seriously considered going and giving a talk about model interpretability, but in the end i decided it was too much work and declined the interview altogether.^i fully support you in that argument. also, it woulda been hilarious to give an hour talk on model interpretability!could you explain where you draw the line on “interpretability” and why random forest misses your definition?are you asking for the hour long lecture?i think we are :pworking on some content right now on model interpretability with respect to gdpr. a lot more gray area than black &amp; white, so i\\'m also in favor of this lecture.not using it for any specific task.  trying to learn as much as i can about it.  let the flooding begin. :slightly_smiling_face:i’m curious which company will be the first to get fined under gdpr, though it might be more fun to guess which company will accidentally drop table when trying to add the ability to remove all of a user’s data at their request…i have a fun announcement: if you\\'ve seen my jupyter notebook talk during the first week or so of insight, you know that i have a lot to say about the notebook. i\\'ve sat down and brain dumped an extra 4 hours of content into a one-day workshop held at stanford through their continuing studies program. jupyter notebook is only one piece of the workshop. lots of other tools and techniques will be addressed. hope to see you there!details here:  hey all, i\\'m on the fb people analytics team, and we\\'re considering various options for knowledge sharing. functionally speaking, we want/need: access control, toc, tags/topic partitioning, relatively simple set-up/admin, and a nice ui. could you please share what your company and/or team uses? i\\'ve implemented airbnb\\'s knowledge repo () locally, and i personally am a big fan - but i should consider other options as well. thanks a lot!thank you - i recently joined airbnb and the fact that the knowledge repo exists is amazing. it\\'s been so helpful in getting up to speedooh i\\'ll have to check that out too, already dabbled with superset i agree - i may be in touch if we end up using it here. there are some issues i\\'ve come across e.g. needing to reboot the server in order to see any new postsi’m curious about that tool - is it essentially a nice way to organize notebook-like outputs in a centralized location?  can answer this better perhaps, but yes exactly, \"centralizing\" content is one really nice feature of the knowledge repo. some other nice features are: tagging, toc, being able to view the raw markdown (e.g. so you can borrow other people\\'s code snippets)yeah it sounds pretty neat i’ll look more into itdefinitely, feel free to ping me about itgreat! i am going to use superset for my new project deepplay - i have to have a look at knowledge repo too.i don\\'t know much about the technical aspects, but i could put you in touch with people who do if you need it. - that pretty much sums it up. it makes it easy to find others\\' work, reproduce it, and use itif anyone is in the office can you yell at my scheduled people to pick up the phone. do you have a name ?noneok, i\\'ll just shout in general then.i\\'m on itplease embarrass the shit out of anyone who is scheduled.  :grinning:  hey, we\\'re supposed to meet at 5:30pm. i haven\\'t received any hangouts requests.call me. 7144250620or initiate on  \"google can change company data in one part of this database—running an ad, say, or debiting an advertiser’s account—without contradicting changes made on the other side of the planet. what’s more, it can readily and reliably replicate data across multiple data centers in multiple parts of the world—and seamlessly retrieve these copies if any one data center goes down. for a truly global business like google, such transcontinental consistency is enormously powerful.\" good morning - anyone ran into problems importing scrubadub in python3? getting the following error: attributeerror: \\'callable_iterator\\' object has no attribute \\'next\\'hello, has anyone ran into  java.lang.stringindexoutofboundsexception: string index out of range: -1 while trying to write to elasticsearch? (not a corrupt dataset problem) thanks :smiley:not sure if anyone else is working with scala but if you are i found a great book for getting your head around functional programming:also, i\\'ll share the writeup i\\'ve been slowly building about how to use spark dataframes in scala uploaded a file: it is probably not completely uselesshi everyone! i am one of the current ds fellows and i was wondering if anyone has aby experience with the amazon api ? specifically i am interested on how to get the reviews without applying for a merchant license.i think there are data dumps for reviews, which might be more helpful unless you need specific reviews. has used amazon data beforethere are!i know there is an old recommender dataset  that would be fantastic as well! you can download a large sample of previously scraped reviews here they come as json … i can also open up my aws db to you is you want to query reviews by asin thanks for the offer !  i think it could be a good experience understanding how to work with json if i hit too many road bumps then i would definitely take you up on your offer!ok, i’ll send you some code to help extracting the data from the json file  thanks! has also used amazon data beforei used this data source: has anyone exported a data frame to excel with hyperlinks?hi all.  i’m trying to use kubernetes to deploy a cluster on google container engine.  does anyone have any experience with that.  i’ve worked through much of the documentation and hello world examples, and am still having a hard time making all the connections and getting things to actually work.thanks!: have you tried  ?president obama was interviewed by google (2007) and he knew about bubble sort :simple_smile: hi fellows, anyone had experience working with flask, pickle, and heroku?  two questions. 1. i have a training model (~40mb)  need to load to make predictions per user request, except host on aws s3, any other way to host this train model file? 2. loading the model take around 6s, any other way i can improve the efficiency? for a production api, 6s is too much for me. you may want to try something like predictionio for automated model hosting. there\\'s some stuff here about using it w/ heroku i haven\\'t used myself but have heard good thingsthank you  use a hosted service or write your own separate model service.  the challenge with using flask to serve models is that requests are meant to be stateless.  that means that unless you do something really tricky you\\'ll have to load the model into memory for every request.  with a dedicated service you have more control about when and how to manage the model in memory. , any example of hosted service?  i am a little bit clueless. fyi, my current api is running, the working flow now is user send request, i load model, using model make prediction, return the prediction. and you are right, now i had to load model per user request. and  with flask you can load any variable as a global one and reuse it - no need to reload for each request.the view functions will be called per request, but other objects can be shared…i’m not sure why you were hosting it on s3 to begin with, why not host it on the local machine that flask is running from?: i think it\\'s more complicated than that.  see the first answer to this question for a really good explanation if one has more than one instance of the serving process running, mutable shared variables are not a good way to go. but having a static variable (the model doesn’t change, does it?) is fine. db connection objects, for example, are such shared objects for multi-process flask applications., thanks for the info. i will try the global variable way. my model does not change. you may want to look into gevent: the following explains how gevent can be used to import read-only data structures into global scope, which can be shared across all requests:good point  i think that might make the case for using spark or more of a programmatic approach. i think  has some xml files he wants to load into redshift and the easiest thing is to turn it into csv filesah! it only runs on stata. it is probably too much of an investment in time to get everything setup to run the scripts right now. but i completely agree with you   that with a few tweaks the approach is very exciting for gender prediction by name and place of residence. also, race prediction refers to the typical american ethnic backgrounds, while i am looking to a diverse international community. i am now looking into ethnea hi all, i am trying to find a clustering technique (in python) than can work with very large data set (about one million samples, 10-20 features), small 3-5 number of possibly uneven clusters. any help would be appreciated. i tried dbscan and aghromerative clustering, but i get memory problem, or they are too slow. i don\\'t know if that\\'s a technique problem. we\\'ve used those techniques and others in python on larger data sets just last week. what kind of machine are you running this on?windows pc with 32g ram, intel e5-2620 v4 2.1ghz 2 processors cpuwe\\'re doing it on machines with 64gb ram, but i would think 32gb would be enough with data of that size.is the data super sparse? categorical? you could try pca or downsampling before running the clustering and see if the results are sufficient.i am already using pca to reduce the number of features to half. i\\'ll try downsampling.  thank you alex. my problem is exactly as hoa mentioned. now we find that i can also try to transfer xml to json file and then load josn file to redshift. i am wondering if you know any good python package or spark package for this?is anyone using google cloud dataproc?thanks guys, will check those videos out.i\\'ve done the scatter plot picker thing in a jupyter notebook with plotly and it was very fast and easy!hello everyone - time for the seasonal insight discount to odsc! save 65% on an odsc east pass by registering with code insight. this discount applies to base prices, and is in addition to other promotions on their end. code expires february 23rd. hope to see you there! anyone have adhd or know someone with it working in tech? loved one is struggling with it and trying to find her a mentor/buddydoes anyone have any opinions on  ?or more succinctly the idea that soft skills are  important on teams rather than technical prowessi remember the wapo article.  i’d be curious to know the roles the non-stem background people have.i imagine a mix or technical + humanities is good for software engineers, and that tech companies have a strong need for non technical people that play a super important role in the company’s direction (*cough cough facebook cough cough election*), and can see humanities people being self-taught in code (like 75% of engineers are self-taught in the trade i once read), but the report feels a bit hard to judge w/o knowing what type of roles they were in.people use the word \"humanities\" but i\\'m not exactly clear what that means.  awareness of the situational context.  empathy?or in particular, what does humanities bring to the table? awareness of the cultural context?  and can these skills  be learned without formally studying humanitiesso when those articles were discussed on hackernews (take it with a grain of salt) one point brought up was selection effect: they have already selected people who pass google’s tech screen, which is a high bar, so given that you have that, then it sort of makes sense that also having “soft skills” is useful.that makes sensethat said, i’m biased towards the idea that diverse backgrounds makes sense for teams!i guess they don\\'t screen for soft skills as well then?or just a lower bar so more dynamic rangeprobably harder to measurealso true, consulting companies try to at leasttwitter is trying to, at least when i interviewed theresimilar to technical concepts, yes, but also similar to cs and software engineering, cs grads have a head start, in the same way humanities majors have formal training in this.we try to, but we don’t have a good wayyes, i think it has to do with cultural context. i also think formal training isn\\'t required, but it simplifies the process of gaining exposure to diverse viewpoints.i think it\\'s a little bit backwards, in that places tend to select first for technical skills and second for soft skills. i get why from a \"risk\" perspective, but really, of the two, i think technical skills can be picked up much faster.why do you think this? i taed lots of students who i\\'d never want writing code i\\'d have to maintain, hah.ever ta a complete jerk?you can do code reviews and learn along the way (until your code is accepted), but what is a ‘code review’ for people skills?yeah, much harder to change a personality than your code.i think about technical skills on a \"months\" scale and soft skills on a \"years\" scale.i think it also is an issue of what has been historically valued and therefore what has been measured. we\\'ve come up with metrics for what good code looks like because we think of there being a direct link between that and $; but there hasn\\'t been much incentive to create evaluation metrics for soft skills because the connection between them and $ hasn\\'t been clear. (which in turn might be an issue of where people are looking.......)i personally think that being intentionally kind to others is not really something that is taught in a curriculum in school, but definately a young person can be trained in it via  supportive relationships in their lives.*shrug* my thinking is we\\'re still conditioning on people who already go to top 20 schools and are probably pretty smart. if you\\'re asking how long it takes to teach some random nice person to code well enough to work at google i\\'d say that takes a while.in this case the engineering ability equivalent to \"complete jerk\" would be 60 year old mom who can\\'t use email.that’s well put alan!i also agree with mark somewhat; before teaching i thought “i can teach anyone calculus!“. having tried to do so… i conclude that i can not. :disappointed:you definitely want to screen for both tech and soft skills, but i would argue we, in general, probably put too much weight on the tech stuff. there needs to be some, but it’s not the only thing.yeah, there\\'s definitely an expectation around technical skill. and because it\\'s quickly learnable, there\\'s also an expectation that an engineer has put in the time to learn that stuff, and it\\'s easily measurable. the thing that amuses me is the idea that there\\'s probably some point of diminishing return on technical skill, where the effort is more effectively spent on improving soft skills. an unsolved problem!bwahahahathat actually looks pretty deliciousour newest blog post is up, this time covering reinforcement learning to play battleship. does anyone have suggestions/advice on a trustworthy moving company (or any to avoid)?  this is for a move from pennsylvania to florida.  did my relo, pretty satisfied especially since i wasn\\'t there to supervise. i think if you do the full-service and let them pack, the items are insured for any damages during transport.thanks so much ! i had a very good experience with moishe’s (). they were very competitive money-wise and did a great job with packing, travel, and keeping me updated along the way. they have a location in philly.i’ve used abf’s pods before. it’s just what it says on the tin. does anyone know if pickle will save the standardscaler as well? right now i\\'m doing pickle.dump(clf,  pkl_file), and then classf = pickle.load(pkl_file), test = classf.predict_proba(newdatafeature), do i need to redo standardization?is the standardscaler in the clf object to begin with? if not, then it\\'s not going to be there, if so then it will.if not, then you can still pickle it, but make the object a tuple and do something like:      storeme = (clf, scaler)    pickle.dump(storeme, pkl_file)    newclf, newscaler = pickle.load(pkl_file)thanks!hi all! if your team hires undergrads directly into data roles or has undergrad internships, could you message me or share a link to the internship application? i\\'m speaking a an undergrad women in physics conference next weekend and would love to share stories of opportunities at the undergrad level. thanks!and happy new year! :tada:thanks ! uploaded a file: i am trying to upload a powerpoint presentation online that retains all of its animation features, ( tried googledocs, ispring), any other suggestions?  (includes animations with movement paths)workaround: record  a screencast from your animation, upload it to youtube, upload presentation without animation to slideshare, insert youtube video between slides: reach out to  in the 2015b session — he found a way to do it with some animations he had. waiting to hear from you worked out for me. culturecatch up with progress in commercial ai -  shares info on deep learning in industry. anyone have experience with numenta and the hierarchical temporal memory framework? i\\'m impressed by the results i\\'m seeing from my poc but there are few details about the method. that\\'s a great uploada real great upload. i\\'ve talked to a lot of really smart people, and they\\'ve seen great upload, and they tell me that\\'s a real great upload uploaded a file:  and commented: does anyone have any good resources for (hopefully) a really simple problem. i have the shapefiles of the nyc taxi data and want to convert their given coordinates to gps latitude and longitude in decimal format. nyc tlc does not say anything about the initial format and i\\'m having trouble tracking it down. i\\'d like to do this in python, avoiding postgis or anything unnecessarily complicated. please pm me. thanks!has anyone managed to plot roc curves for multiclass situations where the features were text data?anybody have advice for interviewing a candidate who is to be your supervisor?ask if the person likes to give raises?ask if they’ve had any correspondence with russian ambassadors?or more to the point, ask them about something related, and see if the spontaneously deny that they\\'ve been in touch with any russian ambassadors... commented on ’s file : this looks like it might be ny state-plane coordinates, but i\\'m not sure.  not sure if it\\'s helpful, but you can reproject using pyproj. commented on ’s file : thanks so much! i\\'ll look into it. do you know if these things are typically a linear projection, or is that an unsafe assumption?i don\\'t have a good answer, but probably more \"strategy\" level stuff: what do you think is the most important thing for the data team to accomplish in the next 6 months? how should the team measure if they succeed in doing so?definitely ask about their management stylethanks for the replies. i thought of those kinds of questions as well. it\\'s supposed to be a technical interview though. :/reverse a linked list :+1:oh, technical! good luck!interesting. i wonder if it would be useful for you to pose a real-world problem you genuinely don’t know a technical solution to. if they’re supposed to be your advisor, hopefully they can advise you. think of a typical problem you normally encounter at work and see how effortlessly the interviewee solves it.. supervisors need to be able to trouble shoot situations as well as the people he/she oversees. if the supervisor-to-be has problems solving something that comes up normally, it might be a red-flag. :confused:here’s a relevant hbr article about exactly this: that might also help gauge how well (s)he will be able to judge the quality of _your_ work.  i would recommend plotly instead for interactive plots. its python interface is very convenient and can be used offline. can plotly take user input and replot things on the fly? yes. you can find examples of this from my insight project (offline now... but the source code  is still good). in particular, \"limelight/app/templates/parameter_importance.html\" has an example of  a 10-line javascript for a dropdown to get data from flask and update plot. thanks a bunch i\\'ll take a look!forgive me for using , but i believe this is an issue everyone should be aware, and hopefully already is. president trump signed an executive order banning entry for nationals from seven countries, including people with green cards and dual nationals. many of our fellow fellows are affected by this order. they can no longer enter the country for at least 120 days. this means if they leave the country, for any kind of reason, they cannot come back. here is some more information: please reach out to anyone you know who has been affected by this, and consider donating to the aclu or other organizations.political rant over.there is a protest happening at sfo this afternoon &amp; evening. i’m headed there now! write your senators and congressmen, too! have a look at matplotlibd3 as wellyou know it\\'s just going to get worse from here, right? these 7 countries are just the beginning. soon enough it\\'s going to cover all muslim countries, or worsecall those elected officials too! both local and d.c. offices this amounts to religious discrimination.   aclu has already sued.   best guide to political action from former washington staffers:   nuggetbot has no chill with how politically driven the supreme court is at time, i wouldn’t hold my breath unfortunately.hi everyone! i really appreciate if you would please call/write your congressmen or senators to stop this unamerican and unfair eo. the visa status of thousands of people currently on student visas, opt, work visas, and even those who have green cards depends on this! i’m one of them..: it ties up their resources to defend it.     is about how to influence your members of congress.  fortunately california is generally leading the way but feinstein, harris and eshoo need to know we are behind them.   senator feinstein:  senator harris:  representative eshoo:   just to piggy back off what  said: one of the guys who wrote the indivisible guide said congressional offices love it when their constituents call them to thank them.also call your state representatives as well!if voicemail is full and it frequently is you can send messages on their websites:  kamala harris diane feinstein:  anna eshoo:  : good luck to everyone from muslim countriesthank you!my friend and president of sbu’s grad student organization is being detained at jfk wow that\\'s shittyaclu just tweeted an emergency stay was granted i\\'m assuming in the immigration banremember to stay vigilant! keep track of whether the government actually honors the ruling.they still haven\\'t been allowed to enterjudge blocks part of trump’s immigration order  thanks for raising awareness about this issue. this is directly affecting 4 fellows at the current nyc insight data science program.everyone gets embeddings! an insight ai fellow wrote a post about how to embed any sort of data. let us know what you think:well, on the plus side you have to log out of your computer firsthas anyone played with facebook\\'s prophet library much? looking for any thoughts around how much data cleaning / quality checks / model validation visualizations come out-of-the-box vs extra coding to implement them welli only explored it briefly, but decent evaluation of seasonal components out-of-box plus well-reasoned prediction visualization. for data preprocessing, nothing that i saw (and fbprophet was a little fussy with the input dataframe).  has more experience with it than me.i think he means you show the create table on each table and you add comments about what each column means to people.basically yes. for example if the code that generates your table is in github, point people to that file as the data dictionary  ah got it. thanks!ask them how to measure something related to your company that\\'s hard to measure.hello everyone! does anyone have experience performing data integration with cloudera?i tend to like open ended questions that force candidates to stretch their thinking. i kind of don\\'t care how many python libraries they can bend to their will at this moment--that\\'s totally googleable. it\\'s way more important that they are creative and curious. our code challenge questions tend to be things like, \"tell us everything you can about what\\'s going on in this video. you have two days.\" a scaled down version of something like that is  usually what i shoot for in an interview. i really like the \"how to measure something...\" question.sure, going to go down the measure sectionthanksnobody let me do this, but i wanted to have our code challenge be, \"order these images of cows by descending cuteness. explain your cuteness metric and show how it is calculated.\"hahathat is awesome . i’m definitely going to use that it\\'s near the end of our fiscal year and we have leftover budget for tech, training, conferences, etc. who has suggestions on the conference to go to, the new tool to purchase, the course to attend, etc?are you an insight data engineering alumni who will be in the bay area/palo alto next week? insight is holding a reunion gathering for all de alumni at its palo alto office. let me or someone at insight know if you are interested in attending.‘what is the probability of a platypus?’jsm? it isn’t until july but you could pre-pay the registration/hotel/flights. bonus: it’s in vancouver this year!would there be one for ds alumni? i’m in palo alto, but did remote session, so i am not part of sv alumns (yet?).does anyone have an opinion on what\\'s the best linux-like command line install on windows, i\\'m looking at babun, has anyone used it? like? don\\'t like?windows 10 actually has bash. have you tried that?does it already? i thought it was yet to come.. havent tried it yet, im running windows 7you could install cygwin, but i was never a fan of the earlier versions. maybe it\\'s better?in my experience, it’s not.or just learn to live with powershellgasp!babun is built on cygwin but it\\'s a customized version with a lot of the useful tools pre packaged, &amp; with decent online reviews. i\\'ll try it for now..good luckcash looks interestinghope you like javascript!good gravy… i’d rather dual boot.hehor better yet, single boot.. in linux.yea me too, but not my call.. i feel your pain. i think i\\'ll be running windows too when i startmaybe i can run linux, and virtualize windows for an occasional appi would have a virtual machine running ubuntu within windows and that worked pretty well for me...i was using cygwin at work.  i would have preferred to virtualize linux but some places will lock you out from using hardware acceleration to virtualize.  if you find yourself in a restrictive environment, it\\'s probably best to ask the person sitting next to you what they\\'ve triedthanks. my co-workers have varying solutions, one uses git bash, which seems like a more limited alternative to cygwin or babun.. i\\'ll look into vm\\'s as well, it might be an optionbash for non preview versions of windows 10 will come in august. though this may interest many of you: , data challenge incoming?  group 4 is just here staring at each other :slightly_smiling_face:got it thanks!wrong team… sorry for the noise, anyone reading this…curious what you guys are doingfirst data challenge of insight!  i can forward you the email if you’d like?the answer is 4.haha, and they’re giving us 5 hours to figure that out! by all means!: are you going to get into it?found this on product hunt today, a way to “generate realistic data for your app”how progressive uses machine learning:   nuggetbot check on kenny_kwankenny_kwan has a happiness average of 0.5287356321839081that sounds about right :neutral_face:nuggetbot check on romanoromano has a happiness average of -0.36what?nuggetbot what?stupid nuggetbotnuggetbot check on mhoovermhoover has a happiness average of 1.8148148148148149: nice!nuggetbot check on kumbhanikumbhani has a happiness average of 0.8666666666666667so irrational:stuck_out_tongue: nuggetbot check on lenkatilkalenkatilka has no happiness average yethahaha nuggetbot check on pushkarinipushkarini has a happiness average of 1.1578947368421053all because of your support : i guess i am not slacking enough :smile: (pun intended!)thanks for all the previous help, but i have a new issue. my website has an embedded folium map which is rendered from an html. i see that after about five minutes that my app starts to reload old maps instead of rendering new maps, as if somehow the html is being cached. this is absolutely not something that i have implemented (and it is not something that happens on my local machine). the application is running on port 80 using gunicorn with supervisord on aws. i see that if i stop supervisord and kill all gunicorns and then restart it then it fixes this issue, but stopping and starting the application every five minutes is probably not viable. any suggestions?:  not knowing much about your app, my first line of debugging would be to explore the network tab on your browser’s developer tools. you can see the parameters that are passed with each request, as well as what is returned. see if anything looks different between your aws and local versions.no issue with the parameters that are passed. the address bar looks fine. it just loads the wrong map (seemingly at random from whatever was entered in the first 5-10 minutes that supervisor was running)also in the first 5-10 minutes it runs finereturns the right address bar parameters and the correct mapwhat i’d be most interested in are the http status codes of the responses (i.e. if you’re getting 304 wrongly). that’s the sort of thing that could cause weird caching bugs, and which might differ from one server to another.though on second thought, it’s probably getter to look at logs on the server side first. i just don’t remember how gunicorn logging works.i just refreshed ityou can play with itstay positive mamdayi’m taking a look…if nothing else succeeds, you can always set up a cron job to restart gunicorn every 5 minutesyeah, but having the web site constantly restarting and potentially throwing errors all the time could also be bad. better than having it return nonsense, but still not great.ah, for some reason i was thinking your app was refreshing the map via ajax. now that i see it isn’t, the network panel is certainly not going to be too enlightening.yeah. it\\'s foliumwell, now i just get \"this site can’t be reached.” i guess i broke it.hmmah. i think maybe that was the cron job. thought i turned that off...clearly still debugging that option...odd. it seems to cycle through stale results when i refresh.to me it seems like something is caching the map (for which the name does not change. it is always osm.html)i thought about assigning a large random number as part of the name of the map to see if it might fix itstring interpolate the datetime to the second?so i’m not that familiar with folium, but looking at the html source, i notice that each map has a unique identifier (something like map_d15934a15f6445b6b30a50e24bc94e20), but when you get the stale map, you’re also getting a stale identifier.yeah. and when the user clicks, it produces the correct map in the directory where i expect on the remote instance and it has the correct identifierbut the page does not load itand the map it is loading does not exist on the remote instancei feel there has to be some other storage that either supervisor, gunicorn or aws usessoumyadsanyal: i think i do not understand your advice?i still don’t really understand folium, but i would be inclined to vary the name of the map depending on the parameters, i.e. instead of  `map.save(‘osm.html’)` something like `map.save(‘osm-[hash]’)`, where `[hash]` depends on the address, the date, and the cost entered by the user.yeah. i think you are probably right. thanks :slightly_smiling_face:anyone know why a column declared as \\'datetime\\' aws redshift is displayed as datetime structure instead of a string ? for ex, (datetime.datetime(2016, 6, 30, 0, 19, 3), 94, 1541, 12382, 6770, 9) instead of displaying time as \"2016-6-30 0:19:3\"found a sweet little book on becoming a quant in the hackernews comments. the brain teasers are solid. :thumbsup:that is a good point. no one here is going to have preferred stock.side-note: i didn’t know sfers called it “the city”. has that always been the case or is that as a result of lots of east-coast transplants? it was already the case 15 years ago when i first moved there (and it annoyed me then)…my guess is it\\'s been that way for a while.good to know :wink:in an effort to improve data flow across the company, we are trying to integrate our testing procedures during manufacture with our general inventory tracking. does anyone have any suggestions of off the shelf software that would work for this? have you build your own systems? at this point anything is better than the scattered spreadsheets we now use, but i’d like to be systematic about it.hi all!a friend of mine at a security analytics startup () is looking for a senior data scientist. you would be their second data science hire, and would be involved in growing the team and building the heart of their product from the ground up. i consulted for this company for my insight project. they’re tackling some very interesting challenges and have a great team.link to job posting: if you’re interested, please apply and let me know! i’ll help to make sure your application gets noticed. i’m sure there are a lot of enterprise-level (i.e. expensive) solutions to this problem, but i’d think it breaks down more to the data input.a hack i’ve seen used several times is standardizing a google sheet as input and pulling in data from there (there are python libraries for that). but you could also build custom apps for data input that output csv or json or something else that can be more easily shoved into a data warehouse (i.e. redshift) thanks. that’s what we’re leaning towards for the testing. the inventory tracking is a pain, though!totally random question, but is there a chem or biochem person out there that could help me with a quick question? if so, pm me. thanks!many thanks to the awesomely talented  for help. :clap::+1: i know some biochem and if your question is too advanced i can also refer you to some chemiststhanks  -- i\\'m all set at this point, but appreciate the shout.any recovering biochemists want to do some enzyme kinetics assays just for old time\\'s sake? makes some great diagrams in this post on time-series modeling. good read, too. anybody going to database camp (free whole day db event) in the un on july 10? database.campthis weekend in san jose: hi insight alumni,i’m looking for a panelist for the data science panel at the beyond academia conference at uc berkeley.the conference attendees are all graduate students interested in transitioning to non-academic careers. your job would be to speak about your experience transitioning into your career. joel swenson, the insight health data science lead at the san francisco office will help moderate the conversation with five other data scientists.please send me a pm on slack if you’re interested in being a panelist. happy to answer any questions. thanks!more info: when: march 2nd, 2018where: uc berkeley campustime: 1:30 - 3:00pm with short reception to followdear insight alums,i\\'m currently in the jan/feb remote program, and i\\'m interested in connecting with people who have gone the leadership/management/product manager route. if you, or if you know other phds who have ended in up in that type of position, please let me know, i\\'d really love a chance to connect and hear about your experiences going that direction! thanks!!hey ml folks, i have a terminology question. say i have a logistic regression model `f: x -&gt; [0,1]` and a sample of labelled instances `{(x_i in x, y_i in {0,1})}`. so the number of positives in the sample is the sum of the `y_i`, whereas the expected number of positives in the sample according to the model is the sum of the `f(x_i)`. what do you call the ratio `sum(f(x_i))/sum(y_i)`? (does it have a standard name?)sounds like it’s the inverse of ppvor inverse of precision, equivalentlyi would call it the inverse of observed over expected?or are you leaving the outputs of `f(x_i)` as the continuous value while `y_i` are left as binary?yeah, i’m leaving the `f(x_i)` as is — not assigning binary labels ooh, ok scratch what i’d said — inverse ppv would be if `f(x_i)` is then mapped to a binary. i would too, i was just hoping there was a standard shorter term for it :slightly_smiling_face:yet another job alert (for senior candidates): a friend of mine is looking to hire a phd data scientist with at least a couple years of industry experience at the intersection of publishing and healthcare with experience in nlp and sufficient experience, and interest, in leading a team (i.e., developing a vision for the team, providing guidance to more junior team members). based in either new york city, philadelphia, or even amsterdam (nl).i cannot disclose the company (rulz!, but you can guess based on area + location), i think they are doing really interesting work and are, as a company, pivoting towards a \"platform company\" (distribution) driven by machine learning algorithms (e.g., recommendation systems, lots and lots of nlp work). pm me if interested! i would probably call it accuracy ( # correctly classified instances / # relevant instances, e.g.,  1s ). however, i would not use this measure since due to the fitting of the logistic regression, one would have the sum of f(x_i) to be as close as possible to sum(y_i), without regard to how well the classification performed.i don\\'t think that\\'s quite right, if i\\'m reading the parentheses correctly -- it\\'s the ratio of the number of predicted positives to the number of actual positives, without regard to whether they were correctly predicted or not.y = [1,1,1,0,0,0], f(x) = [0,0,0,1,1,1], that metric does poorly on this, no?# correctly classified instances = tp + tn# relevant instances = pf(x_i) is a probability, so this is before the threshold is applied. so one can’t really talk about “predicted positives”, i think.^ right. i’m thinking of `sum(f(x_i))` as the “expected number” of positives (in the sense of a probability distribution) rather than the number of “expected positives”and yeah there are definitely cases where this metric could be misleadinglooking at the example given in the wikipedia page (), one sees that `sum(y_i)=10`, while `sum(f(x_i))=9.9997`as you can see, this metric will give a ratio very close to 1 because the model is fitted to do that.i think in practice, `sum(f(x_i))` and `sum(y_i)` will be very close to one another on the training set. this is because the intercept term will adjust so that the base probability of the classifier will match the base probability in the training data. on the test set, i suppose the two could diverge, but i think there are other metrics that would be much better.echoing , here’s a simple sklearn example:```from sklearn import datasetsfrom sklearn.linear_model import logisticregression# create datax, y = datasets.make_classification(n_samples=10000, n_features=20,                                    n_informative=2, n_redundant=10,                                    random_state=42, weights=[0.25, 0.75])# fit on entire datasetclf = logisticregression(c=1)clf.fit(x, y)# metricsprint(‘sum of probabilities’, clf.predict_proba(x)[:, 1].sum())print(‘sum of observations’, y.sum())```i get very similar results regardless of the weights (the base probability)also perhaps worth noting that as `c`goes down (the regularization strength increases), the two actually do divergedown or up? i’m probably confused`c` is the inverse of the regularization strength. from the docs ```inverse of regularization strength; must be a positive float. like in support vector machines, smaller values specify stronger regularization.```so it’s `loss function + (param vector norm)/c` ?no c multiplies the loss functionoh i see confused by how to interpret \\'strength\\'has anyone run a jupyter notebook on google compute engine?anyone familiar with some rules for choosing #bin /width for visualizing big data? looking at freedman diaconis i\\'ve liked:  for bin sizesthanks!here\\'s an excellent blog post detailing it (and giving a nice example of dynamic programming as well): i\\'m an \"alumna\" now! whoo!🤣hello everyone! my team at athenahealth in boston is hiring for a new senior product analytics position! if you\\'re interested in healthcare, want to work for a fun company, and do a wide range of analytics, please dm me for more info!thank you everyone for the awesome advice! :slightly_smiling_face:hello everyone,my name is hyesoo and i\\'m a recent insight health fellow who received a phd from uc berkeley. at uc berkeley, i also worked as one of the organizers/officers at gdso (graduate data science organization) and they are currently looking for data scientists who would volunteer for mentoring in the data science workshop this summer (previously known as cdips workshop).it is a great chance to meet other data scientists, and also help the future data scientists gaining more experience. you can contact gdso () if you are interested in mentoring the phd / postdocs who are interested in data science.below is the email they sent to the previous mentors:the graduate data science organization (gdso) at uc berkeley has taken over the planning of the annual data science workshop, previously organized by cdips. we\\'re preparing for the sixth annual workshop, and we are seeking mentors. we\\'re reaching out to you because you served as a mentor for last year\\'s workshop and hope that you may be interested in serving as a mentor once again.as a reminder, the workshop is a three-week opportunity for berkeley graduate students and postdocs to work on data science projects under the guidance of a working data scientist. mentors can take on varying levels of commitment. at a minimum, mentors should check in with their team twice per week (either virtually or face-to-face). the only fixed commitment on the part of the mentors is attendance at the opening and closing ceremonies, which will be held on july 21 and august 11 at uc berkeley.if you\\'d like to serve as a mentor, please contact the workshop organizers at . more information about the workshop and the gdso can be found at .best,the gdso data science workshop organizershas anyone had problems using spark-submit w/ the --packages flag on emr? i expected it to \"just work\". it downloads the jars correctly, but they don\\'t seem to be on my classpathone thing where i’ve seen rnns beat cnn/bow is sentiment analysis, because syntactic order is so important to understand the sentiment polarity of a sentence.does anyone else out there work for a b2b data-as-a-service company? i’d love to connect and hear an outside perspective on the tradeoffs between model accuracy and data continuity, and how other companies have dealt with it.that’s awesome, thanks !has anyone systematically gone through and downloaded bureau of labor statistics data by metro area?glad i could helpglad i could help.respiratory issues tend to have low numbers so you either have to sacrifice geographical precision or temporal precision to get any significant values. in my case i looked at the yearly numbers since i was only interested in the long-term effects within a geographical location.a good idea could be to study the opioid crisis. there has been some projects around this subject that you could use as inspiration.“all of this advice can be implemented starting now, so that your company doesn’t miss out on the many benefits that come with a diverse environment where everyone — from underrepresented minorities to introverts to parents — feels truly included.”hear hear! i turned it off ages ago, taking control of my own happiness! :wink:hi all, does anyone here have any experience implementing teacher forcing for lstm in pytorch?hi , i have. what’s up?yeah i saw these thingsits some real passive-aggressive bullshitstay positive mpanciathey did it and then they are apologizing for it. kinda seems stupidstay positive ashrithnuggetbot calmdownevery dollar they spend on their bs ad campaigns is a dollar they could be donating to public housing projects or somethingyep, agreed.for all those who got jipped by coin the first time. here is some good -&gt; thanks are in order?talking about jipped. for all those who never got misfit shine/flash to work -&gt; i would have loved to sit in on the design meetings for those airbnb ads: \"we should put up some ads to make people aware of what we contribute back to the city!\" \"great idea, and they should be passive aggressive, sf hipsters love that!\" \"perfect! you really get how the kids think these days!\" :wink:dear airbnb, welcome to society.love,societyis anyone else going to the mit sloan sports analytics conference in boston this weekend?  i’m going and would love to meet up with any other insight fellows there.how buffer() evolved their pipeline: a blog post about predicting caltrain wait-times that i\\'m working on at svds : so cool, though i am partially biased because my insight project was about trying to predict delays (really ‘bunching’ as everybody in my session has heard 1000 times) in the muni systemthe point on sensor placement near burlingame is interesting; at first i thought it had to do with the bridge replacement in that area...`bunching`bunching as a servicebaaswe need not-bunching as a service:  was that attempt (muni only): yan will be there!does anyone have recommendations for good time series forecasting resources (i.e., books or presentations)?: time_series_analysis__with_applications_in_r-springer 2008 free pdf download.  i\\'m working through it now.other free pdf downloads:  time_series_analysis_and_its_applications_with_r_examplestime_series_analysis_with_rthanks don\\'t mention it.  :simple_smile:free one day training workshop on neo4j and graph databases:  i never knew multithreading and multiprocessing in python could be so easy:  now you knowif anyone has an opinion about fisa see if you can contribute : are you thinking of going to the geoweb summit?  if we get one more, tix are even cheaper :slightly_smiling_face:: sadly, i won’t be able to go this afternoon. but i understand why you’re interested. :wink::  perhaps you’d like to go with  ?: looks great but it’s a bit too short notice for me. enjoy!has anyone had weird pip installation issues on a mac where packages are installed in /library/python/2.7 instead of /usr/local/bin ? how can i fix this? thanks!i\\'m wondering if it\\'s related to homebrew path issues but i\\'m not sure: i haven\\'t come across this problem, but i\\'d recommend the anaconda distribution -- it plays nice w/ homebrew in my experience: you might want to check your ```$pythonpath``` and this might be one of the ways you could install things in local ```pip install —target=/usr/local/bin package_name```: just to add to the chorus, it sounds like you didn’t install python via homebrew (or your homebrew installation is busted, likely with a missing path change)even with another package manager like anaconda, you could run into similar issues, so it’s always good to check that you’re using the correct python/pip that you intend to. ``` ls -lh `which python` ``` and ``` ls -lh `which pip` ``` is a good diagnostic to check your setup is correcthad a bit of fun with tensor flow trying to machine learn what neuroscience sounds like. long story short, here\\'s a dnn neuroscience title generator :smile: thanks everyone! i will look into all these options! i\\'m pretty sure i had no clue what i was doing when i installed python/homebrew: \"amplingle-dependent define sexually behavior.\": \"experience of sexually bindyrin intraoxytics modulation of the action enhances taste inflammation.\"another favorite: \"motility and implicit information enhances orbitofrontal ischemic sex.\"could press refresh on that site all day. \"conditional nucleus in the cat/choice.\" is my favorite so farnicely done!does any1 know max download speed for ec2 downloading stuff from s3 in the same region?    only has upload :xit depends on your instance type and the networking mode enableddefinitely interested, thanks for adding me. work downtown. commute to bki work in union square and live in downtown brooklynawesome! i’m chasing several issues at work today so will be some latency in responding... but what do you think of meeting at insight for monthly meetings? seems like it would split the difference between downtown  / midtown and have reasonable office space to host us.if any other ideas (or office space you know of that we can borrow) please share!so the channel name is ml-meetup-cafeel free to joinwho’s going to be at mlconf tomorrow?just yesterday my boss recommended me a tiny book by paul d. allison ‘missing data’ (with some pessimistic view on the field in general :smiley: )must admit i haven’t started reading it yethi all, i was recently laid off at time inc. if any of your organizations are hiring data scientists in the greater ny-nj (or otherwise) area please ping me. i appreciate it.hey everyone, we’re excited to announce the launch of our new website! check it out!   hi fellows! i would like to do time series modeling using many variables (features),  anyone knows about good tutorial/book or links to do that using python?thanks: you can change the weights of the edges or remove them as neededthanks everyone, i think the insight collective intelligence has lead me to a method for approaching this.anyone have experience using tpot to automate model selection and hyperparameter optimization? i’m just curious about how well it works., you may have your answer already, but 2 people in my session also did route finding with constraints.  or  might be able to point you to useful tools: this is not helpful, however your problem did remind me about microsoft’s “avoid ghetto” patent () that drummed up some news a while back. damn it, i guess i\\'m headed back to the drawing board.stay positive ddelosshi everyone! i am trying to combine 3 databases on mysql to be one database combines all the tables from the 3 databases, anyone knows how to do that?: you can export 2 of them with mysqldump and then use those files to create the tables and data in the third database: can you simply run: `create table newdb.table1 like olddb.table1; insert into newdb.table1 select * from olddb.table1;`? this is a bit faster than `mysqldump`, but slower than `load data local infile`. this also assumes that both databases are on the same server.if they aren’t, i would certainly try to use `load data local infile` for speed.thanks for getting back to me  and . another simple question, how to access  3 databases at the same time? because what i do, is use database ;  then i access the tablewhen i do that, i can’t access the other tables from the other databases: `use database` should not prevent you from reading/writing to other databases on the same server. you just have to specify db-name.tbl-name when accessing tables in a different database.ok.. that make sense :slightly_smiling_face:thanks  !hi all, i’m working with 3d accelerometer data to categorize types of motions from a wearable. i’ve got a lot of avenues i’m considering, but i don’t know which ones are worthwhile. does anyone have experience with this sort of problem? thanks!: that sounds a lot like ’s project from the january 2016 session.cool! i’ll look into it. thanks  !: here is her blog post about the project: oh this is perfect! \"engineering features from accelerometer data\"would anyone have any knowledge on how to go about funding a non-profit project ? would love to buy you coffee and have a chat :slightly_smiling_face:numpy/scipy tips for matlab users cheatsheet: insight health data science - boston 2015 session: just saw a billboard from google:\"now needs data scientistsnext just needs you\"uh oh...incoming jobs where we fix all the mistakes people make by using bad data.e.g. a company that goes and corrects everything on 538wow. i might actually find a use for a windows machine. :simple_smile:hrmshi there! does anyone have a good practical tutorial of using hive and pyspark? i will have an interview next week including working with a data set using hive and pyspark, which i’ve never used before. thank you!i don\\'t know of a good tutorial (tell me if you find one) but w/ spark stuff, i always start with the examples dir they have on their github.if you haven\\'t used pyspark much, this might be helpful. this pretty accurately summarizes me trying to refactor code when i haven’t thought it through….repartition(10000)i highly recommend checking out the community edition from databricks.  its free and they let you work with spark on a baby cluster they launch for you and there are tons of tutorials built in. was useful for me when i was learning pyspark ()someone cracked the code: oh no, i am getting 404 error when i click! :disappointed:that didn\\'t last long... here\\'s a google cache: stakeholders is my favorite! :octagonal_sign:thank you!   user research: we figured out a way to finally get paid for our ph.ds.does anyone have any recommendations for great data science/machine learning conferences?kdd, nips?odsc, pydata, strata data conferenceanyone have experience with using dbscan for outlier detection? i\\'m digging the netflix tech blog post from july 2015 but would love to hear about any real-world application    ^i think   worked with dbscanyes!! thanks   :smiley_cat: are you interested?late to the party, but this link is awesome for a/b testing: is anyone familiar with d3.js? i have a working scatter plot, and am literally trying to add a line using canvas, but it frustratingly doesn\\'t display.for my insight project, i\\'m using hdbscan (a hierarchical dbscan) to do outlier detection on medicare insurance claims -- www.fraudhacker.sitealso happy to answer any questions about either my specific application or the clustering method!does anyone use airflow for “applying” their ml models? i am structuring the process as load data, cleaning/transformation, loading trained model, predicting, outputting in a single .py file. i am wondering if there is a way to within airflow assign each chunk of the code as a task, without having to break each of these processes into it’s own individual .py file. not totally sure i understand, but yes, you should have different files to process different tasksis the discount restricted only to former insight fellows, or is anyone welcome to use the discount code?anyone can use it. please feel free to share with anyone in your network.thanks!does anyone have a recommendation on a good interactive vi tutorial?theano will no longer be maintained after the next yearyeah, a little sad`vimtutor` is probably the best interactive way to start with vim. if you want a short intro this should work fine ().otherwise, practice is the keymxnet!thanks!for all you fellows starting interviews soon:does anyone know of a way to automatically reply to texts with an “away on vacation” type message on iphones? there doesn’t seem to be any option/third party app for this!#insightproject :wink:omg yeah- i guess a data engineering thing. apparently apple doesn’t want to let any third party apps do this for security reasons. wankershey all! i have a marketing funnel i\\'m working with right now to determine consumer purchase intent (classification problem). are there suggestions for models for predicting classes that are in hierarchical structures? thanks!just discovered the most useful function in matplotlib: anyone have experience with unsupervised learning on variable length input?this article is a nice encapsulation of what insight drummed into me when they said projects couldn\\'t be \"data exploration\". the simons institute for the theory of computing is hosting a series of workshops next year on “foundations of data science.” if you are interested, i recommend signing up for the mailing list to stay updated. hi all - has anyone here ever tried using kalman filter for missing data?can someone write a simple slack scraper or bot that has access to  and stores a message history to a database or external webpage or something? i hate when i see useful conversation get deleted over time.usually people ahve to pay for that. but you could probably make one pretty easily with crapyer scrapy: i\\'m trying to mess with the slack api but it\\'s currently \"disabled by admin\" for the insight slack. is that something that\\'s free to enable?cool idea!need to sell movie tickets? maybe you should round your ratings in a really bad way! stay positive alex_gudedoes anyone know of a good implementation or blogpost of “attention” neural networks in keras?have you seen this one? have you seen the \"counting your customers\" paper or pareto /nbd models? there\\'s also a package in python called lifelines that follows that methodology to create a probability that a user is alive given a record of transactions/events over timefb data analysis on the dress hey everyone,i had a great conversation with an excellent nyu physics undergrad, chris ick, who has been doing some interesting stats and ml with prof. david hogg. chris describes himself thusly:```data science and astrophysics researcher at nyu. recently graduated with a bachelors in science in physics, with extensive programming and technical problem solving background. research interests include computational data analysis, data visualization, and bayesian analysis. currently seeking work and education in data-oriented programs and positions, and furthering my understanding of data science and it\\'s methodologies.```i felt like i couldn’t give him the most relevant advice on the current data-science job market and how best to set himself up for a career going forward. i have been off the market myself and we haven’t been hiring data folks here at greenhouse (yet) so i don’t have a sense of the landscape. would any of you perhaps be willing to follow up with chris ()? pm me for his cv if you’d like.hey  sounds like he could be a good fit for the insight data engineering program. we recently had someone with a very similar profile go through the program and is now working at bcg. has he graduated yet? that’d be the only caveat otherwise, i think he might consider applying for the de program.yep, he graduated last may, . would you mind reaching out to him about de?hi all, my team in google is hiring for a data scientist in mtv. dm me if anyone is interested in the role. hi all,  i am 3 weeks into my career as a machine learning engineer (aka data scientist) and have to put together a wish list for conferences i would like to attend in 2018.  can anyone suggest which conferences i should shoot for?  there is a finite amount of funds, so proximity to nyc is a plus.hi all. can anyone recommend a toolset for analyzing large datasets? so far i\\'ve been using pandas + scikit-learn for all of my ds work, but i recently started analyzing a &gt;4gb dataset that is resulting in \"out of memory\" errors. i was considering giving sparkml on a cluster a try, but i\\'m curious to hear your thoughts and experience. could try dask as a drop-in replacement for the pandas componentsdepending on exactly how big your dataset is, you might be able to get away with spinning up an aws ec2 or google compute engine instance with more memory, and working with your usual toolkit therethanks, will take a look at that!that also sounds like a good idea!yep -- would recommend bigger box, same tools as an intermediate stepit might not get you too far, but pandas has features like sparse data frames that are more memory efficient, too. depending on your dataset you might be able to just use a little workaround like thathi everyone -- current fellow doing nlp on a large dataset (200k entries) with a small subset (1k) of multi-class, multi-labeled ‘ground truth’ annotations. has anyone done semi-supervised learning on multilabel data? finding a fair amount of research articles but not a lot of python implementations.some great tips here tl;dr: downcast numeric types to save memory, avoid string-type columnsi haven’t used it for text data, but scikit-learn has some semi-supervised methods. you could pass your text data through `countvectorizer` or tf-idf as input into one of these models?i’m a fellow in this session, and i used tfidf as input for a naive bayes classifier to predict sentence classes (dog, shelter, neither).thanks -- i\\'ve been using countvectorizer to construct the feature matrix but the specific issue i\\'m having is that my classes are multilabel and thus don\\'t fit into a standard column vector for \\'y\\'scikit-multilearn has some implementations for supervised learning on multilabel data but i\\'m not seeing semi-supervised approaches in that packageoh, i misunderstood. for that i’ve never tried semi-supervised methods. i’ve only ever done what has multi-label support or neural networksyeah, i\\'ve investigated some supervised classification algorithms with multilabel support but training on 1k samples and extrapolating to 200k seems like i\\'m leaving some information behind. i may try an iterative classification approachdoes anyone have any advice (or things to avoid) for choosing a data warehouse?i also created a resource dealing with nlp classification in general that could be helpful: advice 1: don\\'t tell the engineers they have to build a data warehousethanks, i\\'ll take a lookhi all. currently for my project i\\'m trying to merge aggregate level data (percentile numbers per occupation/county) with lots of missing with individual level data (actual values per person, with occupation and lat/long) that is very sparse on the county/occupation level.  trying to form a complete pictures with very \\'holey\\' data so to speak. i\\'m doing so with imputation and then training a random forest, does anyone have an idea for an approach (that would be reasonable to implement within the timeframe).hi all! anyone have experience using plotly or folium for bubble maps? do you recommend using one package over the other? or some other package? relatedly, i am using geopy to calculate lat and long for us and international cities and it is very slow. i can run up to about 200 rows, when it is larger it times out before finishing. any insight into why it is so slow? thanks in advance for any input!: i’m working on a multilabel classification problem as well. what i ended up doing is using `multilabelbinarizer()` for transforming labels to binary values and then used excellent  ’s resource.sparkbeyond is hiring data scientists. i think they have remote positions for their singapore office. some thoughts for interviews: : we use tableau at glassdoor, moving all our internal reporting there. i like it sometimes, hate it other times.with regard to connecting to data in many forms i find it works very well.  it\\'s also easy to make some nice visualizations, especially if it is simple and you don\\'t have a specific vision. but some specific things require too much hackery. aspects i don\\'t like which are mostly relevant to large projects:1) no way to document the data2) no solid revision history like you have with scripts in revision control -- i\\'ve made changes that broke sheets that i wasn\\'t looking at, didn\\'t realize they were broken until later, and found it very hard to tell how far back i need to undo even if i know exactly what you did.3) so much clicking and dragging and it is trying to update every time you make a change. a colleague was excited about looker but i haven\\'t used it with any seriousness.thanks, alan! my tableau experience sounds similar to yours. if it is straight-forward query-to-chart it is relatively easy and quick, otherwise it gets murky fast.you can turn off the auto-refresh if you are building up some complicated queries and viewsyeah, i should do that more oftenit is easy to forget to do it until it is too lateis there an easy way to convert unix epoch time stamps into regular date timestamps in redshift postgres?i\\'ve found:dateadd(s,somedatetime, \\'1970-01-01\\') as xand also some other worse ways to do it.to_timestamp() does not seem to work in redshift...the regular, ugly, solution seems to beconvert_timezone(\\'us/pacific\\', (timestamp \\'epoch\\' + somedatetime * interval \\'1 second \\'))::datetimecoconut is clearly the worst, but berry at #8! clearly their methodology is wrong and bad!stay positive alex_gudenice to know that cherry lime made the top 5: coconut #20 - guess everyone agreesweird, since coconut always runs out the fastest at my officecoconut is literally the worst.stay positive till_bereally need to rework nuggetbot’s trigger for thatstay positive johnrwalkyou’re not my supervisori’m surprised ‘not’ doesn’t set off nuggetbotthat and typing in all capsfor my project i\\'m going to to sentiment analysis on patients reports of medication preference. does anyone have any advice about sentiment analysis? #sentimentanalysis    hey all, meetup is looking for someone to do contract data viz in jan, possibly longer. we’re also growing out our ds, ml, and de teams. reach out if interested.a colleague at dia&amp;co intends to spend 3 months (jan to march, exact dates flexible) out at our la office and is looking to sublet her quiet and sunny one-bedroom apartment in brooklyn (bay ridge, subway r at bay ridge ave and n at 59th) during that period. if you are interested or know anyone who is dm me and i\\'ll connect you!i vouch for that apartment!kind of the opposite of my hep days: dry, yes, but packed with as many figures as possible. no time for context! let the convener and the one other person paying attention in your presentation interpret the plots as needed.haha, love itthanks  — we ended up asking questions quite similar to that, and they were revealing!a former coworker of mine is looking for data science help for his startup, avvir ()they are working on automating 3d architecture modeling and detecting real life variations from plans using laser scanning drone generated point clouds. sort of a mix of computer vision and anomaly detection.if anyone is interested in hearing more, send me a message please.sounds really boring and not fun at allmy session may or may not have had a team picture maps where everyone used folium :partyparrot:aaaaaand another former coworker is looking for a data scientist:if anyone is interested in the above, send me a message and i\\'ll intro you.score, thanks! most things i was finding for audio were music-focused, so this is great to find!: hey y\\'all!   another plea from me to sign up for alumni mentoring.  we\\'re in mock interview stage starting today.nyc needs a ton of support as we don\\'t have a large alumni base like sv.  take a look at weeks 5-7: :beer::beer::beer::beer::dart: :smile: imo the pgm course on coursera isn\\'t great. she uses some weird software to visualize/interact with her models.: i like this chapter from bishop for pgm: thanks  !  i completely forgot that bishop has a chapter on them! this solves it.538 put together a really cool app for the elections that lets you play with some of the underlying assumptions of their model and see how it changes the output! this was from a recent meetup in philadelphia and some really cool and impressive work at the command line to reduce work burden. hey, does anyone know any good, publicly-available twitter datasets? i have a friend who wants to poke around with one.: the sentiment140 corpus is available for download here: (1.6 million tweets tagged for sentiment)a friend…. riiiiiiiight+1 for  :slightly_smiling_face:  has a good point though.  i think the issue was that the scope of the conference is broad enough that it doesn’t really know what it wants to be, so the talks are all over the place.  moreover, this means some of the talks have to focus at a vendor/business level rather than being technical, but it seemed people were leery of tagging their talks as “beginner” or “nontechnical” so there were a lot of ostensibly intermediate/technical talks that were super basici didn’t go to this year’s odsc west conference, but the one last year was pretty good. at least it was much better than strata which, when i went two years ago, was a lot like   describes in boston: too general.welcome to all new fellows starting insight today!!!woot woot!any tips on building a collaborative data science tech environment? for the past year, i’ve been the only formal data scientist at the company, so my work has been largely quarantined in my own repo. now that we’re beginning to grow out the team (including remotely), is there any experience/wisdom/tools/best practices you can share to scale out wisely, promote collaboration, and minimize headaches down the road? any advice is appreciated!document your code extensively? *hides*use version control, write automated tests for everything you\\'d check adhoc, use continuous integration. do pair programming and code reviews.enforce style guidelines of some sort, refactor scripts into classes, export code from notebooks into e.g. python packages hosted as github repos, provide readmes, confluence docs, etc.you can run a linter as part of your automated tests to enforce style. but don\\'t try to do all this for all exploratory work necessarily.data dictionaries :slightly_smiling_face:thanks  ... those do look interesting.nice...i think i\\'ll probably be going to the ml conf too!i just started a  channel and re-posted these links there. maybe attendees could post their highlights there eventually too: feel free to start a channel for your session and invite members. private groups are also an option.nuggetbot: weatherweather for new york, ny, usacurrently: mostly cloudy -1.3°c/30°ftoday: mostly cloudy until this evening.coming week: light rain on sunday and monday, with temperatures bottoming out at -1°c/30°f on saturday.nuggetbot: weather san franciscoweather for san francisco, ca, usacurrently: partly cloudy 12.3°c/54°ftoday: partly cloudy until tomorrow morning.coming week: no precipitation throughout the week, with temperatures rising to 17°c/63°f on monday.nuggetbot: nuggetbotnopewhat exactly were you trying to do there joshainsley ?get meta with nuggetbotthe nuggets have nuggets!we can add functionality to nuggetbot from here: nuggetbot: pug bomb 2i think this is fantasticjonathan_whitmore: hope we all find it useful!nuggetbot: animate me welcome everyonethe is hereit\\'s happening.nuggetbot: image me jake klamkahaha nuggetbot\\'s the manjust arrived in nyc and ate a ton of pizza to celebrate - see you friday, andrew set the channel topic: :chart_with_upwards_trend: btw, thx for the alumni panel visit yesterdaysame to you  it was a lot of funnuggetbot: image me data sciencecould it be any other image?nuggetbot: image me insight data sciencehahanicenuggetbot: image me data engineeringhahaha lolthanks , i had a fun time and i hope it was useful to the current fellows. sorry i had to jet at 3, but i just barely made the train back to work.see you fridayok back in the fold after a loooooong time awayhi folks, is there a way to know in which way the feature is important in random forest? such as positively correlated or negatively correlated to the predicted value? xiwang: when i was looking into random forests, i came across this package that seems to interpret feature contributions for sklearn’s decision trees/random forest models: ^ i second using ‘treeinterpreter’.xgboost has a model.booster().get_fscore() function to look at feature importancei\\'ve used that. the f-score is just the number of times that a feature was split on in the tree; i\\'d consider that one way to measure importance, but possibly incomplete. you might have an extremely important feature that does its whole job by splitting once, for exampleone way would be in crude bootstrapping; assess importance by removing features and then ranking importance by assessing the difference in model scores based on a validation set. that\\'d be super time-intensive, though. probably better ways than that ...i don’t use forests a lot, so i’m not an expert, but my understanding is that the fscore is the number of splits in a tree _weighted by the number of observations going through that node_. so *if* our model splits on more senstive features first, it would capture the case of a very important variable that only requires one split. is that “if” satisfied in xgboost? i’m not surei don\\'t think so? wow~ so many replies. thank you, guys! i\\'ll take a deeper look. and get back to you!so there are variable importances (also known as vimp) and things like “minimal depth” for a variable. they don’t give you direction though, just importance. direction would require further tackling, as there could be multiple split points for the variable, with influence direction changing, i.e. if there is a parabolic relationship between the features and target variable. thank you! this is what i thought because each time the split might happen in different ways.if i remember right, randomforest in r has cool ways of visualizing the “effect” of the variable to see if it is linear, nonlinear, etc.here’s a beaut of a solution using ggplot2 :smiley: this sounds cool! will definitely try it! thanks!here is a great event about building your personal brand, from some friends at quora: hi guys, i had an aws instance running for three weeks. i could access my web app all that while. then suddenly two days back my app (webpage) wouldn\\'t load. so i killed all processes and reran supervisor. and it started working again! then today, the same thing happened again (page won\\'t load). i log in to aws and see my instance is running (no errors, alarms). so i\\'m not sure what\\'s wrong and what to even google for to resolve this issue. any help information will be appreciated! thanks .i do nmap -p 80 myelasticip and it says the host is up. i guess there is a problem with supervisor then. i have not used gunicorn and i admit i don\\'t know what purpose it serves :confused:stay positive pushkariniif you are using supervisor, you may be able to change the logging level. that said, if you continue to have problems, you might try a more robust server solution, such as apache, or nginx. additionally, if you really just want to keep it up and running, and don\\'t want a lot of extra work, you can try to log into aws and run your server from within a gnu screen session. hope this helps!thanks for such detailed writeup kevin! having a separate environment for each client is a really good idea.it might even be required -- clients will likely want to know that their data is in no way being used as training data in models that could potentially also help competitorshas anyone used secor (by pinterest) to consume messages from kafka and upload them to s3?hey ladies, i registered for the women in data science conference at stanford next friday (feb 3) but now i can\\'t go.  would anyone like my spot?  it was $350 but it will just go to waste if no one takes it so you can have it for free.  dm me!  thanks, , i\\'ll look into this as well!good morning!  does anyone have experience or a good understanding of what to do when you have highly redundant data samples with different target values?   i\\'m working on a consulting project with a pr firm to predict whether a journalist will click on an email heading containing a media pitch.  the data set consists of some emails that are only sent out once or twice while others are sent out more than a hundred times.  any advice is welcome!if you have a lot of redundant data, you may want to look into sampling to reduce the variance, or use a high bias model that ignores/sums some features. additionally, you might want to look into a bayesian graphical model if you want to hand engineer the relationships between actions and outcomesthanks ! those are helpful tips  have you tried jython? or jpype? that way you can at least run everthing in your python app. jython tries to replicate java in python, while jpype also the python vm to to the java vm. you could run a component analysis (pca/ica) on the set of features. you\\'ll lose the interpretability of the feature sets as the new eigenvectors represent combinations of the original feature set. that’s not entirely true, you can still draw on projections of each feature (depending on how many there are) onto the two eigenvectors being plotted and get an idea if clusters in the pca tend to fall towards one particular direction in the original feature space. however, those projections of the original feature space are certainly not orthogonal thanks for the suggestion. i had looked a little bit into py4j, but had not yet given up on finding something available in python. i will definitely look into jython and jpype as well.i\\'m trying to build a foreign language speech classifier for my project, but i\\'m having difficulty finding freely accessible audio data for this task.  if anyone knows of an appropriate dataset, i\\'d really appreciate the tip! thanks!do you need a specific language? a bunch of languages?this might be a decent start regardless: i worked on this project in grad school, which you should be able to request access to: . not a huge number of languages, but has speech samples that are comparable in each languageif you have a friend in a linguistics department, then the ldc catalog has some good stuff. unfortunately, not free hmm--i saw that, licensing those corpora is pretty expensive.  i\\'ll dig in a little bit, maybe there are some subsets that are openly accessible. allstar is actually perfect for  me. i\\'ll request access, thanks.  yeah, ldc catalog would be ideal, as you said, access it sticky.hi guys, does anyone here know how to render a plot in flask?  maybe even do it \"dynamically\" so that the figure does not have to be saved every time? you can try google chart api. i used flask and javascript to feed data into the chart from python. a simpler solution is plotly, but it has limited amount of access for the free version. thanks .  right now i have a simple python function that processes some data and makes a plot, and i just want to pass it back out of the function to an out.html file. just not sure how to save it and pass it back out. here is a tutorial: thanks is also nice for dynamic graphs+1 for bokeh. you can even use it with matplotlib plots.  not sure if this will help but project gutenberg has free audio books  both human read and computer generateddoes anyone have experience creating an interactive map using leaflet in r? i have some questions about creating dynamic buffer zones based on a user\\'s click...(i\\'m a new fellow in boston)does anyone know how i would go about scraping this website? : it depends on what kind of information you want to get. the search form on the product-search tab of the website is simply querying a php script: `&lt;form name=\"form1\" method=\"get\" action=\"/eng/product.php\" onsubmit=\"return checkwholeform(this)”&gt;`. so, if you have a list of products you wanted to search you could just query that php page and save the results.a much slower way is to emulate a browser (for example using `splinter` module for python) and again save the results.thanks alex! i\\'ll look into it :slightly_smiling_face:: also, an easier way might be to just email them and ask for their database-dump. scraping might be against their user-agreement policy, might crash their website altogether, or be considered a dos attack. you never know how much traffic their php script is designed to handle.stay positive alex.tombergokay, good callthanks :slightly_smiling_face:i’m thinking of starting to organize a list of data science case studies after having a bit of pain trying to find some. does anyone know of such a list that’s already out there that i missed?stay positive joshainsleynookyesi willi guess kaggle doesn\\'t count? :slightly_smiling_face:sure! but it’s not comprehensive enough. plenty of companies are putting their data science work out there publicly, but there’s nothing out there that makes all of them easy to find.sounds like a good pitch for incoming fellows project? :wink:perfect! get someone else to do the work for me. that’s upper level management thinking right there.cc my god; what fantastic epicness!!anyone have any suggestion on strategies/documenting software to keep track of tables and their schema at their work?i\\'ve used alation () before, and it\\'s pretty slick.we\\'re starting to use aws glue, which crawls your database and s3 buckets to create a data catalog.  but i don\\'t think it solve your documentation issueso i have no real tips for documentation, but i highly recommend something like flyway for versioning schemas programmatically.thanks!has anyone used google cloud ml? may i pick your brain?we define metadata in the pipeline code that generates tables in bigquery, and then we have a tool like  mentions that scrapes all our tables and pulls the schema definitions and any comments or metadata into a centralized web resourceif any one wants some level of automation in standing up their aws take a look at this.there is an example at the bottom, hope it helps.pretty cooli also like  which you can run outside google with  eg on spark or flink.not sure if this is old news or not, but nasa has open sourced their a bunch of their resources, including data, tools, apis, and repositories:nasa open data:really fun to deduce why each state has that top emoji:hello insight universe, does anyone have experience in biclustering? what is the main difference if you run biclustering on a matrix vs. do hierachical clustering on rows and columns separately and then combine? thanks in advance! uploaded a file:  and commented: we’ve hit 666 :smiling_imp:: correct, although the bug in question was specifically related to cluster correction (initially suggested in worsley et al. i think). had they been using anything else (e.g. bonferonni) or a different approach like mvpa would\\'ve been fine. does reinforce the notion of thoroughly understanding what method (and its implementation) is being used in an analysishi everyone! my company, blizzard entertainment (makers of world of warcraft, starcraft, diablo, hearthstone, and overwatch) is now looking to hire several more data scientists at lead, senior and junior levels. we\\'re located in irvine, ca. if you\\'re interested in more information, feel free to shoot me a message on slack! do you have an office in nyc?if not, would you consider opening one… please?btw, i have recently been using a notetaking software i wanted to share.  a nice alternative to evernote, and it supports markdown. :+1: sharing is caringanyone have the link to the recent draft paper from ds folks at facebook comparing methods of causal inference?nice job at sfiq today people who presented! :thumbsup:hi! i\\'m one of the current ds fellows and would like to set up a way to go around constantly hitting twitter api limits. i know some previous fellows got around this by switching between vpns- if you have some knowledge on this, could i talk to you? or if you know someone who worked on this, let me know. thanks!that sounds interesting , let us know how you go. i like hacky solutions to such limits.  sarah, you had a pretty good solution for api limits in your project no?  if its tweets you need there is a data dump i can help u get -thanks! i’d like to hear more about this data dump. so more specifically- i’m interested in users that follow certain groups (especially political), and would like to get a large number of tweets (200? but could be less) from each of these users. so if the data dump has user ids, who each user follows, as well as some tweets from them, that would be great! also can help you with a massive twitter data dump  -thanks for the suggestions!fellow windows sufferers: what is the best way to export data from ms access to mysql?i had decent luck with navicat when i had an accdb. was on a mac thoughso jealous is your access linked to mysql via pens?no what is pens?i converted to a database/excel format in access and saved it as then read it into python. are you trying to go directly? if you figure it out let me know!!!autocorrectodbcat least that\\'s how i used to do it back when the dinosaurs roamed... via the right odbc driver, you can use access to read/write to almost any sqldb as far as i knowbeen a nightmare trying to install the right odbc driver. after hours of frustration i realized i prob have the click-to-run version (wtf is that?) so driver does not install properly. stay positive jtsitroni dunno. windows 7 or 10?7this helped me... but i dunno. are you admin on that machine?does you windows machine (user/ip) have permission to connect directly to the db?could be a bunch of thingsi do but i think it\\'s the click to rum thing. i\\'ll take a look at your link. thanks!anyone have suggestions for improving performance of mysql queries beyond indexing joins, checking explain for anything fishy, and optimizing tables?can you be more specific about the kind of query you’re looking to optimize?well...they\\'re all select statements on 1-3 tables, many of them involving aggregation and filtering.anyone in the bay area looking for a good data scientist candidate? a work colleague is looking into moving out that way. pls msg me with your contact info and i can hook you up. thanks!anyone go to dataengconf last year? any good? thinking of going this yeari attended last year in nyc, but focused on the “ds” track. found it a bit more applied than some other conferences, e.g. spark summitawesome! thanks kevinwhat about mlconf in march?  anyone know about that?  :q !ah, vim. the easiest way for me to identify other particle physicists:heart:by far the best editor ever conceived.love vim! :smile: gave a 45 minute talk at work about why it\\'s amazing...not many converts still :confused:hear hearit’s hard to appreciate until you figure out how to not destroy all of yoru files and text documents when you’re not in edit mode &lt;-- a great read on \"thinking vim\"is vim is good. my life changed when i discovered `:!ls`also, spend 15 minutes a day here: i was in astronomy for awhile where emacs was king gives you emacs\\' extensibility and plugins with vim\\'s keybindings.question for you all: i have a canonical user-movie rating matrix. however, all users fall into one of five externally labeled classes (abcd or e, say). normally, we\\'d use matrix factorization to find latent patterns in the data, e.g., users who like horror, comedy, etc. however, the goal isn\\'t to predict new ratings, but rather use a new user\\'s ratings to predict which class he/she belongs to. there isn\\'t a 1:1 mapping between classes and latent genres, but rather class a likes some mixture of (latent) horror and comedy, while class b prefers another.the reason i\\'m not doing straightforward multi-class classification is because the data is already in this matrix form (term-doc, user-rating, etc.). i\\'m wondering if anyone knows an approach that could be useful for this case, e.g., could probabilistic latent semantic analysis be useful here?another analogy would be all documents in a corpus arranged into certain \"chapters.\" the goal is to use some kind of latent analysis not to predict the topic/word distributions, but rather which chapter a new document should be assigned to. do you want to write a quick blog post about this?halfjoking...:  it creeps me out you guys are investing in skin cream :wink:hey fellas. i want to record a training video, like coursera videos. i want to be able to record my screen and then edit / insert clips if any errors are madehow can i do that? what sw (free or on msdn) are available. platform windows 7for my o\\'reilly screencasting course () i used camtasiathey also gave a high quality microphone (headset)i recently saw a new course on data science from google, possibly through a linkedin group, but i\\'m having trouble finding it now. has anyone seen it or have the link available?i found their deep learning course on udacity, but i don\\'t think that\\'s it, i\\'m pretty sure it was from google developers.found it, it\\'s this series of videos: boston health economics () is opening a second office in downtown boston. in their own words, \"we\\'ve developed analytic software for large health care databases which were actively and successfully marketing.we\\'re looking for people with experience in both front and back end in thus field.\"   jordan menzin, the head of the software group, asked to advertise; he understands that he might be contacted by either graduates or program directors. lmk if you are interested - ill send over his email.is there any way to do a map-only job in spark stream ? it looks like every \"output\" operation on a dstream forces a shuffle. even map-side operations lke reducebykey() or combinebykey() eventually coalesce dstreams. i have an application where i don\\'t need any kind of reduce() operation at all. i just want to execute a function in each partition locally. apparently this is possible in hadoop streaming.: i thought just running .map on the dstream would work: does running forearchpartition after foreachrdd not do what you need?no it doesn\\'t. to clarify my problem, what i need is he ability to do an output function after after map and combine phase in the map-combine-reduce pipeline. but spark doesn\\'t let you do that. so eventhough i could do everything i need to right after the combiner phase i am forced to incur the cost of reducer phase which means data gets moved from the combiner partitions to reducer partition unnecessarily.someone should do an insight project to find the best way to redistribute tech people across the us to save on rent + change swing states from red to bluei have had this thought as well. as a native iowan who moved to sf, i feel the need for the latter issue acutely. the electoral college isn\\'t going anywhere.i didn\\'t frame it this way but...  :wink:hi insight fellows, i have a friend who has an opening for a ds-applied scientist on their team(amazon alexa), feel free to pm me if interestedwelcome new fellows! if you feel like this, it\\'s normal! we\\'ve all made huge mistakes. :grinning::upside_down_face:a friend has an opening (security focus) at workday if anyone is interested let me know and i’ll put you in touch directlycome to hey  i work at nielsen on 6th at 21st, happy to chatanyone interested in a director-level position for data engineering? what  said applies to me too.hi all, uber is looking to hire a senior data scientist on our crm team. if you\\'re interested do reach out to me :) thank you.what questions did you have?  though not sure i know much about \\'best practices\\'has anyone built a chrome extension before? some of us are working on them, but have been confused on how to get started so far. you did, right?did you find if there was a best way to preprocess images e.g. normalizing the color channels in a 3 ch (or more) satellite image?i’ve done some bare-bones stuff. i don’t know if i’ll be of use but swing by the ai group and we can talk.would any past fellows here have some expertise in building lstms in keras or tensorflow api?  specifically anyone with experience in encoder / decoder architecture with word embeddingsi’ve done some as well, a few years ago. not sure if things have changed much, but maybe i can help too.hey current fellows. i have a mentoring session from 6-8, but i\\'m still on the red line. i am on my way. sorry for being latemade it could someone let. me in?  you did pan sharpening right? i think the type of pre-processing you’ll do will largely depend on how you intend to process those imagescould you share more details about what you are trying to do with satellite imagery and what are the approaches you are thinking about?i agree with dan. it really depends be what you want to do.  or do you just mean turn you 8/16bit pixel  into into a float 0-1.0? i am not an expert but are about to work on it. let me know if you would like to discuss.this has some useful functions you can use or borrow.  has anyone recently could get caffe installed on mac?yeah, sorry for the vague question. and the answer very well might be “try it, see what works”. but, in essence, just color normalization. i’ve seen there’s different recommendations in doing this: - normalize every channel over the max value `img/img.max()`- normalize on a per channel basis `img[...,i]/img[...,i].max()`- normalize by mean and std `(img - img.mean())/img.std()`, now you have neg values- or, if using pre-trained layers (conv layers from something like resnet), just normalizing by their values.i was wondering if this became more important as you moved to multi-channel (4-16) satellite images which include the non-visible bands?if my memory serves me right, you don’t want to have negative values if using relusif you are using e.g. raw landsat images from aws, i’d suggest normalizing them on a per-channel basisyeah, that’s what i was thinking too. though it seems leaky relus are starting to gain favor, i’m not sure if that makes a differencei know each band has a range of values - people from geotrellis are empirically clamping intensity values to [4000, 15176] ()but i think intuitively, it makes sense to normalize on a per-channel basis, especially if you training from scratchotherwise, when you have 8 channels of 8k by 7k pixels,it seems weird to penalize nir band because there was an oversaturated pixel in the blue bandi did a couple of months ago.  ran into a few problems along the way.  dm me with where you’re running into trouble and i’ll see if i can help.okay, yeah that makes sense. thanks !thanks! we’re in the ds group in seattle right now. we’re stumped about how to get our models in python to talk to a chrome extension. we weren’t sure if we should start using flask and try to figure out if we can build an extension that way or we should be approaching this form a different direction.hmm never tried to get python working directly with a crome extension.. i always spun up a server (django or flask) and then made ajax calls from `background.js` in the chrome extension i have some experience in tf/mxnet/pytorch. if your going the tf way i highly recommend starting from the ptb dataset examplesorry to hear about this. i hope you’re ok. you can try contacting some of the higher end local bike stores and ask if they know of a lawyer who handles a lot of these cases. they can also put you in contact with the organizers of the local group rides. sadly, they will almost certainly know someone who needed a lawyer for similar reasons. good luck and i hope you heal quickly.+1 on spending a considerable amount of energy explaining ds, which is not necessarily a bad thing.as a con i’ll say, make sure that leadership understands the investments and expectations of data science. while i disagree that one should come in and demand certain things, i can say based on my experience that not being clear on what i can expect resource wise (setting up servers, or engineering support) made things challenging.how can we refer a prospective fellow to the new data pm program? any available link? you can use this form to recommend a fellow for any insight program: has anyone been to `ai nextcon`?hi. anyone knows of cities that publish live feeds of their transport system that include gps coordinates? most of the feeds i\\'ve found so far seem to only provide the scheduling per vehicle at its stops...allright, it seems like the boston data will do the trick: i\\'ll try fetch it nowhtml related question: i am trying to generate a simple bar plot each time a user enters an input. i currently have the plots saved as images with unique names but they are still getting placed on top over each other in the output. anyone dealt with this before? got it! added plt.clf() - not certain, but i feel like san francisco does this.  on a much smaller scale, i remember princeton providing access (if not publicly then on request) to live gps on their various shuttles.  uc san diego also does live tracking, not sure about feed availabiliythanks, will take a look: try this link for vancouver transit system (requires registration though): cool, thanks , looking into it: it turns out the vancouver api limits to a maximum of 1,000 requests per day... bummer for my plan, since i want to have real-real-time... don\\'t know if it will be possible with any of the feeds out there though...: vancouver api is designed for mobile apps, so that’s why the limit. can you use proxies to scrape more? i don’t think that’ll get you to real-time resolution, but 1 data point per 3-5 mins, maybe: sounds like a good plan. thanks :simple_smile:: berlin does it. &gt; help.exethanks , it seems though that the dataset does not expose the coordinates of each vehicle... also, i\\'m more interested in buses than trains :simple_smile:thanks  -- do you know of good resources/advice on how to evaluate search engine results?what do you mean by that, alexanderjerneckif you mean metrics for the quality of output of search engines, then there are a variety of information retrieval-theoretic quantities you can calculate  (e.g. ): it looks like there is some gps bus info here:  current remote session fellow with a question for anyone about using python 3 on a machine already running python 2.  am i better off installing a vm and running the python 3 strictly within the vm or can these two (and the relevant respective libraries) happily co-exist?  to give context, py3 is a specific requirement for my project, whereas py2 is what i normally use.  reply here or pm me.  thanks!far as i know, they can coexist, as the libraries have separate install pathways (i.e., pip installs to python2, pip3 installs to python3, at least on ubuntu/debian).  might be better off with a vm just for development best-practices though: anaconda makes it relatively straightforward to switch between python versions on a whim.: use conda to create an environment for py3, switch between environments as needed.virtual environments are also a way to isolate versions and dependencies : +1 for virtual envs over conda. in my experience, anaconda can give you trouble with the rest of your dev stack (my made-up statistic is &gt;60% of set-up problems for my session were due to anaconda)stay positive zkermishyikes.  okaybut definitely don\\'t mix conda enviroments and virtualenvs. both sides agree that\\'s a horrible idea. :wink:: \"don\\'t cross the streams\"\"it would be bad!\"stay positive alex_gudei\\'m trying to use stats.models acf and i keep getting an error that my data is numpy.ndarray and is not callable. anyone get this error before and have a fix?stay positive stephgline can you post the line that’s causing the error?----&gt; 1 monthcor = acf(dfmonth[\\'count\\'])typeerror: \\'numpy.ndarray\\' object is not callablesounds like acf was overwritten to be a numpy array i just got assigned to a search engine team, and i\\'m sure (i certainly hope!) they have metrics - i\\'m more looking for someone with experience n the area that can give advice once i get into it. i seeinformal poll here — heard someone recently say it was pretty standard for company-based health insurance kick in after some waiting period, i.e., 30 days, 60 days, etc. that’s not been my experience before across a variety of sectors (education, government, international development, think tanks, non-profits) so was curious what others know from their past/current experience. feel free to dm me if you’re interested or if there’s overwhelming opinion on this, i’ll set up a new channel. thanks!: that\\'s terrible! i don\\'t have much experience, but i\\'ve never heard of that. what are you supposed to do if you get hurt or sick during the waiting period?stay positive berkeleyalmandhunter: you’re not helpful. :simple_smile:: exactly! was curious about a situation like that. seems like a big risk to ask of a new hire. i guess one could rely on cobra from a last job or something of the such, but that’s not applicable in all cases.: this sounds really strange. i\\'ve never heard of this. maybe if it\\'s for a role/company where they have high turnover and a lot of people drop out in the first month or two? for a tech job, this seems really weird.stay positive jeremykarnowski: i\\'m trying.not an insurance lawyer, but new job counts as a qualifying life eventwhich means you should be able to sign up for insurance immediatelyhowever, often, it\\'s the first of the month after you start: the turnover, i.e., someone doesn’t work out, was one of the reasons someone mentioned for doing it.so, if you start the first, it starts that day, if you start the 2nd, it starts the next month\\'s 1st.but it\\'s worth asking about to be cleartalk about bad timing, huh?stay positive mhoover: i’m the most positive person in the world. give me some love for that.also, maybe make it part of the negotiation process? that it should be available day 1? is it because of time to process paperwork?just to be clear, this isn’t something i’m facing — just something i was a casual observer to. didn’t interject as a bunch of others were also saying they’ve not faced the scenario before. but got me thinking... def a negotiation point though if it comes up!this happened to me at jp morgan and also happened to my wife when she got her job in advertisingi think it is just something that insurance companies will negotiate into their contracts with employersi think your odds of negotiating it would be pretty slimyep. i have seen a three month waiting period.can i start a poll? i\\'m new to data science and my boss asked me what my priorities were for learning skills on my new job. i have the following list, but i\\'m kind of intimidated by deciding this by myself. does anyone think i should add or subtract anything to/from my list?                                                                     machine learningbusinesspythonsql and spark/hadooppredictive analyticsmentoring/working on a teamwhat\\'s the job?i say work on the ones that give best roi.best roi for sure. but the python, sql, simple data analytics, and then (predictive data analysis and hadoop or spark).mainly because coding, sql, data analysis are the first things to focus. are there candidates? took me about 6 months to hire set of ds and de there. well, since you have found 2 people, we must conclude that there are :wink: my company also took about 5 months to hire a ds last year. we are currently looking for a senior de -- 5 weeks and counting.posting for a friend: datacamp is hiring for a data scientist position: no problem! glad it worked out!some researchers collected 100,000 “happy moments” from mech turks. pretty cool: i would be really happy to see a fellow project make use of that!^ i see what you did there.does anyone have a good guide on best practices that facilitate data science initiatives, like a data science project lifecycle, that you can share? thanks!maybe not quite a project lifecycle, but this has helped frame my group\\'s work: cool i’ll check it out. thanks!do you guys who work in data science prefer using dask to spark?i haven’t have the chance to use dask; i like sparkdask is nice if you data science team is doing most of their stuff in pythoncan scikit learn be used effectively in spark? (that is distributed) the wait was too long, and i was ready to give up. but yes it takes time!generally, yes. but there are varying levels of implementation complexity depending on what you want to do. there’s a package called spark-sklearn that lets you easily distribute a grid search across a spark cluster: cool!i’ve also done ensemble learning with sklearn distributed on a spark cluster, but it took quite a bit of work to make it happenhere’s an outdated example: the bootstrap module has been deprecated, but the basic concepts are thereamazing thanks!happy to help!that\\'s really cool! as a general caveat, i was told to be careful about mixing and matching python with spark unless it\\'s configured properly, because by default python expects a single-threaded processor. i\\'m working with fellows (and alumni) who are interested in toronto - if you\\'re still trying to hire people to your group i\\'d be happy to chat anytime!we used dask a bit, but then our de team pushed us to use spark and so now we’re all using (py)sparkspark in a jupyter notebook was interesting omg spark is so figidty!!! i’ve been using it in the last month and i’ve experienced so many quirks is dask able to handle bigger data than spark?i had to switch to scala because pyspark didn’t handle some of the features i needed at the timeduring insight,  suggested that i used dask to handle their data. i was doing a consulting project for them. but i never did use dask or spark back then. (my concern was that if i got stuck, i wanted the fellows and the pd () to be able to help me. so i stuck with sampling down and using pandas…i’ve been using (py)spark to handle 400+million rows of data at a time--that’s about 260gb csv file--with reasonable wait time between each execution command (&lt;10 min for whatever you wanna do, including joins). but i always wonder about daski only used dask a little bit, and only as a bulked-up version of pandas. i now use pyspark in databricks notebooks which attaches to decent-sized clusters and does a lot of pretty quick and efficient heavy lifting etl type stuff for me. it’s still a bit of a black box, but i’m starting to get the hang of pyspark and when you should cache dataframes, etc., to speed things up. and i’m getting ok at keeping everything in spark dfs (as opposed to topandas-ing everything!)and in terms of running ml algos, if using pyspark then do you guys typically use mllib, too?one of my teammates has used mllib, and they say it’s worked fine, but i haven’t heard much about it and never used it myself.i mainly use spark ml and stuff everything into a pipeline.i mainly use spark to clean data, then down sample to import things into pandas and do ml training and evaluation therelittle known fact... a little over 63,000 of those \"happy moments\" occurred in the nyc insight office. #themoreyouknow :rainbow:has anyone used/does anyone have opinions on snowflake? we are thinking of moving our rds postgres warehouse.we use snowflake.  our biggest complaint is that their pricing is opaque.  it\\'s so bad that we actually set up experiments to try to figure out how they\\'re charging us.one of our managers actually demonstrated that they are not charging us properly according to their own documentation, and showed this data to our sales rep about a month ago.  no movement yet.to be fair, though, we are still using snowflake, because their service is reliable and can handle our data.i also use snowflake.  i would echo the issues with the pricing and in flexibility.  if you spin a warehouse for 2 minutes  turn it of and then back on for 56 minutes you pay for 2 hours.however, the underlying tech behaves as advertised and is awesome and very fast.i love how data scientists data-science the pricing.there is one open source schema registry utility by yelp called schematizer thanks. i think roi is pretty difficult to determine when you are first starting out, but clearly focusing on coding and analytics are important.google is using recurrent neural networks to predict your messages: one interesting question about this: do they/how do they avoid leaking training data? one obvious way is to train user-by-user, but that would provide small samples.i can’t find it now, but this reminds me of an article i read which was anti- this idea.   the article was about facebook and likes and birthdays, and how it could be possible to have the computer wish someone happy bday on your behalf, and remove whats left of any human interactionnot sure if it\\'s what you\\'re looking for but here at ibm we adhere to the crisp-dm methodology thanks! i will keep a note on that. who else loves rebecca black??!! #friday#at-channelnochannel-tagging 1500 peoplewew lad:panda-dance:oh no! my desk is dirty!here comes the complaining about channel tagging followed by the complaints about the complaints about the chanell tag.:faye:also... spelling.and jokes about faye :stuck_out_tongue:somehow i needed this. which seat can i taayyyyyyk? :notes: set the channel purpose: this channel is for posting stupid youtube videos and @mentioning everyone.so bad, so, so, bad…odsc discount code message coming up soon :eyes::smirk:better stop using logistic regression in production nowman. these people are taught to use regression models for hypothesis testing and forget that regression can be used for prediction, too... and apparently so do their reviewers? oof. lolthat’s amazing!what\\'s the journal??lol j med internet res.no wondereven open source arxiv is way more reputable when it comes to this topics...careful, i published there! ... yeah it\\'s not very reputable.did anyone work through the nlp tutorial ()? on the first line of code from the github, i get the error unicodeencodeerror: \\'charmap\\' codec can\\'t encode characters in position 64-65: character maps to &lt;undefined&gt;it could be python version. are you using python2.7?3.0 --&gt; nevermind fixed it with: output_file = open(\"socialmedia_relevant_cols_clean.csv\", \"w\", encoding = \\'utf-8\\')this notification woke me up!!! and i ain\\'t mad :ship_it_parrot:ugh, this abomination of autotune again xdwell at least it makes me laugh#deadbtw (the upcoming) spark 2.3 should be much faster for converting spark df to pandas df. demos show up to 10x speedup for some cases. this is due to using apache arrow as a dependency, which can avoid the multiple serialisations.ohh!!!out of curiosity, because i’m not ds, why does one convert spark df to pandas df?haven\\'t tried it myself, but there is a topandas() function in pysparkhi all. what is your preferred visualization library? i\\'m thinking of migrating from matplotlib to  to make animated plots, however its documentation doesn\\'t appear to be very good... i\\'m not very good with javascript so d3 is not my first choice... this is really helpful thanks!you\\'re welcome , nice to hear from you!separate question: why is the pyspark api so unpythonic……??!!!i’d suggest using matplotlib’s animation module if you’re already familiar with matplotlib. otherwise, bokeh and plotly are good web-first alternatives.still using matplotlib myself (&amp; seaborn). what’s your use-case? for webstuff, there is a matplotlib d3 library that works pretty well. i’ve been eyeing other vis libraries.a super overview of vis libraries and when to use:thanks for your reply. my use case is to implement a picker for a scatterplot, but the examples i\\'ve found of matplotlib seem to be quite rudimentary: i would love for example to have a hover mode so i know what i am picking, or to be able to do some polygon or lasso picking of my points...we tend toward seaborn and an internal tool build on bokehthanks guys.  how well documented is dash in your opinion?quite well in my experience i feel a lot more comfortable with pandas than i do with spark for manipulating dfs and getting information out of them. pandas is really powerful and it’s what i’m used to, so for anything that’s even kinda complex that i wanna do with a df, i *like* to convert spark df to pandas df, though when the df is huge it can be slow (or impossible to hold all the data in memory in a pandas df)ah i see, thank you for your response, it’s very helpful  np, and of course that just my humble opinion on the topic (as someone who loves pandas and isn’t super comfortable w/ spark)i’ve worked with some dataframes that are larger than memory - and dask dataframes explicitly copy the pandas api while enabling out-of-memory analytics on single cores. seriously good stuff — and it scales to multi-core and multi-node too!in my workflow, we occasionally convert to pandas when we want some function that we can\\'t do in spark. but it is limiting because afaik the pandas df has to load everything to memory at one time, as  mentioned. we can run larger data sets faster when we stay in spark.you may be able to accomplish something fast and dirty with jupyter notebook widgets wrapping a plot function.for hover-mode and all that, i’ve seen better support in d3-based plotting libraries but haven’t used any myself.also, should mention - d3 based plotting implementations don’t have to be horrible - there are some d3 powered libraries that support a python api, including:* bokeh* plotly* altair* matplotlibd3if you watch that video, there are demos for all that stuff which might provide some hints. its a bit of a time commitment at 30 minutes, but i think its a nice overview of everything.i have a friend who applied early decision to the ds program with ny as first choice. can someone provide me the link so i can write him a recommendation too? (assuming that is still what happens :stuck_out_tongue:)i’ve just sent my recs directly to the program directors…is there an official process other than that?yes! here is our alumni rec form. alumni recs are our best source of good candidates, so we love these. letting pds know directly is fine too.  pinned a message to this channel.thanks  ! uploaded a file: ibm lemon awardthey don’t have a lot of market share, but they make up for it in cost!apparently true! they posted the same revenue as aws last year it looks likebest trollhas anyone had experience with any auto-tagging softwares for unstructured data like meeting notes?pyarrow is pretty nicei think this looks promisingi\\'m glad paperspace is on the low end -- already adjusted to it (thanks to insight) and wouldn\\'t mind using it in the future!anyone have experience deploying logstash 5 through kubernetes? i\\'m having difficulty placing the config files appropriately :(have you used this  ?are you getting any particular error messages in the logstash pod?anyone here played with flink?  hassuper cool, will ping him...thanks  thanks for sharing,  ! really enjoyed the piececuz airbnb doesn’t trust insight?! jsilverman: i can’t get enough of airbnb’s policies regarding hiring and growth. hats off.however, i think the article is completely misunderstanding the purpose of this initiative. it’s not because they don’t have enough qualified applicants (come on now :slightly_smiling_face: ), it’s to diversify the types of people who work on data, which is a very smart and very beneficial approach.there’s actually a number of companies who do this.  i think cisco also does something similar for example.hi! my roommate and i are looking for a roommate in sf!  we share an (enormous) three bedroom, two bath condo right on golden gate park / the panhandle.  the open bedroom is big, and has a big closet and shares a bathroom (with the tidiest person ever).  we have a big kitchen, big nice living room, and roof access.  optional garage parking extra, but plenty street parking available.  and there’s a bus stop in front of the house that heads right to market st.if you’re looking, or know someone that is, let me know!  we’re just looking someone respectful and reasonably tidy, and not, you know, raging every night.  rent is $1850.does anyone have an insight on what the pros and cons are to work in a company that does not have a data science team to start your career?i work for a company with a small ds team. pros: 1) lots of low hanging fruit for projects, 2) small projects can have big impact for the company, 3) since you end up working on lots of different projects, you get to see lots of different types of data.  cons: 1) not very many people to brainstorm ideas with (hence all my slack and linkedin posts), 2) some times you have to build new infrastructure to pull data and push your models to production, 3) you end up having lots of meetings to explain data science jargon (i spend at least 50% of my time in meetings).thank you. but low hanging fruit point: how impressive to put those on your resume for the next job/company, etc..?definitely! my first project was to predict which of our 200000 dialysis patients would be hospitalized soon - both low hanging fruit and big impact.hmmm…this is also a unknown startupi can tell you for a fact that what’s impressive in resume is not complexity of project, but impact of results.  if you can spend a week and save someone hundreds of thousands that goes a much longer way than building a complex model for smaller gainsthere’s some jobs where that’s not true, like if you’re more interested in the research side of things, but for the vast majority it isthanks but how do those effects are measurable when the company is a small startup?that’s quite use-case specific.  you should probably evaluate the project’s value befor etaking it onok. thanks! :slightly_smiling_face:+1000 to the answer for \"but low hanging fruit point: how impressive to put those on your resume for the next job/company, \"impact &gt; many other things. data science is a team sport, and if you do some engineering, some dashboarding, some managing, some count+divide, and little modeling, but that delivers value, that\\'s how you starting building credibility on a small team. and with that credibility you\\'ll create more space for yourself to take on new projectshas anyone gotten into a bicycle car accident or have any friends that went through this and got a lawyer/bicycle injury lawyer? i need to find a good lawyer, but i don\\'t have a lot of friends who bike. please let me know if you have any recommendations. thank you!which state was the accident in? you\\'ll presumably need a lawyer licensed in that state. in any case, i haven\\'t hired that kind of lawyer anywhere, reviews on  can be helpful for vetting lawyers.also, sorry that you\\'re needing to do this! :face_with_head_bandage:the accident was in california. thank you so much for the link above! i\\'ll def check it out see reviews from lawyers i\\'m thinking about.i found the o’reilly book statistics in a nutshell to be a good primer for some stats fundamentals that i hadn’t seen in a while. i have been using gan on time-series. while some of the techniques useful for images can be translated to time-series, others just make the model worse.i’m having issues connecting flink with cassandra. i get an io netty error like abstractepollunsafedan blazevski, dr. flinki wouldn\\'t normalize on a per channel.  then you change the color of the object you are looking at, and you will mess up things like ndvi.   if you want to adjust for exposure you should do that across all channels.dr flink :flink:could anyone review my lstm architecture and let me know if it looks ok to them?  i developed one in keras and one in tensorflow.   i’m basically trying to overfit to one sample, but the prediction is not quite what i was expecting and i’m not sure if this is normal or not.what version of flink + connector are you using?divide by 65535 this is the max value for a 16 bit pixel.the flink is version 1.4 and the connector is 2.1 i thinkdoes the code compile?it works on my local machine but not when i upload it to the clusteras in, you see data in cassandra when you write the job locally?does anyone have best practices for peer review of ml models? i\\'m looking for something like code review best practices, but for the model/algorithm itself, not the code implementation.does anyone have experience with django + postgresql set up?on ubuntuit\\'s insisting that my password is wrong and i\\'ve reset it a million times and googled my hands off.hello, has anyone used facebook\\'s api? i am trying to collect profile pictures, and how many likes they got for project validation. thank you!you dont need to scrape pubmed, it has an apiyou can also download medline if you apply for a licensethere are both python and r packages that will hit the pubmed api and parse the output sensibly for youwould anyone working on search engines be willing to chat?sorry.. i didn\\'t mean scrape. just use the api and parse the results into something useful.new public data galore: : i\\'ve been playing with this, it works for multiple search engines: if you\\'re getting any large amount of data though, you\\'ll want to run through proxy servers, otherwise google will ban you, or worse, all of insight (definitely not a way to become popular) :with open(\\'name.pickle\\',\\'wb\\') as f:    pickle.dump(obj, f)can anyone recommend some good tutorials for node-red? i’m learning to build internet of things. thanks! do you love vim?alex_gude: i have been using vim for years only because i was too lazy to learn anything else :slightly_smiling_face: could you just run matrix factorization then use the user\\'s latent vectors as \"features\" into a multi-class classification model?oh yeah! a good vim plugin for pycharm is the only thing that convinced me to switch. :smiley:non-physicist vim enthusiast reporting in, anybody have a good solution when working in a jupyter notebook?:eyes: looks possible -- i haven\\'t used it though. also, their installation instructions might work, but it feels old to me.and of course there\\'s the  project which, in theory, separates the notebook pieces so that you can run via an editor (and not the browser). i know a few editors have used this plug-in some way, but your mileage may vary (and i don\\'t know of a vim specific one). that seems to be the most feasible approach for now. i recently found the following paper about predicting classes from a multi-class domain using matrix factorization, e.g. if movie reviewers had different ages, occupations, etc. sounds relevant, but i\\'m still working out the details -- not sure if it will end up being a better approach:a great use of deep learning: sv peeps — i have 2 tickets to the banff film festival in redwood city tomorrow (friday) night @ 7pm @ fox theatre. it’s super cool (a series of short films that are well done and usually related to nature, plus gear/product giveaways between films) and i’ve gone the last 3 yrs but i need to unschedule some of my wknd so i want to send the tickets to a good home. $24 each (i’d let you have both for $40). let me know if you’re interested! here’s more info and the film list: hey folks, got a job opportunity in austin, tx if anyone is on the job market and open to move. . the official position title listed on the jobs page is senior manager, data scientist, but it isn\\'t a manager position (schwab\\'s weird job title taxonomy). lots of really interesting data to play with. it\\'s a pretty interdisciplinary team, and financial knowledge is not at all needed. we make lots of internal data products. we\\'re starting to mess around with rnns and deep learning. python, scala, and d3 experience would be plus. dm me if you have questions or want to apply.to add more to this. i know that you can use a chi square test say if you can have association between two categorical variables , there is also goodman krushkal test which can be used  for this purpose. but from understanding there is no way show collinearity using the above two methods. just that they are associated.fellow long-time mac user now on windows.  i\\'d check whether they support windows 10, i\\'m stuck in windows 7 and it lacks a lot of osx-like features i notice in windows 10 (linux subsystem, multiple desktops, etc).  i\\'ll admit to being an apple fanboy, but, man i miss my old macbook :disappointed:i\\'ve used vif in the past:very interesting read- an uptick in down syndrome diagnoses was seen with the introduction of new machinery. quite buffling...hi, i am in the same situation. started a new ds position, into my 10th day. :joy:. long time mac user- needing to convert to window based- where do i start? follow-up: has anyone made a correlation/covariance matrix visualization where the features can be categorical or continuous variables?does anyone have a good tutorial/reference on how to identify important interactions using logistic regression?i’m looking into windows 10, and it actually seems like a number of the useful features have been implemented.  more than anything, i’m really partial to the magic trackpad, so i’m looking into windows implementationsis your question regarding collinearity between two categories of a categorical variable (such as cat, dog, other) or collinearity between two separate categories (such as animals vs weekdays)?hello everyone! two weeks from now mya systems will be holding presentations on our nlp and ml research! there will be free pizza and drinks!hey  so i am talking about two categorical variables and not levels within a categorical variable. so take for example the categorical variables race and gender. in such a situation, chi square is a measure of association but the strength of correlation, which is need for collinearity, cannot be acquired from the chi square test. hence what does one use then ?in that example i think a chi-sq would tell you most of what you\\'d need to know (if the level of one var depends on another var). depending on how you\\'re thinking about modeling the data, you could dummy/contrast code the variables (ie convert to numeric) and then just use correlation to check if there are systematic relationships hey josh thanks for pointing this out. so the thing is, this works for the case when you are working with a regression problem, but when you have all categorical variables, you can\\'t really regress for anything. you can define an r2 to vif. am i wrong on this?  another approach i know is to use anova, if your independent variable is categorical and your target variable is continuous, this worksanybody expert in ascii/unicode? i’m trying to create a dashboard inside a terminal and looking at `termui` and blessed` tools.. or the least create tabs as ui within termansi graphics.. that brings me back to the good old days. yes, i\\'m old. : i\\'d suggest using a global optimizer to avoid getting stuck in local optima. nlopt is pretty good and has a lot of options:  . if you can\\'t compute a closed-form gradient, use a variant of direct, that has worked well for me for gradient-free global optimization in the past (but obviously having a gradient will make it converge faster)working part time as an undergrad in a geophysics lab in about 1996 i spent over a year developing an ansi graphics windowing system in c. those were good times (among many…). it was for a test stand to commission a satellite for measuring solar activity. one interesting thing is that years later i now work with a data scientist who spent time as a graduate student in that exact same small lab. awesome. thanks! have you looked at urwid or newt?  [  ]: thanks for sharing! :slightly_smiling_face: will be trying this out, this weekend@ryan: thanks a bunch. i\\'ll try your idea. but i am curious why the connection pool approach doesn\\'t work. i found that idea in the spark programming guide. it\\'d be really neat if it worked.today\\'s the last day to sign upnuggetbot check on ajmendezajmendez has a happiness average of 0: pretty low there, budnuggetbot check on dstonedstone has a happiness average of 0.12280701754385964i am, in general, a pretty miserable personnuggetbot check on meme has no happiness average yetnuggetbot check on higginschigginsc has a happiness average of 0.5227272727272727apparently i’m a happy dude. did not know you could do this: i doubt you will beat the jubilant nuggetbot check on vmgehmanvmgehman has a happiness average of 1.5161290322580645checks outthere you go: positive feed back. i am a happiness op amp.hey to all you more senior data scientists out there: what is your best approach for continued learning and skill building?: try newer analytical frameworks, use r! and try kaggle datasets on your own. also, look at usgs and nih, the routinely put out challenging datasets to work on.  there are also tons of great data science meetups that give talks and workshops.  its useful to see what and how people are using various technologies and pick up new skills that way.:thanks!   shutdownify, the shutdown notice as a service company, is shutting down: wowi literally can’t even.any advice for setting up my scala/spark development environment?the problem is the split between my laptop and the aws node that\\'s running spark.i can install an ide on my laptop, and use github to sync my source code, but how do i debug?: i use pyspark with ipython notebook so i\\'ve never tried these but i\\'ve heard about people using1) spark notebook (), 2) ispark () and 3) apache zepplin ()i assume these work similar to ipython notebook, for which you would ssh into the master node with w/e port the notebook is using and run stuff on your local browser (much much easier than copy/paste ur codes everytime in a spark shell or having to sbt build everytime when u just use spark-submit)i would give pyspark a try, the --packages flag now means u can directly get packages from thanks, guang.  i\\'ll have to check these out.  talk with you tomorrow night for mentoring.does anyone use weka? is it worth playing around with?does anyone have a pdf of the oreilly data science salary report they could send to me? thanks! thanskhas anyone ever worked on fraud detection? i want to discuss some implementation matters regarding the validation of highly imbalanced dataset.  thanks a lot!!nyc insight peeps i hope everyone is ok especially folks from the current session? that was awfully close to the insight office!!at my company since we\\'re often dealing with either regulated environments and sensitive data we usually do things on prem. that way we in theory should have access to all the data we need and they can control all the security policies. there are some cons with this (1. we often need to be on site since they won\\'t grant remote access or the remote access they do grant is awful. 2. we aren\\'t in control of the environment, including upgrades, installations etc. 3. we often don\\'t have access to all the data we need anyway).anybody in sf / the bay area need a...- mattress? - tv? - bike/bike accessories?  (really need better photos for this)- monitor? - desk/table? (no cl link for this but it’s this, $10 )hi all, i\\'m in the process of learning nlp at insight, and i was wondering if anyone could help me out with this problem? i have a set of tokenized words in a dataframe (each set of words represents a single element). i want to tag each set with a custom label that i have, which are represented in another column in the data frame. anyone have advice about how to approach this? thanks!what are these labels based on, and what exactly are you trying to accomplish by tagging word sets?right now, just something basic. positivity and negativity of sentiments. i planned to build on top of that (or modify) once i can figure out how to run a classification algorithm.my question is more related to the fact that i\\'m unsure what data you have exactly, and what you\\'re trying to accomplish, so it\\'s hard to suggest an approach.  e.g. you have a df of labels, where did they come from?  do you have a set of words that is already labeled that you can train on, and then use some trained model to tag new sets of words?i\\'m not sure exactly what you have :slightly_smiling_face:got itdata: i have a large list of reddit titles i from various mental health reddits (~30k right now, but i plan on expanding this). the titles are represented in columns of a data frame, and i have tokenized them (using word_tokenizer()).i\\'m using reddit text to determine if a person is \"in need\" of additional referral to a mental health resource. i.e., a doctor would input some chunk of text from a medical record or type up something they wrote down, press a button, and the app would determine if the patient is in mental distress and should be referred on to another specialisti\\'m trying to start simple: so i\\'ve taken titles with any mention of being in therapy, and ran a basic (vader) sentiment analysis on them. from there, i categorized those titles as either having positive or negative sentiment. these are my tentative test labels....but i\\'m open to additional ideas regarding how to obtain the appopriate labelsfrom there i want to train a model to then apply the right labels onto a test set / new text. does that make sense?almost, what are these \\'labels\\' exactly, like \\'in need\" vs \"not in need\" and it maps onto a specific value that\\'s spit out by your sentiment analysis?i guess it sounds like your labels come from an unsupervised algorithm already, and then you want to use those labels generated by the unsupervised algorithm with a supervised methodwhich is kind of weird :smile:  so i\\'m probably missing something still.  mainly exactly how the reddit data is labeledthis dropped today and seems like it could be real handy dealing with biased samples: tech talk at airbnbi\\'m having some trouble logging onto skype@par.r.callier thanks. we are using a pre-trained model for now. so will do simple addition (and maybe averaging) for now. but, i am thinking of training a new model with the wikipedia.anyone using luigi to set-up / manage their pipelines?just watched this talk:make that  , i apparently forgot what year it is^ the channel i made for the 2015 jan pa insight fellowsanyone going to pa insight party tomorrow from sf?  would like to carpool pinned a message to this channel.for the current fellows using d3 in their webapps:  might be a useful toolhaven\\'t used it myself, but i do remember trying to learn d3 in a matter of days being kind of a dumpster fire during my project :stuck_out_tongue:rolling the dice, anyone here works at d-wave ? i don’t but have some grad school buddies who do (and interviewed with them many moons ago)hi  ! i remember you came to our session :slightly_smiling_face:  would be nice for a hookup to ask some questions dm me with your questions and i’ll see if i can get you in touchhi everyone - i have a data warehousing/data engineering question. we are re-working our data warehouse of large quantities (a few tb) of time series data. we get about 10 million readings x 4 types of readings per day. postgres is getting unwieldy in the loading process (we can’t take indices off because it takes days to put them back on) and we are going to be pushing up against the limits of an rds instance in a few months. has anyone else encountered this problem? anyone have a good (or halfway decent) solution? we are looking at dumping a lot of data onto s3 and using spectrum, but it is very new.any bitcoin people out there?kevinmercurio: they would prefer to maintain their anonymity.i hear someone just made 30mm shorting it :wink:ride the wave, friend:  seems like a redshift or spectrum issue depending on frequency of queries types now in python!seriously?lifesaver?awesome!this is great!!!30 mm oh my god….finally motivated to read that bitcoin paper!that’s pretty nice. i wonder why they didn’t build static type checking on top of the annotation system?i guess you can have pretty complicated types, and trying to do stuff with annotations might get really messy quickly.oh, nm, it does use annotationssilly me which paper ? i actually started working a bit on blockchain in ibm …. uploaded a file:  uploaded a file:  and commented: one of my colleagues here in tn posted an article of  its a small world and insight makes it smaller :simple_smile:hi, i\\'m another current fellow.  i\\'m looking for someone with experience using lstm rnns with time series data, particularly with multiple features.  others with general deep learning experience could be helpful as well.  i\\'m looking for help figuring out how to set up the data on tensorflow backend. from 2017c used rnns.new blog post by  - heart disease diagnosis with deep learning  and interested others, i\\'m working on a project to identify credit card declines with a rate of about 1%, so with imbalanced samples. does anyone have experience using smote  and/or similar classifiers? thanks!hi everyone, current fellow here. i’m looking at interactive mapping options. i have a set of locations (in the us) that each has a value associated it with it. my vision was to plot points with different sizes to denote the value associated with each point, but wanted to display further information when individual points were selected or even hovered over. any platform suggestions would be appreciated. thanks!`bokeh` can do overlays on a map with interactive annotationsthanks! i’m looking into that in particular can talk to 3rd-party geo mappingfor quick and dirty stuff the bokeh sample_data subpackage has patch coordinates to draw a usa state/county map if that helps as wellthat sounds perfect. thanks for the advice! i\\'ve used smote and some cost sensitive learning techniques for class imbalance, feel free to dm medamn. iphone x is $999!!!!,  you might look into using `folium` which is the python implementation of leaflet.jsi\\'ve used leaflet (in r) and found it practical to use for any mapping / gis work.thanks for the recommendation, i’ll check it outfor :  good to know. thanks for the heads up!i’ve been using it for a while at work, it’s super solid. anyone have experience with tracking web mentions? i have a consulting project this session where i want to see if a lot of small businesses that should be dead have any web activity (i expect some are still active). either on big sites like yelp, craigslist etc., or small blogs, or anywhere else. i see there are a lot of paid/quasi-free apps out there, but my idea was to simply set up a ton of google alerts rss feeds and monitor those, as well as try to tap into some apis for the big sites. is there a good free off-the-shelf app in anyone\\'s experience, and are there any known pitfalls with google alerts? what did you do?equifax, bowing to public pressure, drops credit-freeze fees question about d3 and python in visualizations channel.  thanks for the help.hi everyone! i\\'ve created a whole bunch of new channels by cohort if you want to talk to people who went through the session with you. of course, keep the amazing subject-specific and generally random banter going across all fellows as well! and keep creating aggregate groups if you feel those will be helpful too (e.g. sv-2015a)d3 + pythoni may be able to helptomv: they didn\\'t say shiny :simple_smile:aaaah andrew....irony has done a u-turn and smacked me in the facei\\'ve been flasking like a maniac for the last month and a halfall day everydayi miss shiny so much/giphy shinynuggetbot: image me not so shiny anymore10x\\'ing it! thanks obama!on a scale of 0-10, how many x is obama?here is the original source on medium that says nothing about a shortage in data scientists. the opening of the tc article is fake news.hello insighters! welcome to the new crew, and here’s a question for the alum:*does your company have a data science career ladder?*i’m working on a ladder for the ds team at enigma and i’m not finding a lot of existing examples to work from. if you have any experience or insights to offer i’d be very grateful! please reply in a thread to this message. thanks!  shared this last year, about glassdoor: thank you for posting that  -- i was looking for that a couple of months ago and couldn\\'t find itit lives!  -- we\\'re working on something similar for ds/machine learning roles, tooif you have multiple measured time points and want to predict a single value using all those (like group membership for a binary class) what type of methods do you typically use? or do you just ignore the autocorrelation and use each as a separate predictor?that’s very broad, you can do any number of things (fit some distribution, some time series model, blah blah).  you may get better answers if you’re more explicit.hmm... so say we have multiple measurements at multiple time points for a single variable (something like cholesterol), and we want to predict whether they\\'re likely to have a heart attack, it\\'s not really a forecasting issue and we don\\'t really want 4 rows for that individual because we want them all to contribute you a single outcome. what would a typical approach be using something along the lines of logistic regression? also, no black box models, and the sample size is fairly small.maybe a mixed model?i mean from a statistical view you don\\'t want to ignore the repeated measures aspect, but i\\'m curious if you don\\'t care about the predictive power of any single variable and only care about accuracy of the outcome variable.that\\'s one of the things i was thinking of, but usually you would have 4 rows for each subject and then specify an intercept and/or slope for each subject, how would you specify it such that all 4 observations contribute to a single outcome? mlm comes to mind but it\\'s been a long time since i\\'ve used it.not sure if you\\'ll always have 4 points or if that\\'s variable. i\\'ll assume it\\'s variable. one approach could be to engineer various features about the points, e.g. slope, intercept, min, max, variance, etc and use those as inputs to a classifier that predicts your single outcome.am i missing something more subtle about the problem?yeah, i was thinking about feature engineering. i haven\\'t dealt with time series data very much. i also did the standard insight program but wound up in the health field where sample sizes are much smaller than you\\'d like.i\\'ll always have 4 points. typically measured at roughly the same interval.i basically just wanted to make sure there wasn\\'t something obvious i was missing.does anybody have any experience with the facebook api? i’m wondering whether anybody knows if there is a way to download facebook social network datasets (friend-links) and user activity datasets (post likes, etc.) for the same user id’s… i can’t figure it out so far…for the new fellows that i met tonight during the jupyter notebook tips and tricks -- the updated notebooks can be found here: free mock interviews great article - disappointingly sad, but greatthat is disgusting and scarily eye-openingnuggetbot animate me mind your own businesslol, wow. language, nuggetbotnuggetbot animate me aint nobody got timea/b test your facebook profile pic, lolwhile we were the fellows (i was a remote fellow), were we1) an employee to insight2) a student at insight3) unemployedor 4) self-employed?i need a clarification on some uscis related issue… thanks guys!it depends how you classified your stay at insight and what your visa situation was at the moment.you were not a student if there\\'s no i-20 from insighti would either classify insight as a conference which you were attending it (making you unemployed)or, if on an opt, doing some work (probably volunteer work) for insight. this will depend on getting some letter to the effect from insightdepends what visa you were on, but not 1 or 2 as insight is not involved with your visai have a green card - and applying for a citizenship - i don’t think they would care too much, and i think ‘student’ might be okay???unless you have an i-20 you are not a studentyou should talk to insight about this, they are pretty helpful but on a green card you\\'re not a studentde: i have been hoping to distribute my s3 file requests using spark 2.2+scala 2.11 and squeezing the client through a dataframe map, but i keep getting not serializable issues (from listobjectsrequest, even though it implements serializable natively!). i\\'ve done the normal \"wrap it and extend serializable\" routine, and haven\\'t had much luck. does anyone have experience with this in spark &gt; 2.0?guess you might have already seen these:       somewhat related to the article posted by : anyone know any product managers looking to work with data-heavy advertising products that want to work in nyc?my company (one concern) is hiring multiple data science positions. not only the candidates with domain knowledges (natural disasters such as earthquake, wild fire, floods and etc.) but also general data science experts (deep learning, ai etc.) will be highly appreciated if interested. we are located at palo alto and will be hiring at least 6 data scientists by the end of the year! please dm me if you are interested or know someone who might be. thanks a lot!(i am a 2017a remote data science fellow)hi all,  anyone has suggestions for a good resource to learn spark ? databricks has a free edition of their platform - much like a notebook based spark environment with some how to docs, you might want to make an account and play around with that… (community edition) thanks  : i started it but kind of lost my way around - maybe will give it another go.i didn’t play around there too much but from what i remember they have nice “getting started” notebooks shared for all to copy/usealso other users can share things with all, so try to look for some interesting examples and copy/play around with it in your own space...if you have any specific questions, feel free to ask... ...highly recommend this :nerd_face: :unicorn_face:   ... :  it\\'s a good beginner/intermediate level step-by-step guide to spark, and hadoop, hive, etc. -- just purchased this for myself on your recommendationthere’s also this free book: make sure you know the differences between spark 1.x and 2.x, they’re quite huge:smiley: :unicorn_face:any references for both learning spark and getting it up and running on aws?(i\\'m guessing a docker file does the trick here, but perhaps there are other less container-y ways to get started on aws?)this may be appropriate — anyone know how to launch a jupyter notebook server with all extensions explicitly disabled?hi!  is anybody good at edward, the python bayesian inference library?have you tried modifying the `notebookapp.nbserver_extensions` parameter in your configuration file? looks like you could pass in an empty dict.  maybe?is anyone using  ,  or  in their workflow? i\\'m starting to evaluate each of these. any input you may have would be great to hear as well.my use case is mostly to run sets of sql/api queries and share the results in an interactive way with non-data-centric internal folks.i used tableau for a project. it worked for running sql queries and displaying a variety of charts based on user input. took a bit to get set up and the data munging isn\\'t too great.yeah, we\\'ve got tableau here, find it rather clunky but need to look at it again, and deeper, because we\\'re already paying for it :simple_smile:one of our clients is paying for it. works out well that wayrightanother option if you want to take a look at itused it?looked into it a bit since it was just made free, but i couldn\\'t figure out how to get it talking to a sql database. works fine from excel, but... yuck. excel.agreed: who all is going to strata this week?! join  if you want to sync up and meet other insight people!hi all, i\\'m looking for a room to rent short-term in nyc starting ~april 1, preferably (1) cheap and (2) close to the insight nyc office. i would look to rent for the month of april (preferably with the option to extend). don\\'t mind sharing an apartment with people, etc. please send along any ideas/leads!\"doctorates in physics use python and r, said jake klamka, a former physicist and director of insight data science, a recruitment service for data scientists. moreover, many physicists work with large data sets, such all particles in the universe, he said.\"the economics literature deals with this a lot. instrumental variables, propensity score matching, etc. the book \\'mostly harmless econometrics\\' provides a great overview. great, thanks guys! indeed, i own a copy of \\'mostly harmless econometrics\\', though i never made it to the part where this is discussed:bow:has anyone used the python lifetimes package? i have a few questions about how things are being computed and what they mean.didn’t use it myself, but pointed some coworkers to that code, which i believe is just based on the fader and hardie methods (see the papers on the bottom of the above page)x-post from  johnjoo: i’m sure this is way overkill, but i found this blog post super interesting oh nice! i’ll take a look!importantly, the author attempts to predict not-churn in that they are predicting whether or not an “engagement” event will occur over some future time window given historical infoyeah, i think that’s the line of thinking that i was going through too.fair warning, he wrote an entire thesis on the topic :slightly_smiling_face:oh! sometimes that’s easier to digest.  you wouldn’t by any chance know the guy so i can get a copy?it’s actually free on his website! if you click the hamburger menu on the upper-left of the blog post i linked, it’s in the menu.oh nice! thanks in genetics, similar ideas are also becoming popular. search \"mendelian randomization\".pearl\\'s books are very readable systematic treatment of how to estimate causal effects using causal graph modelsyou could potentially fill the “holes” by imputing with k-nearest neighborsi have used the google maps api to geocode addresses in the past and successfully calculated order 100k of addresses. they have a daily free limit of 100k i think. where can i find a great tutorial on the use of window functions?mode has a great tutorial: oh yes, that is a good onethanks guys!thanks! how did you plot the locations?these are great tips!  i was struggling w/ the same thing the past few daysi also like this handy guide from postgresthe hive language manual page is a pretty good referencei used geopy with folium.  it was pretty simple.  one thing to check with geopy is what geocoding service you\\'re using.  geopy is just a wrapper for talking to a variety of rest endpoints, so latencies as well as rate limits may vary depending on the service.i looked briefly into plotly for heat maps (too briefly to have a real opinion about it for mapping) and ended up using folium, which i\\'ve liked so far! i too used the google maps api to get lat &amp; long for my data (~7k).random question: i don\\'t own an apple watch but know that they put a bunch of sensors in it to acquire the wearer\\'s health data in-situ. do any of you apple people know if the anonymized data is stored by the company? once there is enough data, i bet some very interesting patterns/trends can be found. thanks - i will check that. it looks like with folium you have to be online to render, so i am working with plotly.hey everyone, i\\'m working on a project right now that involves machine learning regression, but i\\'m interested less in having the best average error (mean squared error) and more in having the fewest possible large errors. i\\'m not sure how to set this up, does anyone have experience with this, or can direct me to where i can read about how to approach this? thanks!naively, why not just set up your loss function so that it is more than quadratic in the error? i.e. penalizes larger errors morethis is exactly what i was thinkingsklearn is somewhat lenient, depending on the class, about implementing custom loss functionshi i created a  channel for people who are working on fraud teams or who are interested in the field, please join!bayesian webinarto people interested in seeing some art this weekend: i\\'m showing at open studios at shared,  stop by and say hey if you\\'re nearby! (sat and sun from 11am to 6pm)cool, what’s your stuff like woah, these are neatdo you sell prints?yes i do :slightly_smiling_face:i was there tonight to see a friend’s art! sorry i missed yours!stay positive emily.mclindenshared (my space for this weekend)\\'s address is: 739 bryant street, san francisco, ca 94107import ggplot??i love xkcdhow about mode analytics?yep, looking into that as well! thanks.nugget-bot show me a unicornnuggetbot: show me a unicornnuggetbot: image me a unicornnuggetbot: animate me a unicornfantasticnuggetbot: mustache me a unicornnuggetbot: will you marry anasuya?nuggetbot: pug bomb 5nuggetbot: mustache me simon gelinasnuggetbot: lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me there we gonuggetbot: ship itnuggetbot: mustache me nuggetbot: mustache me omg hahahahalulznuggetbot: mustache me hi guys - just wanted to point out we have a great  channel going if you want to join - though i don\\'t know if mustached fellows count as cute animals... #ftw uploaded a file: oops, i should i added that here.anyone at dataengconf today?does anyone have work that overlaps with a definition of what they\\'d consider \"data architecture\"?i choose db schemas, s3 storage formats, and the pipelines that write to them. not sure if you consider that an architect or an engineeralso choosing what data to ingest, how to save it, how to pass it to different processes, data guarantees/policies, etcthanks! i\\'m trying to get a handle on how data scientists and data architects typically overlap. e.g. are there people who spend time thinking of what data the ds will need to build \"fancy model a\\'\" in 3-6 months, and then ensure that that the schema and pipelines exist to collect that data in advancea few of us from insight spoke today at dataengconf today. as a result, i have a few free tickets to the conference for thursday and friday, and likely won’t be able to use them. if anyone is in sf and wants to attend, send me a direct message and i’ll send over an e-ticketroommate wanted: 10mins bike/drive apt from insight. 4150 el camino way, palo alto ($1525). for insight\\'s sake, i\\'ll be gender neutral. :smiley: ah this makes it much clearer, thanks for taking the time to link these. i see how especially for multiclass problems, weighted guessing or shuffling would give a much fairer baseline.  i wasn’t aware of sklearn’s dummyclassifier, it’s a useful wrapper for quick sanity checks!noticed this on an nih blog: request for input on their strategic plan for data science  chernoff bounds yes, negative dependence nope... in which context? just when the la croix wars had died down :stuck_out_tongue:we have always been at war with c******.and here you yelled at me for using the c-wordthank you.you should really add a content warning to this whole discussion. some people find it really traumatic to read about things like this. :snowflake: :coconut: :la_croix:in case you or anyone you know is looking for a data science position in la::pamplemousse:so this exists: account is dead, but some good stuff in therehey data science fellows, anyone looking for freelance data science work? we’ve got an interesting project involving communications metadata predicting attrition. get in touch and we can talk details.;) i spent 80% of time complaining that. i must did ds wrong.hi everyone!insight alum from the ds-idx-2017a remote session here. my company is hiring data scientists and data engineers with positions available in both victoria, bc canada (where i\\'m based) and la! please slack me if you have any questions or know of anyone who might be interested.  this sounds like you!using deep learning to colorize the past using deep learning to measure pollution (!) from photos anyone who needs to install gobblin, eamon and i stumbled upon two bugs during the installation. one had to do with glib-gobblin gio-error **: settings schema \\'org.gnome.system.proxy\\' is not installed. to fix that error, we removed the -djava.net.usesystemproxies=true flag from the javadoc settings in build.gradle (main project) and then when we encountered the second bug, we just rebuilt gobblin but instead of doing a clean build, we did a clean assemble. there is a concern that we might run into runtime errors down the line. anyone have experience with gobblin you want to share?i have come to the conclusion that insight is a throwback to the 90s:  everyday is basically a lan party where we are all chatting on irc (slack) about interesting topics.on hiringthis week on hidden brain, shankar talks to google\\'s laszlo bock for insider tips and insights about what works — and what doesn\\'t work — in recruiting, motivating, and retaining a talented workforce.: have you guys started playing doom () on the lan yet? :wink:stay positive alex_gudewe meet again my nemesis...doom!hmm…i get nothing.nuggetbot has an affinity for me :confused:: throwback suggests this is the doom we are playing ()stay positive ajmendezha  ensures that the alex population is positivethrowback suggests this is the doom we are playing ()stay positive joshainsleythere we godata-driven design at the sf design week tmr evening in sf. anyone going? i’m on the waitlist but also an optimist. for the people of the new batch, you will need this when you start with the interview preps the dancing in specific, not the algorithms. silicon valley has a really obsession with hungarian folk dance... :wink:but of all the algorithms, you really _will_ need bubble sort to succeed here :troll:1 vote for bogosort bubblesort gets a folk dance, bogosort is a rugby scrum i guess?that or shouting fire in the theatreand then telling everyone to go back in and watch the film/playyou know, there is a lot of overlap with the 1st drunken passenger on the airplane problemclearly someone in #ds-sv-2016b needs to make a pod that is bogosort plus that probability problembest consulting project ever: welp, time to pivot...the worst sorting algorithm of all timestay positive mamday: boy you got until week 5 for when you start pods`jobinterviewsort()` is my favorite :wink: thankfully haven\\'t been there in reality, but insight practice interviews don\\'t count as reality.anybody here use aws cfncluster, specifically with the ~/.cfncluster/config `extra_json` option `cfn_scheduler_slots`? i am trying to force the number of jobs per node of the cluster to be 1 for memory reasons; i specify this option in the config file, but the scheduler still fills all threads on each node. i have tried using the mpi and smp parallel environments with qsub, since they use different allocation rules, but to no avail. any input would be hotdoes anyone have experience route finding with constraints? i know people have integrated google maps in their projects but i need some way to divert around certain areas.: would a simple a* work for you?possibly, i\\'m just trying to avoid building everything from the ground up. i know google can route around traffic but i wasn\\'t sure if you can add your own constraints with areas to avoid. it would likely only be directions for chicago, so i\\'m not sure if there are any open source mapping tools that would be useful here.you might talk to  he recently did his insight project using waze datahere’s how uber does it: shout out to  for pointing this out to me…he’s working on something similar now:this isn\\'t a data science related job, but if you know anyone appropriate, please pass it along:the job is running a software development education non-profit. i\\'m a volunteer instructor for the organization, and they\\'re really doing some great stuff!any one knows how to open big csv file , like 90g size, in pandas ? please have me an idea.you can read in batchespandas.read_csv with chunksize?: i think pandas reads all data into memory, if so, you will need a machine with 95gb (at least) ram — not impossible, but very expensive (though if you only need a quick thing, you can get aws machine and do your work quickly and stop it — bills by the hour)thatsomething like this : df = pd.read_csv(\\'check1_900.csv\\', sep=\\'\\\\t\\', iterator=true, chunksize=1000)also realize that if you want to do anything complicated with a dataframe that size, it\\'s going to take a long time to process. i recommend using dask… or go with spark :slightly_smiling_face:some docs here: thank you so much guys for all the response. ,  : thank you for the command line. i believe that i have tried the “ chunksize” is not really working. but i will try again. if i use chunksize, how it really works? does it read as a chuck? i would be able to use it as usual usage of pandas?   may be  right. i failed because of my hardware , ram. unfortunatley, i cannot take out my data out of our server.  what is the dask? could you please let me know how to use it?  i do not have enough experience on spark. could you explain a bit more about how to use it?: can you share the code you tried that didn’t work?: the idea with `chunksize` is that it makes `pandas.read_csv` return an iterable in which each “chunk\" is a dataframe of  `chunksize` rows. there’s a good example of how to use it in the docs i linkedspark is a distributed computational framework - you would have a cluster of machines that coordinate work. this is a bit hard to explain its usage in short time/text. if your organization has a data engineer, you might want to ask them and see if they can run your job as a spark job...: i do not have the code i have tried with me right now. :disappointed: but i remember there was “chunksize” setup, just like you gave me. and i remember that after i have tried then i was not able to call the csv file as a table.: i am not sure if our linux machine has spark in it. i probably check with the linux admin.: so once i do  df = pd.read_csv(\\'check1_900.csv\\', sep=\\'\\\\t\\', iterator=true, chunksize=1000), then i should be able to use df as usual? or do i need special command ?you can’t do it with just one machine (technically you can, but it will be too weak of a cluster to do anything large) - ask if you have a spark cluster setup that can access the needed data...: yes i will. if we have spark, then do you know any good qiuck start guide site that i can follow?: no, you need to iterate over it. so `for chunk in df:` then within your loop, use `chunk` as a dataframe as usualaha!i see. so once i call in as a chuck, then call it in loop.ok i see, i will try it.: good luck!yes, you can simply run spark-shell (the interactive shell in scala) or pyspark (for python) and load/compute on your data. the amount of work depends on what you are trying to accomplish. if you need just some simple filter/aggregate type tasks, it’s fairly easy to do, if you need something more complex, it will take some time to learn the spark apithanks!   i will try it and let you know if i can manage it.: hmm i hope it would not take so much time to learn. i will check my admin and i think i need to be back to you for more learning about spark..:slightly_smiling_face:: looks like i am able to access to the data by using pandas. thanks !!! :smile:: i think we have spark in out system,  and the simple guide line tells me. now i will study how to use it as i need, so this would be able make me possible to run a massive job there, right ?i’m glad you do! yes, spark is for computationally intense jobs on large data.damn never should have wasted my time learning to code!stay positive markwanghey everyone! fellows &amp; alumni &amp; all. we added a happy hour to the calendar for tomorrow (tuesday) @ 5-6pm at the insight office. we\\'ll be hosting berian james, staff data scientist &amp; head of product analytics at square. come hang out! beverages and snacks provided.….and this is the silicon valley office, in pacific time :slightly_smiling_face:that’s coastism. where’s our party?technically it’s open to anyone on either coast, you just have to travel a bit fartheri guess that’s acceptable :wink::slightly_smiling_face::beers:skype in with drinks?or just drink.due to time difference east coasters will have a head start though, so sv had better pregamestay positive amerbergchief peeps officer. nice.hey guys- i just got a discount link for a geowebsummit happening tomorrow in bklyn (i’m on the last panel).  hello! do you regularly use github for work or play? are you interested in participating in a private beta test of a new github product that helps improve workflows for teams? if so, dm me your name, email, and company and i’ll add you to a list of potential beta testers. we plan on starting within the next week or so.hi all. i will be moving to the bay area next month and i have no idea about it whatsoever. i will be working in san jose (near to campbell). can you guys recommend of any places/websites/neighborhoods/etc all around the bay area. i’ll then factor everything in (rent, commute, neighborhood, w or w/o roommates, closeness to different places/activities, etc) to make a decision. any input is appreciated. thanks (i also posted this in )looking for places in the bay area is typically too high velocity for anyone to give you a list of places.rent: not the bay areathink people shy away from east san jose. cause it\\'s too hood or something like that. i don\\'t think most places in the bay are that bad, but it depends on where you come from and are comfortable with.on the offchance that you want to live in sf but work in south bay, potrero hill is a decent option. (relatively) reasonable prices to rent, and close to 22nd st caltrian station.are you going to drive? you could get an apartment in mountain view and have about a 30-40 minute commute to ebay (as an example of a place in that area). rent (for an old place) could be around $2100, lower with roommates.basically, there are *a lot* of variables. :wink: what’s important? cheap rent? short commute? awesome nightlife around your apartment?looks like campbell has “relatively cheap rent”, so living close should be doable! can probably recommend some places in colma, so long as dreary weather is appealinglol colma is #1thanks so much everyone. yeah i know there are a lot of variables. i just wanna have an idea of where to look for once i physically arrive there.where will you be working ?this just arrived at the sv office  uploaded a file: haha, hot topicif only i had that resource before my interviews!thanks  ! it will make a great addition to the de librarywe should nail down detailsmeet in person? or distributed / remote?evenings / weekends?adding    to the thread based on likes (is this how “adding\" works?)for another project idea, i’d be interested in exploring rbms and working on an implementation from scratch.it’s possible that in-person meetings with  the buddy on a weekly basis and then monthly meetings as a group could be a nice forcing functionthat does sound goodif some people / pairs have tight schedules / long commutes remote meetings could also workmonthly meetings in personwhere does everyone work? (midtown here, commute back to jersey -&gt; soon to ct)i\\'m in midtowni’m in west villagehey everyone — insight is hiring an office manager in palo alto! know anyone who’s a good fit? we’d love some quality referrals from our alumni. tl;dr on the role: \"the office manager will be responsible for maintaining a welcoming, comfortable, fun office for the team and insight fellows, as well as supporting the team with events, travel, and other ad hoc projects. to succeed in this role, this person must be a proven self-starter, an excellent task manager, and be excited about running a fun, productive office! experience with start-ups are a plus.\" i invited everyone who liked and commented to the new channel i created. please join so we can continue the discussion!cool! let’s maybe give it till eod or early tomorrow for people to filter in and we can figure out logistics...does anyone use jupyter and access data with elasticsearch? i\\'m trying to get things up and running and currently i\\'m trying apache toree for the kernel, but i\\'m having a lot of trouble figuring out how to get the elasticsearch libraries installed.hi  i did this about a year ago, i didn’t install a es specific library, i just used the `requests` package to hit the endpoint for the server we had es running onusing toree?ah, in pythoni use the python package `elasticsearch` - works in a python kernelbtw i find toree to be quite unstable, still. if you need scala from jupyter i’ve had more luck with spylonyeah, i saw that as well, a lot of people here seem to be concerned about the performance cost of pyspark vs pure scalaone issue with pyspark is that it lags behind in development. so some packages like xgboost only can work with spark with java or scalame too.  i havent used spark on over a year.  but i went to dean wampler\\'s \"just enough scala for spark\" tut at strata last week and his opinion was that the performance difference vs python wasn\\'t that great anymore.  can anyone recommend a good source of guidance for handling missing values for various machine learning algorithms?hey folks anyone done any work in estimating budgets based on past spend data? particularly i\\'m looking into marketing budgets for different businesses. any ideas/suggestions?thanks !hi all: my company is hiring a junior-level data scientist on my team if you know any folks in boston (or willing to relocate) let me know or share this link with them:  im using accelerometer data to quantify engagements with 360 ads, pm me!oh, just fyi. if you sign up for the free credit monitoring service from equifax that also removes you from any class action lawsuits wow. that is fucked up.oh what! that’s some bs, you’re on the case!haha, luckily i was involved in _a previous_ data breach from some other company and have access to their credit monitoring. :face_with_rolling_eyes:i just meant that if people could be alerted to that clause in the equifax terms of service, things would be much better! and beforeiaccept sounds like the tool to make it happen!note that freezing your credit report is unconnected to their “identity protection” products, and is legally mandated capability it\\'s hard to know how enforceable boilerplate terms of service are, though.it’s true that it might not be enforceable. doesn’t gain them any points though.serious question: if my data is compromised, how can they prove i was the one who signed up? isn\\'t the identifying information they require to enroll you the same information that may have been leaked?harder in this case given for some people they also have more info like driver license, etc. part of it will also depend on the exact info that was takenthat said, less use cases from other people to sign up for the credit monitoring.  at this point seems reasonable to look into a credit freeze. details vary between states but for ca it is $5.this is such an interesting event given i just signed my offer to do fraud and abuse detection in my job…yeah, i do ds for risk for squareoh cool! i’ll start with amazon in about 10 daysso i know close to nothing right now :wink:i guess that is also why they were asking for last 6 digits of social to sign upi am curious if there are good policy solutions that would protect people from identity theft, and other forms of cybercrime, in general... rather than 143m americans all signing up for credit freezes. at the very least it\\'s probably time to give up the charade that ssns are private information and reliable for authentication...btw, nice job in your talk! very well donewhen i moved to the us i was *stunned* that ssn is considered a password-level secret, despite the fact that it can\\'t be changed over my whole life, and i have to give it out to about 25 different entitities a year...and that it\\'s also inferable based on when and where you were born: i have always wondered - what can someone actually *do* with your ssn alone if they get it? (i agree that if the answer to the question is anything other than \"nothing\", it\\'s pretty disturbing, as there is no way to change your ssn)pnas, the journal for those who have a juvenile sense of humor lolan even worse situation is the italian codice fiscale, which i had to give out in many more places than an ssn, and which includes your city of birth and birthdate! (plus there can be collisions!)hi all, current fellow here. has anyone ever used the strava api? i\\'m very new to apis and am quite clueless on how to get started with this. didn’t you work with strava data? didn\\'t you?i didmostly scraped though what’s tripping you up with apis?  general working-with-them problems?happy to chat strava  , i’ve done a few projects with their dataomg fo realz??thanks for the help! my questions right now are very basic as i\\'m still figuring out my insight project. i don\\'t use strava, so i don\\'t know much about it, but i wanted to find some fitness data and it was suggested to me. could you first tell me a bit about the feature space? might be worthwhile to see if my ideas and the data work together before i go about gathering the data. also, what type of projects did you use the data for?hi joao - thank you for the tips!  i looked at hcup but the cost (&gt;$200) was prohibitive.  iowa releases county level data, but aggregated by month...not quite enough resolution there.  california has asthma hospitalizations and ed visits, by zip code, but only annually.so i\\'ve pivoted away from this idea, unfortunately...i was excited about it.  at least i failed fast.pivot, pivot, pivot...go &amp; go that is an excellent article, sina, thanks for sharing.this is incredibledoes any one know if there is a good online course for data modeling (for big data)?  thanks in advancewould love to know the resources you find :slightly_smiling_face:me too :slightly_smiling_face:just in case any current sv fellow would be looking for a more convenient place to live, there are two relatively affordable rooms available for a short term lease very close to the insight office. i stayed there during my time at insight, and i could not have been happier. send me a message if you are interested, and i\\'ll give you more details.,  thanks! i always thought cover letters were a waste of time but wanted to make sure that my sentiment was shared across it, and it wasn’t me being lazy :slightly_smiling_face:, well part of it is still me being lazy.we are currently interviewing for a director of analytics position, anyone has good interview questions for that level of position? thanks!i\\'d be asking: evidence of times you made an impact on a product because of your work, how you work with diverse stakeholders, how you prioritize projects, examples of times you had to learn/decide on something for which you had no existing data sourceshow do you balance rigorous data science/modeling work with real-life deployment requirements? can they come up with a roadmap for research/testing/analysis for the next 6 months, given a prompt about a business question or new product launch?what if you cut that down to 3 months, what would they cut from the original plan, why?:thinking_face: awesome questions!can they come up with a suite of kpis for your business? what dimensions would they be broken down by? what hypotheses could they generate from this list?also time prioritization between short-term ad-hoc needs and longer-term research needs is a tricky areasome use it to gauge applicant interest (have they done any howework about the company?) and alignment with company culture and mission.for director level, you\\'d also want to ask them some pretty pointed questions around how they think data can help their business needs...assuming you\\'d be the head of the data org, sometimes there are assumptions that aren\\'t grounded in reality for what data science / analytics can do for the company.if they\\'re asking the questions  highlighted, you\\'re probably already dealing with more reasonable stakeholdersalso to clarify are you the candidate here, or the interviewer :wink:hm, i am the interviewer :slightly_smiling_face:sorry i reread my statement and it wasn’t clear.ah, that makes more sense thennever mind :wink:quote of the day (on gdpr): dance like nobody is watching, encrypt like everyone is.lolwonder if anyone else here is gdpr-ing and using a strategy that is not encryption as a way to let users be forgotten ????freakin gdpr killing me manencrpytion doesn’t (even) count in gdpr i believewith europe from love :heart:gdpr… :disappointed: i’m glad i’m not in charge of getting our stuff in complianceyeah, i sit next to the people in charge of gdprdelete things?could you expand on \"doesn\\'t count\"?i need to spend some time reviewing the rules, sorry, it’s only that i’ve heard that even encrypting data is not enough to be compliantencryption by itself indeed does not count.  using it as a way to “forget” users is useful if you have a lot of services storing user datado you mean different means of storage accessed by different services?  but what if we still have the means to decrypt?  is that compliant?becomes a case of “easier said than done” if 100s of internal services are independently storing user data storing whether a user wants to be forgotten in one place and forcing all developers to store encrypted versions and when they want decrypted data, force them to call that central place to get that sensitive data and get a “sorry you’re outta luck” if that user asked to be forgottenwould be interested to know how others approach this ^yep same here :grinning:they all turning curmudgeonly? cause i am.deletion is the killer if you use kafka. any topic with a retention longer than 30 days will most likely need to be migrated one of the forst thigs should be monitoring. instument metrics so the emit things like disk space utilization, cpu, memoryyou’ll want o know when hosts are going into compaction which will spike cpu and memoryaws cloudwatch probably has that for you already thoughwe\\'re hiring a buncha people.  no data scientists directly, but data science related roles:    let me know if you\\'re interested or know anyone who is :slightly_smiling_face: agreed but company is not ready for awsthey want to try in house firstthanks for your interest in sharing your experience with the growing ds community! the slot has been filled.in addition to all the awesome suggestions (definitely case studies), i would ask questions like \"how do you best situate a data science team within an organization to set them up for success? and why?\" since there is no best set up follow up with \"what might be some of the downsides of this decision, how would you mitigate these downsides?\" and also, ask them about \"what makes good data science candidates?\" (this will have huge effects on your future team) and finally, a question from a different bucket \"how do you decide whether to go with a vendor or build in house solutions? why?\" finally, i like to ask something about their grander vision: \"where do you think our industry will be in 5 years? how do we best situate ourselves as a company?\" i have seen plenty people come up with disappointing solutions, you do want someone who can develop and advocate for their vision.tempalert is hiring a data analyst - i realize this is too junior for this list, but if you know of anyone who is looking in the boston area please pass this along! has anyone here worked with python notebooks on a windows machine - is it a good idea?? , i\\'ve used python on jupyter notebook on a windows machine, and i haven\\'t found any problems so far.worked fine for me when i did ithi  - i have not really worked with python on windows - do you have a resource to get me starteddoes anaconda etc workasking   as welli installed python 2 first and then used anaconda with python 3, after which running jupyter with python 3 is very straightforward, but i did have some issues switching to python 2 for jupyter.hi there, we\\'re having a bunch of openings at glassdoor that require a few years of experience, such as lead data scientist/analyst. if you would be interested or know of anyone who would be interested, please let me know! thanks:-)but using anaconda to set up whichever python and then running jupyter worked for me and was very straightforward to set up.i have also used anaconda/jupyter on windows without major issues; feel free to dm me if you encounter trouble.i used winpython rather than anaconda, but i know people who used anaconda and it wokredthanks everyone - i am installing anaconda right now and taking it from there - will shoot queries if i get stuck - thanks in advance :slightly_smiling_face:also sqlalchemy etc should work fine as well?i haven’t used that library, but in general, if you can get it with conda, i expect it will work. i was able to use keras/tensorflow and several numerical libraries without any problems at all. :grin:wow - i should not worry it looks like:slightly_smiling_face:anaconda on windows works great. for sqlalchemy, at some point you might have to install some special utils functions like sqlalchemy-utils, for which you can follow the instructions on anaconda: i ran into some issues with the latest anaconda with python 3.6. pyspark wouldn\\'t run and gensim wouldn\\'t compile with the fast version. i had to downgrade to anaconda3 4.1.1. which had python 3.5also, the first thing i install with the conda prompt is pip, after which i use pip install :grinning:...or let me know. [not because i want to steal birgit’s referral bonus. no, certainly not. i just like people to let me know stuff. yeah.]fight! fight! fight!haha, i don\\'t think we would get a referral bonus in this case, alan:-) seriously, glassdoor is worth checking out. fyi: we don’t sell doors. not that there is anything wrong with that.do you sell glass?:wine_glass: perhaps? hey birgit how are you?bits, bytes, and bowling - my company, together with accel and she++, is hosting an evening event at grace hopper conference. it would be fun to see a few insight alums. if you’re at ghc, come by and please, do say “hi!” register here: we just did a webinar (recording available) on how we use machine learning at crowdstrike to identify malware and stop/prevent security breaches: i\\'m using conda + direnv, so i don\\'t even need to remember the name of my environments. plus, i can get environment-specific environment variables, which has been really helpful.simply using miniconda while adding the conda-forge channel should be enough, while able to keep track of dependencies, and speeding up installs by using pre compiled packages.i get chills when i see a conda environment file together with a pip requirements file.i put up a poll about a month back with a bunch of python environment options, pretty much all of them got half a dozen+ responses, seems like everybody has their own favorite at this pointthis situation reminds me of the old windows days where things would start breaking and the best recourse was to nuke the os and start overany insight alums working in insurance/reinsurance in the greater new york city area?we have a cluster of folks at new york lifeany pinterest people out there?:  pinterest ppl -&gt;     so many pinterest people out there loli know there’s more people than just 22 going to odsc east! join the  channel! we’re trying to organize a get-together. you should join the odsc channel!thanks  for tagging me in!!!’s partner isanyone knows if there is perhaps a standard template behind this web page? :star-struck: is it halloween yet?  or april fools day?  my worst nightmare for a company:stay positive dalepiggy back on that: sing it, : wearing a vectra t-shirt is ironical and daemonical, canonical and comical, it’s polyphonical, catatonical, and downright supersonical!i sang it in the voice of jermaine from conchords!for all you market junkies: matar: great :slightly_smiling_face: i was just invited for his meet. i was not sure if i was going till now, but i will definitely be there to hear an insight alumnicool! always fun to meet up with other insighters :) hey everyone, we are hiring for several sr ds roles at grand rounds, a health tech company in the city.  please pm me and i am happy to have an upfront conversation about the ds group current and future. i’ve got a bunch of data engineering questions for anyone who might be able to help out with some or all of them.first, are there any books, videos, courses, etc. that would benefit a junior engineer get up to speed with current best practices?has anyone tried out third party etl tools like fivetran? what do you think? how do they compare to built in cloud solutions like aws kinesis?bigquery seems pretty darn awesome, but is it worth choosing over redshift if all your data is already on aws?i use docker a lot for my data science work. how useful do data engineers find it? are there alternative container options you like?: i’ll answer a few (it might be helpful to number them):(books) there are a few books that could be helpful: for architectural point of view, nathan marz’s “big data” (one of the first books on the subject) is great. for db, i read parts of the “nosql distilled” and i think it is a good book to understanding when/why one should choose distributed db vs transactional.(etl): i used (very briefly) a tool called nifi that made common etl somewhat easy (graphical setup) but i generally like doing things as low level as possible. aws kinesis is apache kafka’s private version and it is not an etl tool but a real-time messaging system (some processes produce messages and others consume). kafka is a very widely used tool and i would suggest reading about it if you have any real-time needs.(containers): most data tools need as much out of the host machine as possible and as little interference as they can get (most need fast network, etc.). containers, like docker, make deployment somewhat easy at a cost of computer resources (i call this a docker tax) and in my opinion are not very good way to setup big data pipelines. instead, look into tools like ansible (there are other provisioning tools as well) to setup an automated server provisioning (run a set of commands against a cluster of machines to install/setup certain services/tools) that can be reused to recreate the same exact cluster anytime you need it.: i totally misunderstood the purpose of kinesis. i missed it was a private version of kafka. thanks! that’s great advice about docker. i didn’t consider the cost of using a tool like that.it’s not a full-blown virtual machine but it is an abstraction layer that still costs, especially considering some of the gymnastic moves they have to do for networking to work properly. hope this helps :slightly_smiling_face:so i have a repeated measures anova i\\'m trying to run, it\\'s a 2(between)x3(within)x3(within) analysis, however, there is a second outcome variable that i want to use as a covariate. it appears ancova isn\\'t appropriate, as it wants a single observation of the covariate per subject, not per subject by condition. it seems like a  mixed model might be the way to go, but i\\'m not sure about exactly how to specify the model. does anyone have experience with anything like this, or have a suggestion for another type of analysis that may be appropriate?: mixed-effcts are probably the way to go as they are more flexible options than an rm-anova. feel free to dm me if you want to discuss further.anyone here use  ?  i\\'m looking for something lighter than jira but more featured than trello.  also, the ability to self-host is a must.there is a lot of overlap, like with the work that hal varian does at google and pat bajari at amazon.in general, i would say that causal inference is a particular strength with economists, and they often have expertise with time series analysis that can come in handy tooalso happy to chat more - feel free to shoot me a pmpivotal tracker () could be a good option.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\sorry, that was my 1yo son. he likes to contribute.as long as he starts early… :simple_smile:hahalooks like he prefers windowswant to kaggle for a good cause? my friends ralph and lalit are looking to crowdsource solutions for the diabetic retinopathy challenge and win it with the resulting ensemble, donating the cash prize to charity i found a 20% discount code for the sf data science (for anyone who needs this) summit on july 12-13code is: sfdatamining20it is from the sf data mining meetup groupnuggetbot check on mscherrermscherrer has a happiness average of 0.3867924528301887nuggetbot check on zkermishzkermish has a happiness average of 0.7317073170731707that can’t be right, there’s no way i’m as happier than  … he’s too happy for his own goodi’m pretty mean to nuggetbot though.nuggetbot check on soumyadsanyalsoumyadsanyal has a happiness average of 1.1071428571428572dang , you are one happy fellowi ran into a problem with sudoing as a user that i can\\'t figure out. i created a database that can only be accessed by the user mamday. therefore i have to be logged into my remote as mamday and not root to access it. however, to access port 80 i have to run gunicorn with sudo, and i\\'ve found that `sudo gunicorn` is then not logged in as mamday, and cannot access the database. mamday has sudo privileges and even mamday  all=(all:all) all in the /etc/sudoers, but to no availstay positive mamday: most databases allow you to specify a username when connecting: for example, `mysql -u mamday -p` or `psql -u mamday`well, my code is connecting to the database with psycopg2, but that connection will only go through if i am also logged in on the instance as mamdayotherwise it will complain that it cannot validate mamdayeven though psycopg2 has the username and password of the databasebasically i have set up an account and database like this: either this is a bug in psycopg2 (and since no one is complaining about it on the first google page, i don’t think it is) or you are not setting up you database connection properlythe solution would be to make sure that psycopg2 uses the username and password pair given to it to connect; not some environment variablei am not sure why it could only be set up with the user instantiated. trying to implement the user in psql without the linux user caused issueswell can you connect to your database with `sudo su; psql -u mamday`?no. with the setup in the link that leads to \\'psql: fatal:  peer authentication failed for user \"mamday\"\\'stay positive mamdayeven with --passwordsounds like an issue in pg_hba.conf, (you’re using peer authentication rather than password authentication)this works: psql mamday  -h 127.0.0.1 -d meetup_db . not sure what to do with it to connect with psycopg2the stack overflow you linked explains: \"this is then evaluated as a \"network\" connection by postgresql rather than a unix domain socket connection, thus not evaluated as a \"local\" connect as you might see in pg_hba.conf\"changing the pg_hba.conf did not work for me i thinkbut it seems that you are basically correct about my issuesolution is: gunicorn --user mamday run:app -w 4 -b 0.0.0.0:80apparently this is standard with unix daemons?is there a politics channel?want to discuss this more in depth but don\\'t want to spam generalwell anyways i started it if anyone wants to joni will say there\\'s a protest at sfo today and lots of celebrities are matching donations to aclu if people actually want to do somethinghere is a link to  in case people want one-click access to it.any body had similar issue like this with `apache flink` before: hi , look into the versions of your components and make sure they\\'re compatible. i think you might want to start looking into your build file (.sbt or maven) and then make sure you have the corresponding versions available during runtime. also, check the download_tech file in your pegasus repo for info on versions you have installed. i did check for the versions and even tinkered with downgrading.hey all .. just want to know your opinion of automation of data science using software like data robot or skytree .. and also, any experience to share about skytree? .. i have tried it and found it to be interesting!: alumni - let me know if anyone is interested in an opportunity at tesla (contact me direct).  software/data/devops engineerstopic summarization == visually explore a topic model like  ?, or summarize text in some other way?in case that is what you’re looking for,  is a python port of the package  linkedthanks kevin and john.  those are useful, but i was thinking more like a force directed graph with clusters and more info/structure at nodes.  most tools look kinda crappy, but trying to avoid building something custom if i can :disappointed:interestinganyone have any experience with sentiment analysis? specifically decent performance in out-of-domain predictions? training the model on the amazon, yelp, and imdb reviews.hey folks. any one have experience with predicting start time of the process in the time series data? i have data from the bed sensor and i need to predict when person went to sleep. please pm me. aside from renaming of the input columns, i don’t think there is a lot of preprocessing that is required. i use it to make forecasts for hundreds of columns and it does a pretty good job without a lot of data cleaning or parameter tuning. that said, in the future it would be nice if it had an option to feed in multiple columns at once and to return even a coarse type of performance score given the data you feed it. some of my time series columns have very sparse data, which it doesn’t handle well, and since i can’t visually inspect 100’s of columns, returning that type of metadata could be useful to get a better sense of how it is behaving across all my datasets (in the meantime i’ve written functions to perform some of those tasks myself).tomorrow networks is looking for a data scientist (phd not required). tomorrow networks is a new york city based mobile advertising company. feel free to contact me if you or anyone you know is interested. hi all, i’ve been having some issues with jupyter notebook autocompletion it’s randomly crashing the kernel. wondering if anybody else has faced this? currently using  jupyter 4.4 and ipython 5.5 on python 2.7. also what are your opinions on transitioning into python3? thanks!do you mean tab completion? it has become pretty unusable for large dataframes (~1m rows) in my experience. i haven’t had it crash the kernel but it will keep the kernel busy for way too long.facebook prophet is a pretty cool open source library.  quite stable and can handle outliers and missing data sufficiently well.  however, there are many parameters --intuitive parameters-- defined with default values to accomplish the task of curve fitting (it is not a temporal model), which could make a misleading forecast.  for example one useful parameter is the number of change-points that it defaults it to 30-40 (can’t recall exactly), and the reason being, they are dealing with facebook data of daily samples and 30 or 40 is quite small.  however, in case your sample size is small, the model would basically treat every data point as a change point and the forecast becomes very unstable.  the other nice parameter is the mcm_sample which you would only benefit if you understand its application in their model.  luckily, the paper that accompanies the library is very detailed and quite easy to read, so i would recommend reading the paper carefully before applying it to your work.  once you understand the model, applying the api is relatively a simple task.  the only downside i faced was the fact that the model needs to growing capacity for every data point if your function of interested for the growth model is logistic as opposed to linear.  in many of my use cases, finding that carrying capacity would require the entire path of research so somehow forced me to stuck with the linear growth function.: my transition to python 3 was seamless. the main thing you run into is remembering to put () around print statements and some things that used to return lists now return generators. and actually it has better stability than 2.7 (i had a bizarre problem with something resembling a memory leak in 2.7 which i found out should go away in 3). highly recommend switching. also, i think most companies are making the switch as well. regarding transitioning to python3, it’s not really an option anymore — all major scientific python packages are ditching python 2 in less than 2 years:  yes tab completion hm. i feel like i\\'ve been dealing with 1m+ rows for a while now, it\\'s a very specific circumstance that triggers the kernel crash. i guess i just have to get rid of my tabbing habit :(.  cool! the print part is definitely something that i\\'ll need to get used to. i am looking forward to not having to import division from future though.  thanks. i guess i knew python 2 was getting phased out in 2 years but hadn\\'t read into the scientific part which made me wonder. and f’\\' strings will change your life!definitely looking forward to that one as well :stuck_out_tongue:honestly as annoying as this is, its for the best.maintaining things for slightly different versions of the same language can be a bigger pain in the butt than it needs to be. this forces people\\'s hands as well, which they would have preferred not to have to do, but as has been the case in many projects, if they leave the door open for groups to avoid refactoring, many won\\'t do it at all.what would you like to learn? hey michael, want to email me  and i’d be happy to intro you to the sv team if you’d like.it\\'s for a broad team including analysts, data scientists, and data engineers. and we have a pretty large amount of money, so i\\'m open to any suggestions.i am interested - if others from the last de batch are coming. :slightly_smiling_face:hey  did you get the invite? you should’ve got the invite emailed last thursday…lemme know if you didn’t and we can resend itno i have not seen any emails form insight. can you please resend it ?if anyone else is at scipy 2016 and feels like meeting up, feel free to dm me.:   game of thrones if it was happening in silicon valley (and job titles were houses).. okay yeah it\\'s a stretch but it was a lot of fun with photoshop :smile: *hey new york!* i’m organizing a data-centric meetup on august 25, hosted by my company, . check it out!*a million trees, a billion oysters: enumerating urban nature*how do you count and locate all of the trees in new york city? how do you measure progress toward restoring 1 billion oysters to new york harbor? a crucial part of any data story is how the numbers are collected, and the reasons for doing so. at this edition of data io, our speakers will describe their approach to enumerating nature, with an emphasis on the urban environment.*speakers*jacqueline lu, director of analytics for nyc parkspeter malinowski, director of the billion oyster projecttbdlocation: tba shortly (one of two locations in soho or nyu)does anyone have experience with embedding a dynamically generated html file inside another html file, particularly using flask?  i’m able to do this once successfully, but if i make any changes to the python code that calls flask, the changes are not recognized by my browser, even though i can confirm that the dynamically generated html file has changed.  it seems like some kind of browser cache issue, but i’m not sure.  any help is appreciated!hi  we have a dedicated channel for flask: askflaskcould anyone recommend either a person or ‘firm’ that could help us define data science architecture better? i work for a large org in the healthcare space that’s trying to create a data science layer that spans multiple existing product lines or verticals and we could use some help in figuring out some best practices/architecture. we’re hoping to create a flexible and scalable architecture that can ingest data from a broad variety of sources and layer ml predictions on top. for reference – we have all kinds of data – marketing, crm, content, patient, medical. our challenge has been coming up with an architecture that would work both in client deployments and in our own environment, and that can ingest/source data from multiple sources. thanks for your help and p feel free to dm me if you prefer that.i know some great folks at svds () including insight alumni   . they do that kind of workthanks, looking at them now!a couple of years ago, i followed a hashtag for a few months. the twitter api was super easy to use--i had it up and running in a day. sentiment analysis on the tweets wasn’t particularly useful, but the spikes in the hashtag over time were directly correlated to news events, etc. i’d be happy to pass along some scripts, but be forewarned that i did this pre-insight so it’s not so pretty…sounds like you might need some data engineers! let me know  if you or your company might be interested in insight de fellowshard to argue with gelman.  my impression is that if your trying to create something like that you should probably be hiring a team of des to design, build, and then maintain, because that\\'s a pretty massive undertaking.so there is no issue with centering positive only continuous variables at zero? e.g., what does it mean to have negative age or negative income? agreed on the de part and that\\'s part of the overall plan, but we\\'re also trying to understand what technology platforms we should be looking at, so we can hire the right mix of people.negative=below the mean, positive= above the meana quick update: not using the elasticnetparam (the example in the documentation is lr = logisticregression(maxiter=10, regparam=0.3, elasticnetparam=0.8)) helped with the model, the number of jobs triggered is still a mystery (i should dig in to the source code i guess)#drumpfstanford deep learning for natural language processing course is now underway: apologies for the shameless self-promotion, but this may actually be relevant to a few. if you’ve ever wondered how to get started with neural networks in python, i’ve made an intro tutorial (complete with a jupyternb) about getting starter with chainer. fantastic tj! looking forward to working through it carefully. love the idea of getting bonus points :simple_smile:absolutely shameless jejejeje sounds cool. i’ll take a lookdo you do workshops tj? :simple_smile:this is great  - thanks for sharing, i look forward to reading it thoroughly later!thanks everyone…this post was based off an internal workshop i did at stitch fixyeah i read this the other day , ; have you had a chance to play around with tensorflow at all?i haven’t had a chance yet to thoroughly investigate tensorflow yet, but i get the impression it’s quite similar to chainer in some ways: awesome! i was also going to ask how chainer compares to tensorflow and theanochainer doesn’t require any compile time like theanothis has the disadvantage of providing a bit worse performance, but you’re also not sitting around for 45 minutes waiting to be able to run your training script. so for shorter projects and prototyping i think chainer has a leg up.stay positive tjtorresthanks, nuggetbot…i plan to.:thumbsup:great job tj! quick question: why do the light blue lines bounce around to either side of the final magenta state? is the nn overshooting the minimum of the error surface? just wondering, since the learning rate seems rather small for this to be happening. then again, i know nothing about regression with nns :simple_smile:i’m not entirely sure, but i think in that example i was just going for using different optimizers and ended up randomly choosing a momentum sgd which would likely overshoot a bit in order to try and avoid local minima for loss surfaces which are more complex than the mse in that examplestay positive tjtorresah, got it! never heard of momentum sgd, but now i see that it’s proportional to the learning rate plus an additional “velocity,” which leads to larger step sizes than the learning rate alone. thanks :wink:and go lick a doorknob nuggetbotrecommender system blog post (good flow chart): :clap:hi kathy, salesforce also has undergrad/grad-level data science summer internships. i don\\'t know much about the beginning of the recruiting process yet, but here\\'s a general link: anyone know of good open source tools to visualize social networks? with ability to incorporate additional data, e.g. geographic?`networkx` in python. i wrote a r package as well, `ggnet` that’s at . you can use something like gephi as well.neat : there are also several other very nice r packages for doing this type of thing, see e.g.   yeah… i’d go with either of those two options over `ggnet` — those are really solid. thanks for the links npat the end of the day, if you want geographic stuff, you ought to just use cartodbthanks   hey all, we have a data analyst position at backflip studios in boulder, colorado. we’re a mobile gaming company that produces titles like dragonvale, and it’s a really fun environment full of smart people (and fantastic benefits!). if anyone is interested in applying or has any questions about backflip and the team, please email me at . please feel free to pass along my email to anyone you know who might be a good fit.gotta admit, i was definitely addicted to dragonvale for a minute there(edit) meetup canceled. sorry!! i think there is an online course that accompanies the book \"lean analytics\"its on udemyhi, everyone. did anyone have any experience on transferring xml file to csv file?i\\'ve done xml to json, but assuming you have a simple* xml it should be fine. what\\'s up?* the big issue you\\'re going to run into if you have a complicated xml file is xml is nested in general, and csv is columnar.hi all. i am tying to figure out ethnicity and/or gender from a list of names and no training data. a quick google search returns a couple of options at github, but without any metric for performance. does anyone have a suggestion for where to look, or (more generally) an idea about how reliable these algorithms can be? i do expect a multinational sample. thanks! on ethnicity/race i’ve come across:andhave you already looked at these?this looks awesome. thanks !you’re welcome  ! should be possible to adapt that to gender too, might take some work. i had a task at one point (never got done in the end, super low priority) to build a similar analysis for protected classes w.r.t fair lending.anybody know of python libs/code for entity matching?nicholas.k.baro: what type?dedupe ain\\'t bad stay positive mpanciathis is the most beautiful way to visualize machine learning i\\'ve seen: *heads up*: i\\'ve started  for planning group rides. hopefully this will be an improvement over randomly emailing people. join if you\\'re interested in riding! pinned a message to this channel.in case anyone wants to play around with some interesting python edge cases:plotly also has a library to bind to pandas dataframes called cufflinks that i think is great (very similar to jake\\'s pdvega, but more feature-full). folks are generally wary of  because of the online publishing aspect, but you can also use it in full offline modeif only they did a coconut one…we\\'re hiring an infrastructure engineer and a growth analyst at edmodo (). let me know if you or someone you know is interested at hi everyone!  am putting together a primer for my portfolio companies on diy data science.  would love your tips on how you wrangle/clean your data!nice ! but... no images of battleship boards? i want to see the match!when i have a bit more time i want to make a web app that allows you to playand thanks!what do people use for workflow management? we’re finding trello to be ineffective as the number of projects and people have grown. we use jira on bitbucket. it allows for more organizational complexity than trello.we use asana, but it isn\\'t great for our small teams. since we\\'re pretty self directed, updating it is our job, and no one is really on top of it. :confused:i can see how it would be great if you had a manager person doing itwe also use jira. it works fine, but, i\\'m beginning to think that using it too much is a process smell.i’ve become quite partial to pivotal tracker.jira here as well for the larger group, and confluence for some documentation / collaboration. though i still use trello for my own weekly planningjira for us and with github integration has anyone tried github\\'s workflow management that they launched a few weeks ago?not the workflow management .. tried their new pr review process though. yeah i\\'ve used that, it\\'s light, but still nice!we use jira. it\\'s clunky and bloated. the ultimate irony of a tool used for agile development!also not a fan of jiraapart from some clunkiness i find it’s a little too linear for ds developmentyeah, we have jira toojira/confluence/bitbucket here. it\\'s fine. nothing great, nothing awful. all in how you use it.yodiz/confluence/bitbucket for us - easy to get going. been meaning to look into getting jupyter and confluence to talk nicely - anyone tried any of the macros?hadn\\'t even thought of it! would love to see what you figure out.quick reminder from your friendly insight admissions team: applications deadline for insight data science and insight health data science programs is one week from today on october 24th! make sure to tell anyone you think might be a good fit for the program to get their applications in on time and please do send us your recommendations! . a friend needs help debugging an ios app for nov 1 launch. gig pays $5k. any takers? the app is sport technology - enables multiple smart phones to synchronize and take pictures (perhaps movies also) from different angles .. i have some experience building ios apps. i\\'d like to know more about the app and the bug.does anyone have experience implementing a genetic algorithm to solve the \"nurse scheduling problem?\" any suggestions for good references?dan would. he\\'s back friday?excellent, thanks - i\\'ll hit him upi have some experience if its just the basics. but to be honest its been over 10 years since  i\\'ve used oneare you using deap?i have two tickets for explosions in the sky - tomorrow at terminal 5.  anyone interested?  send me a direct messageiclr 2016 talks streaming on periscope: has anyone built a transit shed map using google maps api or perhaps mta api? i\\'m looking for suggestions of tools/libraries, preferably in python. trying to avoid using arcgis which is slow and buggy.  would you suggest using networkx for smthng like this?  any ideas?  thanks in advance! haven’t used this myself, but it looks promising — *\"dopelearning: a computational approach to rap lyrics generation”*nice, i started doing a project like this last year (with lackluster results)i wonder if \\'rack city\\' was in the training set one thing to consider is how fast would this need to be processed. how much lag can you tolerate 1s, 1h, 1d, 1wk? if you can tolerate several hours or days of lag then batch sounds simpler and easier to build. if the lag has to be lower, i’d recommend building a change capture system that pipes to kafka or a msg queue for other dowbstreams to consume and update their local caches or stateif you’d like to chat more about this later feel free to dm me +1if you just need storage of data that you\\'ll eventually query later in daily batches or partitions, the tools may be much different than if you need to take action on the data as it streams inwhat\\'s the source of the data initially?thanks a bunch  and .  i only need to update the tables nightly, but want to store one row per user per day.  the data initially is coming as a packet from a gsm chip, then stored in an s3 bucket either json, text, or csv (the software engineer i\\'m working with left that up to me).  then i want to take those numbers and calculate daily summaries for each customer.  (it\\'s for solar power systems in the field, so stuff like the max voltage reached by the panel on the previous day and the time it takes the battery to discharge.)  i can tolerate several hours of lag in processing the data and adding to the system performance tables.  we\\'ll likely be surfacing the most recent row for each user to an internal interface, but they\\'ll be accessible with a primary key so hopefully fairly fast.  we want to have historical summary metrics there as well, mostly for modeling (which can be on the order of hours, though would be nice to be faster.)  right now the amount of data is small (on the order of 100k users per day) but i\\'d like to build to handle at least a 10-fold increase without dramatic changes to the architecture.  does that seem reasonable, or should we just build for what we have now and expect to have to make changes?  commented on ’s file : if you go the docker route, dm me if you have questions. i use the jupyter stacks for many products at o\\'reilly and although i highly recommend, sometimes there are issues :)previously had a non-technical manager (background in finance and marketing). was a tough relationship because she never understood what i was doing. current technical manager is much better.not me. my manager is technicalhopefullyhey folks!  coming from academia where there was a very blurry line between professional and personal computing, i’m curious how/if people separate those two in the real world  - do you have a personal computer in addition to a work laptop? i’m debating whether to buy a new laptop to replace the university-owned one i have to return when i start my new ds job.personally, i\\'m looking forward to leaving my work computer at work when i start my first ds job in jan.i definitely have a personal computer because i don\\'t want to touch my work even remotely when i\\'m not at work/on holidayi keep a personal laptop and work laptop. while some companies don\\'t care if you put personal stuff on work machines, others have very strict it policies. you should ask once you\\'re employed. in general, it\\'s best to keep a personal computer as you\\'ll most likely have to return the work laptop if you ever leave..i have a work laptop but the info on it is sensitive enough (hipaa and company info) that i try to avoid doing anything not work-related on it as a safeguardalso depending on company, you can\\'t take your computer to certain countries if you ever go on vacation therehaha, nice - hep has had the same effect on me :slightly_smiling_face:if you want to do any remote working, you\\'ll need to be able to remote from your personal to work compthat would be a huge no go at my company...good point about returning it.  additionally, i’ve been a mac user for the last decade and will be using windows for work, so all the more reason to keep a personal (apple) laptop…something for me to keep in mind when i startwhere/how do you draw that line?  what about answering personal emails?  that’s the one i generally do from work on a regular basisamen to thathep\\'s effect on me: dry, text-laden presentation stylehaha, trying to overcompensate with gifs-a-plentyi just got a personal laptop separate from my work laptop. mostly because i want to do some work on my personal that i don\\'t want to get mixed with work. and my work laptop has an automatic back up that stores everything and i don\\'t really want them to save my personal projects.on a more subtle note, many companies have policies that state any software built using company resources (including work laptops) becomes property of the company. if you like to develop your own code and want to keep it, i\\'d use a personal machine.i do that kind of thing all the timei\\'ll watch netflix on my work laptop, etcbut i\\'m not going to do things like watch my bootleg nfl streams on it, hahahi all, especially those in the nyc area looking for a new opportunity! i have a couple contacts at the startup asapp, where they are interested in growing their data science and data engineering teams. if you’re interested in being connected with them, please send me a message! i’d be happy to connect you.some info — asapp is focused on automated customer service (chatbots) - routing customers to experienced human agents, providing agents the appropriate background infotainment about the customer and call, as well as suggestions of what to say,while moving towards full automation to resolve customer inquiries/problems. they will be doing a lot of nlp understanding, prediction and sentiment analysis based on several decades of call center data, with the goal of saving both agents’ and customers’ time and importing the customer experience / time to resolution.haha, nice dividing line, good to know :slightly_smiling_face:oh yeah, wow, i didn’t even think about that!  thanks for sharing your experiences!seems like it !in my experience, this sentence \"machine learning engineers primarily come from data engineering backgrounds. they’re cross-trained enough to become proficient at both data engineering and data science. a less common route is for a data scientist to cross-train on the data engineering side.\" is not really true. at least at my company, it is the opposite.thanks romesh!has anyone used airflow with docker swarm in aws? i could use some adviceour company (glooko) is looking to fill a data/business analyst type role asap... we need someone who has analytical chops and communication skills. ideal candidate is a recent grad, bachelors or masters, with high growth potential into either data science or product management, depending on candidate\\'s interest / skills. would appreciate if you could refer anyone you know who\\'s interested! would anyone know any solid resources to learn xgboost fast? i would look at the video and slides from the author of xgboostthanks!survey data!  i just participated. there will be so much data in there. that might be a ds project for the upcoming batch :wink:just got an email about this if anyone is interested in fielding a team:dear friends, we\\'d like to invite you to participate in round 1 of the big data analytics world championships 2015 (business and enterprise) on saturday september 25, 2015 (4 weeks\\' time). the current world champion is stéphane sbizzera (kpmg france).     thousands of the best data scientists, engineers, statisticians, computer science and data analysts compete in two online qualification rounds (4 hours each). the top performers are flown to austin, texas usa to compete in the live world finals. the focus is on business, mobility and enterprise data skills with real-world case studies, multiple-choice and short-answer questions. feel free to share with work colleagues or friends that might be interested. register at .50% off ebooks and 40% off books from o\\'reilly with the code: authd  not sure how long it will last...in some cases i have gotten cheaper prices than amazon!call me crazy, but this doesn\\'t seem that revolutionary. they have really accurate clocks. what does that buy you? ^ yeah i got that sense too... but then it\\'s wired writing, so not that surprising.also \"faster-than-light coordination\" made me cringebuzzword bingo but strictly speaking correct - if you can finalize an atomic write operation simultaneously worldwide, it’s as if you had sent information from one place of origin faster than light. of course we know that’s not the way information flows.funny actually, atomic clocks is the naive answer to the consistency problem before you delve into all the practical ways of having eventual consistency but someone went out and actually did it. :smile:\"it’s as if you had sent information from one place of origin faster than light\" i\\'d say it\\'s more just assuming you sent it that way :stuck_out_tongue: but i get youyes, in practice it’s just a careful way of distinguishing event time, write time, and agreed-upon ready-to-read time (haven’t read the white paper so it’s probably called something else). pretty much what dataflow was for streaming, this is for databases. pretty neat still.yeah, physicsy grumbling notwithstanding, it works, i work at domino data lab, which makes a knowledge sharing, access control, automatic version control product for data scientists. we have customers from instacart to allstate. let me know if you have any questions or if i can direct your questions to someone elseon the subject of tools that would be nice to have at work, does anyone here know of tools for managing the integration of ux/ui designs with analytic/event instrumentation? especially one that has the ability to do version control for both designers and the back-end analytical schemas?anyone have experience with \"research repositories\" at their companies? along the lines of airbnb\\'s knowledge repo but less technical (aka supplying marketing/execs/etc with critical facts about company research for presentations and sales) fyi deeplearners wayfair has something called the knowledge base, which is mostly just a wikids opportunities in the uk with  - what kind of metrics do you guys usually use when you talk about model performance/accuracy/… to non-technical people? assumption here is that things like roc/auroc etc will be too technical.% improvement over previous effortsif talking to business stakeholders mapping it to a dollar benefit also goes over well, could also build a quick spreadsheet showing changes in funnel/process and cost/revenue numbers before and after.estimates of course i assume this works best when you have previous models to compare, would you use chance as the baseline otherwise?no, i woudl just compare it to not using a model, and sort of fudge it :slightly_smiling_face:  i’m assuming these non tech people are like c-level or upper managementthey want to see stuff they can tie to roi or improvement somehow i second .  a couple times i\\'ve come up with some baseline to compare to, even just \"assume this is a constant value\".  then you can say the model gives % improvement over a basic assumptions model.cool, thanks everyone. guess the question now is what a basic assumption model is :wink:  there\\'s actually a repo: ah, but it\\'s county level, not precinct &gt;_&lt;are there any canadians here who have any experience with getting a tn visa/ssn? could use some help (pm me)! thanks :slightly_smiling_face:you can also contact insight, they have immigration lawyers: ^^^i basically let adroll\\'s lawyers handle it but feel free to pm me any specific questions. ^thanks everyone :)  i think that\\'s what i\\'ll need to do or at least for the company that\\'s thinking to hire me how to think like a data scientisthow to think like a data scientistdoes anyone have a link to mike’s slides from yesterday’s presentation?thanks for the recommendation  looking at it nowhas anyone used this for photo projects before: ?seems like an incredible dataset: thanks ! :clap:can anybody suggest a good site to purchase a plane ticket to fly international (canada)?aware of the big guys (expedia &amp; co.) but looking for alternative choices...  i use  and thanks for the tips , anyone ever use sqlalchemy to automatically read a pandas dataframe into a mysql database using the \"to_sql\" command?: i just use : yesyes, i did for my project:  . search for engine and you should find some examples. no warranty of correctness or nonstupidity!that\\'s for   btwi used postgres because of the mysql/utf/python issue some of us ran/are running into.thanks  for the extra reference! i was able to get it working with  \\'s helphello! does anyone know about a public data set on precision farming, urban/indoor farming, or more generally plant growth in relation to environmental conditions?no, but perhaps you can find someone who works here to chat:thanks! i\\'ll take a look.unemployment has similar issues, and you can get quite different numbers by using different methods. do you have any connections at bowery farming?no i dont, sorryawesome, good luck!who should i email at insight with referrals for partner companies?is it a silicon valley company?no - clearwater fl. probably something for the remote program.like the actively looking for work qualifier? is a solid go-to for all of insight, and  and  (katie@ and lee@ ) are the right folks for the remote program. :slightly_smiling_face:  thanks !maybe try its a digital repository where lot of plant science journal publications ask to deposit data and its open. doing a quick search for the terms you mentioned, it does throw results with data files.... not sure of the quality though, and i suspect it will be a major exercise to clean and compile them togetherin case anyone is looking for housing in south bay(*cough* 18a fellows?), my roommate\\'s moving out in april. check out  for the post (posting here too cuz the channel looks kinda dead &gt;.&gt;)think insight could get the white house to be a mentor \"company\"? :wink:obama could come for the company visit: get on that!you heard about this, right? yesonly because i keep up with the dj scene:sound::sound::sound::sound:nuggetbot: animate me dj kitty does adding single \\' \\' like : person=\\'{0}\\' help? i also assume using read_sql and read_sql_query are the same? about security though, i think you can also use the sqlalchemy orm to avoid some of the security issues you mentioned. we here at grammarly are looking to hire a data scientist in san francisco. if you know anyone who’d be interested please let me know. the job posting is here:  i know some fellows who may be interested ;) i’m all ears katie!i\\'ll message you!hey, anybody at nips currently? we are hosting an event tomorrow and would love to see you there!a lot of my lab is, but none of the insight fellows unfortunately :disappointed:hoodline is hiring a data scientist! that’s strange to seeeveryone needs data scientists :wink: you willing to move to stamford, brad? :wink:heck nothough they do have an office in nyc too...stone cold svmshi fellows, i would like to use a big csv file, like 80g . i would be able to load as rdd in spark but i would like to use this as a matrix by using spark and run with my algorithm code in python.  anyone has a tip for me please?could you elaborate a bit on what you mean by use it at a matrix?also what have you tried doing so far?and by python do you mean pandas/sci-kit learn?but in general pyspark should work for thatexactly: load the csv onto hdfs first (`hdfs dfs -put`), and then read it from spark.the good thing about doing this is that it distributes your data in your cluster first.once you’ve done that, you could do `sc.textfile(”filename\")`hey crowd, anyone know any good data visualisation blogs? my preference is for simple data presentations or ranty blogs, but i’ll take anything :pray::skin-tone-2: and :    thank you for answering. i put the fule in hdfs  and was able to read it as rdd using  sc.textfile(\"filename\") . and i have a separate matrix calculation python script which i have tested with 10x12 csv file. now i want to load 80g csv file in that python code.  up to now, i am able to read 80g file and can do some basic command. rdd.first() , rdd.take(5) something like that. and also can make the first 10000 line as a matrix. but i have 1.5m lines and 1200columns in the big file.  and i cannot read this as a whole matrix which i need to do  in my algorithm python code.in the algorithm, i have to matrix calculation.     uploaded a file: here’s an interesting piece on ethics in data science: hi everyone! the boston fellows were given a document with references to many data sources. one of these references is mimic () - de-identified health data associated with ~40,000 critical care patients. did anyone use this data set before? if so, how long did it take you to get approved for the restricted access? thanks!: i think  used mimic in her project if memory serves me correctly. one of the boston pds may know if she did or not too. i\\'d check with them (and to also get her email just in case.)she wasn\\'t in my cohort but the one before itanybody has suggestions for 3d plotting in jupyter notebooks? i tried using plotly with jupyter before. if registering with an .edu account, you can get more plots access (or unlimited? i don’t remember)you can also use plotly for free in offline mode, or set up your own serverthere’s some limitations with sharing plots that way though, i’m not sure how easy it is to embed that javascript into the notebook itself, i had some issues with thatbut that was a while ago i know seaborn can do some basic 3d plotting. i\\'m not sure if that\\'s what you are looking for. if you peruse this github for islr () you will find some examples.also: i would assume that bokeh and plotly allow you do to 3d plots (more easily) too.those are more interactive, if that\\'s what your goal is. thanks for the tip!:thumbsup:jonathan_whitmore: the gift that keeps on giving!oh, one last thing : i realized the last link might be sparse in examples, but here\\'s a notebook where someone used matplotlib to do some 3d plots (which can be easily adapted into seaborn etc) good luck!i’m also interested in using this dataset for an insight project and i’m in the process of applying for it. not sure how long it’ll take…does anybody have experience with careerjet api or apis of other job searching websites?does anyone have experience in importing a video in python? pinned a message to this channel. pinned a message to this channel.what are you trying to do?opencv has support for working with video if you want some data on how much stock options can be worth it looks like it takes about a day (i was just granted access). i already signed up for a consulting project though. have fun with the data set!hi guys. does anyone have a lead on a user dataset that i can do churn analysis on?hi everyone! does anyone have experience with using rnns for machine translation? my consulting project has a database of ~5000 credit card financial transactions that include an often-messy string corresponding to the business name (e.g., \"united tx\", \"wells fargo ban\") and human translations of these business names (e.g., \\'united airlines\", \"wells fargo\"), and i\\'m wondering if this would be enough data to train an rnn to correct these sorts of mistakes automatically. the client has a strong desire to explore rnns for automating this task, but i don\\'t have enough expertise to know whether this is likely to work or is simply misguided. any input would be extremely appreciated!!i want to extract frames from the video i would start with some basic feature engineering or compute edit distances and then try a service like open calais for entity resolution (i’m assuming you’re interested in mapping company names to business entities).  in other words, try something simple and interpretable first.  btw, this is a problem we deal with a lot where i work so feel free to pm me with questions.thanks! i also changed my min:grinning:d..jeff: agree that opencv is the best python tool i know of for that. ffmpeg is the other workhorse - it\\'s old, but it\\'s been around for a reason and can take a billion parameters if you need to customize the sorts of frames you want to extract.ffmpeg = command line tool, not python, although you could call it with a shell command from python if you want.i use pyspark with python 3 with no problems…wonder what i’m missing :stuck_out_tongue:just have to ensure the driver and each node is pointed to the appropriate python binaryre the larger question, i’d say python 3.  it’s only a handful of packages that i’ve run into the package compatibility issue, and documentation/writeups for data projects is increasingly in py3do you want to build a desktop, buy a desktop w/gpu, or rent a gpu on the cloud?all options require you have a budget in mind. a great single gpu for deep learning will cost around $1k. i’m not sure what the status of windows drivers for deep learning is (it may actually be plug-and-play) but if you really don’t want to do a lot of configuration yourself, know that setting up the drivers on an ubuntu system w/cuda is a pain in the ass.your best bang for your buck will probably be a gpu instance in the cloud, especially if its for a one-off project.if you want to do longer term personal research/play, rolling you own becomes more cost effective. some people need months, not days, to whip out three functioning apps :stuck_out_tongue: it was a week :stuck_out_tongue: :stuck_out_tongue: and bibli2 took us almost 4 months to setup  :stuck_out_tongue: :stuck_out_tongue:yeah that\\'s way worse than i expected of you. shame!i am getting older little pixipentreath, machine learning with spark.  it is a bit dated but has examples in python, java and scala.  karau, learning spark.  has examples in same three languages.  same comment.  edx has a good intro course all in pythonyeah, individually, by coalescing the labeled data up from the leaves to the node in question.we are looking into whether the leaf predictions can be strengthened via the ancestors models, like done here: out of curiosity is flask recommended for production web serving of dynamic content? or are you better off with a dedicated server with a python cgi plug in?i\\'m hearing tensorcon is a *scam*, so don\\'t sign up! () tensorflow team at google have confirmed they are not hosting it and are not affiliated etca tensorcon con?tensorkhaaaaaaaaaaaaaaaaan!who could have predicted that?predicting cons with tensorflow: read all about it here….do they want you to connect through citrix or something?whoa, my slack was not updating, that message was meant for haha! i have no idea actually. they will clarify for me. but in the meantime i\\'ve found my own work around :) have you connected through citrix?yeah, i have used ssms through citrix or also via an install on my own computer with a vpn. i am sure you\\'ll have some reasonable options although citrix can be really annoying to use sometimes!ok thanks! good to know. maybe i\\'ll bug if i have some issues :)hello i was wondering if anyone had experience writing pyspark processing functions and could answer for few questions? please pm me!i have! ill pm you me too, dm menew blog for data engineering! also front page of hn!:+1: nice!has anyone looked into tools for estimating heterogeneous treatment effects, like causal trees/forests?this article just came out in political analysis (no paywall). includes replication code (in r) thanks! i\\'ll have a lookwe used the `causal forests` r package which was a goddamn nightmare. it’s now called `gradient forests` or something like that. i was inspired by j. faghmous\\'s talk @dataengconf. your team? i was talking to him after his talk and he said that susan athey came out with an updated version of her code ~6 months ago that might be better, but the original version they used was a bit buggy and he would consider using it as a beta sort of thing rather than a prod sort of thing:slightly_smiling_face: i ended up posting my code on their gitter, and somebody tried it and couldn\\'t figure out why it didn\\'t work right.but then i tried this, which does exactly what i want (fitting a gaussian distribution\\'s intrinsic width as well as its mean):  yeah that’s us. we just got a rejection for that paper too!hah, go figure - astrophysics to the rescue :slightly_smiling_face:interestingi\\'m having a hack week this week and thinking about working on a python implementation of something similar to return subpopulations with different responses to a testthat’s a good idea kevin. a hacky backup which i’ve never seen implemented but should work is to split the data, go fishing on one set looking for relevant subgroups using what ever algo, then test whether the subgroups remain significant on the hold out set.what sort of other methods did you try for the first part of finding relevant subgroups?for this project we only used the causal forests packagewhat about propensity score matching?  model the likelihood to receive the treatment then estimate treatment effect across similar groups with similar propensities.  i worked with this package long ago for this and it seemed decent: about multi-level models with varying slopes, one for each group? that way you can get estimates of effect for each subpopulationhey all, wondering if you could give me a sense of where data science sits in your company. what’s the team consist of and what’s the reporting structure like (outside of the team, not internal)?under the product org, which we’re not super happy about. manager of ds reports to director of data (who also has das under him), who reports to the vp of product who reports to the ceo.: you might want to look into aws glue.  i haven’t used it yet myself, but it looks like a promising method to use spark without having to worry about emr clusters.  i believe you can set timers for jobs as well within the glue interface or via the aws cli.all good ideas --  i think in this case we don\\'t know what the groups should be a priori, which is why we were hoping for a technique that could generate the groups. i suppose hierarchical models should be able to do this as wellah, interesting - i\\'ll definitely look into that!hi all.. any tips on how to present data science to senior management so they can come back  with  business use cases? im thinking of having an overview of ds in general in our area (regtech) and showcase some algorithms with some concrete use cases..  the goal is to make management rethink about their problems, pain points, areas of opportunity and frame these problems as data science/analytics problems so we can build a solution for them.  thoughts, suggestions, experiences?thanks jeffrey! we’re currently structured under a chief strategy officer (which i like), but trying to think of a new solution as the company evolves. looking for opinions on reporting to engineering or product or ?when doing this at zipcar we found that the best way, even if it was to solicit top-down project ideas was to show examples of completed projects, and additionally suggest projects we thought would be high roi and could be solved in a data driven way. this made starting the conversation easier, and made it an overall collaborative effort.thanks emmanuel.. yeah.. i was debating a bottom-up vs top down approach in soliciting ideas. but then settled on the top-down since they their buy in is e$$ential not just in getting started but also they are influential on the making the users use the intelligent solution or the data driven product.from my experience, reporting to engineering could lead you to be responsible for building your solutions into apis and data products, while reporting to product will lead you to do more ad hoc work and experimentation to guide the product roadmap via analysis and decision support. of course a company needs all types of work done, so both roles need to exist in one form or another. we have those two roles mostly on different “teams”, so for example a data scientist on a product team might analyze data and hypothesize that a new checkout flow would increase conversion, build a prototype, run a test, then the more eng-focused team may build out a production version of that prototype. we’re a small team (2-4 currently depending on how you count), but situated inside of tech / engineering. the product folks operate in their own space between us and business stakeholders. like kevin suggests, we’re very much responsible for building and maintaining production systems. we work alongside engineering + software development and all report to a cto. cto handles the back and forth between tech and the rest of the business, including product.a note about the top 10 example that niharika, mudit and i discussed this morning. if you look at the program am listing around page 62 an 63, you will notice that the mapper class declarer a map, that maps a \"reputation\" (which is derived from the input record) to the entire record. as it keeps reading more input, the map keeps growing. whenever the size gets bigger than \\'k\\', the \"first_key\" is dropper. after all records are procssed by the mapper, the map contains \\'k\\' elements. now the \"cleanup() is the critical part of the process. it\\'s this \\'cleanup() that takes the map and emits a (key, record) so the reducer can combine all these inputs and pick out the top \\'k\\' elements. if you look at the program, the mapper class has two mthods, map() and cleanup(). presumably map() gets called every time for each input, but cleanup(0 is called after call input has been processed. the cleanup is doing what a local combner does. but it is not clear where cleanup() is getting called. it\\'s clearly not called explicitly from the mapper anywhere. it must be happening behind the scenes my feeling is cleanup() is being registered as the \\'combiner\\' method. but this needs clarification.fyi to whoever is considering a project using airbnb data this session (i heard there was someone but didn\\'t get your name), my project used airbnb data and i\\'m happy to discuss what i did and share my code, i believe someone from the first boston session scraped data from strava. is that what you mean by “running records” or do you mean e.g. world records? trying to remember who it was.:  has done something like thisdoes anyone have a suggestion for a database ide like  ?  preferably something that is free and supports at least postgres, sybase, and ms sql server?: i wrote some scripts to scrape data from strava for runners. let me know if you’re indeed interested and i can point you to them. it wasn’t trivial!does anybody know of a data-set for mooc websites that would allow for dropout prediction? i know there is a kaggle data-set but i do not want to use that :slightly_smiling_face: thank you!thank you   , i will talk to eddie. i think  also scraped strava.i have an invite for the oneplus2, if anyone is interested expires in 24hrs…goneif anyone needs i’ve got an invite.goneif anyone is looking for sf housing, my college roommate has an opening for the master bedroom of a 3 bedroom apartment in potrero hill. dm me if you\\'re interested and i can put you in touch.: i’ll be going, if you want to meet up at some point.thanks , that\\'s what i suspected. make the top level \"god-emperor of data\"and they can have  as their slack profilethanks guys for the tips!and , i will definitely pass along your god-emperor-of-date title :slightly_smiling_face: don\\'t encourage him, he\\'s been lobbying for months to use this title for himself.:slightly_smiling_face:hahaha, sorry, i will stopdoes anyone have experience plotting timelines of overlaid events such as check deposit dates and phone call dates? i\\'ve used simple bar charts but was wondering if there were cool timeline features i was unaware of.and it\\'s totally not my github profile pic &gt;_&gt; i would add that we need gender neutral titles and representations, such as data dragon  fair enough :slightly_smiling_face: hey! i keep hearing good things about plotly, but i haven’t used it myself: any postgres gui recommendations for mac? currently using sql workbench j, which is a colossal resource hog. interested in something that might be more kind to my laptop batterypostico?  i\\'ve used pgadmin3, but it\\'s kind of a hot mess too...  postico looks cool, i\\'ll check that out there are no great solutions. you have to pick the least bad one, the one that irritates you the least.hey all, domino is putting on a data science popup in sf: use `datapopupvip` code for 30% discountpostico does look like it could be a bit more battery friendly… thanks everyone!datagrip is good, if you/your company has $200 to spare here\\'s a list of clients: also, i\\'ve used valentina on linux, it\\'s halfway decent.does anyone have a favorite gui for sqlite on linux (besides sqlitebrowser and firefox)?i do like datagrip i know all the insighters at dia and co will be there.  recent ai fellows   and  are attending and have accepted papers.  insight staff    are hosting a ai careers panel with the above alumni and  .  the plan is to maybe find a time for everyone to have dinner?  jeremy is the point of contact.  sadly i will not be going :confused:anyone else??since the panel is on wed, how about dinner that evening? lmk!ah, nice. i\\'m here with 22 of my colleagues so it almost feels like regular work days! i\\'d love to join for dinner if you figure out a time.i’ll be there and would love to meet some other fellowsmy coworker and i will be there.  would love to join.ha! i floated a very similar idea for my project, but ended up doing a consulting project instead.how about wed before the panel?when is the panel? i don’t see it on the schedule on the nips websitethe plan was to have an insight sponsored drinks and apps after the panel if people are downpanel is not on schedule. it\\'s 7-8 pm on wedsanother decisive victory for alphago! 3-0de people its a must read, i keep revisiting this article for perspective on data engineering. its simply awesome, talks so much about basics of de.the robot just has no idea who he is messing with. :wink:jimmy v. nuggetbot is a match i would watch. jujitsu that talking robot into submission!that robot has no idea how a good i am at dismantling things… and how terrible at putting them back together uploaded a file:  and commented: an astronomer colleague of mine sent me these job postings to share around. if you have an astronomy background and weren’t 100% sold on leaving academia in the dust, maybe these roles are a nice middle-ground.: dust or _interstellar  dust_? (sorry, bad joke; and possible one that doesn’t make any sense!)hahahey everyone! does anyone here have experience deploying a genomics web application? i have some questions about cloud service providers and data security.hey new york folks, my former boss is hiring a data scientist for the growth/marketing team at better mortgage.  happy to make introductions and pass word along.  job posting here: heard that was a neat company, also ^just mentioned in the  channel -  (techstars alumni) are looking for person 3 to join their company and lead their ds and ai efforts.  if you know someone who might be interested, please let me know (slightly more info in the channel) does anyone have experience applying gans to problems other than image creation? i\\'m curious to hear about other applications. here we go!uh oh :smile::fire:quick, clean your desk!this is my favorite part of this slack team!!*so it begins* i guess u can use them for clusteringget your announcements in everyone!at least it was here, not channel … ?don’t forget to sign up for alumni mentoring and in a couple weeks mock interviews!^stay tuned for more lightning talks and awesome alumni events in the near future!, if you\\'re not already in on it - there are a variety of (strong) opinions about using the at here or at channel tags, opinions which are generally re-aired whenever someone uses one of themdon’t forget to cheer for the vikings you have disciplesyeah, i slipped up and used our slack approach from work :papologies to the hundreds of you who just got spammedit\\'s never bothered me, fwiw better add at here to your apology to make sure everyone gets itseems like at here gets less response than at channelseems like a testable hypothesisbut i\\'d have to try it multiple times to get a big enough sample*psa*: go to `notifications preferences` in `channel settings` and tick `ignore notifications for at everyone and at here`here is for active users. channel is for all usersi believe a slack admin can limit usage of both at here / at channel through this page: should at this message :slightly_smiling_face:be my guest :laughing:has anyone worked with amazon sagemaker for deploying custom algorithms? any thoughts on it generally?has anyone worked with incorporating distance based correlations into a model. (for example, i\\'d expect data from people living close together to be more correlated.) or even just encoding zipcode/county/etc data into a continuous variable which maintains their distance. if this is for a project, try to keep it simple at first and use an average of the neighbors as a feature. if it ends up being an important feature, you can go back and change it to an weighted average (i\\'d recommend an exponential weighting) where the decay rate could be a hyper-parameter.i\\'m doing salary prediction for medical specialities per countyi have a lot of data from bls for most combinations, but i\\'m trying to fill in the gaps with prediction basicallyso i was thinking of doing kriging with the exponential model, but i wanted to also incorporate other covariates(from census data etc)which is why i am thinking of how to actually model it by incorporating correlation structuresif i\\'m reading this correctly, you want to predict salaries for where you don\\'t have enough data.if that\\'s the case, devise a distance metric (for actual distance, population distribution, urban/rural breakdown, dates, etc.) and get the most similar cases to the county in case (these are your neighbors). use these to predict. you can later go back and apply an exponential interpolation using kriging or another method you desire.you mean just take a mean of the surrounding counties?the problem is only having mean data per countyso if i localize my models the sample sizes are going to be minisculeyou can also take a weighted average per state, or even by region. try to run a preliminary model with all of these averages and see which one might have a higher predictive power.also, remember that the geographical distance is only one of the distances you can incorporate.what do you mean?what other distances?i have been using a gan to create fake structured data. but will have a blog post out about it sooni mean how apart counties are to each other in other metrics. e.g., by population wealth.certain counties could be very similar, despite being far apart.oh yeah those are the covariates i would be including, which is why i didn\\'t want to use exponential smoothing and simple metrics like that.hello! i\\'m using convolutional neural networks for real-time emotion recognition from videos. i want to display the video in a web app while my python algorithm labels the emotions in real time. currently, i\\'m thinking to use the tensorflow object detection api with either flask or django. has anyone done anything similar? does anyone have any recommendations for platforms/tools to use? thanks!hi, i am trying to download a db share from onedrive, and it is 67 gb. i setup an aws account for it. i tried using curl but it concatanating the file instead of downloading which had worked before. does anyone have any suggestion on how to download a directory from onedrive to a aws volume? data engineering fellow in ny, pamela wu did something similari think i resolved it. thank you! :slightly_smiling_face:hopefully this is relevant: cool thanks!no, i did not. is there anyone to chipin?anyone have experience with  or something similar?yes, i tried that .. it was not great to be frank. i will suggest jupyterhub instead of this. but at the same time, it might fit your requirements better than mine.: we use it internally extensively, it fits the team need very well - we can easily share results with business partners, and keep a record of all the important past work.  haven’t used other alternatives, so can’t speak to a comparison there,a plus to knowledge-repo is that, one of our alumni actual is a main contributor to the code base!congrates on expanding to seattle, insight! some advice on choosing where to work: thanks josiahmwalton , stitch fix is killin it lately with the blog postsdoes anyone have a recommendation for a high-level crash-course or a good pedagogical textbook on quantum computing and algorithms? my phd was in hep experiment, so i\\'m starting from about halfway through peskin &amp; schroeder.still hard to beat nielsen and chuangnot an expert but at a high level data modeling looks like object oriented design without methods.  try stanford lagunita\\'s database class with jennifer widom.  it has a section on data modeling.  for big data specifically, kleppman designing data intensive applications has a couple of good chapters.  what do you guys think of the topic of this session   do you think it’s useful for implementing compliance to gdpr?using blockchain? :wtf:i see. so if you roll your own, then you’ll most likely want to automate the deployment process. be able to spin this up from scratch. pop in and out a hosts with 0 downtime. if you do require downtime be sure that you have multi colo replication so that you can do upgrades or what not to each cassandra cluster in each region without affecting data staleness or availability. for automation you can look into chef, puppet, ansible (config management) for provision of hosts you may need to write your own plugin for terraform. monitoring you can farm out if it’s easier, but look into what metrics cassandra provides already and which vendors they are compatible with (nagios, prometheus, datadog). if you’re plan is to move to aws alwasy keep in the back of your mind, is it worth it to build something specific to your datacenter or generalize it, so that it’s minimal work getting it onto aws.haha i kind of thought so too..our data science team at cz biohub is looking for someone to help us use phylogenomics and machine learning to detect, prevent, and understand infectious disease.dynamic, collaborative, and supportive team.job posting: does anyone have experience with creating a churn predictor?  i’d like to bounce some ideas off of you!: i do. what do you want to know?same. feel free to ping me.same!does anyone know of a good tutorial, book, course or videos that  teach time series analysis. i understand some of the basics but definitely want to grow in that area.: ask :  is a good introduction to a lot of the standard methodsbut i wouldn’t recommend reading up on arima until you’ve also read this blog post: i have a really basic regular expressions question. i have a column with mixed entries some of which are people\\'s names and some are strings of mixed numbers and letters. i just want to filter out the names and tried this names = lambda x: re.findall(\"[a-z]*\\\\s[a-z]*\\\\w\", str(x))but it returned letters in the mixed cells as well: i recommend one of the online regex tools (like this one which also explains the expression you enter: ) to test your regex with some sample datathanks! i may have just solved it but will need this in the future!!another question, has anyone dealt with columns with empty lists in a dataframe? i want to replace them with nans but am having some difficulty. thanks!oh solved it!: also, if you\\'re not interested in time-series forecasting but more on the inferential side of things (same base principles, different applications and practice), you may want to check this out:  this one also has a full chapter on time-series () digested here too (): we use “quantitative forecasting methods” by farnum &amp; stanton, pretty decent and focuses and data mining aspects.does anybody know someone who has been through hackbright? or hear anything positive/negative about the program? wifey is kinda keen to explore it..: insight is hosting a happy hour in la tomorrow evening. if you’re in the area, dm me! i’d be happy to get you the information a recent blog post by someone at my company may be of interest:   i am looking at it.  thanks! as far as i know everybody is fine, we did hear the bang in the office last nighthi guys -- does anyone have experience building bokeh apps in python, and would have a few minutes to chat?we have three positions open at travelers for those experienced in software dev in the ai and text analytics space. they are looking to fill these positions asap. be sure to mention that i referred you.   go to  and look for job id 9881br and 9560br hello! we\\'re looking for a data engineer who\\'s worked with big data systems at scale (aws, hadoop, spark), preferably with some experience deploying machine-learning models. if you know anyone, please dm me!monty python for me :wink:fyi, :cvs: is hiring a data scientist (~3+ years experience, so senior-level i guess) to lead some initiatives on the pharmacy operations side. the position is located at our corporate headquarters in woonsocket, ri. i\\'ve been doing data science work here, in support of the retail side of the organization, for six months now, and so if you have any questions for me please don\\'t hesitate to comment or dm me. here\\'s the job posting:  if i remember correctly, 1 week of insight = 1 year of experience, so everyone from insight should be good!new principle of relativity when applied to insight?cheers!austin and i just posted a blog on cassandra that’s featured on hackernews.  it features fellow projects and code on how to run queries and setup cassandra with spark:link to the entry: &gt; we also got a great introduction to nosql and cassandra internals from insight alumnus mike grinolds and patricio echague, director of engineering at relateiq and a committer to hector - the java client for cassandra. he also discussed data modeling for cassandra, which requires a different approach than relational databases like mysql or postgresql.that paragraph needs a rewrite.: thanks, it’s a bit of a run-on sentencefwiw: i guess the point would be not to make it faster for them but experience how it is to live like regular citizens i\\'m a data scientist working with economists.  does that count?i also know a guy who works as an economic fellow at the fda pm me if you want me to make a connectionthanks! i’ll dmknowledge graph @ apple is hosting a dbpedia community event: drop by and say hello! happy to answer your questions about what its like working at apple :slightly_smiling_face:for those in nyc and interested in causal inference : never mind. i got it to work. has anyone worked with facebook\\'s graph api to track daily social media engagement? so far i\\'ve pulled daily video views and timestamped comments, having some problems pulling daily sharedposts or other proxies of daily social media engagement (most fields can just return aggregated values for the lifetime of a post)so they’re including bonus and equity for employee compensation, but only include base salary for ceo compensation? seems highly misleading.even if that’s total compensation, where i come from that’s still mucho dinero.hi! we are hiring out our data science team at thirdlove. we are profitable/rapidly growing, just hired kim larsen - formerly stitch fix/uber - as our vp of data, and we are moving into a brand new building in sf next week.  if anyone is interested let me know! ( )anyone found a good forecasting library in python? i\\'ve seen prophet, wondering if there\\'s anything else. particularly interested in adding external regression variables to an arima. feel free to dm :slightly_smiling_face:hrm, i now see statsmodel has an arimax with `exog`. anyone have experience using exogenous variables in this way? completely agree. just pointing out that, unfortunately, in many organizations the structure doesn\\'t guarantee a project leader with the skills to advise on technical roadblocks. this is different from an academic setting, where almost by definition, your advisor has more experience/expertise and can steer the project in the right direction.strategic incompetence colin. :wink:anyone have any suggestions for picking up scala/spark? tutorials, courses, books, etc.there is an interactive tutorial with dean wampler called “just enough scala for spark”. if you don’t have a safaribooksonline account, you can use a free 10-day subscription to check it out: this is a great book, but not very introductory: because i know everyone asked this question every day of insight: are any insight fellows at the spark+ai summit right now?so what do people say about the whole github issue. for some reason most of the people around me already started migrating to gitlab ….  are going from my team! there are other insight alumni from intuit going as wellerrr…  is going, nick is on vacation. :wink:\"microsoft will need to really mess things up for me to want to switch\" said our team leadi see people saying “oh there going to pull off the same thing they did to skype and turn it to a crappy product” and others saying “this is a good thing, nadella has definitely been doing a good job bringing life back to the company”  i don’t use much of their products really (except linkedin + now github!), but their reputation + adoption seems to have improved recently.we had engineers planning the migration to gitlab out of paranoia, that conversation was shut down pretty quickly by our ceo ^we have two data analyst positions at rovi  in boston. if anyone wants to apply please send me your resume: .  i can confirm that it\\'s a great place to work. we build an analytics product for television networks that predicts their viewership and optimizes promo and ad placement. we get to work with big data, do machine learning, and all sorts of fun data sciency things.also, feel free to pass this along to other good candidates you may know. the position is entry level so for someone who has a b.s. or a m.s.hi all, does anyone have experience working with mobile phone datasets?  i’ve seen some interesting things come out of academia, but seems hard to obtain.ive heard there are european datasets that are obtainable, but never fleshed that outhi guys.. any data engineers/architects/scientists  in the health care field?  would love to chat about prod issues that you might have encountered in the day-to-day job.&lt;--question to data engineers about “staging data” in etl process, due to technical complexities we can’t have a day behind refresh for our staging databases. i am thinking that having refreshed data is not necessary, and there must be methods were we can test by randomly sampling data? i guess in general, i am interested in knowing how de’s go about testing their etl in staging. thanks!yeah, random sampling sounds like a good idea.for instance user 2017 conference live-stream link and schedule has anyone ever used coursera’s tool called dataducthi  thanks for the reply, when you are testing do have a suite of things to check off?or are you mostly just throwing the whole data set? in the context of datawarehousing.i also think the snapshot operations are not available to us, as i think our ops team would restrict us from doing that.what are the pieces in ur etl? mysql-&gt;redshift?combination of mysql, and oracleto s3, then s3 to redshiftuseful guide for effective t-sne viz: man, that thing has spread like wildfire across the ds worldfood for thoughti wonder which category insight would fall into… i also think that the people who do moocs might be very well practiced in answering technical questions in a similar format to . assuming they did the homework, this would mean 10-20 “practice” questions.i think everyone would like likely benefit from  practicing on  :slightly_smiling_face:the blog makes me think of claims like “people who buy baby books are good parents because they care enough to buy baby books, not because of the content of the books\"* nah, i\\'d rather coast on my sweet pedigree of the university of iowa. who needs to study and practice?yeah, i don\\'t quite believe the causal implications of that lost*posti don’t want to minimize the importance of wanting to succeed and taking action toward that purpose, it just seemed to completely ignore an equally, if not more, important (and very well studied) factor of practice makes you better at things.for the physicists: \"don’t underestimate the power of giving people a common platform and operating language.”  uploaded a file:  and commented: if anyone is looking for something different, general assembly is looking for a full time instructor for their new data science course in san francisco ().  they are also looking for people in the field to give lunch talks and work as tas after work hours.  if you’re interested in any of these things you can contact teresa todoroff at .feel free to spread this around.#makedonalddrumpfagain :smile:anyone going to this tomorrow night?gosh i love 2fai think i\\'ll refrain from resetting everyone\\'s password herebut i encourage 2fa:tada:hi! we have an opening in our group at jpmorgan for  senior data engineer specifically with hadoop and spark expertise. feel free to pm if interested or know somebody.. :grinning:anyone have a good javascript tutorial for crawling websites? i\\'m working with data that\\'s stored on a vendor portal, and it would nice to figure out how to crawl the page and download each file without having to manually do it myself. :stuck_out_tongue:dk about javascript but i’ve done autodownload scripts in python with `requests`i did something using `requests` combined with `selenium webdriver` to go past javascript queries and reach specific pages i was interested in scraping. if that\\'s what you\\'re interested in, let me know.never crawled with javascript before, are you talking about scraping a page w/ javascript.  if you want to run full fledge crawler, scrapy and nutch are both greatwas talking to a consultant, and he says many companies just turn on a ec2 node &amp; have schema registry be hosted there, and producer, lambda, and consumer can all talk to it if it’s in the same vpc.after speaking with 50+ top applied ai teams, we wrote up some tips for academics who want to transition to applied ai roles. if you like the content, could you \\'applaud\\' the post on medium and share it with people who might find it useful? sort of.for some context: the data i\\'m working with is parking meter data (how long a person parked at a meter, how much they money they spent at the meter).right now, the vendors for the parking meters have the data, and make us go through a webportal to download the data.the portal asks for a the meter id, and a date range. then it will pull up all transactions for that date range.there is a download button that creates another popup with some radio button options for how to dump the data. one of the options is just to simply dump it as a raw .csvi might have to pick your brain at some point.pretty amazing deal even if you\\'ll only use a few of the books.i assume it\\'s just the e-versions?yeah, just the ebooks (pdf, mobi and epub). though you also get to donate to a charity in the process, you can even choose to have all the money go to charity.perhaps doing the scraping with python\\'s scrapy would be easieris there a specific reason for using javascript?  would be happy to answer any dataproc question that you might have. my sister-in-law just started a new job that is a bit more ds/coding heavy than her prior post-doc and is looking for a good bash tutorial - any recommendations? this tutorial covers everything you would want to know as a beginner/intermediate. it is paid, but you can get a free 10-day subscription.  can’t compare it to anything else, but this book (you can download as pdf for free) was helpful for me: thanks  and !nice podcast of interview with  has anybody worked  with datasets, where number of features is growing as you do ml?i understand that it is kinda like missing data handling. but in this case missing data has a very clear pattern to it, so many techniques won’t work.i couldn’t find any discussion about this topic.anyone have book recommendations for a ds to learn user research methods?has anyone worked with pachyderm?like ux research?alums: how is data documentation handled at *your* company? can anyone share best practices? where do you store documentation (dropbox? github? google docs?) do you use version control?  what is this \"documentation\" you speak of? :stuck_out_tongue:actually, yeah. what do you mean by documentation? metadata? provenance? uses? change history?all if the abovelet\\'s with data dictionaries (metadata) and we would like to track updates to this documentation. *let\\'s start withi\\'ll just sum up our practices with: :disappointed:we actually don\\'t have a great way of tracking \"raw\" data, but i think that\\'s partially our nature as an org that does something completely different every 4-6 months. for \"raw\" -&gt; \"results\" tracking we use sacred: we have a similar problem to  in that we\\'re a consulting company constrained to whatever our clients have in place. i have yet to see an org do it well thoughstay positive higginscstuff it nuggetboti will say i have seen several startups that promise to help you do it; i haven\\'t found them to be great either. :confused: often it is a lot of overhead, and forces a major change in your work flow.we\\'ve had great success with mkdocs for both general and technical documentation.  super easy to write new pages (it\\'s all markdown) and if you put your site on git then you\\'ve got both version control and easy access for your whole teamanyone use readthedocs? just linked there from sacred, looks coolhost your static mkdocs site on an aws box (or your favorite server) and bam! documentation that anyone can access over the internet :) yeah, we looked at readthedocs but decided mkdocs was nicernice. thanks!  i’ve used readthedocs for academic code i wrote, was easy to use. also creates documentation from your python docstrings which is nice. i imagine mkdocs can also do that aswell.for both mkdocs and readthedocs, can these sites be private? can we set up so that only team members part of a bitbucket (private) repo have access?no idea, sorrydefinitely. all mkdocs does is create static web pages, so the hosting is up to you(it also has a built-in server, but that\\'s more for interactive documentation writing) gotchathanks all!hi all, now that slack has \"thread\" features, can we start using that more proactively in ? i find it hard to follow multiple simultaneous conversations some times and threads are a nice way to keep each conversation modular. please give it a try!agreed. the drawback is the lack of visibility for things that might be of interest to the whole channel. i also encourage the usage the checkbox below to post back to the channel when appropriate.hi insight! we are hosting am r-ladies meetup at flatiron health next tuesday. stop by! current insight fellow in the news!awesome article hey sf folks (although there are also satellite locations in every other place insight has programs), i co-organize a free event in town called astronomy on tap bay area and our next event is on feb 21 at dna lounge at 7:30 pm. it consists of 3 presentations from astronomers, an astronomy in the news segment (hosted by yours truly), trivia (with prizes), and free giveaways. no science background is required and all ages are welcome. because space is always better with beer! for any of you who moved cross country, can you recommend a good mover that loads/unloads. google uses ace relocation for their new employees. i didn\\'t look at the prices, but we had good service with them. data science conference at stanford via pat.r.callier: *”become awesome at technical interviews”**\"practice interviewing with engineers from top companies, anonymously. get better at algorithmic problems, find a job, or just see if you\\'ve still got it. it\\'s like chatroulette[1] for technical interviews.”*[1] but without the dickspretty great footnote: depends on which meaning of the word.i think it works in the senses i can think ofit was soooo awesome!!!! one of my fav conferences everis this a rogue, rabid ai trying to eat us?just make sure no one gives you demonetized 500/1000 rupee notes. when in doubt as for the new series notes.  old series:  new series: : sorry i missed this thread earlier. feel free to message me directly if there’s anything i can help with!when you’re debugging why commands don’t work, it can always be helpful to type “which &lt;cmd&gt;”such as “which wget”the result shows you the binary which is executed for the commandif you see nothing as an output, it generally means that the directory containing the binary is not in your pathi.e. $pathif you just installed something in a shell, but you didn’t reload the shell environment, you can always try opening a shell (which triggers sourcing files like “.bash_profile” and “.bashrc” which modify your $path variable to include searching for stuff you just installednot sure who to ping about this - but when posting a link to insight websites on pages which produce automatic url image thumbnails, sometimes it seems to just pick this one fellow’s image as the thumbnail. not anything wrong with that, i guess, but it might be worth investigating how to set up the page so that a representative image is chosen instead? just fyi. is this your department?just copy/paste a link to each program into a linkedin chat - you can see the behavior.yes, i’m not sure why that photo was randomly pulled from all the ones on our front-page (though i’m happy to feature alvin). i’ve spent some time trying to get this photo uncached, and replacing our social cards and opengraph profile, but never invested enough time to fix this. will look into it more - thanks for the reminder.thank you! i am originally an ubuntu user, and \".bashrc\" was coming built in. looks like, there are quiet a lot of differences between 2 systems. i fixed the bash &amp; path problem with installing brew (which was not straight forward due to the new mac os and new updates); and i was able to install the problem i was originally going for after. so far so good, up until the next problem! :slightly_smiling_face: thank you for all the replies. i appreciate it.how do those headphones even stay on?velcrodj kitty has commitment to his arrartnuggetbot: weather palo altoweather for palo alto, ca, usacurrently: partly cloudy 21.1°c/70°ftoday: mostly cloudy until tomorrow afternoon.coming week: no precipitation throughout the week, with temperatures bottoming out at 14°c/57°f on saturday.nuggetbot: weather nycweather for new york, ny, usacurrently: overcast -3.5°c/26°ftoday: light snow tomorrow afternoon.coming week: mixed precipitation throughout the week, with temperatures bottoming out at -4°c/25°f on saturday.nuggetbot: weather bostonweather for boston, ma, usacurrently: overcast -6.3°c/21°ftoday: mostly cloudy throughout the day.coming week: mixed precipitation throughout the week, with temperatures rising to 6°c/43°f on wednesday.nuggetbot: image me data scientistanyone know a nice, simple, pythonic way to grab street maps by lat lon which dont look gross when plotted over?: you can use geoplotlib to overlay maps on top of street maps; using the boundingbox method lets you specify the (lat,lon) of the corners of the map, so you implicitly specify zoom as well. you should be able to just call the map, without any overlaying pieces as wellalso, more for  thanks for the rec! someone lost a phonestay positive thang and  : thanks for the help guys. for now, it seems like i have a good enough fix. will check sunlight if i need  better quality data.a good small tutorial on how to use flickr\\'s api using a python wrapper! a good tutorial on reading tweets and doing some natural language processing on them via python: flickrapi is great; it\\'s what i used for whereto.photowhereto.photo is great.  i wonder what happens if i try to find pictures of \"yoga\" in sf?or maybe i want to go to a \"fair\" in soma.inappropriate:disappointed:now i\\'m going to get firedi was going to let the inquisitive minds find it for themselves.  no need to make it easy for them.news-reading habits dataset: lol  your hot dog post disappeared!cia take it down?:wink:: for those interested in scraping data, this is a very easy to use tool.. just create an account and click buttons. no code required. the example they use looks very apropos for  cool good read: looking in to it now - also conversing with  channel creator,  what could go wrong with 500+ insight fellows having access to the api?  :wink: anyone interested in going to a deep learning/tensor flow meetup in sv?just signed up. thanks  !insight will be at strata this week! looking forward to seeing you all there! hey fellows! if any engineers are currently looking for new opportunities, my company rally health is aggressively hiring software and data engineers for our core team! here is my referral link: positions for data engineers aren’t currently posted, but if interested please let me know and i can refer you in person. thanks! for 2016 you\\'ll have to scrape it from the secretary of state on every state, or ap\\'s election service api  not sure if still freeanyone using zeppelin with spark? or just using zeppelin in general i suppose?i\\'ve got the team here at placeiq going on it and i\\'m finding it pretty decent if you need a notebook that works well with yarn(we had too many problems trying to get jupyter working)we’re doing some work on azure, and you can configure jupyter notebooks that connect to a cluster thereso we connect locally and it uses yarn in the backgroundthough i do want to get into zeppelin at some pointi’m also supposedly learning graphx — have you all worked with graphs as well?i don\\'t know exactly the issue we had with jupyter on yarn but i think it was around user queuessince politics are part of that channel...here is an interesting article worth sharing about false info. super fun! neural nets recognize your drawings...   other onesreally cooldamn, quickdraw would kick my ass in a game of pictionary :stuck_out_tongue:stay positive ramalldfany headless-browser pros out there know how you’d go about building a crawler that can tell you whether a given site has a mobile-optimized version?well for crawlers i like splinter hey all: does anyone have any good resources for marketing data science (eg attribution analysis, customer segmentation)? looking for more methodologies than cheerleading (finding a lot of the latter)liverence:  papadimitriou.c: awesome! thanksi can share a bunch of academic papers with you via dropbox if you\\'re interestedmorning, are there alum working on online grocery delivery business? i’m working on a similar insight project and am eager to learn about this business. your two cents will be my precious gem! thanks! :slightly_smiling_face:hey jeff, i used opencv in python to get frames from a video for my insight project. you can see it here. warning though - really poorly written code: i am doing an insight project to identify trolls on twitter. i have some data and i’m pulling from the api but if any fellows have twitter dumps they would be willing to share, i would be grateful! i am particularly interested in @replies. i hear you might  … i believe had a twitter dump? has a twitter dump too: nicole wong () from my cohort did a similar project, might want to ping herthanks!i didn\\'t cache any tweets so unfortunately i don\\'t have any twitter dumps. happy to chat abt any other q\\'s you might have about the project tho! i have grabbed an additional pile of tweets that i’m using now so i may be close to having what i need. i am curious if you did any sentiment analysis. i’m going to be working up a model to classify users as troll or not.does anyone have experience extracting information from pdf tables (of a range of different formats)?kevinmercurio: yes please! email is i\\'m not claiming it\\'s comprehensive or well-organized, but there is some content :wink:willfrost: it\\'s hard. i used image processing techniques to clean up the pdfs first and then ocr (tessaract) to read in the datai used  for a small personal project and it worked well except that i had to manually tweak its automatic bounding boxes to get the last rows in my particular tables.  thank you! i will have a look into these options.hey kevin, mind sharing those with me too? email is i used pretrained vader sentiment analysis, it was pretty good imo. it was developed specifically for twitter and social media analysis. a friend of mine made this recently and it was a hot topic on hacker news. i haven\\'t used it though and don\\'t know if it will suit your needs. if you come up short i may be able to dredge up a tweet dump from awhile back but i am unsure if i archived it properlyplease feel free to contribute more :slightly_smiling_face:if you\\'re familiar with bash, i\\'ve used pdftotext with a combination with sed to get information from very old pdfs with tablesjust use `pdftotext -layout -eol unix &lt;file&gt;` to get the pdf translated into a text format that you can later filter the information you wantyou might want to check facebook as well as you can get the post sentiment and reaction built-in via likes and the sorts. especially for facebook posts from news sites and other public-oriented entities.  might be able to help.always interested in learning more. share away at latest that came to my table was :+1: for opencv. i\\'ve used it in both python and c++ for image analysis and features recognition, and works great!although if you are only looking to get particular frames from a video, there might be simpler solutions to it. e.g., vlc is able to save any number of frames every x frames. it allows you to do it graphically or via terminal (as ffmpeg)have you tried fuzzywuzzy?maryamtaba: insight pds should have some example datasets and i are planning to go climbing at brooklyn boulders somerville, ma tomorrow at 11 am. anyone interested in joining us? check out stephaniemari: will do. thanks! i don\\'t have a twitter dump (yet), but just wanted to mention that i\\'m also going to be doing a twitter project that will (in part) involve sentiment analysis. so let me know if you want to swap thoughts/insights (pun intended) over the next few weeks. that would be great! maybe we can find a time where we both are awake and free next week and do a skype call? actually, i’ll message you and share my week1 files…cyucheng: no can do for this weekend, but i\\'d love to get back to it. my climbing gear has been collecting dust :cry: i have a blood feud with bkb.  come to metrorock instead :stuck_out_tongue: mind sending the link to ?thanks everyone! i was able to get what i wanted with moviepywould love to have a copy too. , thanks kevin! last time i listen to a ground hog...the snow/slush is starting to accumulate. at least in parts of jersey.^ that pic in the article is from a south indian movie “enthiran” (robot in other languages). i was hoping for some reference to it and found this instead: uploaded a file: omg what could this mean??(former) theoretical physicists out there: there’s some string theorists down the street (in sf) collecting 3 million dollars a piece:hey all, i’d love to have some honest first-impression feedback about my resume from fellows in the industry. i’m not asking for a detailed analysis, just your quick opinion. pm me if interested, it’d help a lot. thanks!!is anyone familiar with negative dependence/chernoff bounds?does anyone have experience running pyspark jobs on an emr cluster using spark-submit?  i have a job that hangs indefinitely in \"running\" mode even though it should finish in about a minute (which i know because it worked ok inside of a jupyter notebook) and would love some advice on how to debug...maybe  \\'s territory? is it possible that another job is being submitted to the same cluster? i imagine that amazon emr will split the resources if there are multipile though. also see if you can get any sort of logs our of emr that may point to the problem. possibly permission issues accessing data.another austin! :wink:#replicaskubernetes pods of austins?sure!thanks everybody for your help!glad to hear it! could you expand on what you mean by random shuffling and what it helps with ? very curious about that.anyone at strata?  if you’re here come by the  booth :slightly_smiling_face:sure. you simply do a random shuffle of your test labels (`random.shuffle(y)`) and then compare against the unshuffled labels. this will give a worse overall accuracy, but it gives you a starting point for f1 scores for the minority classes. it is essentially choosing randomly given the relative number of each label in the dataset.hi there! any de\\'s mindd giving a noobie some advice? i think most of the de channels are kinda dead, but these seem active so i thought i might try my luck. specific questions get specific answers! try it out :slightly_smiling_face:i didn\\'t want to spam the channel w/ a convo, so i was hoping to just pm a couple people with specific questionsnobody knows if they\\'re the expert you need unless you describe the expertise you need!is anyone else a data engineer at a startup w/ less big data needs creating data pipelines from the ground up?i had some architecture ideas that use big data frameworks, but since there aren\\'t foreseeable scaling problems, i think in some regards they\\'re overkill, but as far as integration w/ other software they might be a good fit despite maintenance overhead / cluster management.mostly because glue has transforms that can translate into spark calls, andd those spark calls can be edited later as requirements change.that’s very cool, i’ve never encountered that before! do you happen to have any references to the method i could look up to dive deeper?whoa... will be checking out right awayiqvia is looking for a tech-lead for our data science platform. if you are a ml engineer or data engineer looking for a larger role, we would love to have you on the team. dm if you have any questions about the role or the company.role is in philadelphia or possibly anywhere in the northeast  (remote)i don’t have any great references specifically on shuffling :confused:.  i originally saw the idea of shuffling in the context of bootstrapping p-values for hypothesis tests in mine cetinkaya-rundel’s statistics course (e.g. section 1.8.2 in [1]). for a comparison of uniform random guessing, weighted guessing (which is very close to shuffling, but would be based on your training data distribution), and guess the most frequent; see [2]. also sklearn’s `dummyclassifier` can easily do all of these methods [3]. i suppose if you wanted a weighted random baseline, only predicting based on the training data would technically be best practice (and now that i know about `dummyclassifier` maybe that’s what i’ll do in the future). shuffling is just so quick```y_shuffled = y.copy()random.shuffle(y_shuffled)print(sklearn.metrics.classification_report(y, y_shuffled))```[1] [2] [3]  posted a nice tutorial:  fwiw, i\\'ve had good results forecasting thousands of time series using the `spark-ts` package. i think it\\'s available in pyspark, so low barrier to entry if you don\\'t work in scala. :wink:we (salesforce einstein) are looking for senior data engineers with scala/spark experience. see posting here: feel free to contact me if you have any questions!anyone here use jira to report bugs and track tasks? we currently use asana and very much don’t like it… we use jira for task tracking for data and engineering folks at my shop, asana for client facing folks.jira is also horrible but i don’t know which is worse since i haven’t used asanai\\'ve seen a shop that used a combination of github, trello and slack integrations for engineeringthanks for the feedback!if anyone is looking for an apartment in an excellent location (greenwich village), we are looking for someone to take over our lease sometime in the near future (trying to buy a place). no broker fee is pretty cool! or if anyone knows anyone... tell that someone! was looking for somewhere i think?anybody here going to data engineering conference in bk today and tomorrow? will be there! as wellyay! hope to see you there :slightly_smiling_face: i’m there now! hhh also is here! as well as jinchao and practically the entire capital one team! nothing special, ease of use mostly.  just want to play around with streaming and processing. wasn\\'t sure how easy/hard it was to get going with different brandsthat apartment is super cute, and on the plus side makes me feel less bad about my rent in silicon valley! :smile: best of luck renting it out! i think it shouldn’t be too bad. when you plug in the webcam it typically registers it with the computer as a camera option. you just request a frame from that device. maybe borrow one and try it out?stay positive jeremykarnowskii\\'ve had problems connecting to the internal webcam on macs with opencv in the past, but this was using the c++ interface and an older version of opencv (you using 2.4.13 or 3.1?)it seemed more stable in linux (which was a surprise).. sometimes it depends on the webcam manufacturer.. some high performance usb3 webcams are still problematic due to driver issues on macs.any experience on using arangodb. or are there any other graph or multi model dbs you can suggest?  any war stories on neo4j and orientdb?neo4j is really great in my experience, i can’t compare it to other graph dbsbut cypher is real nice for sure uploaded a file:  and commented: selfie courtesy of alihello all! does anyone have experience with domon dashboards here?#domopossibly i’m supposed to learn the capabilities as well :wink:this must be nuggetbot’s finest work. these days your isp is almost 100% determined by where you live. are you going to be in sf? nyc? at least here in nyc your options are either cable from time warner cable or fios from verizon (but only if it is available for your building). i would probably lean toward fios if you can get it but it is really just picking from the least of two nearly identical evils.maybe, (hopefully?), some other insighter has some other secret knowledge to jump in with?ah... in australia everything is federated, so one has a lot of choice.i\\'ll be jumping into the sf area, probably around san mateo... who are serviced by comcast. is adsl comparable in performance? more federated?(thanks btw)there are a few recent start up isps that have been trying to compete with the likes of comcast. i don’t think any have made it to nyc but hopefully the story is different in sf. i haven’t seen any talk of adsl in years so i think it is mostly dead in favor of cable or fiberah... it\\'s interesting that that is almost the opposite of the situation in australia.cable ultimately seems to have fizzled in favour of adsl / nbn (fibre)anyway... thanks for the insight.:wink:: might help: my rough (and highly opinionated) summary of the situation in the bay area is as follows:- comcast is the fastest and most evil- at&amp;t is slower, but less evil- wave is not so evil, and pretty fast- sonic is not evil, but often just resells at&amp;t so the speeds are comparablesonic is the only one in the area without datacaps(i use sonic dsl in mountain view and get 22mbps consistently, which is what they advertise at least, but also as fast as they offer down here)thanks alex! looks like sonic is the best choice :)new insight data science blog post! check it out when you get a chance. eli talks about the need to look at your data as your very first step.  has anyone used scikit-multilearn with text data? hi, has anyone here had the experience of installing a virtual box on windows machine ( windows is the host)  and then using cuda-gpu on the linux guest platform?! i\\'ve had difficulties installing cuda on ubuntu! you likely need pci passthrough working, which tends to be temperamental. are you on windows 10? can you do the things you want in bash for windows?its not possible i think. i use virtualbox on windows 10 (host) and run ubuntu as the guest os. the gpu is not fully exposed to the guest os. for example, the maximum gpu memory you can assign to the guest os is 128mb.your best bet is dual booting or using cuda sdk along with conda on windows. that’s true.. i shall perhaps think about ways to make comparisons with this. thanks for your advice :slightly_smiling_face:has anyone worked with trying to detect anomalies in time series data, or, for example, trying to detect statistically significant differences in a metric across ~100 ab tests where you have daily measurements? #multiplecomparisonfunhahaha if only there were such a thing! :parrot_mustache: :beers:thanks. i am on windows 7. i\\'d like to use pytorch, but i should have linux installation for that.thanks nishan, i think your suggestion of dual-booting is the most feasible solution for my problem.sheareraj: wut! that corgi is amazing... this is from 2013 :slightly_smiling_face: :hourglass_flowing_sand: what income on the side???  show me the :moneybag::grinning:greetings, i would like to contact former fellow yuhong wang to ask about a job at radius intelligence, but i can\\'t seem to find her on slack. anyone know the best way to get in touch with her?hahaha it actually belongs to i think she was a fellow in the silicon valley ds sessions. her name is remy :slightly_smiling_face:kevinmercurio: i did a lot of time series anomaly detection in grad schoolaxl002: hey alvin. yuhong left radius a few months ago. let’s touch bases in a biti see! perhaps we can chat this week?hi smart people!! i do financial forecasting at wayfair. one of the challenges i face is the financial info is constantly being updated, so i end up doing a lot of gymnastics to recreate the past. we\\'re in the process of scoping out a solution so that we have snapshots of the table across time. i\\'m wondering if anyone has a smart way of tracking changes to order-related financial data across time.  the data is currently all stored in sql, but is quite large (i.e., the base table is 400 gb) and getting larger. thanks!!take snapshot once every week and then store change logs the other times?sounds like the abc of functional connectivity analysis in neuroimaging :smile: what\\'s significant when you have pair-wise correlation of 15k time-series.my company, driver, is hosting a tech talk next week: \"curing cancer as a machine learning problem!\".  the event will be at our headquarters in san francisco on thursday, june 22nd from 6-8 pm, and presenters include our ceo and members of the engineering team.more details and rsvp at   hope to see some of you there!\"this is the last century that our children will ever have been taught that one times one is one,\" he says. \"they won\\'t have to grow up in ignorance. twenty years from now, they\\'ll know that one times one equals two.” - terrence howardread more: lolwut^ i wonder if it’s possible to choose different axioms and construct an algebra wherein 1*1 = 2. what would the axioms have to be? what would the consequences be?axiom 1: do a bunch of shots.axiom 2: accept terryology.the future is already here: brad.deutsch: in a ring (cf.  ) this can only happen if 1 = 0, just look ```1 = 1*1 = 2 = 1 + 1 =&gt; 0 = 1 ```provided your definition of 2 is 1 + 1the existence of a thing is questionable, but there is a lot of virtue in behaving as if it does exist (at least if you want the thing to be a field -- there might be exotic rings that have strange zero divisor)bad news terrance: we already know why bubbles form spheres and not triangles... :disappointed:stay positive alex_gudehas anyone calculated nuggetbot’s false positive rate? : thanks. another mathematician friend of mine agrees. he suggests that this type of structure would be a semigroup since we can’t find an identity element, and would be fairly uninteresting. ah well, i wanted to give mr. howard the benefit of the doubt!not strictly ds/de related, but this paper shows mathematically that under very modest assumptions about error and bias that we should expect *most* medical science studies to generate incorrect conclusions. (no paywall)stay positive brad.deutschstep in front of a bus thanksgiving data from 538!is anyone familiar with n-back tasks? for example you\\'re presented a color on each trial and have to remember if the color n trials ago was the same. i\\'ve been tasked with classifying the difficulty of different patterns. for example, for n=4, a pattern could be blue red yellow green blue. i have quite a bit of data, but aside from some arbitrary cut off of difficulty ranges based on average percentage of correct trials i\\'m having trouble figuring out the best way to classify their difficulty. i was curious if anyone had any suggestions for methods that could be used classify the difficulty of these different patterns for a wide range of n-back levels (1-12). another issue is that as the n-back level increases, so do the number of patterns, so coverage is light beyond n levels of about 4. one standard thing to deal with low sampling in the levels above 4 would be a permutation test on bootstrap values. as for the performance estimation goes… if this is a change detection task (you mentioned observers need to say if the color was same or different on n back trials) you could probably figure out a way to use d\\' as a measurement of sensitivity [d\\' = z(hit rate)-z(false alarm rate)]. another idea: to measure the performance on this task (or the classification difficulty) one way would be to reduce this problem to 1d space and play with the distance between accurately classified colors in n back conditions in the cielab color space. or perhaps look into using pse (point of subjective equality) which is basically the moment at which observers respond randomly to “same/different” question on your experiment. anyway, feel free to pm me, probably no need to get into details here :slightly_smiling_face:d3 question. has anyone ever passed d3 strings to flask? it doesn\\'t seem to like quotes. for example i have a list of dictionaries like {\\'label\\': \\'ruby\\', \\'value\\': 345}meant strings from flask!oh figured it out! {{var|safe}}why not use stan?oh nice, i see edward is at least 35x faster than stan and 6x faster than pymc3wow that\\'s quite a speed-upi\\'d be very interested in hearing more about edward... looks very promising...it does seem really powerful!  which is why i\\'m trying to learn how to use it, although nobody else at my office has done it.  the gitter forum for edward is good for help, though.does anybody have experience making customer-facing reports with embedded python scripts? suggestions for what to use?oh boy do i! what is your output format? `jinja2` is a good start for many use casesright now i want to just output to pdf. would you recommend it for that?caveat that i haven\\'t used it, but i\\'d probably start with the `fpdf` library.depending on what you are outputting, matplotlib has a pdf backend the library reportlab is more comprehensivei’ve actually used `simpleidml` to generate adobe indesign pieces and then gone to pdf from there, simply because indesign is much better at…designfor the purposes of making pretty reports, i meanthanks all!i’ve played with it a bit but found it very confusing. can’t tell if it’s actually confusing or due to my poor bayesian abilitiesknight foundation, omidyar network and linkedin founder reid hoffman create $27 million fund to research artificial intelligence for the public interest   hey insight people: looking or have a friend looking for a job? - i recently joined a data analytics startup called kofa in san francisco, and we’re looking for more engineers to join our growing team! we’re a self-service analytics platform that prevents companies from reinventing the wheel when it comes to data pipelining, analysis, and modeling. we’re backed by top-tier investment firms such as accel and sv angel, and the founders previously sold their first company to palantir. all of us are ex-stanford ee/cs excited about working with new tech and building a company from 0-&gt;hero. over the past few months, we’ve doubled our team size and nailed down contracts and pilots with companies across the country. we recently closed a six-figure deal and will be doubling the number of companies using us by the end of q1 this year with the intention of raising series a by decemeber.we’re actively looking for talented and driven backend/data engineers/fullstack, and we’d love to chat about the product in more detail if you or any of your friends are interested. you can see the full job description at , and send me a message at  if you’d like to learn more. hello insight friends, our company is hiring machine learning engineer in san francisco, and here is the job description  feel free to apply using the link above or contact me directly if you are interested. thanks!friendly request that we not blast 800+ people in the general channel with `@here` or `@channel` messagesokay :wink:hello insight fellows:  ibm is looking for volunteers for ux/ui testing for a cloud platform.  there is an honorarium if you participate!  here\\'s the info:we\\'re currently trying to interview some data engineers about this idea to find out what your biggest challenges are.   would you be interested in talking with us for 45 minutes about your experiences trying to ensure data quality, low latency, and scalability?  qualified volunteers will receive a monetary honorarium as a thank you for your feedback.  contact me at  or .  thanks!there\\'s a new movie about snowden if you\\'re interested in the silk road case, deep web is a pretty interesting documentary that covers the investigation and ulbricht\\'s trial.domino is having a conference for data science leaders. nate silver, cathy o’neil, and wes mckinney will be speakers. there will be a practitioners track and a leadership track. use `dominodl` for $100 discount off the current early bird price.we’re also still looking for good speakers, so let me know if you’re interested or know someone who might be!what are the conference hours each day? 9:15-5:15cool, thanks!:+1:hi i am looking forward for this conf. is the dominodl a promotional code to get the $100 discount?, yes that’s should be the code. let me know if you have issuesok thanks for thishere is an upcoming opening for product intelligence manager in my current company (liveramp). let me know if you are interested:manager, product intelligence san francisco, cado you love working with incredibly smart people who inspire you to be better and work harder every day? do you thrive in highly cross-functional roles that work across engineering, product, and sales? are you excited to contribute to building a product that will transform marketing? consider joining us.product intelligence is a newly-formed group charged with deepening liveramp’s understanding of how customers experience our products and also with helping pms make strategic, data-driven product decisions. you will:lead the product intelligence team, shaping its charter and priorities to maximize impactprovide direction to stakeholders on how to measure the impact of product changesscope and manage your own analyses and those of your teamcreate frameworks that enable your team to execute analysis quicklyserve as coach and mentor to analysts on your teamcontribute towards building a great product team and great companyyour team will:deliver actionable insights to pms, engineering teams, and other stakeholders according to the priorities you set in conjunction with stakeholdersbuild and maintain reports, dashboards, and metrics to monitor the performance of our products, understand the customer experience, and track relevant kpisabout you:you have 3+ years of analytical experience in business intelligence, data science, sales operations, business operations, management consulting, or investment bankingyou have built models and dashboards for large audiences using tools like sql, ruby/rails, python, r, looker, tableau, etc. you are amazingly detail-oriented, but can also bottom-line analysis for an executiveyou can understand, manage, prioritize dozens of projects guided by a strong sense of priority and focusyou can communicate clearly and quickly. you synthesize effortlessly, and always answer emails and calls within 12 hoursyou have a bachelor’s degree in computer science, math, physics, engineering, or other quantitative field.you have a type s(tartup) personality: smart, ethical, friendly, hardworking and proactive you have a desire to work in san francisco. we will relocate you if necessarybonus points:experience leading an analytics teambenefits:people. work with talented, collaborative, and friendly people who love what they do.food. enjoy catered meals, boundless snacks, and the occasional food truckfun. we host events such as game nights, happy hours, camping trips, and sports leagues stock. every employee is a stakeholder in our future. health and saving. receive the benefits of comprehensive health, dental, vision and disability insurance along with a 401k matching planlocation. work in the heart of san francisco and take advantage of our commuter benefitsmore about us:liveramp is the leader in data connectivity, helping the world’s largest brands use their data to improve customer interactions on any channel and device. we thrive on mind-bending technical challenges and value entrepreneurship, humility, and constant personal growth.there is so much more that we want to build and that we could continue to improve. we value strong engineers who are agile enough to hit the ground running and tackle challenges.to all recruitment agencies: liveramp does not accept agency resumes. please do not forward resumes to our jobs alias, liveramp employees or any other company location. liveramp is not responsible for any fees related to unsolicited resumes.liveramp is an affirmative action and equal opportunity employer (aa/eoe/w/m/vet/disabled) and does not discriminate in recruiting, hiring, training, promotion or other employment of associates or the awarding of subcontracts because of a person\\'s race, color, sex, age, religion, national origin, protected veteran, disability, sexual orientation, gender identity, genetics or other protected status. qualified applicants with arrest and conviction records will be considered for the position in accordance with the san francisco fair chance ordinance. is there an extended version with more details? :stuck_out_tongue:i\\'m guessing you don\\'t mean it but to tell you the truth, this is all we\\'ve gotit hasn\\'t been posted anywhere yet so pardon me for the crudenessjust read about youtube incident. hope everyone there is safeapplication for speakers: this worked fine, thanks for the promotion.posting this for a friend. please let me know if you’re interested and i’ll forward on your resume.it is a machine learning engineer role.hi all - does anyone have any deep experience with gsuite domain management? we’re running into a problem where we’d like to change our primary domain and email records with no service downtime (i.e. the company’s name and domain is changing) but it’s apparently not that simple. private message me or email me at  if you’d be willing to get on a call (for $$).sorry i\\'m late to the party gang. i applied for a tn in fall 2017, and tried to use the physicist category. the person interviewing me said that was a no-go because there isn\\'t any category that is just \"physics\", they need to tick a box for \"astrophysics\"/\"atomic physics\"/\"geophysics\" etc... none of these will adequately match the job description for data science or engineering.similar to the other fellows above, i had my category changed to mathematician, which matched up with a minor in math on my bachelors degree.if you have more questions about the tn application feel free to reach outmaybe not so applicable to insight fellows but still interesting ^+1 for that medium articleinteresting premise, horrible analysis:  from the comments:&gt; i like it when statistics are properly applied to support a theory. unfortunately, the author makes massive statistical errors.&gt; the new york city (nyc) graph shows a strong seasonal effect in a time series. the ordinary least squares regression line (i\\'m going to assume those are the solid lines) are strongly influenced by where the series starts and ends, especially when looking at series of just a few seasons.&gt; for example, a series of temperature in new york city starting in january of 2011 and ending in july of 2015 will show a positive slope, one starting in july of 2011 and ending in january of 2015 will show a negative slope.&gt; indeed, the real story of the nyc graph is that there are strong seasonal trends in taxi cab complaints (probably driven by nyc weather). without accounting for those trends, further analysis is meaningless.more importantly, the author never compared against the null hypothesis, which in this case wold be to evaluate a single linear regression (higher order seasonal trends aside) for the entire pre- and post- uber periodyes, clearly had a point to prove, data be damned!haha yesunfortunately this seems to be a trend at the atlantic: (long read) : just started  channel to cover all things rdbms/sql, inspired by my presentation/tutorial with the ds-ny-2015b fellows yesterday. feel free to make the first post!awesome: is your presentation/tutorial available online?the tutorial part is just ah it\\'s modei can post my notes in the  channelthankswe had derek heregave a little intro that perhaps was usefulproject-basedah, okay, so you went straight to the co-founder!:simple_smile:haha :simple_smile:i\\'m looking for a python guru to contract for a short series of on-site lessons/tutorials at my company in nyc, with an emphasis on data science workflows - any recommendations/referrals out there?finally, i’m a real data engineer :simple_smile:  congratulations david.  this is super awesome!what is the best way to make sense of json files using pandas dataframe? read_json() results in error ( valueerror: mixing dicts with non-series may lead to ambiguous ordering.) #json  #pandas: that is awesome!!! thanks for mentioning insight!:+1:&gt;this is another secret in the hacker community. written instructions and online videos are only useful up to a certain point; to get really good, or to make something really fast, you have to be in the same room with people and you have to talk to them face to face.fun read: i\\'ve never loaded json into a dataframe, but i have loaded it into a sparse matrix. if you load your file into a list (or iterator), you can json.loads() each element to get a list of python dictionaries. you can then use the sklearn dictvectorizer to convert that list of dictionaries to a sparse matrix. so this is good if you want to do machine learning or something, but not if you want to explore the data.a word of warning: json with integers (rather than strings) as keys is not, technically, valid json and json.loads() will throw an error. there is a yaml parser that can load such json, but it is pretty slow. just fyi since i happened to have this problem recently.stay positive miothanks  it was a technical warning, not a personal warning, but if you don\\'t shut yer bot hole...stay positive miololnuggetbot animate me high fivehahahah bonus plug for partially derivative because i once got a beer suggestion shout-out. :)new stitch fix blog post that i wrote just went live. deep style: inferring the unknown to predict the future of fashion. nice! pinned a message to this channel.:   here’s a free event in palo alto on quantum computing by prof. maria spiropulu:i\\'m not sure if you guys have seen this, but i thought it was really neat. they can simulate how objects would move when interacted with based on short videos. : i’d be interested, too! for a good, in depth overview of agile workflows, i enjoyed learning agile  (in the insight library, btw, if you’re back in town).: try the mythical man-month (). good for developer and managers.good morning fellows, my name is yiyun, a current insight fellow at boston. my project is related to user retention and engagement and i am planning to quantify webpage traffic by drawing a web heatmap with a slider control. the heatmap will change as a function of time with the slider so it will show which part of the web is most hit at specific time by users. i have plotted heatmap before but not sure how to put these together. any suggestion is welcome. thank you in advance!: i’d use a jqueryui slider () with a change callback that retrieves data for the selected time and refreshes the heatmap accordingly. (i’m happy to explain in more detail if that’s not clear. let me know.): this looks awesome! thanks for the suggestion!:  this might be a useful way to visualize something similar : wow, that is fancy. thanks jonathan!hello. i\\'m one of the current boston fellows. anyone have experience with a modeling package written in python called numberjack? it\\'s supposed to include a c/c++ solver called mistral. the installation says everything installed correctly and numberjack import works but when i try to import mistral i get a \"module not found\" error.: i don’t have experience with numberjack, but for solving combinatorial problems/linear programming i use pyomo.  its super quick and the best i have found outside of commercial languages. not sure if this is what you are quite after or not but might be worth checking out.  do you know what class of constraints you have?  if they have a convex relaxation you could check a corresponding feasibility problem with something like cvxpy: thank you   and  ! those look like good alternatives. pyomo even has an example that\\'s exactly what i want to do.hello. has anyone used flask socket io? i\\'m wondering how to call an outside function, after i receive a response. i want to create a custom response depending on what\\'s been received. i tried some things but keep getting errors when running the app.thanks  and !hi all, does anyone have a recommendation for a digital marketing course? ideally, i’d like something that covers a good amount of theory and practice in digital marketing and analytics, perhaps using the google analytics or salesforce ecosystems, and not spending much time on rudimentary data analysis. thanks!happy thanksgiving, fellows and alumni :slightly_smiling_face::turkey: :+1:gobble,  gobble! happy tofurkey day!:stuck_out_tongue: virtualenv is your friend for multiple version of python.  the curse of minimal viable products... scary stuff. stay positive kumbhaniyou seem to like... nmf. have you tried item-item cosine similarity?anyone want to grab mexican food on/walkable from castro st. mountain view in ~ 1 hour?yes.hello! has anyone used the mimic data set? how long did it take to get access and how long to take the course? thanks!? i think you should request it right now just in case, as it takes a few days to access it. also used it in her project to my recollection tooi requested access last night, so i\\'m hoping it doesn\\'t take too long. the course takes ~2-3 hours to complete, depending on how fast you read.it took me a few days but i had to choose project before that happened so i chose something else. i haven\\'t investigated this beyond a quick google search, but i know that casinos have used facial recognition software for many years.there also seem to be a handful of face recognition api hooks offered by big companies. great for building a project quickly, but if you go that route, i\\'d think about coupling it with something you build yourself.and as always, think about your target audience and what business problem this addresses.i guess the thing i\\'d worry about *more* is how you plan to validate this.thanks guys!now get back to work everyone!!!anyone interested in using the mimic database should be aware that they have demo data available which doesn\\'t require a waiting time for access: there isn\\'t much data in there, but it does give you an idea of how the data is formatted.superwoman has woman only screening, now it has clown only screening.  victory for clowns!  and the best part is that this time mras can attend! #itburnhave anyone previously done an accelerometry-based activity classification project?try  for some relevant experience. from my session did something like classification of boxing punches based on data from accelerometers in boxing glovesthis is true! from my session did something like this, iirci want to think someone from one of the bos health sessions did something with fitbit datahi former fellows, does anyone know of a good repository for terms of service and/or privacy policies for popular apps/services? trying to stand up an nlp project for 2017 remote session. much thanks! used iphone data was that for hykso?yep! could you please send me a link to your app or demo?who uses rnns or cnns for nlp classification tasks? i have read a lot of articles on nlp classification and one thing i have noticed is that it is really hard to beat bag of words and the models that do are by a very small amount.  i\\'m just curious what other people do in practice. i work at  we provide moving help for packing and moving stuff. if it was a short distance move we could have tried to get someone to pick truck up for you. if you are still looking for help, shoot me a message or call us at  and someone at concierge can help. also, if you did decide to use us, i can shoot some coupons your way :wink:i still feel like that in week 8. :sweat_smile: welcome seattle!i would start by considering a discrete version of the kolmogorov–smirnov test, comparing the percentiles. if you\\'re not aware of the number of data points from each distributions, you probably have to consider the number of percentiles values as n.more info here: interesting, , thanks for the link! i\\'m not sure ks test is appropriate. i\\'m asking for specific percentiles. like given two distributions, is there a significant difference in let\\'s say the 90th percentile, etc. currently i have the harrell-davis estimator on the table because it gives l-estimators with standard deviationsbut it\\'s computationally expensivehey all, any recommendations for a large text summarization dataset (paragraph -&gt; sentence)?*any with fields for citations, click rates or commentsif you\\'re comparing just a specific percentile from 2 distributions, the harrell-davis estimator is definitely the way to go, as it gives a more accurate estimate of the percentile and an error, which you can use a welch\\'s t-test to see if they are significantly different.but given a sufficient number of percentiles from both distributions, you want to compare all of them at once, i.e., you have to take into account that the percentiles comes form the same underlying distribution.ks test might work given enough percentiles, but there\\'s probably a better way when one doesn\\'t have enough percentiles (e.g., only every 10%)there\\'s at least one reddit bot that summarizes news articles. you could scrape the text of those articles and the subsequent summaries.here\\'s a python package that implements several algorithms, i think: some are slow, i think. gensim also has some functionality like this.thanks!awesome! thanks for the links anyone getting through registration for grace hopper?does anyone have experience using pymc3 for time series? generally i\\'d like to include some kind of time lag feature but not sure what\\'s the best way and can\\'t seem to find good examples using autoregression in the pymc framework check in with austin rochford on twitter - he might have a good blog post on this! uploaded a file:  and commented: another question in the que! has anyone ever worked with character cnn? do you have a implementation or resource you could share for the following architecture?for fellows and alumni in boston: the city of boston is having a data challenge using data collected from speed feedback signs. the kickoff is happening at cic boston on july 26th from 6-7pm, on the 16th floor. there are prizes for winners! :slightly_smiling_face:link here: also  has a lot of great nlp features including summarization.  i’ve always wondered if those worked…i didn\\'t realize they saved the data!if anybody does this i\\'m super interested to know how it turns out! vision zero is near and dear to my :bicyclist: heartof course you didn’t looks aroundwho, me? why would i know anything about government surveillance of cars?:stuck_out_tongue:demo: i use spacy and stanfordnlp for tokenization!also, it looks like apple has launched a ml journal today: ‘journal’apparently they do! if only there was some one in city government that would (not) do anything nefarious with that data....hey guys, has anybody worked with synthetic lift studies (where one tries to establish a causal link for the effect of an ad directly from the observational data, without performing an experiment)? all i have found are a bunch of papers by some yahoo folks. if you have any lead, please pm me or add a thread. thanks!i was like \"wait, i know someone who would be interested in that as well\" :joy:might be worth checking out propensity modeling to build a synthetic control groupwill you be there ?how does one tell if a query is cpu bound or more so memory/disk intensive?hey bay area folks- new insight sessions are around the corner in sf and palo alto and i\\'ve already had some questions about housing. if any of you are looking to sublease/rent a room to a fellow starting in august or september, send me a dm!i think i\\'m required to be there! :blush:quick question... are the lightning talks available to the general public? or just insight fellows/alums? that video is a big part of my motivation to make the survey in the first place. the other was this general unease/uncertainty about how to assess what a fair salary in the area. feel free to bring a friend / some friends!sweet article: it was nice seeing you this weekend….  for 5 secondspretty nice:: that is both beautiful and nauseating: 5 seconds in heaven is better than 4 seconds in heavenhah.  heaven’s overrated theni’m looking forward to my 5 seconds in hellstay positive alex_rhee: tony is my colleague at h2o :smiley:nicewonder if anyone has seen this internal error in spark stream:  error jobscheduler: error running job streaming job 1467073992000 ms.0org.apache.spark.sparkexception: an exception was raised by python.  it attributes it to an attempt to run foreachdd on a dstream: \"user_accident.foreachrdd(lambda rdd: rdd.foreachpartition(processaccident))\". and it\\'s fairly easy to reproduce. \"user_accident\" is a dstream from a kafka stream( a slow one) and it comes about after a simple map operation to extract some  fields and put out  a (k, v) pair.stay positive sathyabas i suspected python is generating an error when the dstream is empty i believe. i found this discussion in   in . the poster was asking if there was some way to avoid calling the foreachrdd(). is there way to check if a dstream is empty ?stay positive sathyabthe bootcamp setup has windows drivers by apple for the magic trackpad. you\\'re right, i missed that part.isn\\'t collinearity a concept only applicable to continuous variables? it implies that some change in one feature might result in a proportionate change in another. however, for categorical variables there is no connection between 2 different categories, right? that\\'s why mapping them to numeric and calculating the correlation is not valid.i think in this case, you are best served with the simple chi test with cramer\\'s v test, or the anova test when you have both categorical and numeric variables.otherwise, depending on the number of different categories within each variable, one can one-hot encode the variables and apply the generic numeric correlation procedure.you mean, how to identify important interactions that you can then add to your logistic regression model? one can train a shallow xgboost (2-3 depth) model first, then feed the most important paths as nonlinear features.just finished the signal and the noise, really loved it! started on black swan this morning. :+1:yes, i\\'m interested in identifying important interactions.that\\'s an interesting approach. thanks for suggesting it. i imagined i could interpret \"important\" interactions as those with high coefficients after fitting a logistic regression model with interaction terms. however, cursory google searches aren\\'t pulling up much on finding interactions using this approach. now i\\'m suspecting there\\'s something fundamentally wrong with it.thanks again for your input carlos!hi everyone! the insight sf health data science team is hosting a meetup this thursday at 6:30pm with invitae! we only have a few spots left, so sign up if you\\'re interested. food and drinks included! details about the speakers in the link below.if you\\'re referring to \"important\" interactions between a feature and the target variable, then the magnitude of the coefficients, as you mentioned, is absolutely it.yes, that\\'s what i mean :wink: did this during his project. from what i remember, he just created a metric boatload of interaction terms and used a regularized model to find the most important.hey insight data scientists and data engineers — do you love video games? i have an opportunity for a data scientist/engineer at activision blizzard in sunny santa monica ca, to work on building an analytic platform for some of the world’s most popular video games (e.g. call of duty, destiny, skylanders, etc). this specific role calls on the ability to foresee and support business and enterprise-level questions for franchises and titles as part of building a web-based data platform. this team is looking to hire someone very soon. send me an email at  if interested. you get to play video games at work! nuggetbot u so swoony. what a hot mess. this is really interestingamazon machine learning advanced coursewhile that blog brings up valid points, it wouldn\\'t address the not serializable issue. often times, that issue arises due to scope issues, such as creating a database connection in the master and then trying to pass that connection to a worker. maybe post a code snippet?thanks! yeah, those are why i thought it might be possible.i think you may be right---i was definitely creating the client on the driver and passing to a worker. this was what the blogs were doing, so i thought it wouldn\\'t be an issue. i\\'ll try instantiating it within the map unit and report back. thanks!that fixed it! lesson learned. thank you *so* much!great to hear ! now consider creating it within mappartition and then passing it into map. this way, you are creating it in a worker and then sharing it among the threads rather than instantiating it for every worker thread. the whole insight team is on holiday until jan 2, but  is correct, we\\'d be happy to help you sort this out! can you drop us a note in the new year? you can reach me directly here: thanks~ david from your office contacted me and explained things this morning….*awesome!* that is a cool optimization. implementing it got me a lot more familiar with how to effectively leverage mappartition.does anyone know of any companies doing internships in data science for undergrads?thanks for sharing . this is terrible but also a good reminder that these things happen everywhere and that we need to be vigilant. perhaps legendary entertainment in boston, i heard they have some internships at a panel discussion, but not sure of the level at wayfair we definitely have internships and co-op programs i know square brings in lots of interns for data sciencestate farm has some available  a call to all bay area folks and especially those who have experience working both on the east *and* the west coast: i am going to be on a panel to talk about the tech, data science, and machine learning community in nyc and i am curious to hear how others think it differs from the community in the bay area. what do you think?i have nothing useful to contribute, but i\\'d like to say i imagine you guys (in nyc) wear more ties. :wink:… haha, and fewer hoodies! :white_check_mark:this was making the rounds amongst physicist-turned-data-scientist friends:physicist turned data scientist is about 60% of my friends... i thought the article could basically be summed up as: \"why does deep learning work so well? because taylor series work so well at describing the universe! but we don\\'t know why that is...\"&lt;-- biases particle physicists who only knows how to solve things through series expansions i agree and i did not very much understand that jump in logic(to why this explains that neural nets only have to work in a very small corner of parameter space) yeah, my local theorist also passed that around. : ) i agree about the jump; _intuitively_ it sort of makes sense \"oh, the universe is in a small part of the function parameter space, so obviously neural nets only need a small part of the parameter space!\", but it sure isn\\'t a rigorous demonstration. i worry it makes us come off like this:  :wink:righti think theoretical particle physicists in particular, in the spirit of ken wilson, think the rg flow applies to…almost everythingin our defense, it was hugely impactful in condensed matter and particle physicsyeah… max tegmark is basically that xkcd in a nutshellformer biologist here.  i frequently see that cartoon passed around (half) jokingly.  =pyeah, max has gone outside of his realm of expertise a lot recently…i think the only reason that arxiv paper got a call out in the mit tech review is max is a prof there now…so grain of saltbut i was very amused to see the cmb and “cat pictures” on the same figurehe really has… like a less famous michio kakui don’t remember him being _that_ wacky when i took relativity &amp; cosmology from himhaha…indeed, but with probably more impactful work before he went off and started being a philosopher  \"a less famous michio kaku\" what an un-flattering comparison! (note: do not take that as me disagreeing with it. ;-))for solving tab completion/kernel hanging issues try this at top of your notebook: `%config completer.use_jedi = false`  &lt;-- solved the issue for me!hey , we\\'re looking for something like:  or have anybody used crunchbase api before? how long it usually takes for them to give you a user key since you submit the request?hey, i registered a couple of weeks ago and still haven\\'t heard anything...what kind of strategies do you all use for isolating your \\'production\\' database from your \\'research\\' database?(e.g. clone + custom tables?,  read-only slave + ?,  data-warehouse layer like redshift?)advantages? disadvantages? they can work depending on your use-cases.  it becomes hard to optimize for very specific things or build custom versions of an algorithm that may be better-suited for a specific data set,  with these general-purpose model platforms.  on the other hand they can save time if you know you need something general, and i think skytree specifically i remember being fairly performant also.i have very little experience with it thoughanother way to think about it is that it is not aimed at someone who needs ‘superuser’ level of control of models, but you may not need that for many or most things you do. thanks till (and others) for posting this. we are all concerned and uncertain of our future in the us. my sister went on a vacation to visit our parents back in iran a few days ago and now she is not so sure what\\'s gonna happen to her. i am a de fellow this session and quite frankly it is very challenging to focus on my project with all the uncertainty revolving our lives in the us. seeing all the protests and support today was very heart warming. thanks to all for showing us your support. i couldn’t find an appropriate emoji to react to your message, but i just want to say that i’m glad you’re here and hope your sister can return safely. please know that many of us are fighting for you and all immigrants.then i don\\'t think i\\'ll be able to say good bye in person! best of luck back up north, and let us know if you come back to visit!hey guys, would anyone with some expertise relating to nlp be willing to chat about project validation? my project deals with identifying topics of interest from unstructured customer reviews and assigning a sentiment value to them. any help would be much appreciated! i don\\'t know if this has been shared already or not, but open data science is matching aclu or united nations refugee agency donations up to $5k per person.hi everyone! i\\'m trying to scrape some data from a javascript query that seems to send a submission form via post. however, i\\'m not sure how to get the responding page or what the keys should be. here\\'s the page: i\\'m trying to look at what possible keys they are to send through a  function, but i\\'ve been unsuccessful so far.does anyone know how to get that, or another tool that might be more helpful, considering that the resulting webpage is also a javascript page?joao: have you tried watching the \"network\" panel in chrome\\'s developer tools (view&gt; developer&gt; developer tools)? that will show you the different requests that are made, and you can click on them to see request and response data. why post when you can get!  for instance, just a little playing around chrome\\'s developer\\'s tools and looking at some source code...   geog = ctyevent = ased | asho  (for emergancy department vists vs hospitalizations)year = 2000 - 2014race = totl|afam|aspi|hisp|whit|othr (for all races, african-american, asian-american, hispanic, white, otherage = totl|0004|0517|1834|3564|65ov|0017|18ovsex = totl|feml|malemodel = conv|bbym (for conventional vs spatially modelled)type = r10k|cr10l (for age-adjusted vs crude adjusted)are the form names:just iterate through them all and profit!ex: what he said! :stuck_out_tongue:thanks. i\\'m looking at it now trying to decipher it. from what i had looked before, it seems like the query page sends a post request via , but i\\'m not sure about the keys being sentawesome!thanks a lot just watch out since certain combinations of options may return an error (according to the website). good luck!by the way, where did you find the list of the form names?any advice on how to render a page with some javascript generated table, before being able to scrape it?i\\'m trying using pyqt5 but not having luck so farselenium?not sure if it\\'ll dump the pagehey sf bay area folks, anyone need a great bike? i bought one brand new from mike’s bikes in palo alto about 4 months ago, used it to just get around palo alto, and it’s in near perfect shape and i have all the original paperwork for it. it’s a raleigh 2016 detour 2 (), red, size 17. i also bought rechargeable front and rear led bike lights and a pretty heavy duty u-lock. all that stuff combined plus tax came about to just under $500 but i’d be willing to sell it all for $350 (or best offer!). anyone interested should feel free to dm me (and i have some pics of it as well).i was finally able to get it via selenium and chromedriverhad to get the chromedriver, but now i can simply render the page on chrome on demand :slightly_smiling_face:cool beans! don\\'t forget the little people when you become rich and famous off your demo!lol. i think at the top of my priorities right now is just to get some sleepfor future, you may want to consider splinter.   swears by it.  selenium is a very round about way.i do enjoy splinterworked well for interactive scrapping @ ny  help!  i need a place to stay from october 5th.  please let me know if you hear something.  thanks^ \"what it\\'s like to be on the data science job market\"anyone interested in checking out the mayweather-pacquiao fight tonight in mountain view? i\\'m thinking of heading to st. stephen\\'s on castro st.what time does it start?i think it starts ~8pm,so i was thinking of getting there ~7pmfor folks in bay area, i have a clipper card loaded with a caltrain monthly pass for september, zone 4 to zone 2.  i paid $190 and willing to sell at half price. pm me if interested.   anyone have experience with testing database schema? my team basically wants tests in place such that when someone changes one of the schema, our scripts don’t all break. i think that counts as an integration test? any suggestions or references would be super helpful :slightly_smiling_face:what type of database is this?exploring the spacenet dataset using digits: (shout out to my lab\\'s sister lab: cosmiq works!)further promoting: they just started blogging about some of their works on _spacenet_ here: sheareraj: what are “our scripts” in this case? etl?here\\'s a quick informal survey: what is your company\\'s policy on the use of and contribution to open source software packages? if you need to go through a formal approval process for certain licenses not pre-approved, what is the turn-around time? thanks for any input!biased answer: my company open sources all of my teams work; we regularly contribute to open source and the process is \"hit the pull request button on github\". i expect that is on the crazy open end of the spectrum. a little history: we do this because we work with a bunch of collaborators and \"we just open source everything\" was an easy way around fighting about ip.stay positive alex_gude yeah, etl-type stuff, analyses that get repeated periodically, really a little bit of everythingnuggetbot check on alex_gudealex_gude has a happiness average of 0.5905172413793104i know, i\\'m as surprised as you that i\\'m not negative yet :wink:i mean...nuggetbot check on scstarkscstark has a happiness average of 0.24100156494522693 ^turns out i\\'m just a lot more positive! :wink:also that\\'s a lot of sigfigsnuggetbot must be precisenuggetbot do you know the `rats()` matlab functionnuggetbot check on kumbhanikumbhani has a happiness average of 1.02  here come cheap ingredients for robotshi guys, i had a question about project validation i was hoping someone could help me with. my insight project deals with extracting interesting (most talked about) topics in online reviews, and assigning a sentiment to them based on the context of the sentence they appear in. the reviews do have a 5-star rating attached, but i was told they would like to get more \"granular\" and parse line-by-line rather than just using this overall rating.does anyone have any good ideas on how i could validate this procedure? i don\\'t have a \"sentiment-labeled\" review set or anything unfortunately.if in doubt, start with textblob: cool paper published yesterday on nature: application of deep learning to classify different types of skin cancer, with similar or slightly better results than dermatologists. ring a bell, ?how to not data science :open_mouth:wait. someone thought it was a good idea to name a project skynet?gotta admire the gallows humor in the defense sector...on another note, does anyone know anyone with experience in data governance in hadoop environments — including the creation and enforcement of security policies within hdfs via ranger or other methods?. also curious about encryption-at rest and in transit- tools and other meta-data tagging capabilities. we use edgenodes with our internal hdfs cluster with active directory integration (i think via kerberos)i’m not sure of the exact implementation details, but files/folders on hdfs have groups associated with them for which we get access to with a separate request.any former fellows out there have a good starter tutorial regarding working with flask? this might help . step by step instructions here hi guys! i’m having some problems deploying flask with aws (see screenshot below). if you guys have some time, i would really appreciate any help!!! thanks so much in advance!! :heart::sunglasses: uploaded a file:  and commented: what does this mean……?! thanks so much for any insights! what\\'s in your requirements.txt file? looks like it tried to do a pip install of a dependency and failed. did you check the log file?does anyone have any experience comparing data from two samples both from a negative binomial distribution? i\\'m considering a mann-whitney u test, but i\\'m not sure if that\\'s the best approach. i\\'m also considering negative binomial regression.i\\'m also thinking about simulation but i\\'m not sure of how to best approach that given that the negative binomial distribution has two parameters.  permutation test? then you\\'re not losing info by converting to ranks like with m-w.a pretty cool tutorial - (warning it’s long)i\\'m not that familiar with permutation tests, given that the negative binomial distribution is not symmetric i believe that randomization tests aren\\'t appropriate, what specific type of permutation tests should i look at? you could probably use the kolmogorov-smirnov test after adapting it to your discrete data. however, i\\'d probably use the kullback–leibler divergence.on the other hand, if you already know the data comes from the same distribution but with possible different parameters, a m-w u-test or even a t-test will do the trick, as there\\'s no need to compare the whole distribution.here\\'s more on our upcoming 4-year reunion gathering in palo alto open to all  insight de fellows next week: my company (finance firm) is restructuring our office floor plans! do people have insights into what types of office set ups (eg, open floor plan, meeting rooms, other types of work/gathering rooms) tend to work well in terms of productivity and enjoyability?insights for insight office set up :stuck_out_tongue:but seriously, i found your nyc office in flat iron so enjoyable!that was open plan, with a few meeting rooms and a fridge full of beers  &gt; private offices with doors that close were absolutely required and not open to negotiation.oh maybe i should clarify - i’m not working for insight. i’m a boston alum working for a finance company (ie not traditional tech office floor plans)read “quiet”, decide where your team falls on the introvert/extrovert/sensitive spectrum, then decide. finishes that reflect sound make open plans horrible because you can hear everything. white noise is often necessary (i’ve seen people install white noise machines in open offices, literally)    text to speech in python 3.  haven’t tried it yet.a series of pieces by nyt called “reimagining the office”.pt 1 is a really good piece about what google learned from its quest to build the perfect team.  also part 4 deals with diversity.  highly recommend.it really is a good series of articles. i enjoyed reading them.current idx fellow,  will be on all things considered today around 4:44pm et. she’ll be speaking to her academic position and how the brain responds to teleportation.pretty cool ()if i `pip` install theano on macbook air, how many cores will it use out of the box? is there a way to set this parameter in (say) a jupyter notebook? i know a comp sci graduate student at uic that might be interested, but its harder for him during the semesteryeah. its urgent. only 2 weeks timealso, someone with slack admin rights might want to delete old files since we are over the limit ()i need to post dog photosit’s urgenti\\'m not sure if everyone is already aware of these, but i found them quite a bit better than the default: ooo prettystill waiting on promised dog photos over here uploaded a file: the dogs of adrolli present mochi.her favorite things in no particular order are1.  food uploaded a file: this is lady.  she is from thailand.her favorite things are, in no particular order, are:1. food2. her owner alligood dogshe is also very fashionablethxi got you i’ve been stalking the office dogs for youalsothe white one bit meloli was poking it thoughand by poke, i mean petting it to get its attentionwho knows a lot about probabilistic graphical models (pgm)? someone i know (at mount sinai in nyc) is looking to chat with someone, who knows about those pgms. i believe it’s location agnostic … and … for a good cause!hi all. i am involved in a project where a non-profit is looking into possibilities to take advantage of their huge database for some (undetermined) advanced analytics in the near future. the current discussion with the database engineer is about various data warehousing/lake solutions, as the first step in the process. they have all types of data from sql server, to csvs, to unstructured reports.  i need to make sure that any solution proposed would allow analytics in the future. (they currently only display various information in their website and standardized reports.) . does anyone have thoughts about things to avoid or make sure are implemented for data storage? thanks!ps. they are currently considering salesforce. delayed response here, but i use files dumped to a semi-organized structure in s3 and spectrum to join that data in to other sources in our existing redshift cluster and it\\'s pretty greatthe advantage of the files just living in s3 is you can also do more specialized analyses via spark in addition to having some standardized queries run regularly with redshift/spectrum thanks! i was reluctant to do the switchover based only on the press release (which makes it sound great). i hadn’t even thought of the advantage of using spark with it too.yeah, i was exclusively doing things via spark until spectrum came out and spectrum has made my life a bit easier (no worrying about tuning the cluster parameters for memory issues, etc) i had held a few bitcoin for a number of years but i actually sold them earlier this year to help pay for housing during insight~~zkermish: how do you decide between redshift or spectrum? do you use spectrum for data transformations that you then throw into redshift or are these just big one-off or seldom queries for stuff you’d rather not put in redshift?btw, we do similar dumps to s3 and loads to redshift, with occasional spark jobs for data transformations.the data we now link through spectrum was large enough that we had issues loading it into redshift…also, it would have brought our costs upwe have enough one-off queries to do that it’s worthwhile to link to redshift via spectrum and just run those as neededand eventually the hope is to have some regular queries done that right back to redshift for longer term use (overall descriptive metrics to condense the information in the raw sources) i guess for us it’s a good way to explore a larger source of data before having a clear process around it and a way to load in some distilled info from that data into redshiftall at a lower costawesome, thanks. we’re in the process of offloading some in-redshift transformations to spectrum (instead of spark) to keep redshift resources dedicated to our bi tools.and the migration from redshift -&gt; spectrum is much cleaner/faster than  redshift -&gt; sparkcoolhi  what is spectrum? ?“redshift spectrum”no  , it’s an aws thing: ah, i thought it\\'s called athena!i’m sure it’s all spark/hive/presto/whatever on the backend that they manageathena is very similarbut that doesn’t link to redshiftah!subtle distinction…they’re actually super linked. if you make a schema accessible in athena it’s there in spectrum too. the advantage is you can use your existing redshift cluster to run the queries / join to other data that’s therecost is the same too (they both just charge per tb scanned in the query)what is a good tool that you guys have used for schema management?but spectrum queries aren\\'t as fast as redshift, right?some things to consider, maybe being able to separate out personally identifiable and non personally identifiable data and store in 2 different db?spectrum queries use athena and redshift under the cover it’s all parallelized under the cover so it’s quite fast. the only thing missing is a dist_key and the associated optimization but my sense is spectrum approaches that by using compute cycles on the storage nodes…not entirely sure..but it’s all fasthey past fellows! does picking a la croix flavor get any easier? :confused: ending the 4th week and i’m still having troubleben_stclair: ask  , he\\'s a lacroix connoisseureventually i just went with one of the beer flavored ones.haha just try them allexcept lemon. ew.when in doubt, don\\'t feel blue, go green!citrus are the clearly superior options. berry used to be... but unfortunately i think it\\'s gone. :disappointed:there is only one la croix its all about the mango (or anything that comes in a tall can)it does get easier. lime is the clear winner.just stay clear or coconut and lemoncoconut and mango and berry are the only acceptable flavors coconut? what the hell man, i thought you were coolhahaha, i know, i’m very much in the minority on that one….thanks, everybody! this is really helping my decisions be more data driven :r: the nakamoto paper  … i’m very much a noob :slightly_smiling_face:would be very interested in any references you could recommend though!definitely coconut. #teamcoconutprevious fellows: at week 4, did you start feeling excited for demos but convinced this real-world job thing would never happen? it seems to be a common fear in some circles.100% coconuti’m with cocoi go with the two flavor ones usuallypassion fruit for life!! :  maybe the current fellows want to start a channel to discuss their project ideas with the alumni?good ideaor even if they\\'re stuck with a particular issuemaybe someone can have some helpful input: it would be good for more alumni to sign up for mentoring slots too (especially us nyc folks!). jenainsley would really appreciate it!how many alumni are actually on here? i think it might just be me and british matt from my session.there\\'s maybe 10 from my session (nyc 2014) i thinki was really just pestering them :simple_smile:i just updated my profile to include insight info.  that could be one way we know who\\'s from where and which classslackers = a exp(-k(now - session date)).  find a and k!good idea talldave , i\\'ve done the samek = i: thanks for the mention!if anyone has questions about signing up for alumni mentoring spots (project feedback or mock interviews) - email me! :sparkles:nuggetbot: animate me welcome everyone!:  is a good package. i used it in my project: www.nerddit.club/graphalso easy to use with neo4j :simple_smile:happy  international women’s day! check out our blog post! lol…i love that the two slack icons of women used to “like” my post above are called “ok_woman” and “information_desk_person”. how…ironic?: i used the ‘information_desk_person’ because previously a friend used it to indicate (in their words) ‘it’s like a hair flip heyo\\'so it was supposed to be chill:okman:oh, okay..if anyone wants 20% of the upcoming dataengconf in sf, we have a discount code: ids20xthey also run a regular (free) meet up for data engineers: and in ny as well: this match is fun to watch! deep learning + reinforcement learning against the world\\'s best! alphago beat lee sedol!i did not expect that.how is x distributed?fun data on bacteria in nyc subways he that was my supervisor :simple_smile: did you help gather data by licking poles in the subway tom? tell us how you guys wrangled the pole data!!? no, nooooo not *those poles* :speak_no_evil:gosh.also the github which has all of the code used with mimiccurrent remote fellow here: shot in the dark - anybody have experience with audio/speech processing in python?  i’m building a web app that processes audio of the user giving a presentation and does nlp, sentiment analysis, etc. does! or  have done some as well sounds good :slightly_smiling_face: might be useful to take a look at  in addition to whichever other tool you use (i recommend just going straight to `requests`)is any one going to the google cloud summit in seattle on wednesday?  trying to get in some networking as i look for my first ds job! pinned a message to this channel.current fellow here working on my project...have any previous fellows used the  api?  i\\'m having trouble getting the vehicle api key and was wondering if any past fellows might be able to help. thanks.  ? i did some for my insight projectgot it. that\\'s two strong reasons to go for it then (i\\'m using python). thanks!hello a new job posting in my company not exactly related to data science, but in case anyone is interested hello all!  i’m starting a ds position at a place that gives dss an option to work on a windows or ubuntu machine.  i am traditionally a mac user, and like the idea that ubuntu has much more overlap with mac — but am not sure i’m up for all the fiddling that i’ve heard can be required with ubuntu. what are your opinions on the two options?if it\\'s windows 10, you\\'d have access to the bash subsystem, so even less of a difference between them.i made the switch from mac to ubuntu last year. not much fiddling at all. 16.04 lts feels very much like osx did. and if you ever used homebrew on mac, the linux/ubuntu package manager system is easy to use. no problems at all installing &amp; using anaconda, tensorflow, etc.i have a windows 10 machine &amp; run a virtualbox w/ ubuntu. best of both worlds for scripting + ease of use with microsoft products (ppt, skype, etc.) that can be challenging on ubuntucongrats on the ds position! i was a long-time user of mac and linux from my time in academia, and i am now working at a place that operates exclusively under windows, but i was very pleasantly surprised by how minimal the hassle was for me to use windows to do my job. that having been said, i do wish i had the option to use linux. either way, i think the fears and concerns with one versus the other are generally overblown.very good post on ai...congrats on the new job! i would carefully examine how the it infrastructure supports ubuntu vs windows. daimler (my company) is historically a windows company, and has recently been transitioning to support mac. there are a lot of network issues that are purpose built to be easy to handle for windows and are ultra frustrating to deal with on a mac or linux box (i.e. proxy settings, email server, shared directories, etc). if your company deals primarily with web-based stuff, then i’d recommend sticking with ubuntu, but if you need lots of productivity software (ms office, etc) it might be easier to just use windows.just go windows. besides the bleeding edge, you\\'ll be able to use most data science tools just fine and you\\'ll have less hassle producing results you can share more broadly.if your company maintains its own fleet distro, then it\\'s a strong indicator in favor of linux - patch tuesday still sucks decades on-going.a contrary indicator would be if everyone\\'s account is office365 and activedirectory instead of say ldap.hey people, hope you are all having a great day. i just had a general question, for a classification problem, is there a way to check for collinearity between categorical variables. been looking around for a method, thanks in advancewhile this channel is still reacting to faye\\'s last @ channel blow-up, i have an actual question for everyone. my team is in the process of interviewing for a new data analyst role, which is an entry level data position. i\\'m building a rubric to help my team judge resumes. i was wondering what you all look for in junior positions like this, at least on paper. any ideas would be great! :slightly_smiling_face:what are you hoping the person will work on? experimentation, dashboarding, ad-hoc work, modeling? how much will they need to get their own data and write etl? design experiments? do they need to work with engineers in python/scala? how do you share data internally, via tableau, jupyter, something else?frankly, they will be doing most of the above (our team is small, so most roles we hire for are generalist roles).there will be a hefty amount of guideance in the new role, but i expect the new analyst to help with dashboarding, aid in modeling (even if they aren\\'t versed in it) and do etl as necessary in conjunction with our data engineers. we are pretty platform agnostic, so any knowledge of r/python is a plus.usually data is shared in jupyter, rnotebooks, and tableau.the biggest thing for this role is their willingness to learn and interest in government, since they won\\'t be on one single project with one focused task.anyone have any experience with speaker separation / diarization? i\\'m having moderate success with the typical approach of agglomerative clustering, gmms, and structured perceptron. would love to talk details with someone who has some experience though.samba tv is a long-standing insight partner, but we just had a de (insight alum) decide to move on to salesforce, so we have an opening: does anyone have a suggestion for a resource (anything from a blog post to a coursera course) to understand the basic things to consider when deciding on a data pipeline?  i want to set up a pipeline to handle a few tables with about 10m rows added per day, and i want to figure out whether a typical s3-&gt;spark-&gt;redshift pipeline can handle it.  i\\'m not a data engineer, but i definitely want to learn more!anybody interested in latent dirichlet processes? or if you have worked with them, please let me know:grinning:fantastic!does anyone have experience with the facebook api?  i\\'m pretty sure what i\\'m trying to do is really simple, but i\\'m getting lost in all the set-up documentation.what are you trying to do?i just want to get the number of likes for a list of pages.  i have the addresses already.hmmm...unless you have access tokens for all those pages, might be impossiblewhat endpoint are you trying to hit?does anyone have a link to the mentoring sheet for tonight?pretty cool:  uploaded a file: yeah! we were surprised by how well that worked...i really got to start trolling for meetups... :confused:hey everyone, if anyone is looking to pick up some o\\'reilly books, here\\'s a 40% off discount code, i just ordered a few and it worked fine: wcyazif anybody wants a chance to learn tableau without paying the $1000 for a license, you can get a free (and legal) key by registering for this competition:  . (the site doesn’t say how long the key is good for, but mine says it expires in mid-august, which is considerably better than the usual 14 day trial)for those of you still studying, there is a decent documentary on netflix called algorithms that explains some of the sorting algorithms, pagerank, and decision trees using fancy made for tv graphics.\"any bias contained in word embeddings like those from word2vec is automatically passed on in any application that exploits it\"tableau licensing is notoriously expensive. part of their pricing model assumes that all potential users of the product should be factored into licensing, and the scope of a potential user is very broad (e.g. an employee that may use excel -- in other words, everybody -- might use tableau, thus they should be factored into the license cost). it wouldn\\'t be uncommon to see tableau offer a site subscription to a small/medium organization at a rate of ~$1m+ for 3 years.consider your use cases and existing tooling... power bi is improving quite a bit and many organizations already have licensing arrangements with microsoft. also take a look at qlik, which offers their desktop product for free (windows only). the main limitation with the free product is sharing/publishing/collaboration.after talking with a lot of teams,  and i wrote down what we learned about transitioning from software engineering to applied ai. let us know what you think :slightly_smiling_face: does anyone every use any gui tools for debugging extremely long, hard to understand sql queries?awesome post.  but, what is ai? can you give examples that are ai, and not machine learning?  is reinforcement learning - ai or ml?we interviewed someone recently and reading my colleagues comments i was reminded of a gap i have (e.g. i could not have answered the questions they asked this candidate) --- particularly, the trade-offs between all the different ways we can store data (think broad here, not just sql vs. nosql, but that\\'s a piece) and how to think more deeply about schema choices.  when i asked my colleagues they uniformly said something like, \"i learned it on the job\" or \"key is to try the different options and see why it doesn\\'t work\".  i\\'m learning some of it on the job, but it seems like we are in a place where we already made a lot of choices and i want to understand why better... of course, i don\\'t have infinite time --- maybe a chart to start with, or say one section of an online course i can listen to on a bus... perhaps this can help: provided you\\'re using postgresany opinions on whether this is worth attending?went last year for free two of the days. some decent talks.i have a question about data science platforms .. my team requirements are simple: have a collaborative workspace supporting multiple languages and easy to deploy. i know of tools that solve the pieces of puzzle but i am looking for something more comprehensive .. i looked into dominodata labs and it helps, but i think its very expensive for something that is completely open source tools. any experiences or leads into other projects or tools? what do you mean by \"collaborative\"? does \"we share code on github\" count? or do they need to have like jupyter notebooks that anyone can edit at any time?predicting wheat rust in ethiopia with the bill &amp; melinda gates foundation: : lets say a big more than we share code on github .. commenting on files and general discussions .. i think many of the tools like bitbucket, gitlabs do that. but do suffer from a lot of complexity in ci/cd.i think bitbucket plus docker is what i am looking for ..seems likely. we used domino and went back to docker + github on our local servers, but we\\'re weirdhas anyone ever used the python package lime? my team is working on ways to easily explain individual predictions from a model uploaded a file:  we\\'ve been testing lime. we found it worked under... exactly the circumstances that they presented in their talk. :confused:it falls flat on anything complicated (or with dimensions that are not explanatory, like word2vec dimensions)thanks  any thoughts on how well it worked with random forest of (ballpark) 10-100k data, 50 features? other methods you have liked?i don\\'t believe we used it on a random forest (but that seems like a model that it would excel at). we\\'re still exploring options; our boss is a big believer that model explainers are the future. (which means it is one of the things we\\'re tasked with looking at.)more frost this morning...  uploaded a file: here\\'s a little gem:  sublime text key bindings for jupyter notebook!(add the following to `.jupyter/custom/custom.js`)hey, i\\'m looking at your chsi data hanlerand i have a couple questions whenever you\\'re around.what\\'s up?i figured it out in the mean time sorry i didn\\'t get a notification when you messaged me!it was very useful, i pulled out tons of interesting predictors.oh i did want to ask in your ridge regression, why did you weight by population size? is it because you were looking at state wide effects?if you\\'re running the regression to develop hypotheses of causal relationships at an individual level (which is a sketchy thing to do -- what with causation not being correlation and all that -- but it was assigned to me for a data challenge for a company i interviewed at...) then you probably care more about prediction error in counties with more people because they represent more individualsyeah, makes sense. i was just curious. usually we only weight by population size when we\\'re trying to weight survey sampling by strata.i wanted to make sure i wasn\\'t missing that i should do it with my county level inference. :smile:did anyone ever encountered virtual box problem such as \"the virtual machine \\'ubuntu\\' has terminated unexpectedly during startup with exit code 1 (0x1). more details may be available in \\'c:\\\\users\\\\selin\\\\virtualbox vms\\\\ubuntu\\\\logs\\\\vboxhardening.log\\'.\". my windows restarted itself and now my vb, therefore ubuntu is not starting.hyper-v has been much more stable for me than virtual box. i was getting blue screened every time i tried to launch a ubuntu virtual box instance recently.yes. pretty much any other virtual environment is more stable. after 4 hours of troubleshooting, i fixed it. i think it is time for me to spend money on some nice macbook, and never risk something like this again. btw: the problem is caused by windows 10 update, windows 10 firewall was attacking the saved ubuntu status files. for many people, removing the firewall was helping, but it did not help me. i fixed it by going back in time. :slightly_smiling_face: i\\'ve been doing all my development in wsl and it\\'s working a dream.might be an option for you.i will check it out! thanks!anyone want to do some nlp on emoji? witness the sputtering of the hive mind.that would be fun!anyone else interested in future insight alumni hackathons? once we (the current session) get some free time we\\'re thinking of doing this. join at  , did you ever find that voting data?anyone looking for room mates?if so,come join the new  channel!!!does anyone know a good *zeppelin* tutorial that walks through working with a dataset? i\\'m tasked with comparing jupyter and zeppelin, and i\\'m a complete zeppeling n00b. :wink: most i\\'ve found so far focus on installing it, but i\\'ve got that solved with docker.horton works has a gallery of notebooks that seems like a good start: : check out toree, which literally runs in jupyterit’s still in the incubation phase though...anyone know what happened to nuggetbot?don’t wake the beasti\\'m not sure how to stay positive without his friendly remindersph\\'nglui mglw\\'nafh nuggetbot r\\'lyeh wgah\\'nagl fhtagn.agreed stay positive, jollyhrothgar! all will be well.:stuck_out_tongue: dyfrig - could you please send the name of the textbook for time series analysis?: : you could also check out google scholar - they show h-indexthe arxiv has an api too, im not sure if it has citation information: the arxiv does not have citation or precomputed h-index infoinspires is much more rich, but limited to astro and hep: many thanks. google scholar does not provide any api, and it blocks if requesting too often -- based on the info i search online but didn\\'t try out myself.i got something to start with from inspirehep. thanks a lot, guys i looked into this at some point. all of the citation data are owned by either scopus or web of science. google has a deal to make it available basically as a community service, but they\\'re not allowed to have an api. for medicine/biology you could scrape a well formed pubmed search to get publications. :  if you need to scrap pubmed talk to   he did some nice things with pubmed data. here is an example scraping pubmed with ri have a perl script for it as well... did anyone (or know of anyone) go into machine learning/deep learning academic research in industry? i was curious what the career trajectory and market compensation is like. please dm me. thanks!can anyone help with getting a postgresql database set up with aws? i have an e2 instance running ubuntu, and i\\'m able to connect and load my app. i also have an rdb instance running postgresql, and i\\'m able to connect and query the database on my local machine with sqlalchemy and psycopg2. however, when i try to run my app on the e2 instance, i get the following error after running `gunicorn flaskexample:app`,```operationalerror: could not connect to server: no such file or directory    is the server running locally and accepting    connections on unix domain socket \"/var/run/postgresql/.s.pgsql.5432\"? ```if i comment out the line where i try to connect to the database, it will run. i\\'ve changed the permissions on amazon rdb, so it should be able to connect. i\\'m stuck, can anyone help?hi all, i fixed it! it turns out i wasn\\'t accessing the rds database as i originally thought. i was still accessing the local database, which is why it didn\\'t work on my e2 instance.hi everyone,  does anyone knows a good tutorial to deploy flask apps in redhat?  (beyond the ones you find googleling)does anyone have a suggestion for an hourly rate for data science consulting work? a large range -- perhaps $100-250 depending on if your client is a startup or fortune 500 :wink: many many factors can influence the rate thoughthanks! :expressionless::expressionless::expressionless:thoughts?it’s strange how we see computer scientists, politicians, and economists arguing that gender differences are a result of biology, but never population geneticists.note that google’s response in internal memo is on the motherboard article now (as is the full text of the original) .in terms of social issues, i find that it\\'s (un)surprising that we hear critique from (presumed) white guys who feel like they aren\\'t treated equally. regardless of background. we\\'ve always used \"biology\" as a way to argue for exclusion historically, unfortunately. you have to wonder that if there\\'s a general consensus  about what this guy asserts, where are different minority groups in supporting this statement? /sjwseems like a typical corporate response. i wish they took a harder stance. :thinking_face:i found his screed to be an exhausting rehash of garbage ideas and talking points. for nearly all of his arguments: [citation needed]. and i\\'d bet the farm this guy subscribes to red pill and men\\'s rights on reddit.yes, i thought the response sounded like bsjust read it. i thought those [#] markers in the google email were citations.turns out they were footnotes with more ranting.lolcontroversial male opinion: just read the whole thing. i think the pseudo evolutionary science explanations for differences in representations are bullshit, or at the very least difficult to prove. however, i agree with his point that there\\'s a culture of trying to shame people who think non party line thoughts on diversity and that it\\'s unhelpfuli find that pretty suspect and am curious what your examples would be. for this specific case, the medium article is pretty clear in describing how toxic this kind of ideology is.  it is important for leadership to take a stance against it so the organization can continue to function instead of tearing itself apart with suspicion and lack of collaboration. there is rightful anger against this person for perpetuating  the sexism and racism that has troubled our society for so long, and if they’re shamed by that, that’s their own fault.i\\'m going to sorta sound like a bitch here, but i\\'m going to say it anyways: there\\'s a distinction between compassionate educating and \"justified\" calling out. but i\\'m generally, if you are perpetuating sexism, racism, islamaphobia, homophobia, transphobia etc, an someone calls you out, i\\'m not gonna massage your hurt feelings. sometimes we say things or believe things that are hurtful (and/or wrong) to others. and sometimes we need to learn from those mistakes. i place myself in this statement too because i have also have done and said things that have been hurtful or off base.i usually don\\'t bring my own identity into the discussion, but i will here. i applaud aggressive inclusion practices and believe we need more of it. as a *trans* person, i have no effing patience for people who perpetuate oppression or discrimination. i\\'ve spent my past and current life being on the fringes of society, and puke when people who are not trans or have not consulted w/trans people try to speak for me.anyways, hopping down my pedestal now. 🤣i guess my example of would be the reaction to this postin the medium post itself the guy says \"i need to be very clear here: not only was nearly everything you said in that document wrong, the fact that you did that has caused significant harm to people across this company, and to the company’s entire ability to function. \"the idea that this one person\\'s opinion can cause significant harm to people seems like a pretty strong assertionand if we\\'re working on that assumption then the follwing \"if you feel isolated by this, that your views are basically unwelcome in tech and can’t be spoken about… well, that’s a fair point.\" seems justifiedbut i guess i question the underlying assumption, and think if we really do think he\\'s wrong, we should engage with him, not threaten to immediately crucify him and anyone who might be within 50 feet of himie, don\\'t be like yale i don\\'t think the crux of this debate is whether callouts are justified, it\\'s whether this employee (and perhaps others who think like him) should be straight-up fired (and perhaps blacklisted by the industry) via the usual social media dynamics. how far do we need to shift the overton window in order to have an environment that\\'s considered sufficiently inclusive to whatever groups?so it sounds like your argument is that you think his statement is not that harmful and therefore he should be spared all of the criticism he is receiving.  statements that support the kinds of discrimination and oppression that are the norm in technical careers (and elsewhere) are absolutely harmful. that system is absolutely harmful and we are trying so so hard to improve it. every time someone makes hateful and uneducated statements like this, we take a step backwards. given the public forum his statements were made in, criticisms absolutely must be in public forum as well. if someone said this stuff to me privately, i would argue them privately but not post it on the internet. but he is the one who made this the the forum. i have zero pity for him. also, the fact that you claim people  “crucify him and anyone who might be within 50 feet of him” is absurd hyperbole. he is being countered, as he should be. he might be removed from his job, once again, as he should be if he has prevented himself from being a functional part of the organization by attacking his colleagues. warning i am not going to  respond any further to this thread since i don’t actually know you and don’t want to go down an internet argument black hole.also, not to continue to belabor the point (against my better judgement), but what reaction would you expect if someone aired their misogyny (or other awful opinions) to a wide audience? would you expect everyone to be all ok with that?i think it\\'s worth understanding why many women got up in arms about this manifesto and made their grievances known, rather than being reactionary about it the grievances themselves. this is generally also a good model to follow for any social justice issue.i would say that there is room for both anger and critique on the discourse we have about gender disparities (and other disparities in general). it\\'s not an either/or. i find it slightly disengeniuous to compare gender disparities in tech (especially with regular headlines through this year) to an incident at yale. we know there is icky stuff going on in the world place at many tech companies, and is actually a good reason for people to be less than pleased with every new piece of news that comes out.like  said, i have no sympathy for this guy. he aired his opinions company wide. he should\\'ve been prepared for critique, angry or not.i think i misspoke in my original comment so i\\'ll clarify: yes, of course people should be able to get angry. but in the medium post that was also posted, yonatan says, \"you would have heard part (3) in a much smaller meeting, including you, me, your manager, your hrbp, and someone from legal. and it would have ended with you being escorted from the building by security and told that your personal items will be mailed to you.\"which basically says he would have been immediately fired for his opinions. judging from the near universal praise of that piece, i think many people here think that\\'s an appropriate response, and i don\\'t agree. the original piece might be wrong, but i think instantly terminating him doesn\\'t remove the underlying biases that lead to the piece. it just drives them underground so the next person who feels this way will just talk about it in redpill on reddit. if we\\'re serious about trying to convince people that diversity is good, how can we ever hope to convince anyone if they\\'re afraid to share their opposing views because they\\'re afraid they\\'ll get fired.so uh, mea culpa for making it sound like i think it\\'s wrong to get angry or critique him. that\\'s a totally appropriate response and no one has a right to tell anyone not to get angry. i just don\\'t think he should be fired or sent to the basement for rat clearing duty.+1 to that answer!hello all! can anyone recommend any tools for labeling data sets that have worked well for you? (and possibly ones to avoid) thanks!what type of data are you labeling? i used a java tool to do image segmentation… i can try to find ithi everyone (if i get enough thumbsups, i will hit the dreaded at channel on this) i am aggregating data on parental leave policies in tech startups.. hard to find data on small companies, and since scraping glassdoor is not the first thing i would like to do :wink: i am trying my luck with crowd sourcing! it is extremely important for startups to have parental policies in place if we have any chance at truly diversifying tech! women (and men) should not feel the need to work at select big companies if they want to make babies.. especially since having a family and kicking ass in tech are not mutually exclusive! so please help me aggregate some data since we all know data is power :slightly_smiling_face:  i started a sheet with the big companies, please add to it any small company whose policy you know of and any more data on big companies the better!wow super useful. thanks!so if we work at a non-startup should we enter our info or not?yes! please also enter! but would love to get some data on startups.. so please also include the number of employeeswhat’s the :+1: threshold for going at-channel?can we get more music fanatics in ?if you want a pure python solution for image segmentation, bleeding edge scikit-image has that capability we\\'re experimenting with  for text labellingthe developers are super helpful with deployment/customizationhehe.. good question soumya.. 30?anyone aware of openly available data on hiv anti-retroviral treatment (art) adherence with associated social/demographic/medical factors? i have an old project i’m trying to revive and get a publication out of, but out outcome (art adherence) was poorly measured and subject to a lot of bias, and i’d like to try and train a model on decent data of art adherence that i could then use for these empirical data we’ve collected to predict art adherence. i’ve been reaching out to researchers based on publications that might have data willing to share, but wanted to tap into this collective bundle of genius as well. i’m open to anything, but if it happens to be data from sub-saharan africa, all the better. if you’ve got any thoughts or leads, pm me. thanks!for those who do a lot of exploratory analysis or ml algorithm prototyping, what kind of tools (internally developed or externally available) do you use? the reason i ask is we are thinking about building some infrastructure for this internally (leveraging what\\'s available as much as possible). what are the considerations you would suggest before embarking on such a project? i\\'m thinking about, for example, infrastructure for database connection, scaling compute resources vs. quickly seeing if an analysis is promising vs. how easy it is to move from exploration to production. those might not be all or even the most important things to consider, though. basically, before we waste time building infrastructure that turns out to be useless, i want to see if there are any best practices for an exploration/prototyping platform?i have a potential co-founder opportunity if anyone\\'s interested. i\\'ve worked with christian for over 2 years now and can attest to his judgment and ability. he\\'s recommending his friend (sam\\'s) soon-to-be-startup:```sam is launching a startup and looking for folks with deep data science/statistics backgrounds, to be a cofounder or an early employee. he\\'s building machine-learning tools that help companies make better people management decisions, starting with hiring. the venture is very new, but they\\'ve signed up 6 relatively large companies for pilots and have assembled rich workplace data on 50,000+ employees. they\\'ve got a few hundred $k in funding and are opening an sf-office this summer. if you\\'re interested, sam\\'s email is it\\'s simple: if you want to start a company and build a data product in this space, you should do it with sam. he\\'s everything you\\'d want in a co-founder, and also everything that you didn\\'t know you wanted but would if you knew the future. he\\'s really smart, super organised and hard working, and most importantly posseses an outstanding character--he will always have your back. the only reason i\\'m not joining him is that i already have something else going on. do yourself a favor and reach out to him if you or someone you know is interested in building a data product that could improve how companies hire.```i think that python 3.4+ has been supported since spark 1.4thanks everyone, this has been super helpful!thanks !postscript: i tried applying the updated `prophet` r package (v. 0.1.1) to my forecasting task and it no longer crashes. hahathat would be funny uploaded a file:  commented on ’s file : kevin take some more for me? thanks!videos? uploaded a file: here\\'s a short clip: what\\'s the conclusion/summary from this salary slide?my takeaway was that salary differences by gender for outgoing insight fellows is much better than the industry norm  uploaded a file: not to be a buzz kill but is that true? i was under the impression men/women mostly got paid the same for the same positions but just men got promoted sooneranyone in this group interested to form a team for this event? or if you know someone who might be interested? might be a long shot, but i think any phd who currently maintains student status (even defended) is still eligiblepm me if interested :slightly_smiling_face:::  \"note: the nba basketball analytics hackathon is open only to undergraduate and graduate students studying at accredited universities in the united states.”you can do it anthonyif you believedon’t dream it, be it~/giphy rhpssquare is hosting a data science meetup for anyone who interestedhey guys, there are various ml meetups in the bay area; i was thinking to start a ml meetup with insight fellows. we can meet sometime on the weekend at the insight office. let me know if there is interest!on ^, who wants one for nyc?anyone have experience with large-scale product merchandising? we have a client who manually curates groupings of similar or related products that would like to start automating the process i have some experience, not sure if applicable, but feel free to dm me with details so i can see if it’s relevantwould be interested!same!i’d be interested. the quality of meetups in nyc is very…mixed, so i think this would be an improvement.super! let’s collect names and set something up.agree with varun. there are so many meetups of very little utility. how can we ensure ours is unique + useful? … because _we’re_ uniquely awesome?personally, i’d be more likely to attend something that involved actually writing code or building a project over time to learn a new skillsi upvote that(heading into meeting, will respond shortly)like working in a group of 2-3 people to build some new prototype for a month, then sharing it with the group, then switching projects and switching partners:+1: :+1:how about open source issues?perhaps as one option, though in general that might be too narrow for me depending on the librarymore options the betteri have some issues i’ve been wanting to finish for statsmodels, maybe also a small contribution to mlxtend or sklearnwhat other sorts of projects would you want to work on?user embedding?potentially, though i’m not quite certain just yet as things will change soon i love the idea of building a project on a small team. count me in!looks like we’re gathering a crowd!tell your friends!interestedi would! but live in oakland.  if we switched between the  two offices that would be preferablesign me upi\\'m interested, but i am a simple data engineer. what would be focus of meetup? so there is a lot of cool stuff happening in the ml world right now, and basically all of it is freely accessible. i work on some stuff on my own, but i think it would be more productive and more fun to do it with peers :slightly_smiling_face:for example something that we could do is do the stanford deep learning nlp course that is available on youtube. but tonight i\\'ll make a new channel and we can discuss what people\\'s interests are and what they hope to get out of something like this. we\\'ll come up with something :slightly_smiling_face:we can also form a kaggle team :slightly_smiling_face:all of our conference rooms have glass walls to encourage transparency and openness--if those are important aspects of your corporate culture, i highly recommend them! also i love the gentle hum of the industrial hvac  :blush:does anyone out there know of a good e-mail dataset with labels other than spam/non-spam?if you work near campbell, i would recommend living in campbell or even better at los gatos. campbell should be more affordable, though.have you looked at the enron email dataset? not sure about labels thoughhey  i’m going to be working at velo3d (4 year old startup on 3d printing) as a ds starting in july :slightly_smiling_face:yes, so far that has been my best option. i would be interested to hear about the tech they use at velo3d. i did research involving metals 3d printing in grad school :slightly_smiling_face:they are in campbell ? then i\\'d def live in the south bay or peninsula, or even santa cruz:thumbsup: uploaded a file:  and commented: machine translation is hard.(how does dishonored 2 become rayman 2?????)hey all, i\\'ve got a question on classification (and possibly anomaly detection): i have five known classes with labeled objects. in the future i want to classify objects as they show up, but there is the possibility that something will be anomalous (in a way i can not predict now). such an anomalous thing should not be put in one of the five classes, but should instead get a \"weirdo!\" label. what are my options? any good readings on this?two different models might be a good option. one for classification, one for anomaly detection.that certainly seems reasonable!i’ve had a similar problem in the past, and that was my solutionit worked out fine as this wasn’t something that needed to be done in real timedo you have any weirdo’s in your current dataset?no :confused:stay positive alex_gudecan you simulate some?how complex is the data?i can actually tell you the whole problem. :wink: we\\'re looking (well another team here is) at network traffic between devices. classes are things like \"web server, dns server, phone, laptop, apple tv...\". basically, everything on the network is already know. if another laptop is added we\\'d like to be able to say \\'that network traffic looks a lot like a dev laptop!\\', and if something weird shows up we\\'d like to say \"weird new thing on the network!\"so simulating anomalous network traffic is outside my knowledge basei am making this up on the fly now but what would be the problem if you were to model 5 models. 1 for each class and if all five models are pointing towards not its own class maybe there is your weirdo?just make it really weird then. add 1 billion to everything:wink:\"anything that connects to port 1,000,000,022 is weird!\" :wink:hahawhen i did this previously, i used a one-class svm for the anomaly detection where all the example data was used for training. then i used a multiclass model for the actual classification. can’t remember that one...yeahthat seems like the right way to attack itit\\'s sort of inline with tolgao\\'s idea, except instead of training n classifiers, you train 1 that says \"anamolous\" or notit wasn’t something running on streaming data, so the added time from that two step process wasn’t a big dealyeahyeah, in theory our group hopes to run these classifiers on the sdn hardware... but they\\'re looking for ideas, so anything helps :wink:hello smart insight peeps.  i need to get up to speed on tableau quickly.  any suggestions?run away :slightly_smiling_face:seriously though, i never came across any great learning materials out there, what was most useful to me was to reproduce tables and viz that i had produced using other tools like sql+pandastableau has some nice tutorials nowthere is a feature where you can set up the data source and then look at the generated sql, i found that usefuldamienrj: got any links?looking for the one i usedi liked this one, got me up and running pretty quicklythanks. jonneff: what are you being asked to produce via tableau?not sure yet.  :grinning::grin: got called into boss\\'s office at 4 pm to ask if i could repurpose an existing tableau app and write some python to automate alerts.  saluted smartly and said yes sir!it\\'s going to be a poor man\\'s rule based expert system.  anomaly detection without the statistics. alerts without statistics sounds a little dangerous to me. we’ve had issues with some “dumb” alerts in datadog that were wrong often enough that people just ignore them now even though they may indicate an actual problem some fraction of the timestay positive andrewzirmit\\'s rules like \"chip card in chip card country being used as mag stripe = fraud\"fair enough, as long as you aren’t building the alert-that-cried-wolfi will say, jonneff, having lived in “chip card country”, sometimes you just gotta failover to the mag stripe. :slightly_smiling_face:i think that the problem will likely be that it is a real wolf but he doesn\\'t approach the pasture the same way next time.  :eyes: dat wolfdoes anybody have experience with nginx? i\\'m using it as an http proxy to make calls to a rate-limited api. the api service gave me two more keys, so i can now make three times the number of calls from three different servers. if anyone knows how to configure nginx to rate-limit _across_ concurrent http proxies (not individually), please dm me. thanks!fyi, this _may_ be what i was looking for. still playing with it, appreciate comments if anyone has done something similar. sure. i’ll give you an idea once i’m there. i did my phd on this (3d printing + ds) so i’m excited about it yes they are in campbell. thanks for the advicefun post from yhat:  dear all.  happy hour tomorrow thurs, starting 530p @ b-restaurant and bar in sf.less data more drinking!2016a-sv is hostinghello! has anyone had experience opening outlook email .msg files in python? i\\'m having trouble figuring the best way to do it. thanks!!!(   )thanks!!has anyone used databricks? opinions?it\\'s sweet: i have and use it quite extensively at work.  its a great and easy way to launch and maintain spark clusters for various data pipline tasks and analysis.  the notebooks are very similar to zepellin and work well with different languages.  they have a pretty complete and free community version that you can use to test it out.i took a spark class last month that used it. it’s pretty nice for handling a lot of the background configuration required by spark auto-magically in the background. i get the sense though that if you need to do something with a special or unusual configuration, it might not be so nice. that is wild conjecture though—it could work just fine.also it’s written in scala, so things tend to work in that language a little better and a little earlier than they do in python. i didn’t have any problems using the python bindings though.awesome. thanks for the input! i\\'m playing with the community version now and like it so far. i\\'d like to play with sparkling h2o also.&gt; i get the sense though that if you need to do something with a special or unusual configuration, it might not be so nice.can affirm ’s hunch. we have a some pretty specific requirements that we struggle with on databricks… but i will say, their support is really pretty responsive. they have shown us some backdoors that are useful. i would also second the use of scala over python (we use python almost exclusively) — there are things we have done in scala because it’s possible and not  possible in python. but, all in all, most days, we are thankful we have db managing most of the backend magic for us.  : reddit popped up in my seo demo. would\\'ve been funny if that happened a day earlier during the actual demo there. can you say \\'awkward\\'? :flushed:hey all, have to move out of my current place on july 16.  if you know of any available shorterm furnished rental opportunities please let me know!  thanks these are just my thouhgts so far.. you suggested s3 -&gt; spark -&gt; redshift.  given your goals this makes sense to me but i\\'d want to understand more what do you plan to do with spark?  if you are planning to work mostly with tabular/structured data (csv one option your engineer mentioned) then it seems to me that all you need is s3 -&gt; processing daily in python/pandas on a relatively high ram machine -&gt; redshift.  spark will be needed if you want to build ml parallel models using mllib, and/or if your data is coming less structured and you want to do some parallel processing of it... otherwise it may be adding more infrastructure w/o need for it... on the other side redshift can be considerably more expensive than spark and if you are planning to grow to 100 mm rows per day you\\'ll need some relatively large redshift cluster... if i were you i\\'d try some combination of this, starting with the simplest s3 -&gt; pandas -&gt; redshift and based on that decide... for that i have used this with no problem...  good point about spark possibly being overkill.  i can definitely write the code first with pandas, then switch to pyspark later if need be.  thanks!does anyone out her have a favorite weather information api? i just need basic stuff like temperature range, humidity, air pressure, precipitation (basic local news weather report stuff). it would be really great if their coverage also included more rural areas, and not just urban centers.i’m also thinking of heading in this direction for a project, so i’m interested to hear what you have found so far!noaa has an api.  if you’re still working where i think you’re working, then we have a similar use casesee for example. i use the `requests` library to go hunting and parsing for information; i’ll leave the rest to you(ps you’ll need an api key from them)if you are going from audio files, bilstms have been state of the art from my understanding (a paper example here ). you can basically cluster on their hidden states once they are trainedi recently joined a top-vc backed ai &amp; cloud services early stage start-up in palo alto. it is in stealth mode so i cannot share so much, but it is an amazing opportunity to work with people who have had several successful exits in the last decade. basically they know what they are doing. and needless to say, the stock option is going to be pretty good. we are looking for senior backend data platform engineers in java / scala to build data stores, pipelines, micro-services, data orchestration, communication, distributed systems management, availability and scalability.  if you are interested, dm me.and machine learning engineers and data scientists, as well.nice. do you know if the noaa api  has wether information for just the us or all over the world? in either case, i\\'ll look into it! thanks, !great question. it is a national service…haven’t tried internationallyi\\'ll let you know...looks like it does... which is nice. did some of this for his insight project.first to chime in on the jira discussion --- (1) it can integrate with github and atlassian and for that it is nice that i can write a spec doc and link my tickets. (2) it is not really designed for data science, but i find that it can be helpful to find info on things my colleagues are doing that i need by reading their descriptions and comments --- when they fill that in --- so please do that. (3) i am in the middle of my first \"self-assessment\" as part of our annual assessment and i found it really helpful to look at all my tickets since i started (next year since my last review).  it took some help from an se with more experience to know how to get that, but now that i know, it is pretty cool.   i don\\'t have experience with other systems outside of using trello in insight for personal stuff.second: i have googled and googled and googled and i need to sleep, so i need to ask for help and i thought i would start here.  i have found lots of pages that note that using string formatting for sql queries with sqlalchemy (and more generally) poses significant security risks, but what i can\\'t find is how to properly identify the parameter in the string and in the sqlalchemy call.  what is most important here is that i\\'m using mysql.everything i\\'ve tried either throws and error, or returns an empty dataframe and i know that the query itself is fundamentally sound, it is simply formatting the parameters correctly that is the problem. uploaded a file:  and commented: here is a super simple example.  assume i have already successfully set up the engine.  the question is how to correctly indicate the parameter in the query string and now to correctly enter it in `read_sql`.  i\\'ve read the docs, so know that tuple, list or dict are options, and i\\'ve had the same results with `{person}` and`@person` and it has to be a basic formatting issue --- everything suggests this.  but none of what i\\'ve found is mysql specific.try `%(perons)s`i think the interface is fairly standard and i know that works for the mssql connector (used it literally minutes ago).hi fellow fellows!i\\'m looking to hire 1 (maybe 2!) data scientists for my team at coach, inc in nyc.i\\'m a director in the customer intelligence and data labs group. we\\'re focused on customer-facing analytics and data science to grow our multi-brand (coach, stuart weitzman, and maybe more soon), global fashion retail business.there is a great team in place right now with 4 data engineers and 6 people focused on analytics and some basic data science. it\\'s still a pretty green group as far as machine learning goes, but everyone is very excited to dive in and learn. they have really invested in strong, clean internal data and coach itself is a very numbers driven organization. i report to an amazing vp (best boss i\\'ve ever had) who reports to the ceo. we\\'re well positioned to get some really cool stuff done, it\\'s just too much work to get done myself. if anyone is interested, i\\'d be happy to share an official job description, answer questions, meet for coffee, talk about our employee discount, etc.stahpkeep it classy bartwatching a spark tutorial and wondering why the instructor keeps referring to some package called \"skippy.\" later realize she\\'s talking about scipy... :open_mouth:hi,does anyone have any experience with armax models?i do  anything i can help you with?hi all, i’m wondering if i can pick the brain of a data engineer who has experience with in-memory databases (redis, memcached, etc.). i have a few questions regarding why you would choose one service over the other. this looks like a good read for comparing redis and memcache . i\\'ve only worked with redis in the past and it\\'s super easy to get started. we have had issues at insight with security in that the default port (6379) on redis has been vulnerable to getting hacked  i\\'d be interested in learning about your experience with armax models! i created a #time-series-model channel if you don\\'t mind having a conversation therethanks austin, i’m hoping to take things in the redis direction, but i have a few co-workers who want the decision made with a bit of evidence, this is perfect, thanks!what if we\\'re not trying to convince people that diversity is good but trying to send the message that we don\\'t tolerate supremacist ideology? because i think you might be mistaking the point of these reactions.ok i read everything before weighing in.  1. biology is not destiny.2. if we devalue empathy, why should i give a rat\\'s a$$ that a conservative feels stifled?3. google makes tons of money.  affirmative action isn\\'t hurting their business.  4. another clueless male with a high paying job complains about being mistreated by programs that help people who have been held back by centuries of prejudice.  ps i predict appearances on fox news, a book deal and a job offer from trump\\'s equal opportunity office. i don\\'t feel sorry for him.  great resource for redshift tuning my team has implemented some hierarchical classifiers in the healthcare setting (classifying onto a medical ontology that is hierarchical).in a tree-structure, we have implemented hierarchical classifiers, where each internal node has a classifier for its children, and the classification is done by starting at the root, and traversing down via the node-wise classifiers, until a leaf-node has been established. we found that this is usually a bit better than a flat classifier on the leaf nodes.we are also looking into some alternatives. let me know if you find some sweet ways of doing this.hi leifur, thanks for your msg! do you mind my asking how did you train each classifier at each internal node?individually or you can somehow optimize the entire tree of classifiers all-together?any bostonians in this channel interested in presenting their insight project to a broader audience at pydata boston meetup? i\\'m in contact with one of the organizers, who says there\\'s a slot for a bunch of alumni/a/ae/arghcannotevergetitright to present at the end of august. are you after international companies?no just the usa.. the rest of the world has paid leave.:sad_parrot: :burn:thank you for doing this! also this is depressing :sadpanda:hi all! does anyone have insight on advantages/disadvantages between chartio and mode analytics as bi tools? any specific reasons you would choose one over the other?would also love to know if anyone has used aws quicksight and has thoughts on it. thanks!hi all, has anyone done basic calculations on geospatial data, let\\'s say i have a table with lat, lon and a float (i.e., phone signal), and then i have a shapefile with boundaries, and i\\'d want to know the average(float) for each polygon in the shape file... the table with lat, lon could be billions of records (so could be quiet large)... i considered postgis as an option but if my table is too big it\\'s not the ideal dbcan’t edit notifications away  :stuck_out_tongue:loli later remember the non //here policylolhow fast do you anticipate your table to be growing?not that fast... the table currently has 1 bn+ recordsat most 100 mm / moor something like thatif it’s for exploring/prototyping, i’d suggest giving postgis a try firstfrom what i’ve heard, it’s usually not the number of records but the total size of your table that slows postgis downsince you only have point data, it should be able to handle lots of objectsif postgis, doesn’t work, you can try geomesa for which you can choose various backends (e.g. cassandra, accumulo, hbase, etc)but of course, it’ll be a lot more overhead to maintain itmmm nicelet me give this tools a tryof course, if you have lots of unused gpus lying around, you can give mapd a try :slightly_smiling_face:(they have since made mapd open source)my team uses chartio, but now we’re looking to host our own bi tool\"current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong ai. to achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. to demonstrate the essential role of such models, i will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.\" - judea pearl ()thanks for your response ! any specific reasons you weren\\'t satisfied with chartio?because we want to reduce an extra step in the data pipeline where chartio needs to copy over data into it’s own data store, and also for cost savings.  we found open source tools to be pretty feature complete as it is.i work for the city of ny and we get zero paid leave. in fact, after cuomo\\'s latest paid leave law went into effect in jan, unionized city employees are the only workers in the state, private or public, without paid leave. oh and i just had a baby, used all my vacation and sick days and am already back at work 6 weeks later. yea i it writes to cassandra locally. could it have to do with maven?hello everyone - i\\'m currently executing a pyspark batch job on a cluster. my data transformations are a series of map, flatmap, and reducebykey operations. it seems like my driver is the executor for every single task, and the reducebykey operation is taking a long time. i know that reducebykey is a wide task and will require shuffling, but i\\'m confused as to why the driver seems to be doing everything. any thoughts/gotchas i should look into? make sure you have executors alive on the spark master ui and when you spark submit be sure to set the spark master flag to the master ip:7077. it’s possible your job is executing in local modethanks! didn\\'t set the --master flag, so i was running this on local modenice. glad it worked! hope your week 3 goes great!i’ve checked out lifelines but it still seems to require the definition of a “dead” user before the modeling phase. but i have not seen the pareto/nbd paper. will check it out!this is such a perfect depiction of how i think we all feel in seattle right now!!oh actually, i have checked out the pareto/nbd models too. can’t remember why i thought they wouldn’t work now though… i’ll have to revisiti have not, but will take a look. thanks ayseni, for one, welcome our new algorithmic overlordsi’m surprised nuggetbot didn’t have anything to say about that, carloswhy would nuggetbot have anything to say about algorithmic overlo- oh my godhi all, some good friends of mine at ucsd are beginning a funding campaign for their non-profit startup that is focused on bringing the online education revolution to offline communities. they\\'ve been at it for a few years now and made huge strides (near the beginning: ). this is their first public fundraiser. they would love to get some donations before monday (when they will make a more public push) so those that come to the site will see funds already being donated. if you are at all interested in bringing the education revolution to everyone and not just to those with internet, please consider donating. also, if your company does matching contributions, that would be awesome for them. thanks! : well, it was nice having a job while it lasted. :wink:\"stay positive alex_gude\"(where is  when you need it?)nuggetbot helpnuggetbot adapter - reply with the adapternuggetbot animate me &lt;query&gt; - the same thing as `image me`, except adds a few parameters to try to return an animated gif instead.nuggetbot echo &lt;text&gt; - reply back with &lt;text&gt;nuggetbot help - displays all of the help commands that nuggetbot knows about.nuggetbot help &lt;query&gt; - displays all help commands that match &lt;query&gt;.nuggetbot image me &lt;query&gt; - the original. queries google images for &lt;query&gt; and returns a random top result.nuggetbot map me &lt;query&gt; - returns a map view of the area returned by `query`.nuggetbot mustache me &lt;query&gt; - searches google images for the specified query and mustaches it.nuggetbot mustache me &lt;url&gt; - adds a mustache to the specified url.nuggetbot ping - reply with pongnuggetbot pug bomb n - get n pugsnuggetbot pug me - receive a pugnuggetbot the rules - make sure nuggetbot still knows the rules.nuggetbot the rules - make sure nuggetbot still knows the rules.nuggetbot time - reply with current timenuggetbot translate me &lt;phrase&gt; - searches for a translation for the &lt;phrase&gt; and then prints that bad boy out.nuggetbot translate me from &lt;source&gt; into &lt;target&gt; &lt;phrase&gt; - translates &lt;phrase&gt; from &lt;source&gt; into &lt;target&gt;. both &lt;source&gt; and &lt;target&gt; are optionalnuggetbot weather - get the weather for nuggetbot_dark_sky_default_locationnuggetbot weather &lt;location&gt; - get the weather for &lt;location&gt;nuggetbot youtube me &lt;query&gt; - searches youtube for the query and returns the video embed link.victory - display an victorious imagenuggetbot animate me ninjas fighting(i’ll accept it)nuggetbot tell me rules of the internetfyi we have a channel dedicated to nuggetbot  *raises hand*but ask  i think he\\'s taught a class on itare you trying to track a fixed value or will your set point be moving around a lot?yep i have! not an expert but i\\'ve mentored students to run pid controllers on roombas that get discrete location data my strategy was to hold off mentioning this until the end, otherwise you risk getting cut by an hr person who doesn’t really know how easy it is. getting a tn visa is easy easy easy and your company will hire a lawyer deal with it. i gave the guy the letter from the lawyer, talked to him for like 5 minutes and i was in. lol, i keep telling people to use it but no one listens to me.everyone wants to do some optimal control bs.    i’ve also heard some stories of major delays with tn at the border, so just be sure to do your due diligence on your specific situation :slightly_smiling_face: the set point will be moving around a lot.  i’m just looking for something simple, better than the baseline of nothingmoving around a lot compared to how long it takes you to compute the control signal?if the computation time is orders of magnitude faster than your dynamics you should still be finemodulo some nonlinear thingscomputation time is much fasterdo you know of any frameworks that would fit this problem?what problem?  pid is a pretty general control algorithmthe problem for most engineers is gain-tuningi.e. picking the parameters of your controllerthere are some automatic algorithms, but most people can get away with heuristicsthe algorithms won\\'t do much better anywayone of the big advantages of pid over more sophisticated methods is that you can tune the parameters online and by handi can pm youre: tn, definitely give yourself lots of time for the application. i got pulled into secondary screening at yyz and had to run to the gate to avoid missing my flight. never had any issues once i actually had the tn stamp though. on lawyers’ advice i’ve brought my original diplomas with me every time i cross the border but i’ve never actually been asked to show them. was also given the tip that they usually have more experienced immigration officers on duty during regular business hours (9-5 m-f) and you’re less likely to run into issues with them, so book your flight to the us then if you canstay positive bradit’s hard sometimes, re: tn, don\\'t snooze on it, there may not be a nafta agreement in the near future that\\'s a happy thought :disappointed:  .. though if that were the case, it would still buy 6 months (jul 20) at the very least, if it were revoked on day 1 of the trumpian era... so more than just 6 months...demandbase is hiring a data scientist and data engineer! our triumphs in machine learning end up directly affecting the quality of our product, which has been super exciting (demandbase is one of the pioneers of account-based marketing). let me know if you\\'re interested, or if you know anyone else who would be! graphs, python, machine learning, spark, aws, etc.has anyone used open street map data before for routing and encountered issues where the shortest_path function gives really poor results?does anyone know if the data is bad enough that roads in manhattan could end up being disconnected?stay positive mattkrems:  had a similar problem with his project.: my issue was that searching for the nearest node to an address would often return something like a node on a foot path, which didn\\'t seem to be actually connected to the street grid... i \"solved\" this by instead querying for the nearest intersection node (node with more than one associated way) and using that as the origin/dest of the path search. not sure if you\\'re facing the same problem, but hope this helps!if anyone is using natural-language processing for their projects (or alums that still work with it), just made a  channel for sharing ideasanybody have experience with/currently working on  clustering (hierarchical or otherwise) that involves non-numeric data (ie string-delimited categories)???? can you give an example of the kind of data you’re interested in?what is the usual procedure to standardize images of different sizes and shapes to the same size and shape, if i want to feed the pixels into svm etc. as features?  resize the images and pad using white pixels?hi , feels to me will be best to go into the fourier domain and then revert back with fourier interpolation onto the plane region you aim to cover. personally do not like the artificial white background you\\'ll be adding. note i have no idea how an svm will respond to your patching idea.: i’m not sure what the usual procedure would be, but photoshop has some pretty good options for batch resizing. i’m sure you can specify things like “keep aspect ratio” and fill background with white (whatever tasks can be done by clicking in ps can also be automated with a script)  i\\'m trying to cluster data that involves fec filings/ super pac expenditures - some of the data is numeric, but i also want to factor in expenditure type, candidate, ect... i agree that the padded white background is not ideal... i think i get the rough idea of the fourier transform idea; do you have a good reference or good package to perform that on images?   i have the images as numpy arrays so padding should be relatively straightforward, i hope: one good way to deal with categorical feature is the \"one hot\" encoding scheme which transforms one discrete feature into several binary features. sklearn has a function that does that: : pandas has a feature pd.get_dummies() which is more straight forward than sklearn.preprocessing.  if it’s really sparse, you might want to consider using sparse matrices: of the top of my head all ones does is ft, zero padding in the frequency domain to satisfy your size requirements and then back to real space. you could use numpy to achieve all this but  the  suggestion may save you some time.  as to references      best bet is to google for fourier interpolation of @d signals and similar2d signals i meantdear all, does anyone has some publication data with bibliography information which could be used to calculated h-index? many thanksbtw, tangentially related, careful when doing interpolation in the frequency domain (if you ever need to do so).  fourier amplitudes decay fast with frequency thus posing challenges to most familiar interpolation techniques. here a link with a nice comparison (only sending the conclusions page):   thanks!: what field are you looking at?  has loads of particle physics and astrophysics data, and an api to boot(and counts citations and computes h-indices of authors)many thx, dstone.  any field would work. i\\'ll check it out.is there a channel for posting job listings?my parent company is looking to start a new data team and want to hire an ‘experienced’ data scientist to lead itis that a job with the tender team? i would love to work for tender.classic example of \"too much hackery\" in tableau: my colleague wanted to show zero rather than nothing in a table of numbers when there is no data. instead of a friendly formatting option \"show null as zero\" or similar the standard prescription () is a new calculated field with zn(lookup(sum([sales]),0)).for tableau fanatics this is second nature \"sure, that\\'s easy, just create the following three new auxiliary calculated fields...\" but for most this cuts into the easy-for-anyone aspect of the product. my colleague who is a very smart, hardworking guy just said \"uh... no\" and now hates the product a little bit more.oh, also i forgot he also went and looked them up and asked, \"so, how is it that tableau is $6.5b company?\" my response: we just paid for an enterprise license.thanks ddrum001 and johnjoo ! (and andrewmo for posting)got my last job from someone i met there. :grinning:also i remember a session on hidden markov models that was pretty good.  of the general data science conferences i’ve been to, odsc was the best. that was odsc west two years ago. sounds like either data architecting (higher level) or data modeling (more detailed).  for architecting try covert, \"how to make sense of any mess.\"  for schemas try data modeling section of stanford\\'s free database course on lagunita. my friend justus, is coming to palo alto on aug 23. he and i will rent a place together starting sep 1st. between aug 23 and sep 1 he will need a place to stay. anyone has a room to rent out on such short notice between aug 23 and sep 1? or a couch to surf? thanks!anybody in the bay area with experience and interest in building recommendation engines in production, hit me up. there\\'s a role opening up on vevo\\'s (music videos) personalization and recommendation team for a senior ml engineer. hi all, i\\'m looking to make a transition from my current position, so if anyone knows of any opportunities for an experienced data scientist/machine learning engineer at your company in the nyc area, feel free to pm me. thanks.a thing i’m working is is identifying events in remote places as they happen. anyone here do super hyper targeting of individuals and events? i’m wondering what sources of info you might use outside of twitter/fb/youtube/google trends? - i’m looking to hire a spark/scala guru for part-time consulting, needed now through the next week or so at 10-20 hours/week, with opportunity for ongoing work at fewer hours. we’re in nyc but remote is fine. pm for more info such as rate and scope. thanks! have you looked at gdelt events data?this looks very useful, thanks !: acled is another event dataset you might like thanks ! , 80gb doesn’t sound that big. can you just put it in  big aws machine? they have instances that have 2 tb now  thank you, yeah, i agree, i do not think 80g should be handled by spark.  the matter would be, the manipulation in spark. do you know any good source to study this?why are you trying to manipulate with spark?if you’re trying to prototype, you can either run spark locally or use something like pysparkling, if you’re using spark with pythonnot in the spark, maybe, but in my python code, there is matrix manipulation. the problem is , i think , how do i load my matrix  from my data.i’m saying that you could just load up an 80gb dataset in a big ec2 instance and use python instead of having to deal with sparkdomino has all this set up for you. pm me if you want to try it out as john suggested in the past i’ve just used a notebook environment on an ec2 instance for stuff like that, i forget what such an instance would cost to know if it’s in your budget, but it’s probably the most straight-forward way to work with that data in python. i have my data in our system , cannot take it out side of the system. but i can work interactively. i may want to try what you said.  by the way, what is the different between running python script and running interactively?if you mean script vs notebook, the only differences are what you want to do - i didn’t read your initial goals, just the memory aspect.  i was just saying in the past when i wanted to do exploratory stuff in a notebook with a similar data set size i was able to do it on a notebook environment set up on an ec2 instance with lots of memory.  an alternative is to see if you can come up with an algorithm that allows yo uto somehow do your operation iteratively, not sure if this is possible, but if it is you can then maybe stream the data bits at a time.  thanks harry, “if it is you can then maybe stream the data bits at a time.”  i am now thinking if i can make this happen, do you think parallel  calculation would work?it depends what you want to do - for basic linear algebra computations like matrix-matrix multiplication you certainly don’t need the full data in memory all the time so if you can figure out how to partially load the parts you need, do the relevant parts of the computation, dump them out of memory, and load other parts, then you can get that to work.  i would bet someone has already written stuff like this evenif you’re using python, you can use daskat some point it becomes worth it to use spark.: thank you, what is the \"dask\"? dask:   thanks!!  a team at ebay needs to hire data science engineers in nyc.  it\\'s not my team but i work with them.hey all, i\\'ve got my first post up on the lab41 website, would love if you\\'d take a look! it\\'s about the datasets we\\'re using in our recommender challenge.  disclosure: i have an editor writing the headlines, and he is a bit enamored of buzzfeed :wink:hey all, if you are interested, here are the sentiment results from the super bowl last night for every 5 minutes of game timei am still working on mapping the plays to utc time and found a better avenue using the coaches film for each playi\\'ll confess mickey: based on the words being used by the panthers fans in my living room, i think your sentiment might be \"overly positive\". :wink:haha you need to tell them to get on twitter more!the most likely outcome of that would have likely been phones thrown to the floor, so perhaps it is best that they were not.: where is the positive spike for puppymonkeybaby?one of the best salary negotiation posts i\\'ve ever read, fwiw: twitter news:  +1 for the floodinghi jeremy - if you are still looking for some indian rupee change, give me a call or e-mail.  i am not as active in slack, so bit slow to respondhas anyone worked on spatio temporal models for irregularly space data (i.e., wind turbines in a farm field located in random positions, some meters away, some kilometers away to the closest one).  .. on a more general topic, anyone working on spatiotemporal models?oooops keeps forgetting not to put the at here sorry everyoneandrew ng’ s new book---machine learning yearning - chapters 1-14 —  a colleague combined krieging and facebook’s prophet modelhi all, i\\'m a bit confused with the newer spark api (dataframes and datasets)... does it still make sense to use mapreduce or is it better to let spark handle the operations under the hood? thanks! good hint... makes sense, sounds like a good approach, would you have some more details on the models used/approach, etc.sure, she wrote a blog post. (incidentally she is also an insight alum) nice article!  love the classic \"stay tuned for part two, coming... never!\"lol, indeed. the work is done, but she just needs to write it up :slightly_smiling_face:wow nice!thanks for sharingyou bet!hey program directors - i’d like to see if its feasible to drum up a consulting project relationship between insight and the smart-electric business unit at mercedes. they have a pretty rich data set which i’ve been working on as a side project and it seems like an fruitful place for fellows to get some experience with a consulting project. might also lead to job opportunities for a associate to mid-range data scientist position. ping me if there’s some interest and we can set up a meeting.so cool!nice! this should be interesting.thanks for mentioning it,  could probably help arrange thiswhich language are you using? if  python, you probably want to stick with the dataframe api as much as possible. that’s because you will get near-native (scala) performance when using the dataframe api with built-in functions.also, for machine learning you’ll want to be using dataframes, since  will only support the dataframe api going forwardhi everyone! i\\'ll be a speaker at odsc east in boston in a couple of weeks, if you are attending, come and say hi! excited to meet other current and former fellows! is anyone familiar with apache hive? i’m trying to run a cross join with a temporary table, but having some errors.my company was just acquired and our new corporate overlords are not doing a great job of assuaging the doubt and uncertainty that comes with. it doesn’t help that there is a cultural divide (it turns out the job market and job culture are very different between a corporation in minnetonka and a start-up in boston) and they are taking away many of our benefits. has anyone dealt with this? any advice for dealing with the stress/knowing when to jump ship/anything else?i was at a publicis-owned ad agency when it merged with another publicis-owned agency. it created a fairly long-feeling/anxiety-producing period of about 6 months where everything felt stalled.all goal setting at the company level had to be re-evaluated and took ages, people expecting promotions were basically told, yes it\\'s still going through but wait, keep waiting, then eventually they came through, which was great, but it took a while. several higher-ups left as they didn\\'t have a clear place in the new org.recruiters usually pounce on people when they hear the news as wellafter that 6 months, things were pretty much back to normal in a cultural sense though. my girlfriend also went through something similar with her company having had a majority stake go to a pe firm this year.we also both jumped ship at least in part due to the inertia / uncertainty that it causedthanks . this is really useful.somewhat -- at least to the extent that spark supports hive queries.i think the general wisdom is to not run cross joins without any join conditions, but i\\'m aware that may not be helpful :slightly_smiling_face: i feel ya. i just went through/living through that. my company was a child company and hq decided last year to fully integrate. imo it was/is a poorly planned process. many left, i have visa issues  that made the jumping ship harder. i am on month 8-10 and things haven’t been back to normal, but on the positive i have started to do ds which was something i was unable to do consistently when i was part of my original company. to be honest the uncertainty kills, and oftentimes i’ve just wandered figuring it out how to advance my career within the chaos. also feel that this was not helpful :stuck_out_tongue:probably best to look around in the job market and leverage it to keep your benefits? most of the times in these takeovers, the most valuable thing is the employees, not the product.not sure how to deal with the uncertainty though.i\\'ll send you a dm.yeah, cross joins are to be avoided unless absolutely necessary.one advice is to have all selects and where clauses done before and then use the cross join on the resulting tables. if you have those conditions after the cross join, the cross join is done first and then the result is filtered.that shouldn’t matter. predicate pushdown should actually do the right thing regardless.i’ve actually seen worse performance trying to optimize predicates in ctes or subqueriesthough it’s been a while since i’ve used hivethank you so much, will definitely experiment with ithello insight community! looking for a great way to give back to stem education? check out letters to a pre-scientist, a pen-pal program connecting professional scientists with middle school students in high needs areas! this program has officially obtained fiscal sponsorship by a 501(c)(3) organization and is pursuing their first ever fundraiser. check it out and spread the word: this is useful - i’m in collecting data phase right now (because i have to be a scientist about this) but hearing different experiences just normalizes everything i’m feeling and seeing. check-ins with me from 10:30-12 tomorrow. reply to this thread if you want to chat - i can plan to see 9 peoplelol here we gooh no i know what\\'s gonna happen now:popcorn:ahahahahah no, i\\'m good.woooooo!  real career advice time!clean your desks everybody :)i\\'m down to chat!:oncoming_police_car: @-channel alert :oncoming_police_car::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice:ruuuutttttthhhhhhhh:bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji:2016a has still got the only emoji game in towni’ve been honing my skills for 18 monthspoor faye 🤣twitch had a  guy-fieri-themed @-channel incident a few months back. it was carnage for 16 hours straight.the frosted tips jokes alone must have lasted for weeks...my mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the chat. uploaded a file: ohmygod, i am so sorry. this happens every single time i want to @ channel our session from my phone. am mortified. [goes to corner and cries]nuggetbot pug bomb 10did not know pug bomb was a thing; i support the pug bombmixpanel, amplitude, and heap do a half-decent job of ingesting events data and letting you do basic analysis on top.  however, you always end up running into their limits and wishing you had more flexibility and direct access to your data.segment is good to collect events from your site and it can get them into redshift for you.  or you can use their webhook to ingest it yourself using kafka or kinesis.if you want data from multiple sources (events, apis, other databases) there are companies like treasure data, fivetran, astronaut, or alooma.  but they are just piping this data in and tossing it into your redshift cluster, you get to deal with creating a system to update derivative tables.(this is where the pitch starts :slightly_smiling_face: ).  i work for a start-up that’ll get all of your data from these disparate data sources, put it into s3 and redshift (soon spark, maybe hive), and make it easy to create derivative tables that are automatically updated, hook up jupyterhub or a sql dashboarding tool (metabase), or any tool that can hook up to redshift.  and! pipe it back out again to another database or apis.hopefully some of this is useful.  i got involved with this start-up because i was tired of doing all these things instead of actually working with the data.  i wanted to build this tool so that i would never have to deal with it again.nuggetbot animate me  oops. i had trouble searching \\'@ruthtoner \\'. try later.as luke was told to go to dagobah, so you shall be told to go to nuggetbot animate me this is finegood bot invitae bought good start genetics (boston-based) in august and we\\'re learning a lot about how to do things better next time.  - reduce uncertainty by making sure everyone has full-time offers (except the people that you need to cut right away)- benefits etc... should be clearly communicated possibly honored the old style through the year and then transitioned to the new company- people are needed to maintain existing systems and the company should be willing to pay to keep people around.  if you lose too many key people then the point of the acquisition could be meaningless.- going from a small company to a big company is hard.  people won\\'t have full insight into all the products and that\\'s just the reality because with a bigger company things are more complicated.  that\\'s hard for people to get used to.- speaking as someone from the other side, if you can see they are making an effort (and you believe in the mission of the company), i would try it out for 6 months.  it\\'s not too hard to find a new job. - you can also use this as a time to demonstrate value to the new company by stepping up to help with the integration and make yourself more valuable, if you believe in their mission.   hope that helpsdoes it need to be in person? can we do it by phone or skype? :laughing: :faye:thanks guys! cross join was running fine in athena, but it\\'s giving me error on hive. also i am trying to cross join a temporary table. still don\\'t know the exact reason., i\\'m flattered that you want to check in with me to see how i\\'m doing, but i usually reserve that for partners that i have more investment in (maybe after the third or fourth date?)i wish you the best though!aren\\'t there close to a thousand fellows on this channel? i think you might have your work cut out for you if you\\'re only doing 9 at a time.hey all, you using data science for social good? come share it at bloomberg\\'s data science for social good exchange.  abstracts due aug 1st! looks like \"the data science handbook\" is finally out. preface by , featuring alumni chris moody and diane wu:wow very coolit’s all about topo chico.hi all, just curious, what are the policies regarding database access for you? my company decided to hand us 90 days database credentials that are ugly and random just wondering if this is the norm for data teams. :slightly_smiling_face: thanks!/giphy topogigiowait… you only get access to your data for 90 days?if so, that’s kinda… broken.: does that just mean that each 90 days you get a new set of credentials?yeah :disappointed:every 90 days i have to request a new set of credsit’s self service which is not that bad except i have to update all my creds for python and other sql stuffstay positive kenny_kwanwant to know the standard for people like usthat sounds annoying, but not unworkable.:stuck_out_tongue:: i just started a free trial of the slack “standard plan” that runs to october 1st. so, enjoy the unlimited message history and other perks of standard while it lasts!: create a file called `.credentials` in your home directory and store all of your credentials in there as bash export commands, yaml/json-encoded python dict or whatever. add a line to your ipython startup script and to your .bash_profile that reads this file and loads its contents into memory as either a python dict (in ipython) or environment variables (in bash). now your scripts will always work as long as you have a valid `.credentials` file, on any computer you work with. thanks! i am already doing so i don’t have to copy them around, but was interested in knowing more about what the community faces. another point that i have thought is that probably in startups these things are more lax, whereas in my case where all our policies are set by the parent company, then things have policies and all that fun stuff...: that\\'s great! while we have it, could you also perhaps setup archiving at ? that way we can continue to have full history even after the trial runs outon second thoughts... that archive is public :confused:: yeah, i was just looking at that. do you know if they have a private version?: 90 days is better than 60 days :slightly_smiling_face:there’s this: good documentary, i watched it this weekend. made algorithms fun and accessible. upcoming data eng meetup at netflix on wednesday april 27 at 6pm: : is the meetup recorded?i do not believe it is: confirmed, it will not be recordedsad.. okay thanks a lotwow this has been there since 2009.  interesting algorithmwow what a break through with deep learning… i read about itinterestingi’ve built a couple slack reporting apps using r and python. they report some various metrics on a daily or weekly basis. the upfront time investment is minimal and it’s a very easy way to keep interested people at my company informed.`corgi_gif_generator %&gt;% slack_spammer `stay positive mpanciathe american statistician association weighs in on p-values and hypothesis testing misuse:they make 6 statements. summary:1. p-values can indicate how incompatible the data are with a specified statistical model.2. p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.3. scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.4. proper inference requires full reporting and transparency (e.g. p-values and related analyses should not be reported selectively).5. a p-value, or statistical significance, does not measure the size of an effect or the importance of a result.6. by itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.*other approaches*in view of the prevalent misuses of and misconceptions concerning p-values, some statisticians prefer to supplement or even replace p-values with other approaches. these include methods that emphasize estimation over testing, such as confidence, credibility, or prediction intervals; bayesian methods; alternative measures of evidence, such as likelihood ratios or bayes factors; and other approaches such as decision-theoretic modeling and false discovery rates. all these measures and approaches rely on further assumptions, but they may more directly address the size of an effect (and its associated uncertainty) or whether the hypothesis is correct.\"no single index should substitute for scientific reasoning.\"thank you for the summary,  !there was a reporting article on nature:we need to have a go board at the office, : i have an extra one! i\\'ll bring it to the office on friday and leave it there!alphago and lee sedol are playing game 2!hey, if any of you are attending the nips conference in montreal and want to hang out, let me know!current ai fellow here. anyone interviewed and / or interviews at netflix? i have a onsite next week (and it\\'s the last opportunity in my pipeline). looking to get as much help as i can get!  :pray:role: analytics engineer for original content promotion, topics: product metrics, a/b testing (stats). thanks in advance! (please dm me if you or you know someone else that can help). i\\'m very very new to them, so just getting started was throwing me. i found some helpful beginner videos though and am feeling much more confident about them now. just finished week 1 of this fellowship, so i\\'m sure i\\'ll be back here freaking out again soonnot now nuggetbotformer fellow (16a boston) tamas madl launched a website tonight for the debate that will be checking and plotting hillary clinton\\'s and donald trump\\'s heart rate in real time.  tamas’ insight project involved being able to detect an individuals heart rate using only a webcam, based on the subtle fluctuation of skin tone on an individual’s forehead. the url is awesome! i look forward to watching it with that in the background!^in week 4 i was excited for demos and terrified of not getting an interview while simultaneously being terrified of getting an interview (because i was incredibly intimidated by the interview process). btw, i got a great freakin job as a ds at samba tv in sf wholly because of insight.also, how is there not a coconut emoji?! i think the first demo (and the leadup to it) was the hardest just because it was such a shift from the project phase coming out of week 4 (and because i was still tinkering with my presentation right up to the wire, so i didn’t get to practice it much :stuck_out_tongue: )though it’s definitely a breath of relief when the first callback comes…joyceschan: schema registry is great. we use a lot of kafka+spark streaming internally and sr was the obvious choice. it\\'s a common belief at week 4, but stubborn reality always proves the belief to be wrong. you\\'ll see... by week 8-9, you\\'ll start to wonder why were you ever worried in the first place. also... past fellows seem to be determined too to prove the belief wrong! :wink: [that said, ny fellows: looking forward to your demos in a few weeks!! ]i think at this point, i was still tinkering with some visualizations on the webapp, but was ready and excited to show it off already. the most stressful part was waiting for interviews (not preparing, just waiting), especially since it\\'s such a contrast to the fast-paced project building stage. once the first interview comes, it\\'s a sigh of relief :relieved:besides, imagine where you were 4 weeks ago. probably can\\'t even fathom how far ago that was. imagine now 4 weeks from now. that\\'s what insight is all about: quick and sudden changes :laughing:you will get call backs after week 7. :blush:although, getting call backs and getting a job is a different issue. for some folks, a job comes within a few weeks to a few months. which is awesome!or you could be in my shoes and have to go essentially three months before landing something. not to be a negative nancy, but the road has a possibility to be long!to piggyback..it is a matter of fit in many cases and some will find that sooner than others but don’t let that discourage you.  i took a bit longer than most to land something but in the end i found a role that was well worth the wait! :slightly_smiling_face:for those where it took a bit longer, how did you spend your time? more work to refine the project? other learning on the side? or perhaps get some much-needed rest? any suggestions (just in case!)?also...focus on the things you can control and don\\'t worry about the ones you can\\'t. there\\'s lots of variables at play how companies choose their employees and a lot of it has nothing to do with you (like...office politics).  that being said, your interview performance and attitude (positivity) will play a huge role obviously- so make sure you are ready for interviews and nail them! ^ def keep up the practice :slightly_smiling_face: we did a round-robin setup of giving walkthroughs/lectures/whateveryouwanttocallit on topics we had particular knowledge of during interview prep, e.g. one fellow goes through more straight-cs algorithms for coding interviews, one does stats, etcthere’s always time for that :stuck_out_tongue:i think in week 4, this feeling kind of creeps in  because after the crazy pace of working on the project, ranking and demoing at companies reminds fellows the real reason all of this is happening. i agree that it is best to work on what you can control and approach things positively. i was having a lot of visiting and demoing companies because it is a genuinely exciting experience getting a behind-the-scenes look at all these companies that play a role in our daily lives. advice re: beyond the 7 weeks, i enjoyed preparing for subjects i was weak in because it was a real opportunity to acquisition new skills and do it together with others around me. it helps to do it with others. also callbacks and interviews are pretty unpredictable, in my case, most fellows in my session already had jobs by the time i got callbacks, but i ended up with multiple offers coming in, in one week and ended up at the top company in my wish list!study and practice practice practice..but def take time off, or even a weekend off to replenish yourself.  its a draining process!#teamcoconut :team-coconut:  with regards to callbacks. given the variance in the process, trying to figure out how it all works will just result in a headache (at least it did for me). i know it\\'s difficult since we\\'re trained to look for patterns (more so than other job seekers), but without knowing all the relevant parameters, you tend to over-fit based on your small sample size. :stuck_out_tongue:  remember, as long as this process takes, it\\'s still far shorter than what you would have to do in academia.: i was doing alot of studying for my weak areas, specifically cs and whiteboard programming. usually working on leetcode questions everyday and/or reading about data structures and algorithmsi also structured part of every other day or so to comb through job ads, and bringing them to the insight pdsmichelle_tat: great ideas, thank you! only because you mentioned it: do not work on refining your project. it’s really important to focus on simply presenting your results (however crappy they might be…mine were) as best as you can and move on to practicing interview-relevant skills. everyone thinks their project can be a little bit better with another 1-2 days of work (or that their project is horrible and needs another month) but hiring managers rarely care about / notice the marginal improvements you’ll be able to make now.hi everyone, we’re looking to collect some feedback from a handful of practicing data scientists.  this would involve you taking a 20 minute survey, and the results will be used to directly inform the roadmap for our product.  we’re also offering a $25 amazon gift card to everyone who participates.  if you’re interested please dm me :slightly_smiling_face:  thanks!^lack of coconut emoji fixed!thanks everyone for your interest, our survey slots are now full :slightly_smiling_face:  appreciate it!hey everyone! one of the ai fellows just posted a blog post about his work doing audio super-resolution (recovering high frequency audio signal from just the low frequency audio signal). could you please heart on medium? interesting, there was a similar attempt at google brain but in video:the coconut one smells like sunscreen :laughing:what my grad school education consisted of:  applied math, optimization and estimation theory, orbital mechanics and control systems.  what it should have been:  how to make pretty charts in powerpoint.  hey!  hope all is well! i was wondering if anyone works at nordstrom or has any experience doing a data scientist screen with them. i\\'d love to hear about your experience...thanks much! :slightly_smiling_face:\"the result is called a tensor processing unit (tpu), a custom asic we built specifically for machine learning — and tailored for tensorflow. we’ve been running tpus inside our data centers for more than a year, and have found them to deliver *an order of magnitude better-optimized performance per watt for machine learning*. this is roughly equivalent to fast-forwarding technology about seven years into the future (three generations of moore’s law).\"i wonder what physical considerations go into optimizing for that sort of thinghow much of a step is it over designing gpus to be optimized for linear algebra (low-rank tensors)they don’t make it clear in the blog post, but i believe these do 8-bit integer processing and probably only get the 10x performance / watt for inference tasks. for learning tasks gpus probably are still the way to go. there is some discussion of this on hacker news.caveat: i know nothing about gpu designi know that graphics cards were also pretty powerful at bitcoin mining and were most commonly used by most miners. however, asics have been around now for years that easily break 1000x the performance of the fastest gpus ($1300 asic vs. a tesla  gpu). as long as the task is specialized enough i could easily see how an asic could outperform gpus and be cheaper in the long run. it may require optimizing the code to run on it, but i don\\'t see why people would see it as being hard to believe.here\\'s a breakdown of the speeds of asic/fpga miners, you can see graphic card speeds at the top by clicking on \"non-specialized hardware comparison\". almost by definition, an asic implementation can always be faster than fpga or gpu or an any pre-structured architecture. the problem is that the fixed costs of asic are huge, and redesigns are equally costly.stay positive alanasic -&gt; application specific integrated circuitasic -&gt; fpga -&gt; general purpose processor  gpu / cpuin laymans terms, a dedicated chip for your specific use casethe costs of building your chip include many phases of application definition, chip design, lithography/tape-out/fabwhereas with an fpga or gpu/cpu, you are using something that already exists and writing an application (e.g. code) to work within its architectureyes (though you can’t assume fpga is always better than gpu or cpu. they are more flexible of course, but for what they are designed for a gpu or cpu is always going to be faster.)with an asic, you are allocating, by design, all of the registers, gates, etc to implement the logic for your applicationwith an fpga and your hdl + libraries, you are allocating a finite number of reprogrammingable registers+gates to implement the logic for your applicationwith a gpu/cpu, all of those elements are already allocated into a the processor design and you are writing instructions to be executedyou have execution units in a gpu/cpuin real world practice, the ability to develop good software/applications is affected by things such as the developer/development team’s ability to work with the tooling/language, speed to execute and iterate, etcwhile an asic or fpga offer theoretically better performance, the ability to materialize that benefit in practice tends to not happen as oftendoes anyone have experience on  how to build a sci-kit classification model that mix numerical/categorical features and text related features.(that generated with tf-idf)?brad: uh oh! does this mean you\\'re leaving us! :disappointed:yeah, i’m moving back to the 6ix and working remotely from there permutation tests make no assumptions about the underlying distributions/data types, which is their beauty. (also, the m-w is actually a randomization test, but it converts data to ranks first).it looks like if there isn\\'t random assignment it requires symmetric distributions. i\\'m still trying to determine if their randomization technique is sufficient. i\\'ll have to look into it more.well best of luck! when do you run off?maybe more relevant in a few weeks for the new fellows :wink:these are actually good / fun questions, i wish that’s what my experience was when interviewing at most places haha, instead it was mostly dice / coin problems and reversing python stringsyeah, a lot of silly code questions.  still worth knowing though, i reckondoes anyone here use crisp-dm or similar methodology structure?  interested in getting some thoughts about that from people using it.thanks! i fly out on tuesdayhey guys, had a question. i need a python program to call a function \"outdata\" from a different python program called \"process_data\" (\"outdata is within \"process_data\").if i say \"from process_data import outdata\" everything works fine. but if i *change* the contents of \"outdata\", python does not recognize those changes even if i import again. any ideas?you are sure the path you are importing `process_data` from is the same as the path where you are editing its contents?also what changes are you making? on the fly?if it’s in a jupyter notebook, you have to explicitly re-load the package, re-running the cell with the `import` statement is not enough^ use `reload()`yeah reload did the trick!hey all, i am organizing a hackathon to download government data before it disappears :scream_cat:   eventbrite link to be added to the event, but feel free to share in the meantime! for nyc folks, there’s an event on feb 3, but it’s all full i thinkanyone looking for a mattress + bed frame? practically brand new, $100 each or best offer, i\\'m just looking to get rid of it!  jackieb: it’s coordinated with the other efforts out there?yep!all run by a lab out of upennafaik no other bay area events yetok, so i need to writeup a quick example of the various ways to use the ipython magic `autoreload` but this gives a good starting point:  regarding \\'s point -- in python 3 (if you\\'re using it) `reload()` is no longer a core function, it must be imported explicitly before use: `from importlib import reload`anyone in east bay, my dog got let loose by maintenance. 17 lbs blond freshly groomed female lhasa apso yorkie mix with purple harness. named circe. piedmont area kingston ave. please call number on tag.if anyone sees her pinned a message to this channel.found her!glad to hear. that was fast. hey guys, does anyone have experience using bokeh for apps/dashboards in python? they have some really nice example templates, but if i try to alter them at all (i.e., try to plot a histogram in the app rather than a simple line) the code breaks. is there a good tutorial on customizing these apps other than just the bokeh site itself?took 3 hours, but glad it ended wellone of my friends team at uber is hiring for a backend software engineer role. please pm me if interested and i can put you in touch with the right person. here goes the description \"if the world of maps interest you, there\\'s a position open in uber maps team for a backend engineer role. the position is similar to this one, and focuses on maps data. #softwareengineer - #maps #infrastructure, #data, and #services... \": it might be that the page (or part of it, say the map piece) is being cached by either the browser or the server. you should look into disabling caching for both. there are some html tags that do this (it’s just a suggestion) for the browser and there are settings that are server specific. i’m not familiar with gunicorn settings, but try to find the caching and disable it.making the map name different every time worked, so i am positive it is being cached somewhere (but i am positive it is not the browser). i will look into aws caching...i have found that some files are turning up in /var/tmp so that subtly suggests to me that it is probably supervisor/gunicorn (but i am not certain)linkedin open-sources a url detector and parser using finite-state machines. blog post by tj from the anti-abuse team: stay positive carlosfahamanti-abuse nuggetbotstay positive carlosfahamloldude someone needs to rework the sentiment analysis nuggetbot usesseriously!nuggetbot check on mscherrermscherrer has a happiness average of 0.3925233644859813i’m so super happy, life is so wonderful like rainbows and unicorns. amazing!!!!nuggetbot check on mscherrermscherrer has a happiness average of 0.5370370370370371: more decimal places pleasedoes anyone know of a clever way to read files out of an s3 path with wildcards like you can do on a local filesystem with glob? i poked around on stack overflow and came up with nothing. i’m doing a bunch of string manipulation in python and kind of getting there, but i can’t shake the sensation that i’m doing this the hard way.the  —include —exclude flags?yeah, that’s kind of the best way to do it. i keep foolishly thinking of s3 like it’s a file system...i’ve gotten spoiled by how easy this is with glob. sorry to bother everybody...anyone use the databrick’s spark-xml parser and elasticsearch run into problems with accessing an s3 bucket? we have a fellow who successfully uses the spark-xml parser but the minute he tries to also import elasticsearch libraries, he gets runtime errors complaining about s3. i know there are sometimes problems with accessing s3 with spark and hadoop, and we\\'ve been able to fix that (setting the classpath to add the aws jars) but the minute he tries to import elasticsearch, he gets the error. any suggestions?stay positive reporterhoacan anyone recommend some good java tutorials?umm, i normally use this:i hope it\\'s helpfulany nyu alumni that want to go to the nyu alumni day on oct 25th?nyu alumni day? what\\'s that?: just some fun activitieshey all - i\\'m a current health data sciences fellow in sf; looking to get less-limited access to the locu api for some restaurant menus. if anyone has an \"in\" with a contact at locu, any chance you\\'d be willing to put me in touch? thanks! -andy hey alumni rockstars, did anyone use tensorflow in docker and do you know how to grab the output so that it can be used/displayed in a web app?you can exchange data between flask and a web front-end using jquery/ajaxis that what you\\'re looking for? you can mount a data volume and access data from outside the containter, there are a bunch of examples explained here:thank you guys for the links!hi everyone! we are looking for a spark expert with hands-on experience to consult on a project. we expect around 10-15 hours. paid. please dm me if interested or know anyone who might be interested. thanks!dear sage data scientists, how do i extract a representative data subset (~200k rows)  from a large sql table (20m to 90m rows)?  what would be the best way to proceed if i need a representative data subset from  four or five related sql tables, each in excess of 20m rows?`select * from table order by rand() limit 200000` ?rand() ! brilliant. thankstake care if you have some rare but important things in there :slightly_smiling_face:mdhardy: some db use random()i think this solution has somewhat poor time complexity (o(n log n)), because of the sorting. there’s some discussion in this thread i believe `select * from table where rand() &lt;= .3` (replace 0.3 with your actual percentage) should be better if performance is a concern.(although this solution has the disadvantage of not returning exactly the desired number of rows.)friend of insight karl heibron on 23andme\\'s parkinson\\'s team is launching a microbiomics meetup in sf:it will likely be rather data heavy! definitely check it out on february 27th if you can!i will definitely be theremy girlfriend is looking for a job, ideally in boston (where i am!). she\\'s an nlp engineer with three years of experience in search, a master\\'s in computer science, and a phd in linguistics. she also speaks french and german fluently. much appreciated if you have any leads, e.g. friends working on search teams.not too get too political again, but all fellows currently on opt should contact their employers to explore alternative visa options. there is talk that opt will be either scrapped or modified, so extensions might not be possible by the time you need one. see: , paragraph: \"limiting legal immigration\"varunrajan:  thanks! rand() &lt;= threshold-percentage worked nicely.hey guys, had a question about jupyter notebooks. i looks like some of my figures aren\\'t rendering in \"dashboard\" mode after a certain point. anyone else run into this before? i have included the %matplotlib inline. could you give more details?in my notebook i\\'ve written a bunch of code snippets that generate figures in different places. if in the notebook i click on \"view\", then \"dashboard preview\", they all show up fine up to a certain point. then for some reason subsequent figures don\\'t render past that point.okay, now it does show up somehow. i have no idea what happened: wanted to add one more thing. i had in fact tried the idea of \"rdd.foreachrdd(lambda rdd: rdd.foreachpartition(processrdd))\". it appears as though that should take the rdds from a partition and apply processrdd() to each of them without causing any shuffle. but it does cause approximately the same amount of shuffle read and shuffle writes (you can see this in the \"executor in the :4040 page of the spark master) as an alternate approach that was doing the processing after the reduce phase. what it means is that the foreachrdd() happens *after* the reduce phase. we want it to executed *before* the reduce or rather in lieu of it. when i try rdd.mappartitions() instead, it thinks there\\'s no action  on the stream stream at all and ignores the input completely with the result that the incoming rate of this stream drop to zero !does anyone have any tips about getting a unicode data frame in pandas (collected through webscraping) into mysql? i have a function that uses sqlalchemy and pymysql which works, but not for this dataframe in particular. i\\'ve tried a number of data type conversions, but can\\'t seem to get anything to work. looking for any kind of work around, i only need to get one df with 10 or so columns into mysql. thanks!:  this is what my pandas to mysql code looks like (using mysqldb):def pandas_to_sql(df, database, table):    import mysqldb    db = mysqldb.connect(host = \\'localhost\\',                         user = \\'root\\',                         passwd = \"\")         db.query(\\'create database if not exists \\' + database + \\';\\')    db.query(\\'use \\' + database + \\';\\')                                                                                                                                                                                        df.to_sql(name = table,                    con = db,                    flavor = \\'mysql\\',                    if_exists = \\'replace\\')                                                                                                                                                         db.close()        return \\'sent \\' + table + \\' to mysql.\\': did you look at my post  ? i show how to use utf8 encoding to get unicode into mysql. in some cases you need to set the mysql character set to utf8mb4, not just utf8 (see e.g. ) mentioned a file: .thanks so much   this looks great! i\\'ve been struggling with unicode to mysql for way too long :simple_smile:what\\'s reddit\\'s button all about??: the button!  :smile:our ceo (who’s non-technical but really smart) asked for resources to learn more about data science so she could be more informed when in discussions with the ds team.  does anyone have suggestions for books, blogposts, or talks?this has been making the rounds recently: some of the relationship managers at insight gave positive reviews of data science for dummieshi team, i am building a classification model for imbalanced data (1% positive prevalence). we have a lot of data so i am subsampling the negative cases to match the number of positive cases. i plotted the calibration probability plots following this link () and the subsampled train &amp; validation curves look good. however, the test data (not subsampled) is essentially just a flat line near zero (slightly upward sloping). does it make sense to plot the calibration curve for imbalanced test data? has anyone used calibratedclassifiercv with imbalanced data? the use case for this model is to provide a ranked list with highest probability at the top. unfortunately due to the imbalance, the number of false positives greatly exceeds the number of true positives at the top of this list. any recommendations would be helpful. i can provide more details if needed.if you have high false positives, i would recommend using an ensemble model approach. train several different models on different undersampled subsets of the data, and then take a majority vote of all models. just make sure your test set is not in the training set for any of the models.you might still be able to do a calibration curve if you have a mean probability cutoff instead of a voting cutoff for the models. one huuuge caveat - if you\\'re using a boosted classifier this will likely push all your probabilities to 0 or 1, and i would not recommend using a calibration curve, or using a probability cutoff instead of a voting cutoff for your ensemble model.thanks  and !thanks. i forgot to mention my \\'client\\' wants the top 5 contributing features to provide insight into interventions. i\\'m currently using xgboost and grabbing the top 5 features when pred_contribs =true. i\\'m unsure of what pred_contribs = true does - but you might want to be a bit wary if you have correlated features. in feature importances for a scikit tree model, that will affect their rankings. we are looking for ds and de interns at tempalert (also in boston).thanks for getting back to me everyone! very helpful.hey guys, a technical question: is there a way/algorithm to partition a continuos variable into a series of intervals, while maximizing the predictive power of this new variable? let me make an example: you have a time variable that you want to discretize into, say, 10 intervals. how do you choose these intervals so that most of the information of that variable is retained?haven\\'t tried this before, but off the cuff, can you just do a series of binary splits using a similar information metric as in rf?would it be relative to one dependent measure?you might be looking for something like bayesian blocks: sounds very much like what we did in physics with continuous physical variables to try to maximize signal / background ratios across several bins of a given variablewe did it similar to what   recommends — automated a loop over several possible binning configurations and chose the best one. perhaps there’s a much smarter way, thoughthanks guys, this is helpful. ideally, i would like to have an algorithm choose the best splits, as  suggests, rather than testing a bunch of binning configurations. will look into the bayesian block stuff, this is new to me. thanks again!you can probably use sklearn’s decisiontreeregressor (or classifier equivalent) to find the first split, then search the two resulting splits for the next best split, etc. but beyond the first split, i’m not sure if you can somehow pull the information metric out of it so you can compare the next steps.  but it’s probably not too tough to figure out how to get that.got it. will give this a try and maybe post an update in a few days...i’m very intrigued by the bayesian blocks thing, very cool way to do thatthe bayesian blocks is pretty awesome, i think this is the current up-to-date implementation: originally here ya that looks righti was trying to find an implementation that wasn\\'t buried in `astropy` but couldn\\'t find oneyeah the astroml package is linked in the article you posted ``deep learning ftw uploaded a file: that\\'s hilariously terrifyingthis is the stuff of my dreams. and nightmares.wowjobsalaryreminds be of meowpurrdylast day for abstracts!has anyone dealt with issues with decoded_latlngs = google.maps.geometry.encoding.decodepath(latlngs)?what sort of issues are you encountering,  ?are job postings on-topic here?hi, ii\\'m current fellow, and i need twitter graph data (who follow who ) outside of snap. any help is greatly appreciated. thanksdoes anybody try using scikit-learn model across different  http requests on flask? i want to train a model based on the first request and predict something based on the second requst. new donald trump programming language seeks to make programming great againicymi, a recent de.ny fellow just posted a nice post on cassandra for “newbies”: dear respected elders, do you have any suggestions for a new fellow like me on how to best pitch an api key request to a company?  if you were at an api-key-granting company, what sort of request would you get excited to grant vs. what would be red flags in a request?  i humbly submit this question to the oraclenot sure, but likely the words \"academic purpose\" could be helpful yeah? right on.  this company states “we share our api with a limited set of corporate, education, and research partners”they’re more into academic projects than business ones? i guess i just don’t understand why they’d turn somebody downlike, they’re worried i’ll take their data and use it better than them and make all the money?katie suggested name-dropping insight, but that kinda seems contrary to your suggestionour company would view the insight connection as positivewe consider our data to be both sensitive (it has pii) and monetizable. so we would be looking for partners whose use wouldn\\'t compete with us, and who wouldn\\'t disclose our data (especially to the general public) irresonsibly.nice, thanks! i’ll try to angle it with that in mind!hi guys, i am trying to get tripadvisor api key but there is a limitation: \"tripadvisor grants a limited number of api keys to official tourism organizations and select other websites. it does not grant access for purposes of data analysis, research, testing, or similar uses.\"  any advice?is there a specific reason why you are trying to access tripadvisor data? is there another data source that\\'s similar you could use?yeah, i guess i can use wikitravel and expedia data instead but tripadvisor also has these reviews that could be usefulwould scraping the data be a viable alternative?hey cats - anyone at google? just put in a request for google flu data - hoping somehow that could be approved or expedited. also hoping to know the geographic/time \"resolution\" of it (zipcode/weekly/etc.) would greatly appreciate any help.hi fellows, has anybody seen a dataset that has diagnosis or medical records that have valid date and geographic information? i\\'m thinking specifically for respiratory issues and zipcode or hospital level spatial precision, so i can correlate with metereological information. (the mimic-iii database \\'dedates\\' the time information and adds a random offset to the date so it falls between 2100 and 2300, making it unsuitable for this purpose.)  this data set is likely not for public access because of the risk of identifying individuals.  thank you!what kind of reviews are you looking for? yelp has a handy data api. also, google might as well.welcome new fellows! anyone who has even the slightest chance of using the mimic-iii database should request access right now as it takes a couple days to get access to the data.you could probably try hospitalization data from hcup or the children\\'s database at kiid and associate the hospital zip code to the patients.however, depending on the condition you choose, it might be very hard to get a breakdown by zip code. much more likely to get it by county, but is very dependent on each state\\'s department of public health.try to focus on a single state and, if given enough time, expand to the us and eventually the world!i actually did something similar studying pediatric asthma.hello fellows, i\\'m in the first week of insight. still brainstorming the project idea. i would like to propose a project doing the face classification, i.e., like different disney character, or different famous people, etc. any advice is welcome. thank you.this blog is a great resource for image classification that doesn\\'t require nn using histogram of gradient features,  hello! hope you\\'re hanging in there during your first week. i guess my question to you is, what is the use case for doing face classification? i imagine this has been done by several fellows before. thinking about your use case and what kind of practical and/or business value of your idea should be your mindset when thinking about your question.there is a huge celebrity face data set, celeba is this for the ai program or for the data science program? if it\\'s ds, think about how to productizeeither way, actually disney!!! :slightly_smiling_face:hehe, i don’t think i’ve seen a disney-related project ever before…thank all for your comments. very helpful. p.s. this is for ds project.the local bike community or cyclist union should be the best bet for that information. i\\'m only aware of the ones in boston though.new blog post by  featuring some alumnae giving advice to their younger selves. check it out: hey friends - if anyone\\'s interested i\\'m giving a talk on using data science to disseminate off-grid solar in sub-saharan africa in sf on april 4: people who work/live in the south bay, if you\\'re interested in and enjoy art and work in technology, the palo alto arts center foundation (paacf) is starting an initiative of art and tech ambassadors. if you (or someone you know or work with) is interested, sign up here: i\\'m on the paacf board of directors so feel free to ping me directly if you have questions. :point_up:encountered this today, missy elliot came to the rescue. nuggetbot animate me missy elliot flip it and reverse itberkeley grade average by department: also, cal data: hey, domino is doing another data popup in nyc. sv folks know angela, who comes by to advise fellows often. she posted a blog about the popup domino did in sv earlier this year: .the nyc data popup is on june 14, . if you want to attend, here’s a 20%  off discount code: datapopupnycinsight alum, , will also be presenting!new seattle fellows?? :grin:woot! welcome seattle :slightly_smiling_face:dear seattle fellows,don\\'t worry if you start feeling like this very soon. it\\'s normal. :laughing: seattle fellows, plus the first cohort of ai fellows in new york!:wave:coolthis is awesome. what\\'s the location of insight seattle by the way?seattledamn, edited too fast!right by pike place market, i\\'ve been toldyep, right by pike place - 1st and lenora. come visit? :slightly_smiling_face:are there snacks?there are! :la_croix: :cookie: :coffee:i live close by (3rd and battery) will do yay! i want to come visit too!hey all, random expertise request.is there anyone here familiar with tv programming/advertising in argentina?please dm me. thank you! pinned a message to this channel.has anyone here done significance testing with comparing percentiles between two different distributions ?does anyone have any recommendations for an egpu setup? if you\\'re mac inclinedapple is also has one has anyone generated google static maps requests via python’s ‘requests’ library?i think its mutating the parameters passed in via the ‘params’ keywordi.e. r = requests.get(&lt;api_url&gt;,params=some_python_dict)tried unicode encoding, no dice, =(oh, nevermind, stupid error on my partplease ignore the above, :sadkeanu:yeah, it is pretty handyi really enjoyed nate silver\\'s book. he does a good job of showing that some fields have become excellent at updating models over the past 20 years (e.g. weather prediction) while others have not improved but recognize that (e.g. earthquake prediction) while other fields still have lots of players engaged in puffery (e.g. politics/elections)another book i recommend is \"the visual display of quantitative information\"has anyone here filed for a tn visa? i had a few questions? what category do i file under ? i was planning to file under physicist.fyi: my phd is in physicsawesome, thanks ! i\\'m in chapter three right now and really enjoying it!  and thanks for the tip on the tufte, i knew it was famous but not sure if it was actually worth a read, i\\'ll put it on the list!depends on your job.. many data scientists are under mathematiciani came in under mathematicianah thanks guys ! if you have a phd, you should file under the activity you are performing. uscis is very particular about this. make sure that your employer\\'s letter with a job description matches as closely as possible nafta\\'s definition for that category. start by looking at -&gt; to \\'s point, mathematician (or statistician, which falls under that umbrella) is a commonly aligned description.  (also, you may want them to file that in your behalf to ensure that it gets approved easily, as of this year, uscis has started to get really picky about tns)hi everyone,another boston 18a fellow here like pawan. what if my phd in physics is still in progress? i have an undergraduate degree in engineering.thanks and regards,abdullahthanks marco !+1 to applying ahead of any attempted border crossing. they still asked a lot of questions (which they continue to do every time i re-enter the us), but it was a big help.out of curiosity, has anyone ever heard of “equal first-author swap” on a cv/resume list of publications? i just saw this for the first time and after some googling it appears that it’s a thing people sometimes do when the first 2 authors of a paper “did equal work” but the second author is “allowed” to list themselves first on their cv/resume, even though they are listed second in the actual reference.most people online seem to think this is super sketchy and i tend to agree… thoughts?why do you think it’s sketchy?  is this not a situation where the publication supports co-first author?many publications have status for that, where it does in fact mean they did equal work, without quotes :smile:it does seem odd to me to do something other than publication order… but then i come from a field that puts little emphasis on author order for papersi had this come up once in a very specific instance... during grad school, another graduate student and i ended up doing nearly identical experiments, completely independently, and unintentionally. when it came time to publish, we talked about the idea of publishing our results in a joint paper and explored this as an approach to having equal credit as first author.in the end, we decided it was simpler (and more impactful) to publish separately, so we ended up publishing in the same journal at the same time.i could see it being a reasonable approach sometimes, in fields where author order is important.(i\\'m coming from ecology and evolutionary biology)i come from a field that puts emphasis on the alphabetical order of authors last names. first author swaps are not allowed. at least in astro, if you list a paper in your cv/resume and you’re in the top ~3 listed authors, then it’s assumed you did a lot of work on it. if you’re listed first, you’re assumed to have done the plurality of the work. if you’re in the alphabetical section, then you probably didn’t do much. the sketchy view comes from the idea that what is stopping me from saying i “split” first author credit every time i’m second author or a trio of authors when i”m third?i think it’s totally fine in cases wehre the publication supports and validates co-first authorship.  at that point i might as well be questioning their finding or p-values.  but if they do it on their own it’s sort of weirdif you want to explain how much work you did when you’re “only” second author, then add a bullet under it? i would just read it and assume, ‘cool, second author, they did a decently large amount of work’yesterday a paper of mine got accepted in a journal where i share first authorship with another postdoc.  i fully agree that we put in equal amount of work and were equally invested from the beginning.i’ve never seen a publication in astro support this in any waythink of it as pair programming.there’s a bunch of quality ones for biology stuffdo commits of pair programmed code have both author’s names on it?i’ve done second author work that can range from a week to like 1/3rd of the work^so have i!if i am co first on something (i’m not) it woudl be because its’ very close to 1/2 and due to much fewer papers in that field it matters(monkey work stuff)no, they would only have one.i know in deep learning/ml papers they often have little bullets saying “authors did equal work” for the first `n&lt;=3` authors. i suspect that is what some people are saying when they say “the paper supports it”but coming from a field that doesn’t have author ordering… y’all weird! :wink:i bet its a function of “time / resources per paper” with the more required, the more likely for co-first :smile:`academic politics are so vicious precisely because the stakes are so small.`does anyone know of any good vendors/services/workflows that keep the docker image together with the dockerfile that created it? have you looked into kubernetes?have you looked at jfrog artifactory? others that come to mind include nexus sonatype and aws\\'s container registry (ecr). i think you can use ecr without actually using ecs.i’m not sure if i follow the question exactly, but if the main concern is linking a docker image to the code that originally generated it (perhaps long ago), then i’d: store the docker images in any container registry (docker, jfrog, ecr, etc.), use git to track changes in the dockerfile, and then tag each created image with the hash of the latest commit in the git repo containing the dockerfile. that way, if you have a random image and want to find the code that generated, you just check out the repo at that point in time. yeah we use kubernetes internally but that’s more container orchestration. i’m thinking about a more dumb service than that , yeah we currently use a combination of circle ci, github, and  for this. i saw that  would let you build images on their service from dockerfiles but was disappointed that you couldn’t link the image id with the dockerfile so that if you changed the tag, everything would follow along. i would have thought there’d be something out there that did that elegantly.  i’ve used ecr but not ecs with ecr. my impression of most aws services is that they do a low level thing really well but don’t really take features beyond that. i’ll check it out though  they have a link to an 8h-long youtube video of the whole thing(which is actually a playlist of many 30min talks)anyone else hanging out at the scipy 2015 in austin?does any one have experience with parallelization for assessing model performance on ~100k different data sets? spark seems like a good option, but it seems like it would be limited in terms of library selection. (i\\'m using python)help! there seem to be so many possible options and approaches while it\\'s not memory efficient, you could run them via multiprocessing if you have a decent machine that can handle it. how big are the datasets? can you hold nvirtualcores x dataset_filesize in memory? if so, the model is static and you\\'re only applying it to different data sets, you could pass model and the filenames of the dataset and let it go. if the data live in a db, it\\'s even less io intenstive. do you need to run it on a cluster?does anyone have experience with creating technical training materials for software use at a startup? my company would be interested in doing a lunch &amp; learn :)i’ve done some, what technical level are you thinking?more specifically, i’ve given the “lunch and learn” talk about some tech i’m using (but that’s kinda one-directional), as well as created some training materials for our analysts you could try`luigi` as well, i’ve used it for task graph specification.  sounds like you basically just need to queue up the same model run for a bunch of different self-contained datasets?  like said most approaches are going to be memory-limited, but at least luigi makes it pretty friendly to run arbitrary python code for the task nodeswhat about dask?i faced a similar problem during my phd research. i concur with lonnie, dask is an excellent way to approach this problem. it has a really easy syntax for constructing arbitrary directed acyclic graphs using dask.delayed, interfaces well with scikit-learn, has distributed dataframes like spark,  and scales well up to about 1000 cores on a cluster.can some insight fellow make a chrome extension that reads your jupyter notebook and automatically suggests pages on stackoverflow that are relevant?brilliant!!! but... how do you validate??? :stuck_out_tongue_closed_eyes:copy-paste til it works:wink: for building it, i\\'d say you have a feedback loop sort of like how search engines determine what links are effectivewould be hard to build the model / solve the cold start problem maybe*consulting solicitation - image classification guru*i’m looking for an image classification guru to consult on a short-term project, 2 - 4 weeks, starting asap (like, next week). it’s a very interesting project for a fortune 500 company, hourly compensation at competitive market rate. pm me for details!surely google can help solve the cold start problem?assuming they’re logging everything you type into chrome...?anyone around who can help me think through a problem with time-series data? :point_up:does anyone here have experience calculating regression prediction confidence intervals with sklearn models? although this doesn’t answer your question, i believe `statsmodels` has prediction intervals, so if you’re doing something simple, that might be a good choice. happy to help  as well if you need ithi all! does anyone have experience with javascript? i have none. what i\\'m trying to do is:1) have a drop down menu populated with states.2) once a state is chosen, have a second menu populated with counties.3) once the county is chosen, query a database i have on the (county, state) and display the other values in that particular row.i would like to do all of this without refreshing the page since there is a relatively expensive map that takes a little time to render. feel free to dm me if you think you can help (code snippets, good tutorial, examples, etc...)!look into jquery, that should be able to help youhi guys! any podcast suggestions for big-data? i have about an hour commute per day and would like to use it listening to something more meaningful :simple_smile:partially derivative is fun and good about giving you and update on the recent news in data sciencewhat’s the point from fivethirtyeight is also good about current topicsi’ve also been listening to linear digressions and talking machines which seem to focus more on teaching specific techniques/algorithms/toolsdang, beat me to it on those.everything else i tried was just… not very good. so that is a list of 4 (ranked in order of how i’ve enjoyed them) culled from a list of like 10there’s also select episodes of: oop, you got that one too! nice job, paul.brennerhaha! i’ll dance on your grave!or… maybe that was a bit too intense.haha, yesnuggetbot: animate me that escalated quicklynot the one i had in mindsorry, i’ll back that up… but i’m laughing at least looks good but i haven’t listened yet^ oreilly data show/giphy anchorman that escalated quicklyi just couldn’t get into data skeptic. the subject matter is great thoughthanks guys! i’ll give those a try...*hi all* - i’d like to connect with ds/de alum who are working in *data-driven finance* or *commercial insurance*, and who’d be willing to chat a bit about sourcing data. pm me if you can spare a few minutes to talk at some point. thanks!for the ml people in sv : :  try cca since you have an output variable.   this is better than pca in this case. while pca will give you components which explain variance in x, what you want is the components of x which explain variance in y.    pca is not appropriate for this and may bring in features that have unnecessary noise, or don\\'t correlate with the output.hello! geopandas question! i\\'m having a weird issues where i now can not import geopandas in ipython. i\\'m getting errors like failed `cdll(/usr/local/lib/libgeos_c.dylib)` and warning:shapely.geos:failed `cdll(/usr/local/lib/libgeos_c.dylib)`failed `cdll(/library/frameworks/geos.framework/versions/current/geos)` and warning:shapely.geos:failed `cdll(/library/frameworks/geos.framework/versions/current/geos)`failed `cdll(/opt/local/lib/libgeos_c.dylib)`  i\\'m really bad with directories and knowing if i am missing folders or if my libraries are in the wrong place. any ideas?stay positive stephgline: we already had a channel for scala! it is called so fun: this algorithm unpacks the rappers whose rhymes influenced the musical hamilton .  plus some details on how they built it here: : nice! we\\'ll migrate hi people. does anyone have any good book recommendations about project planning and management? :partyparrot: good talk from last week’s spark summit regarding data culture you should also send this info directly over to    or anyone else on the data engineering team at insight! it\\'s the best way to quickly find which current fellows might be interested in joining your team! :raised_hands:san diego is trying to poach tech talent from the bay area.event on feb 1: (the event’s in mountain view) thanks！oh dang, i\\'ve been going around saying thistimeseries finding trends example: computing rolling averages for timeseries: another good tutorial, the resample function looks incredibly powerful: a chance for free ebooks:has anyone heard of predictive analytics innovation summit? is it worth attending or being a speaker for? nicolestrang: storing the changes of the table across time should do the trick (similar to a version control like git). however, it would decrease read time, as you\\'d have to get the base table and then the cumulative changes. nonetheless, depending on how little it changes with time, it would save plenty of space. take a look at `log triggers` for more information.dear data science sages: i\\'m interviewing w/ a few companies and have seen a couple of startups using a ruby on rails stack. what would be a good way to integrate python into such a set-up? - micro-services.  most companies are polygots (multiple languages / stacks etc) and let teams choose the one that they are most productive in.  so the way i have worked in such environment is make the data science model as api and let ruby on rails app just communicate via the api and get a json response just like any other prediction api services out there.anyone have experience passing google maps autocompleted addresses from javascript to python?sampathweb: - thanks ramesh! i was thinking micoservice applications but wasn\\'t sure; it\\'s great garnering perspective from a professional - thanks! i really appreciate it.sure. when/how?classic papers determined by googleif you’re ever alone on a friday night wondering what to doanyone have any tips or favorite packages for creating web dashboards? i\\'m currently looking at bokeh, data will be coming in from mongodb and postgresql. videos, tutorials, etc. are appreciated.shiny is pretty quick for adhoc stuffrshiny depending on what you want to do with itsuperset is pretty awesome. the built in admin and roles is hard to pass up especially if you\\'re deploying for your own company. user interface was a bit strange but gets super easy once you\\'re used to ittinkering with superset now, debating that vs metabasenb it doesn’t *yet* work with nosql databases. i’ve had issues with mongo. there is active contribution for elastic search (which i’ve never used), thoughinsight ai fellow  shares her project on separating overlapping chromosomes with deep learning: who\\'s on  !!???so i\\'ve found that, when preparing for contests where you are hopelessly out matched, it is best to talk a lot of trash. so: come get some you junior university trees! go bears! :wink: :bear: :football: well that worked out poorly. are there mercy rules in college football? :wink:so cold in nyc... must be nice to be in the valley...probably warmer than nyc, but colder than i\\'m used to!if it’s any consolation, boston is colder than nyc now. but i’ll take a bit of cold over measles outbreaks, dysfunctional transit, and absurd housing policy. (signed, a former bay area resident of 6+ years)&lt;— smug former boston resident in sv here….i saw it was in the high 40s this morning... a real weenie shrinker (la story reference)this is a great series of in browser demos of deep learning with javascript: pretty fun to play withwhoops, sorry the above link was to a specific demo. here is the full list: the paper has a bunch more, and is open: : i thought that plot was really important tooevery difference that they list before that last one they report it like: (62.7%) (χ2 (df = 2, n = 2, 473, 190) = 492, p &lt; .001).or 74.6% [74.56%, 74.67%] 95% confidence intervalbut they don\\'t list the values or the confidence interval for the gendered outsider difference (it just shows up in the plot). so is this not a statistically significant difference?haven’t read the paper, but reading the bbc pr\"\"women have a higher acceptance rate of pull requests overall, but when they\\'re outsiders and their gender is identifiable, they have a lower acceptance rate than men.\"maybe their paper makes this clear… the figure doesn\\'trather… when people don’t know somone’s gender and  know the person (insider) men and women’s rates are equal within their errorstarting the y-axis at 60% also distorts the overall size of any differenceswithout scrutinizing the stats, the outsider numbers seem “significant” but the results are either within the error bars or very close to being within the error barsstay positive jsapangood try nuggetbotgoogle embeds more words in more vectorspaces making *swivel* (obviously they tried pretty hard for that acronmy): knock if off, dudesleave words as strings¡backronyms ftw if you are interested in attending odsc west in san francisco (11/2-11/4), please use the following link to get 60% off your tickets : . (the discount code is insightdatascience)woooo! please see some earlier comments regarding using the at channel here. you effectively just pinged 1.3k people from a lot of different time zones.#thoughtleadersso much is wrong... starts with the hair@thushy works for insight. i think she meant to ping the entire channel. just sayin\\'.i’m sure she’s super sorry about providing 60% off tickets to one of the largest professional data science gatherings in the usi don\\'t find the notifications that annoying? but i\\'m biased ;)clean your dishes it took me way too long to figure out what odsc stands for. define your acronyms people!!!this.any data engineers or data scientists working with streaming pipelines that are attending strata in ny this week? interested in attending the apache beam dinner/talk on wednesday night? let me know and i can send you an invite :slightly_smiling_face:: sorry for the 2 notifications from me today. trying to share the discounts to as many people as possible.lolwill the @ channel vs never-@ channel debate become the great tech rift of this decade, much like vi vs emacs or tabs vs spaces?or the great la croix wars of 2017? it’s telling that there’s only an emoji for team coconut edit: nm there’s a pamplemousse we\\'re trying to keep this channel a civil and safe place, so please don\\'t use the c-wordi only have a few truly deeply held convictions, but most of them are about flavors of foodi think there is a civil war happening at fellows desks right now.i suggest that all current fellows clean up all their crap. :stuck_out_tongue_winking_eye:actually all of them are about flavors of food except for the one about the direction toilet paper rolls should be fed on the roller thingsi think the tp thing is only important in households with cats or toddlers#provemewrongdoes clark count as a toddler?we both know the answer to that questionspaces. always. spaces!!!!!team coconut is tearing this insight family apart...uhhh husbands who do it the wrong way are definitely a problem #justsayingmy wife thinks i do it the wrong way, but i\\'m not convinced there is a right way.oh do the different colored la croix cans taste differently? i never noticed. team tap water :potable_water: why is there a debate about the direction of toilet paper rolls?because people do it wrongbut... there can be only one way!exactly, that’s the right way* unless you have a toddler or cat that\\'s why you get one of these: not the best of reviews thought...this could be gamechanging...if you’ve forgotten about all the mvps we keep obsessing over at insight, we wrote a blog post to remind you to always start with a baseline :slightly_smiling_face: would love any thoughts you all have on that role^ tell yo friends? how excited are you about the ipo?yes. great post!for a really simple baseline, i also like a random shuffle of the labels. i find it gives me a useful starting point in multiclass problems.i liked how you mentioned the baseline you choose depends on the data and the task. i feel like you could write more great posts (e.g. your post on approaching nlp problems) with that idea. for example, i didn’t know i should reach for a u-net if i have an image segmentation task.canadian fellows: what can i do in vancouver in the next 36 hrs?jira is the bane of my fucking existencei use jira for backlogging things and loosely organizing sprints and it’s fine. that said, i’m glad i’m not managing it and setting it up. seems a pain. convenient when it’s integrated with github.haha i don\\'t mind it in itself so much. ppl closing my painstakingly created tickets without actually finishing the work would drive me to tears of barbaric nausea however we used to use jira... painful. switched to zenhub (which integrates with github), amazing! a happier life is possible :slightly_smiling_face:i definitely prefer jira to asana, but there’s probably options that suck less than either of themexcellent info! what pain points did zenhub alleviate? (hate jira too)^yes! it seems like no one’s a real fan of jira or asana, but maybe zenhub is good?does anyone know of tools to track per-dataset bigquery usage across a gcp account? i.e. identify which individuals in an organization consume which datasets?it doesn’t seem like anyone is given any concrete reasons why they hate jira but i would give my impressions. my experience: we use jira at glassdoor but when i started we were using rally. i’ve tried trello on my own time and heard some talk about asana (my wife has used it a little in non-engineering contexts).• it is built to be extremely flexible and do all the things but for many uses this is way overkill (boards, filtered boards, projects, epics, versions, labels, issues, sub-task - how do i map each to our org and workflow? why are issues not called tasks, why are sub-tasks not called sub-issues?)• its defaults and structure are very engineering oriented, but as an engineering org it is still going to require much configuration• advance search is the only easy way to find stuff, but of course there is learning curve before this is actually easy.• when i got started i found i could fairly easily create issues and then not be able to find them, this is because it is super easy to create issues that won’t be visible in the current context. in my experience - shared by the team as well - jira is extremely inflexible precisely for the reasons than  points out: it is built with software dev/engineering in mind, and much catered to the agile religion... it requires task hierarchies, not built to be very intuitive, things get lost, blocks certain functions if not all task classifications are filled out... a lot of complications and unnecessary time investment if you need a simple tool to track your backlog, report issues, and track progress in a very flexible manner.while zenhub _could_ also cater to the agile way, it it not built to trip you if you don\\'t. so very, very flexible. ui devs in our team use it as jira-like, the ds team uses it more as a backlog tracker, the general team uses it as a mixture of both. since it\\'s natively integrated with github, it\\'s seamless to link your backlog with commits/pulls. we also use it to document issues and the solutions that have been developed. this is extremely useful when you have offshore teams in different time zones, contractors coming in and out of projects, or kt when a project transitions to different teams within the orgyes, it sound like pie in the sky... but for whatever it\\'s worth, the whole of ds and it here transferred to zenhub, and we\\'re not planning to go back...awesome...thanks  ...makes a lot of sensewe’re sending out dev setup recommendations to incoming fellows and currently suggest python 2, vote below if think we should stick with that recommendation or should start suggesting python 3. feel free to comment if you have strong arguments either way!bmregner: while i think 3 is unarguably better, i\\'m still seeing 2.7 in much wider usage.i’d say 2 since more packages are compatible with it.  i try to use 3 when i can so i’m not part of the transition problem though :slightly_smiling_face:i’ve heard the package compatibility argument, but i haven’t run into that problem at all, any example packages worth noting?the big ones are mostly compatible, but there’s various small ones that aren’t - though i don’t have a list handy.  should be able to do most non-niche things with python 3 packages.we have some internal stuff that isn\\'t py3 compatible yet.also spark i believe only works with python 2.7yep, unfortunately pyspark is only compatible with 2.7though maybe that’s less relevant/giphy such visiondocker and ci question: does anyone use (or know of) a continuous integration service they like that is \"smarter\" about building docker containers? none that i know of cache the various layers, and so a change anywhere triggers a complete rebuild (at the cost of about 15 minutes).great question — ?i don’t want to speak for austin, but worth noting he is out of office through next weekis anyone here going to be at nips in december? that gif is from `heat vision and jack`, btw, which everyone ought to watch immediately: did u try go-cdgo has job, stage and task layerswill take a look!quick question, does anyone have any recommendations for a web cam that is good to use with something like opencv? you should be able to connect to any cam connected to your computer with opencv (afaik). anything particular you need from one?yay!on another note: internet, you\\'re grounded!y2.016k bug.hi all. i\\'m going to be arriving back in the us soon , moving from australia. can anyone recommend an isp? and which modality (adsl vs cable) i should best consider?ddeloss: you could probably use an arima model where each measurement will influence the final result dependent on some parameter and how long ago was the measurement taken (akin to an exponential decay with a certain half-life). that way you can sum up all the time-related information of one variable into one quantity and use a simple logistic regression afterwards.created  in case anyone wants to share tips and materials on back-end architectures for ad-serving, chat about targeting, audience-building, yield optimization, etcthis is interesting:and here is a related  nytimes article, “what a million syllabuses can teach us”: i also stumbled across plotly’s dash () and dashboards () a week ago and they didn’t seem bad ( limits the number of private plots though unless you’re a paying customer…depending on the balance of engineering time and desire for interactivity, it might be cost effective). but haven’t used them myself eithertoo bad the data from the open syllabus project is so limited. :disappointed: cute for playing around with their dashboard, much less useful in general though.i am working on a project where i need to determine the utc time of nfl plays. i have the short text description of each play and the time of the play on the game clock. i am trying to match each play to one of the 5 minute intervals from game start to game end. for each 5 minute interval, i have the text from several hundred tweets within the interval.so far, i can predict distinct events like halftime by using a tf-idf/sgdclassifer pipeline in scikit-learn. however, i am having trouble matching specific plays to the intervals, since the description of the plays are very similar to each other. can anyone suggest alternative methods for converting game clock time to utc time besides brute force?:  pandas has some functionality to help with this.  thanks for the info! i am able to bracket into 5 minute intervals with pandas, but i need help matching the game time to utc based on the tweet text and play text.:  can you convert the utc time to the appropriate time.i think an example of a row of data would be helpful. :wink:: hey alex.hey!so i have a list of plays in an nfl game with the time on the game clock, which does not correspond to real timeah, so like \"4th quarter, 5 minutes left\"?exactlyand i have a pandas dataframe of tweet texts with corresponding utc time bracketed into 5 minute intervals: can you show us several rows of the nfl data time stamp.: you can just copy and paste into here. uploaded a file: : yeah i can see this is very hard. do you have any reference start times for the clock and times for commercials.so your big issues is going to be that the play clock doesn\\'t match the real worldunfortunately, there is no timestamp associated with each play - only the time on the game clocki tried treating it as a classification problem and i do a good job of predicting distinct things, like halftime: yeah without a reference time to index against - and also without knowing times for commercial breaks. it seems like a hard problemyeah, i’ve been struggling with it - i might just estimateadd 2 minutes for every scoring play for commercials or something like thati guess i should have chosen the other football for my project :stuck_out_tongue:i don’t see a way to do it without knowing a lot more. injuries, time outs, the occasional power loss, etc.stay positive joshainsleyi have injuries and time outs as wellis pulling from another data source an option? or are you trying to infer this strictly from tweets?i checked other sources, but none include the actual timethe closest i got was the nfldb, which has utc time of database insertion - varies wildlysince this sounds pretty difficult to do accurately, is there something you can do to approximate? just add on times from the expected game start time?: i think going with your heuristic is the way to go. ..or getting another data source.or even figure out if it’s really necessary and if there is another way to get at your goal.i’m leaning towards just adding up the time and adding time for breaksi was just wondering if there was a more elegant way to do it: without reference times or another data source, i don’t imagine that there is….you can determine the average number of breaks for games and the average length of breaks and use that.. with some cis..use all the data to create a distribution of how long each type of play takes and then probabilistically sample from it? is that elegant? :stuck_out_tongue::  i like that idea…: good luck!!i like that too, i only have to get each scoring play within a 5 minute windowthanks for the advice!i hope it works out! good luckhi everyone ! does anyone have any experience deploying a shinyapp that contains rcharts? not in years, but have you seen this? keld_lundgaard: pricing for pro accounts is prohibitively expensive for a team of this size. something like (at least) 2-3k / month. if losing message history diminishes the value of this slack enough, we’d probably have to charge users and i think that may be a non-starter as well. the other option, that a group like this should be able to handle, is to build a searchable message archive via the slack api.sounds like an awesome de project :wink:my office uses rocket.chat, an open source slack knockoff. the downside is it needs to be self hosted, which involves some expense and maintenance. does anyone know the problem with```-bash: brew: command not found```i have a new mac and can\\'t seem to work with standard bash commandsneed to install `brew`i was doing that but this line is giving a problem, even though the directory exists: ```$ ls -l binbin/wget -&gt; ../cellar/wget/1.16.1/bin/wget```you’re just typing `ls -l /usr/local/bin` right? you’re not adding the other stuff?i was adding the other stuff as wellbut i think the error was popping because the homebrew is at 1.19.4_1 version now, and it comes prelinkedi followed the other steps and then tried installing the package i was trying by brew install ...  looks like it worked.cool!very random, but from what i understand, women entering workplace/gdp numbers are very flawed because unpaid housework isn\\'t counted as gdp but paid housework is, so if a former house wife enters the workplace, takes a person\\'s job, and the person becomes a paid houseworker, then gdp has now gone up even though no additional output has been addedbut that\\'s only something that could be known if you knew the details of how gdp in this instance are calculatedanyone know of other statistics that are only seen as misleading if you dig into the methodologyi used dbscan mainly for cluster detection with the purpose of not assigning clusters to a lot of the \"noise\" in my data. it\\'s really helpful if you don\\'t know / don\\'t want to know how many clusters you\\'ll have beforehand, and also if you don\\'t want all your data points assigned to a cluster.hey all, does anyone have any good tutorial/reads on how to model rare events with historical data using statistical inference and creating simulations?you can try zero inflated models, like zero inflated poisson, or negative binomial. you can make inference and simulate from such a process using bayesian models. i can\\'t think of any specific tutorial nowgood start for me to look into, thanks tadaishi.hey! i\\'m leaving tomorrow and would like to get rid of a cheap working bike. i\\'m at the insight office (in sv)  now. anybody who might be interested please message me directly.fyi the rally positions are in sf and dc. perhaps all hiring messages should start with a square bracketed line with a 2 letter country code an optional region, a city name, and a general capacity (role). eg:&gt; [us az mountain vu - cashiers (wfh)]&gt; excitingly creativ naeme is rapidly expanding and looking for people to help process the bulk transport of large cash assets. familiarity with usps and envelopes a plus.woah i\\'m in:  interruptible map function for multiprocessinghi all, anyone here with a computer vision background? (physicist asking) looking to define feature vectors to characterize images details in binary images (black/white) while keeping the vector spec as robust as possible to geometric attacks such as rotations, scaling, translations, etc? hints?...  might helpthanks for that . will take a pick. still on the mvp phase thus building very simple features that my common sense can recognize. thinking about histograms of sizes for connected (contiguous) areas of black/white patches.  the winning entry for that is a conv net  . it probably wont be very useful carlos unless he comes up with a decoder version - carlos doesn\\'t have labels.  this is doable but tough.  however, would recommend reading the forums.  there might be good tips there  apologies should have clarified, take a look at the forums, specifically “so what were your approaches” some of them didn’t use convenets, hope it helpsdoes anyone have experience with svm handling network derived metrics (pagerank or community (louvain/walktrap derived)?i come from a more “statistical” background and am more comfortable with permutation based/ bayesian approaches to modeling networks.thanks in advance: here is a cheat map that i find helpful for choosing  ml algorithm according to the given data:anyone a master of putting nested json in to pandas? — dm, i do this for work pretty frequently: used recently with twitter feed data. you can email me @ @general: happy to connect with you on linkedin. feel free to send an invite. welcome new fellows!: thanks - it\\'s really helpful. i was looking for the map a few weeks ago, but didn\\'t know where to find it. really basic strptime question, how would i parse out year month day from 2014-06-05t15:48:00:000? the time stamp part is making it tough. thanks!!the boneheaded way is just do yourstring = 2014-06-05t15:48:00:000; year=yourstring[:4];  month = yourstring[6:7]; etc.ahhhhh! thanks!in general, i have used the parse function from the dateutil.parser package which is pretty amazing in parsing dates.`pandas.timestamp` has a very robust parser toothanks so much!```import pandas as pdt = pd.timestamp(yourstring[:-4])t.year, t.month, t.day```no problem :simple_smile:i am trying to import a string into a text datatype in a mysql database in python, i get the following error: \"unicodeencodeerror: \\'latin-1\\' codec can\\'t encode characters in position 0-5: ordinal not in range(256)\". it is due to trying to enter: \"богота\". anyone know how to fix this?hi guys, here are some cheat sheets for python, matplotlib, and pandas that i\\'ve found extremely helpful. hope this helps: has anyone played around with scraping reddit data in python using praw? 2 points: a) i think mysql is not very good with unicode/can handle it with the right config (feel free to correct if anyone knows more).b) try pythons decode () with error=\\'ignore\\' or the like to get rid of it problems in a simple, hackish way.  : you might be interested in looking at textons. a good start would be to look at the visual geometry group at oxford (many other nice little piece of code to get you from zero to hero) :wink:hey all, in our fall session of insight, we made some custom insight-themed emoji for our slack channel.  i\\'m adding them here as well.  examples::insight_left: insight :insight_right::insight_left: john :insight_right: :john::insight_left: wafa :insight_right: :wafa::insight_left: zach :insight_right: :zach:if you had other pds you\\'d like to add, the process is super simple and takes about a minute, see iderjerneck i was trying to avoid the hackish way but might just go with that for now. thanks!whoops, above was meant for does anyone have experience using the meetup api? i\\'ve posted on unicode in mysql and python before in the sql channel. if the infos not there anymore dm mea bit like kaggle competitions, but for forecasting world events and trends: does anyone know a good, automated way to get approximate latitude,longitude boundaries for a city?like an approximate square area (for example) if you can get the boundaries of cities, try the python shapely library anyone know how to integrate variables into urls you are reading into python? i\\'ve been  using requests.get(my url here) but i am using a url in which you can query so i would like to add variables within the url but am having trouble concatenating them. thanks!can you give a specific example?( ^^):  try requests.get(\\'\\' % yourvariable) # yourvariable\\'s value is substituted into where \\'%s\\' isalso : maybe you can move your questions to ds-sv-2016a?what, and deprive the rest of us of the opportunity to help? :stuck_out_tongue:but yeah,   would be more appropriatealso, check out `str.format()`. you can use named tags like:```s = \"hi, my name is {name} and i’m a {occupation}.”s.format(name=‘brad’, occupation=‘data scientist’)```: i got (very) approximate city bounding boxes from google geocoding: does this help:   (at least for the us)does anyone know if there if there is a not completely roundabout way of getting postgressql running on an aws linux instance?`sudo apt-get install postgresql`?fair enough, they just had a quite involved white paper about the whole thing, that i probably need to just pick through a bitbut thank you , that’ll probably take me 90% of the wayand apologies for the sillinessjust spin up an ubuntu ec2 instance and 3-4 commands later you\\'re good to goayedoes anyone know where to find granular (e.g. precinct) voting data for the 2016 election?judging by the sources listed here you have to get it from individual countiesor beg the authors of that page i had a great experience with two men and a truck.  they are on the pricier side, though.is anyone interested in a job as a data scientist at a consulting company in nyc? my former employer is looking for someone new. the work would be focused mainly on the energy and pharma industries. send me a pm if you\\'d like to know more.an old but relevant blog post by  hahaha, i was just thinking about my insight project as we were rewatching the force awakens on friday. “this movie got me a job!” :nooice:thank you for the kind shoutout!spoiler alert:  random forest works pretty well for nlp. :grinning:does anyone in the bay area have indian rupees they\\'d like to exchange with me? i\\'m heading to india soon for a trip.this came out, has some interesting things perhaps even for those that are experiencedyep! especially on the attitudinal side like surveys, interviews, diary studies, ethnographic studies, focus groups, participatory design*hot off the press*: we just starting looking for more data scientists at samba tv in sf! samba’s an awesome place to work and long-time friend of insight, so if you might be interested in working with us on gobs of tv data, dm me! hi alumni,my name is joel swenson and i’m a program director for the health data science program at insight. i would like to invite you to our san francisco office for a data science meetup featuring color genomics.food and drinks will be provided.please rsvp here () by this friday (11/10) as space is limited.hope to see some of you there!joeldomino is doing a data science management survey. chance to win $250: has anyone had experience with saving and embedding an image from pyldavis in flask?nevermind- found a solution!:  :wink:i\\'ve always thought that the last post in the thread on this comic should say, \"nevermind, got it to work!\" -- would be more true to life.agree!for anyone curious about setting up neural networks with gpus in windows i recently published a post about it.profeshcould someone help me interpret something… i’m trying to install xgboost on osx following these directions:  and have run into trouble at the last step. i think it’s because i use osx el capitan. it says: \"note: if you use osx el capitan, brew installs gcc the latest version gcc-6. so you may need to modify makefile#l46 and change gcc-5 to gcc-6. after that change gcc-5/g++-5 to gcc-6/g++-6 in make/config.mk then build using the following commands.” can someone help me interpret what that means?i think it is just asking you to edit line 46 of the makefile in the xgboost directorythat might be out of date, it looks like that change has already been made in a previous commit but i’m on yosemite and had no trouble running makestay positive andrewzirmalso, what brad said :slightly_smiling_face:ok so i guess that wasn’t the reason why i’ve been having an error, but i’m going to try something else… thanks for the help!thanks mike. i saw the supervisor log file and noticed an error right before the site stopped loaded. apparently there was a problem with flask (i found this on googling the error message). updating to new flask has solved the problem right now or it seems like. i will look into your tips if i have more problems.stay positive pushkarinip.s everyone else in current session, you may want to update your flask (you may notice an update in your werkzeug installation).glad to see that solved the issue!nuggetbot animate me \\'log file\\'haha :smile: :smile:hi! i am interested!! i am at an airbnb in east palo alto and there was a shooting and cops outside my door last night…i gotta get outta here! thanks!: article on recruiting data scientists (and how we do it at sailthru):thumbsup::thumbsup::smiley_cat:thanks dvats, very interesting articleyes, now get everyone in the world to change their interview process please :simple_smile:\"working on it\" -- data scientist at  :simple_smile:re-post from  :nice! sounds like a couple of interesting discussionspreetika [11:18 am]tamr cofounder michael stonebraker won the turing award..neatsean\\'s project!hahgood read. insight gets mentioned 2x!! also, interesting point about code and data sets being considered in academic advancement. at o’reilly jupytercon in nyc last week, there was much discussion about how jupyter notebooks will help with curating and publishing such code / data. (if you have a subscription to safaribooksonline, you can access the conference talks in about 2 weeks.)anyone work with ads and dealing w gdpr? i know you\\'re excited to talk with me about it\\'s impact on the ability to personalize advertising for a better user experience if anyone’s interested in entering product management, one of my friends is giving a talk about being a pm for linkedin not sure, but  _might_ be using / have used gcp dataproc feel free to dm me. work as a data scientist in a prod capacity at health tech startup.thanks  and  ! will dm.. thanks  for the cartodb suggestion! :simple_smile:yes, cartodb is wonderfulhappy new year! hey all, i have a good friend in the bay area (and who’d like to stay here) who’s been doing webdev/ui/ux/design stuff for a few different companies for about a decade now and she’s really not pleased with her current company. she’s also recently gotten interested in and a little experience with doing some analytics (usage stats, etc.) related to website designs and stuff like that. if anyone knows of any possible positions that she might be good for, please dm me. thanks!airbnb seems to have a great data science on boarding education. lovely! they also just open-sourced their data exploration + viz tool: ai and the competition for talent in silicon valley uploaded a file:  and commented: really great piece of research, just the best, reallydoes anyone know how to export hive query results to parquet format?i have used pyarrow for sql to parquet. might be worth a try. thanks dr neff.how about this:  create table a stored as parquet as [sql].  then pull the file off hadoop fs.  you can use spark to do this :slightly_smiling_face:that\\'s amazing seiji, thanks for sharing! uploaded a file:  and commented: my company gac r&amp;d has an opening for a research position oriented on ai, experience in robotics or control desired but not required, in the automobile industry (domain knowledge not required), see the post here commented on ’s file : good ideas! thanks!any one has a good way of building data dictionaries for their databases?hire an intern?  i actually have yet to see anyone have a good data dictionary.  i always have to build these out for whatever data i have to use, and it\\'s a painful process of exporting schema and then poking at engineers and jira tickets to fill it inwhat do you document it on?just a google doc?a spreadsheet with an intro sheet describing each table and then additional tabs, one for each table.  i color-code them based on data source if the warehouse is integrating multiple sourcesahi was hoping to avoid spreadsheets but…it is the inevitable thing to do.what else did you have in mind?i feel like describing a table is easy to do in a... table :smile:ha! good point. i was thinking i guess of something more in the lines of having it being part of a general database wiki document.you can always link to it but i feel like document format for something like that would be pretty impractical to navigate around for a complex warehousei would recommend having the data dictionary live as close to code as you can, so you don\\'t have to update field descriptions in multiple places when things changequestion: i need to explain random forest to a bunch of analysts in my company. i want to convey the main idea and not get bogged down into the nitty-gritty. does anybody have a good reference for this?i always hear it explained in layman terms as \"wisdom of the crowd\".  analysts may deserve better though, they\\'re probably somewhat technical :slightly_smiling_face:i\\'d like to go further than that (but not too much)... ideally i would like to show some simple visualsi like the idea of drawing an example of a 2-d feature set + classification label, then thinking about how you can draw lines in those dimensions that would help you best separate the classesthanks , will check this outand make the connection that the lines are the splits in your decision treei.e. just start with one treethen explain random forest after that have you seen the r2d3 demo? don\\'t think it necessarily talks random forests but it\\'s good for visualizing decision tree learninggood ideas guys, i\\'ll check them out. last time we did a mini-lesson on decision trees, so that should also helpa long while ago i made this for a business client who wanted to know wtf i was doing uploaded a file: it\\'s very rudimentary but it worked pretty well, they \\'got it\\' they feltcan you elaborate on the “as close to code as you can” part?hey, i got roped into doing an technical interview for an analyst position. anyone have a analytical question they like? commented on ’s file : not sure what you mean by linear projection.  as i understand it, state-plane is based on \"flattening\" the state\\'s map so you can do point-to-point distances in 2-dimensions.  i don\\'t think it would translate easily into wgs (what google uses) commented on ’s file : thanks! i expected it was not so simple a problem. i\\'m looking more into pyproj today and will let you know how that goes. the help is much appreciated.supervisor does not an advisor make this was just open sourced. looks like it might be a cool alternative to bokehoof. outstanding name!hey fellows, i got a friend from the uk who\\'s thinking of joining the program. he was doing a phd in vancouver, but he\\'s grown rather disenchanted with it. he\\'s looking for options for visas, and i\\'m wondering if anyone here has been in a similar situation, and how they made it work. let\\'s start a thread, or reply to me privately!actually, i think that is a critical role of a supervisor. they have to have useful thoughts on the path forward.update for those curious: i basically just did an interview i would give an insight fellow but at a faster pace, covering more topics but trying to stay at a relatively high level. no whiteboarding, but she did bust out a pen and paper to start writing some equations and sketching graphs. i did delve a bit into management and recruitment style. i also did a longer real world scenario of an experiment i\\'m planning to pitch on a real product we\\'re launching. apparently she was so impressed that she talked about it in her interviews with other people. that\\'s all cool--but now i\\'ve been roped in to interview two other candidates tomorrow and monday :cry:: i saw this and thought of you. we\\'re hoping it sends people to our blog! :wink: (where, of course, our lizard people mind control rays will get then!)anyone thinking of giving it a crack?i do not have time at the moment, but if anyone decides, you can ask me for tips. i did my phd related research in helioseismology. there is a overlap with planetary seismology. is reddit famous!hey team insight, long shot: anyone working as an economist, or have economists in your team? i’m interested in what they’re doing. - recent fellow  was working as an economist at the ftc before joining insighti’ve used it, but i don’t have much brain to pick. i retrained the tensorflow object detection api for new classes. training was expensive, i whistled through $250 without even realising. since google wrote the training scripts it was easy. i can really appreciate the workflow they have set up.glassdoor has a chief economist (andrew chamberlain) and one data scientist assigned to his team. much of their work appears at i worked as an economist pre-insight :raising_hand:\\u200d♂️thanks ! that’s super useful :slightly_smiling_face: hey jesse, do you have any thoughts on econ v ds? like, is there much/any use for econ roles in industry?meh. they would cause such a hassle with security it\\'s probably worth not having them take it. i would be curious to hear the thoughts of someone with a neuroscience background on this article. boffins and luvvies eh? was this article written at hogwarts?: !: i also felt like this article needed to be translated from english into american. i don’t know what a boffin or a luvvie is either.: smells a lot like the look elsewhere effect (and related p-hacking/p-value misunderstandings) being abused: in my personal opinion i would say that article is about 80% accurate. there are some brain regions (mostly sensory/motor) that activate pretty reliably but a lot of the behavioral economics and social neuroscience research is on shakier ground. i don\\'t trust the results of a neuroscience study unless they\\'ve been replicated at least twice, and that happens very rarely. probably partially bc results aren\\'t real and partially because there\\'s little grant money and interest in replication. and of course no one wants to publish null results or replications. ugh. and  that is pretty much what i thought. thanks!stay positive pamela.clevengeri totally missed the article that started the thread  (and  and )  i just brought us full circle.  sorry about that.stay positive pamela.clevenger: have you seen similar issues with erp? i was in linguistics/cog sci and n400 and n600 are used super frequently\\\\m/hey, i remember brianna talking about not trusting neuroscience results during the session!: i don\\'t have enough experience with erp to know, but i\\'m sure there are other people who could comment!this isn\\'t just about people not correcting for multiple comparisons, the bigger issue, which they only link to and barely mention in the article, is because of a software bug that existed in very popular fmri software. there was a  bug in the software that was used to estimate the probability of false positives, so it means even some people who were trying to account for multiple comparisons could have spurious results because the software they used had a serious bug in it....for 15 years.  i am hearing terrible news from new york. i hope all of you guys are safe and sound.  everybody here is safe!: thank you. it is getting uglier everyday.    glad to hear everyone is ok. stay safe!!anyone know about the command line split (to split a large json files into multiple files)?i\\'ve used split for json before. what\\'s your question? so when you do (for example): \"split -b 40k myfile segment\" does this mean each segment will have 40k at maximum? so if my file is 165kb will it make four 40kb files and one 5kb file? is there a way to split it into four even (having the same amount of data) segments?do each of your lines have the same amount of data or is it variable? you will need to determine if your ‘json files’ contain a single record per line of the fileif that condition is true, you can use ‘split’ to divide that file into chunks, by line (see the options for ‘split’ on how to divide a file by lines)divide/partition  yes the files did contain a single record per line so that was the perfect solution. thanks!ny! thanks for your hospitality to us boston fellows today. great to meet you and see some familiar faces too. if you\\'re ever in boston, hit us up.wardrobe question for the dudes: my husband is interviewing for a job at a startup, the role is on the business (as opposed to technical) side. would khakis and a button-up be too casual? was thinking that a suit wouldn’t fit with the culture. definitely wouldn\\'t do suit, i\\'d say+ sportcoat! (not the top part of a suit, but an actual sportcoat!)+1 for sport coat/blazerand very shiny brown shoes that match the brown of beltthat plus button down, nice high quality jeans and the right shoes have a magical ability to look professional and casual at the same timeidk if i\\'d go shiny shoes thoughthinking somewhere between tier 3 or 4chukkas are a go todepends on where the startup is locatedeast coast more formal, west coast less formaldo any of you data scientists develop your ml workflows in an object oriented programming paradigm? my code gets messy very quickly so i\\'m thinking of adopting an oop design for my projects, and i\\'d be very curious to hear about some existing paradigms (what classes and methods to use).i use scikit-learn’s pipelines[1]. they mirror the level of abstraction i would use to describe my model to someone on a whiteboard. i’d also suggest writing your code such that the data structure passed around to different transformations is _effectively_ immutable (i.e. your transformers should take in an object and output a new object instead of mutating the old one). this makes parallelizing the code easier, and, at least for me, makes the code easier to understand. [1] +1 to pipelines, or at least using them as inspiration for rolling your own classes+1 on the immutabilityhi fellows, we just launched our latest insight fellows program:devops engineeringmany insight data engineering alums are moving into the field, and we\\'re seeing a lot demand for these roles, so we\\'re going to run a dedicated fellowship starting in july in silicon valley. if you know of any engineers or systems folks who are excited about getting into devops, sre or infrastructure roles please send them our way. if you\\'re working in this field currently, please definitely reach out as well. very excited to be expanding our engineering fellowships!wow ! you guys are on a roll with launching new stuff!a :trophy: to the person who can most succinctly define devopsthe team holding up the walls and ceiling around product / ds:austin_and_his_puppy:^^ shortest definition of devopssuperherosanyone know java? can you point to good starting materials?“site/database reliability”head first java is pretty goodthanks guysyeah, that one was good\"devops is intersection of lover of cloud and hater of wake up at 3 in morning.\" allbirds, good jeans, button updevops: the people you call to redeploy your code after it breaks from changes made to the environment by devops people. :troll:me, i like apache beam\\'s abstractions; but they\\'re not necessarily oo.hi all, preetika just wrote a great blog post on how she chose here project and technologies.  check it out here::thumbsup:hi fellow fellows!! my team here at memorial sloan kettering is looking for  a comprehensive public/government database of death records. the social security death index used to be the best source but since 2011 they have stopped integrating state level records so are woe-fully incomplete. someone else suggested the ndi maintained by the cdc but the timeline for accessing that data is doesnt work for us. the data cannot be an api and has to have identifying information. any thoughts or suggestions would be great!!: ^^i would also be interested in this, , let me know what you come up withfacebook algorithmic factory in 3 parts - a neat read: hi folks.  i know a marketing guy with lots of experience in tech startups who\\'s looking for a new position.  if you know of any openings feel free to pm me.  thanks.  at fast forward labs, we are hiring one machine learning engineer and one data engineer for a short, 6 months engagement to develop a novel prototype using quite fun nlp techniques, clustering, outlier detection etc. it’s a fun opportunity to develop an entirely new product from scratch and an ideal role for folks who want to dive into new techniques to e.g. level up for the next role. we’ve got strong partners: prehype (an incubator) and “an international news organization” (i can’t say the company name but, let’s say, you’ve heard of them). here is more: send me a pm if you’re interested!we’re hiring another data engineer for the data science and machine learning team at crowdstrike:my roommate is likely to move out by november. if anyone is looking to share a 2b/2b condo in central redwood city ping me!  ^not sure if this has been talked about here or not: anyone have experience building interviews for data leadership positions (ie the person running a team)? looking to create a list of questions/discussion topics for an interview and would love some advicedoes anyone have experience with `sklearn.cross_decomposition.cca` module, especially in the ways that it relates to the `canoncorr` implementation of cca in matlab? i have some matlab code that i am trying to reproduce in python, but answers never seem to be close. thanks in advance for any pointers! i asked this question a couple weeks ago. i can give you advice about how it went for me. what is your relationship to the candidate?hey, oh awesome!that would be great! basically we have a ds team and a ba team and we are looking to hire a high- level person to both run ba as well as deal with upper management on behalf of both ba and dsdamn, looks like my question slid off the back end of yeah, insight too cheap to buy real slack :wink:yeah, sounds pretty similar to my situation. we are/were hiring a director of analytics and insights (de/ds/ba). i was interviewing for technical depth in terms of data science and analytics. what worked well was to run through a series of high level scenarios that mimic the kind of work your team handles (or that you anticipate you will handle). ask them how they would approach the problem, and basically ask them to explain the approach as if you were a business stakeholder, then a junior analyst/ds who would have to do the work, then dig a bit into their technical depth on the problem, then bring it back up to the stakeholder level--asking how to interpret some fictitious results or ask them to communicate some bad news to you (my favorite) as if you were the stakeholder.that way you get to see how they would communicate with management to receive requirements, how well they can explain things at a basic level, how deep their expertise goes, and how well they can synthesize and be diplomaticyeah that totally makes sensethis is what i was imagining doingwhat about more high-level stuff? questions that assess management style, culture fit, that type of thing - nontechnicali don\\'t have any advice in particular about that, sorryi think these are the women who will be speaking at/organizing this event, not all the women who work at the company :wink: . also, two of the eight are directors, two are data analysts, one is a data engineer, one does analytics, one does data visualization, and the others represent different roles that are integral to building data products. it doesn\\'t seem like the lack of a specific title is diagnostic here.all good manthanks for the advice on case studies, i think that’s an awesome direction to take  it seems to have decreased how big figures and fonts appear for the same size values!hooray backwards compatibility! :smile:\"the available shortcuts are currently limited to command mode, but we invite contributors to add support for edit mode!\" i would kill for a good vim implementation....+1 on the case study questions. i\\'ve had to interview someone for a position like that on my previous team (and we also designed case study questions that were almost identical to ones we were trying to solve on our team) and i\\'ve also been interviewed myself for a position like that (they too asked me case study questions)it helps both sides to understand the problems and whether the person is the right fit for it or notin terms of management style/meeting deadlines, they looped in questions like \"if you had a problem like this, what would your thought process be like? how would you set goals/deadlines for your team?\" etcyou can also just be direct and ask them about their management stylesome people will give vague answers like \"i like to follow the agile process\", but be sure to ask them to elaborateha, just read \\'s response. yup, basically sums it up, well put! :slightly_smiling_face:i think culture fit pretty much becomes apparent through their mannerisms thanks so much! :smile:all good points, definitely understand and will keep all of this in mind (we’re hiring a head of data science)hi , cape analytics is hiring data engineers!  we use machine learning and deep learning on geospatial imagery to help property insurers \"see\" the built world at scale.  if you or anyone you know is interested, feel free to ping me here or at  and i\\'ll  tell you more :slightly_smiling_face:there _is_ a vim mode (though it hurts for me to say it since emacs is da best). the edit mode uses codemirror.js, so you can just turn out the vim mode there: that feeling of disappointment when you get a slack notification and it was an @ channel :sadkeanu:  that or wondering why the hell someone from work is slacking you at this hour.  like getting one from your boss and you’re just like “i’m at a bar, i just did a shot of hot sauce, _we will deal with this in the morning_\"shot of hot sauce?like tabasco or is that a type of shot?no like literal hot sauce…i am not a clever manshot of johnnie walker would have been betterthat’s a little on the nose, don’t you think?just pulling your leg:stuck_out_tongue:but i’m glad we see eye to eyei’ll be that insight staff member and caaaasually point out that anyone on this channel can always hire a data engineer / scientist _through_ us instead of posting a job announcement directly to the network and connections we’re providing for you :slightly_smiling_face: * refreshes inbox for eager alumni emails who want more team members * * but seriously you know where to find us and obviously this is what we do *anyone know of a good online/remote masters program for data science?  i have a work colleague (who\\'s a good data engineer) who wants to do a masters focused on ds. u. illinois cs dept has an online ms in data science for $19k.  i can\\'t vouch for the program but the cs dept has a good rep.  also georgia tech has on an online ms cs and you can choose a ml or ai focus.  it is much cheaper and still a good school.  hi  arm is hiring in us for two roles: an analyst role and a data scientist. if you know of anyone that is interested, please ask them to mail me at . here are the links : , sorry to do everyone here ..i think uc berkeley has a great ds online programhey  -- please scroll up to see my message a couple posts up. this looks like a role we could help you fill via our current fellows (it appears to be entry level / new grad?) you can message anyone from our team directly when you have open roles, rather than posting widely to this network that we connected you with (including new fellows we\\'re currently working on placing).hey  .. sorry did not see your message. was in a hurry, will get in contact with team for these positions. do you want to learn about quantum computing as a general thing ? or something specific ? if general then your really can’t beat  nielsen and chuangok i am never testing anything on vanilla mnist again   nielsen and chung is good for basics like knowing what quantum gates are and such but depending on what you are interested there are huge varieties of good intros.. are you interested in annealer based algorithm or universal related algorithms? are you looking at particulars like shors grovers hhl and such.. or even more basic like techniques?   i don\\'t know what any of those things are. so probably basic.  i guess then it depends on what your interest and how much time you want to invest in it. neilson and chung is great but it is a whole textbook. if your interest is more casual there are some great posts by rigetti computing staff. i personally like ti read review papers and wikipedia when i need helpanyone here working in ds roles in canada (vancouver, toronto, montreal)?i\\'m not but i\\'m interested to hear of anything/anyone particularly in mtl or toronto (any other :maple_leaf: folk feeling like moving home lately?)friends of mine are looking to hire a data scientist in toronto. let me know if you\\'d like an introduction.canadian feeling vancouver hard here.  hi! is anyone good with debugging website issues, or know of anyone who is good with this? the web app i built for my project was working perfectly a week ago, but starting monday i started having really weird bugs with embedding instagram posts to my site, and i can’t seem to figure out what’s going on. any help would be greatly appreciated! -- you may also want to have a chat with , she\\'s got a pretty good grasp on the canada ds scene! but he rarely checks slack any pointers in the logs in the webapp?i’m currently looking through the errors with dev tools in my browser, as i’m not generating log files for the webapp i am in toronto, but i don\\'t really know any companies that are hiring atmany errors in the chrome console?hi , i’m getting errors like the following: uncaught error: minified exception occurred; use the non-minified dev environment for the full error message and additional helpful warnings.    at k ()    at ka ()    at  uncaught error: minified exception occurred; use the non-minified dev environment for the full error message and additional helpful warnings.    at e.exports ()    at v ()previously, embedding posts worked fine with the embedded html instagram’s api returned to me, but then all of a sudden this week it stopped working. you can check out the (broken) web app here: the formatting of each post is what’s not working. they’re all different sizes, and overlapping. previously they were coming in perfectly formatted yep if there\\'s a military parade on all the tv channels i\\'m going to definitely do a few searches on job sites...i can see in the dom that it\\'s assigning a fixed row-height of 26 to those \"overlapping\" bits; unsure if it\\'s inheriting that from elsewhere.looks from your github that you tried to manually loading the embed.js, but when you do that you need to get the html you have to use the omitscript parameter in the request url some of your embeds get the js from  and some get it from . see what’s different between those posts toothanks for taking a look! and yeah, i did try loading in the embed.js manually. when i did that i also used the omitscript parameter for the request url, and it didn’t seem to helpdid you get any idea about why some of the embeds link to a different js file?no, and i suspect this is the cause of my issue. i have a window open from last week, when the app was displaying properly, and it only doesn’t have any errors and lists embeds.js from within a “scripts” folder. now, my app with all the errors points to an embeds.js from an “extra scripts” folderperhaps the instagram urls look different for these two embeds?  too!has setting `set node_env=development` helped?, no, as far as i can tell, the urls look the same. i’ve been spot checking a bunch of them, and nothing stands out. , i haven’t. i’m not familiar with that, but i’m willing to give it a shot. where exactly do i set it?this would be in the shell that you started the webappso you’d have to take it down, then run `$ set node_env=development`, then bring the webapp back upit should provide you with more debug info hopefullyawesome, i’ll give that a shot!after a whole lot of frustration, i was able to get the site back up and running! , , , i just wanted to thank you so much for taking the time to offer some help! i really appreciate it!anytime!: one of the fellows from my session ended up doing something along those lines -- the startup he was working with ended up giving him anonymized text messages that were like `aaaaaaaaa. aaaaaaaaaaaaaaaa :)`, he ended up using the emojis a lotp(excitement | number of a’s)?very highimodoes anyone have experience with mobile app analytics platforms like segment, mixpanel, amplitude, mparticle, app annie, and appfigures? i\\'d like to hear your impressions of any of them.shit, i’m supposed to be using amplitude right now but haven’t played with it enoughstay positive lafonjstay positive lafonjstay positive lafonjsorry, had a little issue with the robot...stay positive i try but a robot telling me how to act...stay positive *data scientists in healthcare:* i host a weekly youtube live show on ml in healthcare and am looking for someone to interview this week. the show is on thursday from 11:30-12:30 pst (including sound set up). if you’re interesting in discussing:- your work- ml in healthcare generally- the past or future of ml in healthcare…please let me know! we’d love to have you on the show! ?has anyone attended the open data science conference? if so, would you recommend it? i’d like to attend at least one conference on the west coast this year, and was wondering if anyone had any suggestions on conferences to attend or ones to avoid. thanks!btamadio: i just went to one at boston. i don\\'t plan to go again. the one in boston was mostly targeted at business folks, with a mix of vendors and some practitioners as speakers. audience was mostly business folk new to data science and students. it was ok, some specialized talks, but most were very general, i would suggest looking at the schedule of the boston conference. dm me if you can\\'t find it. of the workshops at the conference, only the platinum pass would have been worthwhile (spark training) for me. i signed up for the gold pass before knowing what they were training on, it was on scikit learn. good refresher but not really worth the time and money.btamadio: if you can tell me more about what you\\'re looking for, i can tell you if it\\'s a good fit\\u200b or notfor what it’s worth, several in the boston insight crowd have decided to submit talks for next year at odsc. it was a little rough this year, so we want to make it better :slightly_smiling_face:hi all, due to visa issues, i am going to be leaving the us this  august. does anyone know of any companies that are hiring for remote work? also, not necessarily opposed to other countries outside of the us as long as immigration work can be done. thanks!this announcement may seem familiar to some of you! it\\'s a re-post with different start date.a colleague at dia&amp;co is looking to sublet her apartment for a few months, here are some details:a quiet and sunny furnished one-bedroom apartment is available for sublet in brooklyn (bay ridge) for 2-3 months, starting early/mid february. actual dates are flexible.dm me if interested and i\\'ll put you in touch!i’ve never specifically worked from onedrive, but when i had to transfer large files (100+ gb) i always used `rsync`hey, if there are any alumni looking for senior data engineering roles in boston, please let me (or someone at insight) know. thanks!is there a way to download all the county data sets at once on census fact finder when you\\'re looking at the demographic (age vs race) tables.you can download specific tables and variables here: and if you\\'re an r user, the tidycensus package is quick and easy yeah i\\'m using the factfinder download center uploaded a file: so for example, i get to this page: so it is making me go to each county through that dropdown to download it, i was wondering if there\\'s a way to expand the table so that it has each county as a columnhi guys, i am making mood-based playlists for indian classical music for my insight project. i am able to pull out data from spotify api and analyze them using clustering techniques, however, since my data is unlabeled, i am considering getting \"human in the loop\" as a mechanism to establish ground truth. simply put, i am asking you guys to do me a small favor and fill this survey form for me: . i would send out one form every day; each form needs you to listen to 10 songs and provide labels to each of the song wherein you rate a song on the basis of energy and positivity. i am also thinking of extending my work to give user-based recommendations for which i will ask you whether you like a song or not. send this to whoever you think would be interested in participating in this project. a big thank you to all of you in advance! :slightly_smiling_face:sounds like a really cool project!  so, are you aware you can get energy/positivity measures from the spotify api?positivity -&gt; \"valence\"yup :slightly_smiling_face: i used them for clustering my data points :slightly_smiling_face:cool, so then you\\'re trying to validate spotify\\'s measurements of those quantities, is that the idea?yes, in the context of indian classical music. i may have to come up with different set of features eventually since indian classical is quite different from genre-based western music.i see, very interesting!thank you! :slightly_smiling_face:playing around with that r library, looks promising. thanks.planning to listen later, but had a suggestion on the form - having dropdown menus with the expected response choices to choose from would standardize the inputs and could make your data much cleaner...  even though we\\'re all phds in my experience people aren\\'t super consistent with following complex instructions...that\\'s a great suggestion! i was considering that but then i will have 50 items to choose from for each song. that\\'s not very pretty :disappointed: any thoughts on how to optimize selections?i was picturing just three drop-down menus per song, one with [\"calm\", \"mildly calm\", \"neutral\", \"mildly energetic\", \"energetic\"], one with {\"dark\", \"mildly dark\", \"neutral\", \"mildly positive\", \"positive\"}, and one with {\\'like\\',\\'unlike\\'}or the \"linear scale\" response option might work tooeach song could be a \"section\" in the google form, with 3 questions in itjust from experience, if people have to type things in there will be misspellings, capitalization differences, etcchanging the format now! thanks a ton, jamie!! :slightly_smiling_face:great! glad to be of help :slightly_smiling_face:done! :slightly_smiling_face:would something like this help? just fyi the order of the questions doesn\\'t match the spotify playlistnow it will. i had asked it to shuffle before which i removedthat website was where i took the screenshot from. :slightly_smiling_face:hi guys, i\\'m working on extracting features from blood pressure waveform signals and performing classification (e.g. diseased vs. healthy) based on that and i wonder if anyone has ideas about what features might work best for the classification task. i\\'m thinking about morphological features and frequency domain-related features. please let me know if you have any insights. thanks!what’s the pm track?hi all, i was wondering if any past fellows have worked with satellite images? i have a couple of “best practices” questions as well as questions about available data that i was hoping to askinsight started a project manager track for “ds ai ml” stuffjust when you though they couldn’t fit anyone else into palo alto :wink:interesting. i guess i assumed that woulda been announced somewhere… has all the details... it\\'s kind of a pilot.yeah, i only learned through the grape vine; it think it’s very much a betahappy to chat with anyone who is interested - just message me.^i’d be curious in the executive summary, personally!hello current and past fellows. would anyone who\\'s worked in the ad-tech space be able to have a quick chat? i\\'m looking to verify that my project idea is a practical use-case. thank you!this is so importantplease let us hire them ;)  we can talk tomorrow too i have, briefly.  did a project using satellite imagery.thanks. i spent two years at the oracle data cloud ad-tech firm, so let me know if you want to chati\\'m technically in an ad tech space and curious to hear about your project!not sure if this talk would be beneficial or not (in terms of available data): thank you all for your quick replies.  and  i\\'m currently part of the nyc session. i\\'m open to chatting over any platform  (i.e., slack chat or skype call) that works best and can accommodate your schedule. thank you again.awesome, thank you all!:  we just contracted domo for a few use cases for internal clients. their pitch is flexible aggregation, storage and great visualization. i am talking to then about options for hosting code on their platform which they don\\'t support currently. unless you have a small dataset to download from the api and compute on locally and push back results, it becomes impractical very quickly. they are open to supporting a few use cases, and are currently doing so for julia. python is still not supported.anyone know a good reference for statistical testing?i found these notes to be useful: i’d like to share interactive matplotlib figures with business people. does any one have suggestions on mpld3 v. bokeh v. something else?never used mpld3, but big fan of bokehi just learned tableau via . you can very easily create interactive graphics via data you load into it. it\\'s all click and drag, no coding. udemy thanks guys!i’m getting errors in converting my complicated figures to bokeh (“unboundlocalerror: local variable ‘axis_l\\' referenced before assignment”), so i’ll see if mpld3 just workstableau’s good for simple plots but i find that if you want to customize it much then you have to do weird hacky thingscant stress enough how valuable this post is, especially if you are in the startup world.  shared a file:  commented on ’s file : this better not be a rick roll: well there is only one way to check :trollface:aww. only snow on sunday...anyone has got access to windows event logs - dataset and is in a position to easily share it? could you let me know? thank you.naive regression question, has anyone ever used lowess regression to predict future values? if so any tips on implementing that in python?:  (though statsmodels is notoriously buggy, so if the module doesn’t work out of the box, i’d google other python solutions…chances are someone else has implemented it)how important is it that we show error bars or p-values in our figures since we are working with less than ideal data especially if large (yet significant) error bars take away from the aesthetic presentation of the data?: as all things in life, it depends: this seems like a philosophical question of fundamental importance.  i’m with keats on this one, for what little my opinion is worth.  \"beauty is truth, truth beauty\"hello, is there anyone who has successfully gotten a bokeh server working on an aws ec2 instance? i can get it working locally to produce interactive plots on my site but am having trouble once deploying it to aws.anyone at o’reilly strata london this week?anyone have any favorite resources for learning scala, most directly related to data pipelining jobs?scala cookbook!!has anyone had to use mongodb for analytics and dashboards? from what i understand it\\'s kind of a pain, but i\\'m curious if there are any decent resources out there for learning it. the best i\\'ve found so far is this: folks, thanks again for participating in the comp survey. results have been broken out into sheets according to seniority. i\\'d like to make this survey a somewhat regular thing, to serve as a resource for all fellows during performance review and job offer evaluation. if any of you have feedback on the kinds of questions you\\'d like to see on the survey, clarifications on the results, or other feedback - please pm me (or better yet, start a thread so people can see the discussion).thanks again!hi everyone, i was just wondering if anyone had resources for good measurement ave attribution techniques beyond ab testing? *andwhen i google measurement analytics without ab testing i only get results about ab testing :dgraphical models? amassari: looks promising, i\\'ll take a look. thanks andrea!! when did you stop being maxos :di’ll never stop being maxos! i’m maxos on the sv16c slack descriptive stats global\\'s total comp min and max are wrong because the row order of the base comp has min and max swapped, but the cell references used to compute the total comp point to the opposite cell. also the total comp min and max (didn\\'t look at the other summary stats) are not correct as computed. the totals should be calculated per individual and then the max taken on that--not taking the sum of all respondents\\' max for each individual categorythanks for pointing that outerrors fixedi;m getting a giant black box for some of the statsmeh, if natural language generation ever gets up to a human level, software developers will be out of a job, since the expressiveness of human language is a proper superset of the expressiveness of programming languages.gac r&amp;d (the company i work at) is looking for a senior data scientist: with experience in the data engineering part at production level (with knowledge of bigdata technologies) .here is the post:please contact me if you are interestedhas anyone had problems adding maven packages to their spark runtime w/ zeppelin? i tried adding comma separated repos in the maven format to thespark.jars.packages option under the zeppelin spark interpreter, but it doesn\\'t seem to be pulling in the library at runtime even after a restart.does anyone happen to have any resource materials on how to parallelize python lambda functions consuming from kinesis firehose ?get data scientist as a role. anyone who cares will read your resume, anyone too dumb (riding the hype) or lazy(or overwhelmed) to look into it will glance over it. also this:i\\'m a data engineer though, so, that\\'s just my grain of salt.a good list of machine learning resources on hn today: went for the 15$ package :slightly_smiling_face:lots of talk around this, similarly lots of talk on the pending cookie-pocalypse coming soonholy moly! this is amazing! thanks for sharing! $15 for medoes anyone have any connection to ds/ml in fashion industry in nyc? would love to grab coffee/chat. pm me if available. thanks!macys, dia&amp;co? talk to  , toodoes anyone know if avro schemas can handle unknown fields?  i know that json can do that.  was tying to find some info on this.does anyone know if this is an accurate representation of dl research?can\\'t comment as i\\'m a developer but i know the swordfish hacking scene is a pretty accurate portrayal of my life i haven\\'t used it . i came to know about it while i was presenting my insight project at yelpdoes anyone have a clear sense of what sort of data will be \"off limits\" for targeting? it seems to be a bit opaque from what i\\'ve heard. for example i don\\'t want to target anyone based on their pii, but if i have data about a user\\'s past engagement with ads (e.g. you click on a lot of native ads in finance) it would seem strange to consider that off limits for targetingthe youtube video shows him working on ipython, noice!addictiveyou\\'re welcomewhat have you done here you goi grew up in la crosse, wi! the brewery there makes crap beer, but at least we did something right.stay positive bmregnerthat sounded pretty positive to me nuggetbot. shut your mouth.poor little nuggetbotnuggetbot is a beer egalitarian and didn\\'t appreciate my beer snobberynuggetbot, o\\'douls rocks!nothing.nah nuggetbot is a bigger beer snob than the rest of ushi guys and gals. i\\'m looking for insights into this error metric called the \\'gini coefficient\\'. its routinely used in kaggle competitions (particularly related to insurance). how does one train a machine learning algorithm differently in order to get a high gini score (as against say mse or roc)? any insights will be appreciated! more about the metric here: the tree based methods in scikit-learn minimise the gini coefficient  by default, maybe that a place to startthanks chris. the \\'gini index\\' (also called gini impurity) is used in tree based classification methods to determine what a good split is. i\\'m not sure it minimizes the overall \\'gini coefficient\\' which is a different quantity. also, i\\'m working with a regression problem.okay, i see.: the _gini_ _coefficient_ is a measure of variance of a random variable. basically, it is 0 for a constant r.v. (uniform distribution) and 1 for a delta function r.v., all other positive random variables should fall in-between. i always thought of it as an approximation to the entropy. i am not really certain how you can use it to measure the _error_ of a regression.: i agree! but since that\\'s the metric used in the kaggle competition, i wanted an insight into what kinds of error metrics would give a good gini coefficient. essentially, what i want is predicted outcomes yhat which have a \\'distribution\\' similar to the actual outcomes y. the exact values f(x) don\\'t matter, only the distribution of all the yhats does (and this distribution should be close to distribution of the y\\'s). thanks.i don’t think it’s so much the distribution of yhats that’s important as their _order_so, obviously, if every yhat was exactly the corresponding y, you’d get a perfect score. but you’d get the exact same score if every yhat were double the corresponding y. and in fact, even if every yhat were the square of the actual y.however, the extent to which you’re penalized for deviations from the perfect ordering _does_ depend on the _actual_ y values.anyway, i don’t have any real answers here, but i’ll think about it a bit. uploaded a file: : this book chapter might be helpful to your questionhi all - i’m speaking at strata+hadoop in san jose on march 15 about my insight project. i have a friend of a speaker discount to the conference - dm me and i’ll share it with you!i am speaking there on the 4th. if you guys are deciding on a place, then may be bukowski, or the pour house boylston/newbury?would love to meet the insight boston crew can bootcamp be deployed on pc hardware? i was under the impression bootcamp was just for running windows on your mac.i found this to be a pretty good speed comparison of different methods to save/load dataframes: anyone got some insight on how `feather` compares?i did some quick benchmarks a few months ago. for a 50 gb dataframe i tested, mixed categorical, string, and float data, pyarrow to parquet was the fastest (available in dask and recent pandas versions). the second fastest was feather.i think pyarrow was faster because it compresses data by default, and feather does not. i could be wrong, but i think they use the same in-memory representation.anyone have experience with facebook\\'s prophet forecasting tool? i\\'m trying to determine if the scalability aspect is as good as claimed.thanks, `parquet` is indeed a good option if given a large amount of data.good to know that `feather` compares really well to all these other methods, although it\\'s still not recommended as a permanent storage method. lasso is another approach that could help me identify interactions. to be clear, i\\'m trying to find interaction effects between features for a classification problem. the point is to both tell a story and to identify the most predictive interaction terms to add to a different model.very excited to launch insight in toronto this morning! expanding to my home town has been something i\\'d always hoped we\\'d do, and today\\'s the day. to read more about insight  in canada check out the blog post we just published, and if you know any great scientists or engineers who want to work in data in canada, definitely send them our way  anyone familiar with the hidden markov model and know of a good resource to learn quickly? my boss suggested it today and i had never heard of it before.hi  dm mehi  --i have some experience with prophet.  happy to discuss.  i found the first few versions to be buggy and crash-prone, but it\\'s gotten better.  we\\'re running it in production now to forecast many thousands of time series, each with weekly or monthly frequency.i’d love to hear any responses/suggestions you get.you don\\'t need to run bootcamp on your pc.after bootcamp boots into windows, it has a mount of special apple drivers (for windows) just for apple hardware. that\\'s squirreled away in some part of the bootcamp image.which recommends:  but ymmv because that assumes you\\'re trying to make windows the main os on your mac hardware; which is not what you really want.it begins: you’re gonna scare them off_types and quickly erases “blood for the blood god\"_ data scientists for the data science godlet\\'s save the sacrificial references for tomorrow\\'s panel, ooookaaayyyy -_-i will bring the microphone and special effects boxwill there be cartoon hammer boings and fart sounds? i really hope so...for those interested in black box inference : has anyone ever tried scraping data from sites that consistently change their css class names (i guess in an attempt to block scraping...). this site i\\'m trying to get data from seems to be generating new random class/id names every hour or so, which means i can\\'t find the info i\\'m looking for that way. any clever strategies to counter that? can you drill down from a static class or id?hi alums+, anyone work or worked with google analytics data and have any tips? particularly about how to access the raw data? thanks!thanks, i’ll check this out!does anyone know something about the deprecated method .inc(word) of nltk\\'s freqdist() function? in particular, i have heard that word_freq.inc(word) has to be replaced with word_freq[word] += 1, but what if there is a second variable (which is an integer), as in: word_degree.inc(word, degree)?your probably going to want to store degree in a separate dictionaryi.e. word_freq[\\'word\\'] += 1, word_degree[\\'word\\'] = degreeor word_degree[\\'word\\'] += degree depending on what your trying to do with degreei suggest you take a look at what freqdist() does to better understand how you should be using it:  i access google analytics data via big query fairly frequently, and it works well. that may be a premium only feature, though. i am not an expert on ga generally.hi all, does anyone have experience working with druid? i\\'m trying to access a db using pydruid, but i\\'m getting stuck almost immediately...thanks a lot!i\\'m trying to create a random subsample of data from a dataset on bigquery as follows:select * from(select rand() as random, * from [fh-bigquery:reddit_comments.2015_01] order by random)limit 1000however, i get an error saying: \"resources exceeded during query execution.\" what am i doing wrong? is there a better way to accomplish this task?has anyone run into a python blockingioerror? doing a lot of image processing and think it’s running out of memory.: if you are an r user, i highly recommend the rga package. for python, i would recommend the python version of the api  you can probably put the limit 1000 right in after order by?bigquery has issues with running queries that return a lot of rows, you can circumvent that by storing the results in your own table temporarilyso many punchlineshillarioushi all, i was asked a question about the size of our corpus needed in order to have statistical significance in our models. i\\'m not entirely sure how to do that... so would you just train models with different sizes of corpus and see at which size the performance starts to level off? this also depends on how well the data represents the domain and the type of algorithms right? does the training corpus normally has a statistically significant number?what type of models?unspecified - it was a general question posted to our sales team. in general we do binary and multicass classifciation, and regression.are you thinking size of corpus needed to train a tfidfvectorizer/word2vec or number of samples needed for classification? regardless, i would plot performance vs size and see what it looks like. if it continues to increase, you could add morei\\'m thinking samples needed for classification (which we need to determine which target to classify). so this doesn\\'t sound like applying the hypothesis test to corpus right? that\\'s the other place i\\'ve heard of \"statistically significant\" being used.an academic paper on this topic that i was reading recently that you might find relevant:this is more about classification performance rather than statistical signficance, so as  said you might just subsample and look at p-values (or whatever significance test) as a function of subsampling size and look for an asymptotei ran into that paper too. thanks.the other key stats concept to read up on and keep in mind is  \"effect size\", for example, in regression, what\\'s the strength of the relationship you\\'re hoping to detect? if the effect you\\'re trying to measure is big, then you don\\'t need as much data to confirm it exists, but the more subtle the effect you\\'re trying to test for, the more data you\\'ll need to collect.always wish my stats were stronger... thanks i will read up on it! is it worth ordering a hardcopy of this if it is based on java 5.0? i was going to make a similar comment :slightly_smiling_face: even in social science a lot of people don\\'t use/don\\'t know about effect sizes.this would help get acquainted with a bunch of the object oriented principles of java, which i found helpful.  i basically complemented this book by google searches when looking up specific syntax, which after java 8 could be a bit different w/ all the fancy functional stuff it introduced in java 8.  can’t say that’s worth getting a hard-copy (but if it’s like $20, not a bad investment for a physical copy)this is so cool. hackernews didn’t really love it, saying “why not just use the api this businesses have?” but that’s the whole point, most business do not have an api except the phone!anyway, i hate calling businesses so i hope this works as well (eventually) as they demoed)can an insight employee let me know who is the best strategic partnerships point of contact (email) for desv and whatever ya\\'ll are calling devops sv?^^ yes! david@ is working a lot on devops too, for msp it\\'s ericgreat, thanks kathy!agreed, plus businesses robocall me all day every day, time for me to return the favor!hi all,hi all, i\\'m wondering if anyone has used apis for peer2peer lending sites such as lendingclub or prosper before?: ^alsowelcome to all the new fellows!ozan, i\\'ve used the lending club api before. shoot me an email at  for more infothanks  !i’m thinking about doing a project with the angellist data, maybe we can discuss nice to meet you all today - feel free to email me if you want feedback on project ideas or if you have any questions about silicon valley data science. cheers!  if anyone interested to do project on a bitcoin blockchain please let me know. there is a trend to use the blockchain as a public ledger for storing timestamps and signatures. i am thinking about analytics on the custom data that stored in the blockchain transactions (so called op_return type of transactions):  has, recommend dropping him a line at if interested, we use azkaban + gradle here at linkedin ()though probably luigi is much faster to setup and get runningi recently found a price of registrar by tld comparison site that i think is handy.  .nyc is there too but has restrictions.happy mid-autumn festival to all!!!!supermoon + eclipse tonight!yeah! it was always cloudy in austin for the eclipses... it is happening!moonrise is in like 20 minutes i think. sounds like it\\'s pretty cool on the east coast right nowit\\'s about 50% covered now!amazing sight from the east coast !it\\'s super cool!what do people here use for serializing and persisting machine learning models? sure pickling is great for a handful of models but what about a large number of  different models that are each being loaded and trained by an end user and then saved again?i have tried yhat, azure ml, and custom deployments written by software engineers i work with. i’m not sure about how many models is large for you, but those methods can work for tens of models without too much trouble.maybe someone at a bigger company will have better advice than me. :slightly_smiling_face:not at a bigger company, but this is something i’ve been tinkering with.  i’ve been having good luck with modeling frameworks that have inbuilt serialization, especially cross-language implementations (xgboost, h2o)alternately, could try passing around pmmls?: have you tried pmml?   just wasn\\'t sure how well it worked in practice and how extendable it was to custom models i.e. models they dont already have the xml defined for. also thanks for the tip about xgboost and h2o. will check them out. i haven’t, just something that came up at the new job.  at least for out of hte box stuff i think it has apis defined for the common frameworks, sklearn, xgboost, etc.xgboost and h20 have java apis so i can dump out to something that works in our engineering backend with like no overhead, it’s nicei\\'m mostly using python 2.x for no real reason. jupyterhub looks like it might be a good enough reason to switch to 3.xhaven’t used jupyterhub yet, thanks for the link. i’m in the midst of scoping out collaborative ds tools for a growing team, is anyone in love with any of the platforms out there?i am also interested in thisas a ds team of one, for the time being, i haven’t evaluated any of the collaborative tools yet, i have looked into several bi tools (looker, chartio, zoomdata, mode analytics and tableau)there was that thing that came out recently that does everything, what’s it calledalso, i’m in the python 3.x camp as of last falli imagine there’s fairly broad interest, or that it will be a topic that comes up for many teams at some point. i’m scoping out the ecosystem and talking with reps from many of the providers - i’m glad to post what i learn.i messed around with it a little, it was janky but maybe that was a result of it just having been posted to hnyeah,  looks interesting but i haven’t taken the time for a deeper dive yetdoes this deserve it’s own channel, or would it get lost as channel 28+?stay positive tcmorannuggetbot is trying sentiment analysismy initial impression of  is that it’s a pretty nifty mashup of notebook-style collaboration and github-like version control, but it’s killer-application isn’t totally obvious.experiment: fuck you, stay positive tcmoranit worked!hahahai’m a happy unicorn, no love.do you want nuggetbot to tell you to go negative?fishing for positive reinforcement. a little atta-boy, that’s all.:simple_smile:nuggetbot: image  me atta-boyanyone at this event today/yesterday? : i assume you’ve seen this too we use python 2.x with from future import thingsml in mario!  uploaded a file:  and commented: hey there! if you or anybody you know is looking for a new opportunity in europe, brainpool - a london startup founded by a founder phd colleague - is looking for a head of machine learning. feel free to pm me for an introthis is really great, snappy thoughts from a google engineer who has seen many ml systems go into production: hi to all fellows (past/present)~ i\\'m doing a project on removing blinks from photos and i was wondering if anyone knows of any databases with photos of people with eyes open and closed (with the same people so that i can test my method)? i\\'ve found some with different people but that isn\\'t quite what i\\'m looking for. do you know of anything like this? sorry, i\\'m not up to date on available databases. cool project, though ;-) thanks so much! hopefully they can release the data :smiley:hi all! new health data science fellow. i\\'m working on a project using nlp. i\\'m still waiting on getting the data for the data i\\'m consulting for. i was wondering if anyone in the ether knew of a nice primer on doing nlp in python? thanks in advance! :slightly_smiling_face:check out `spacy`, and in particular this tutorial: `spacy` is nice, but in my experience has a pretty big learning curve. depending on what you want to do, you can also look at `nltk`, `textblob` and `gensim`.definitely a +1 for `gensim`. in fact, the tutorial i linked to uses it heavily, especially for topic modeling (lda) and word2vec. `nltk` is best for general language processing (textblob is basically a wrapper around its capabilities iirc), `gensim` is better for numerical heavy lifting with vectorized texts.  there should be a copy of the nltk book in the insight office libraryi\\'ve also used `fuzzywuzzy` for fuzzy string matchingyeah `textblob` just has a slightly nicer api than `nltk` in my opinionoh yeah, simplifies all the defaults by far.  nltk is if you need to get into the internalshas the vader sentiment analyzer built in too, doesn\\'t it?oh also  could try reviving the  channelguys has anybody experience how one can read or add the *.bacpac file (microsoft azure sql backup) to my local database using unix and linux platform i\\'ll echo what others have said recommending nltk and gensim.  nltk is kind of bloated imo so i generally restrict to tools that are also in the corenlp package.  you may also find somethings in here useful: the algorithms in the datasketch package aren\\'t really optimized for performance but it should give you enough breadth to experiment at least.  for instance, we use a variant of minhash to detect algorithmically generated text...has anyone here used the isic archive? i know staplet did a skin lesion project, but not sure on the dataseti was thinking of more focusing on the melanoma vs. benign nevus anglehi y\\'all, in case anyone is thinking about buying a new mac, i happen to have a pretty big apple giftcard that i am looking to cash in :slightly_smiling_face:whatever you do, don\\'t buy a new macbook pro. those things are trash trash trash. i\\'ve been dealing with them for 4 days since starting a new job. one died on the first day. there are no (useful) ports, the touchbar is a travesty. just..ugh.what types of models do you guys typically use for a/b/n tests for seo data with factorial designs? i was thinking of a zero inflated negative binomial regression but there don\\'t seem to be any packages in python for this. does anyone have any experience in this area, and if so can you point me in the right direction?aws emr question — i am using pyspark with python 3.4 in my config as i created a new cluster.  i am using spark streaming.  i already set my pythonhashseed in all my nodes and restarted my slaves in ec2.  as i run a simple spark-submit (not as a step, but in the console), i still get `randomness of a hash string, should be disabled in pythonhashseed` error.  any thoughts?my next plan of action is this starting a new cluster and adding the pythonhashseed in configurations.can you confirm that the worker process can “see” this variable (value)?i’m not sure how you set it, and most likely there is a description on aws, but it is possible that it was not set as systemwide env varso i think this was asked, and then fell off the edge of the world, so i\\'ll ask again:does anyone know where to find *district* level elections results for the whole us? 2016 necessary, further back a big plus! :smile:thanks  .. i was able to see it as the `hadoop` user in the `hadoop` env (confirmed it by running a simple script that hashes a string). no bueno there with my spark-submit job even if i see it.  but setting it in `hadoop-env` which is apparently different from the user `hadoop` did the trick.  i added the export var upon start of a new cluster. so happy dance for me. for now. :slightly_smiling_face:: i’m curious about beaker. let me know if you think it’s worth trying: i really like it so far - it runs on top of ipython notebook (and you can even open ipython with it), but it allows you to use multiple languages and share variables between the  languages. the other thing i really like is that it is easy to annotate your cells and collapse whole sections of the notebook.: thanks! i’ll check it out: i unfortunately don\\'t have a solution for you, but would be interested in hearing about anything you find!spent half a day on these tutes yesterday:  pretty great documentation for an initial release: did you do anything with it beyond their tutorials? i\\'m wondering how easy it is to make something new with it.as long as it’s easier than theano, i’m in.not yet, just tinkering around the edges. i\\'ll let you know what i find offline if you\\'re interestedyes, i\\'m definitely interested. thanks: can you make me a recurrent neural net real quick? i’ll wait...: i\\'m assuming that they did better than theano. hinton is in this group, right? maybe they learned from past projects. it did look easier from the brief glance, but i won\\'t know until i try something. :simple_smile:it would be fun to do something with people using this!twist bioscience in san francisco is currently looking to hire an experienced data engineer (this would be a great fit for someone looking for their second post-insight job) to help organize the data collected by their custom dna synthesis manufacturing pipeline. their product is incredibly cool and the company is growing rapidly. here\\'s the actual job posting: please contact me here or at  if you or anyone else you know is interested.if you have job opportunities you\\'d like to share with the alumni network, please email or message the insight team directly! we may also be able to connect you to data scientists and data engineers with *senior-level* skills and experience! pinned a message to this channel.&gt;4+ years experience in pharma/biotech, or experience with biology/chemistry datathat\\'s a tall oder:)what about more junior level positions? i frequently get asked about jobs at that level and i’ve posted for a few. i know they are not aimed at insight alumnae/i specifically, but they are useful connections to make.great question! i think it\\'s true that most of those positions wouldn\\'t be a great fit for insight fellows or alums. feel free to post :slightly_smiling_face:and one more question - if we are interested in a senior level position, should we contact insight? is there a specific contact person?if you are interested in a senior level position, for yourself, anybody at insight is happy to talk about career growth with you! you can email any team member from your session, or send a slack message, and they can help you get connected to the best party.looking for ds infra advicehow big of a box (or cluster) should i spin up for jupyterhub? for now, mostly python with some spark. eventually gpus for some dl stuff. a data team of ~10 including a mix of de/dshey bay area ds folks! the data science teams over at fitbit wanted us to extend an invitation to insight alum for a ds happy hour they are having at their office next thursday night. more details and registration here ^sweet! i can finally ask about my interview status since they never said anything to me!playing around with neural networkswould anyone who did the remote program be willing to chat with someone finishing their phd?hey everyone, we’re announcing that insight has officially opened applications for our first data engineering session in new york city, which will begin in september!  please spread the word to any one looking to become a data engineer on the east coast::  don’t drink and derive. cool graphs: pymc?????? help neededdoes anyone have any advice about sites the are difficult to scrape ( using  software): try a headless browser like selenium for pythoninteresting article, but worried about significance of finding and authors jump to causation (\"the more they buy, the less likely the product will succeed\"):for all the physicists in ds: a follow up to our discussion in “women in tech” panel: : rand wilcox (usc psychology) and his students have done a lot of work on this (c.f., _introduction to robust estimation and hypothesis testing_). from what i recall, they generally recommend 20% trimmed means, based on simulations that attempt to jointly minimize type 1 and type 2 error rates.20% kind of makes sense conceptually, too. standard parametric tests (e.g., t-tests) generally use all the data, which is 0% trimmed. standard non-parametric rank-based tests (e.g., wilcoxon) generally use the median, which is 50% trimmed. 20% trimmed comes close to splitting the difference, erring a bit on the side of keeping more data.hope that helps.does anyone know of a decent probability course? aside from khan academy i can\\'t find much out there that has full lectures and is reasonably in depth.hi everyone! i wanted to get feedback on a way to deal with a machine learning problem that involves text classification with multi-labels. so i have text data, and they are labeled according to certain categories; so imagine for example they are labeled as: “delivery”, “complaint”, “account”, “cancellation. ”however, some of my data has multi-labels assigned to them. in other words, we have some text that is assigned more than one label (for example, it’s about both “delivery” and “complaint”). for now, to train my classification models i have been ignoring any data that has multi-labels assigned, so that whatever i fed into the model would be “pure”- aka one label. but now i’m in a situation where a significant portion of my data has multi-labels. i was thinking of trying to treat this as a regression problem- so for each label, try to predict a score… does anyone have any feedback on this, or has anyone tried other approaches? thanks!oh and regarding the above post- i would rather not go for unsupervised or generative methods for the moment...you can do any type of multi-class classification and rather than getting back the class pull out the probability for each class (labels in this case) sorry i should have mentioned i’ve been doing that… but there isn’t a way to input multi-labels for one training sample…how about using the same corpus with both labels in the training? i have to think about the drawbacks of this. what is the final outcome of the model? multilabel systems should be able to handle appropriately formatted multilabels, like with `sklearn.preprocessing.multilabelbinarizer`.  going from that to naive bayes seems naturalassigning only one label to the output or it can be up to 4 that you mentioned? the final output should be a probability score for each label... also to clarify: one data pt can have up to 3 labels assigned to itis the issue that you want the ‘target’ of the training to be all of the labels?  essentially you can train multiple classifiers one on each label in that case, if you don’t have a ton of labels.  or use some type of matrix completion methodand if the labels carry information about each other you can pass in the other labels as features when you train a model on one of themnot sure if youll have that data in your actual test data though (e.g. if some labels may be present sometimes but others missing).so you probably don’t want to do thatthe other thing you can do is to create new labels based on the combinations that take place, like complaint&amp;cancellation can be a separate label. specially, if on the business side, complaint and cancellation alone have different meanings that two of them happening at the same time. look up multi output classifier in sklearn, or train a bunch of binary classifiers in general. i did that in keras for my ingredient recognition model. that would be a great idea, except i have a lot of labels (20ish) and not all of them are in “natural” groupingssorry to throw deep learning at this, but this naturally falls onto a neural network architecture where you want to predict a final \"layer\" of your model consisting of 1\\'s and 0\\'s mapping to the multiple labels. there\\'s a conversation on the keras github about this -  it\\'s probably overkill if your data is small, thoughyeah if you want to go the nn route it’s more suited to train on vector targets where each vector is a series of 0s or 1s for each of your labelsinterestingly, that was the first thing i did with an nn, but the vector target was series of outputs from neurons in monkey brains :smile:nns on neurons - how meta :slightly_smiling_face:thanks everyone for the input!does anyone know where i can find a shapefile for all of nyc not subdivided by boroughs etc. i just want to see if coordinates are within nyc or not and all the little bridges between boroughs do not overlay with the borough shapefile. thanks!osm has a list of ways to get various shapefiles: a few places claim to have premade ones (like ) but unclear what that covers...oooh! thanks!hope it\\'s helpful! i haven\\'t worked with city level shape file in the past. mostly country levelthat was an excellent read  and it\\'s a channel by channel setting!!yes, +1 on attention to detail for slack ui teamanyone around with some experience engineering features from / analyzing cookie data?does anyone have any experience accessing the fitbit api from the aws server?solicitation for advice on distributed modeling:i’m finally getting around to doing my first actually large size distributed model training and could use some advice. i’ve got a local xgboost model that i’d like to extend to a distributed training framework. i know xgboost can be run in a distributed fashion through spark or flink, but i also know there are other algorithm options out there that may be more conveniently built in to a distributed framework (such as spark mllib or h2o, both of which have gbm algorithms).  can anyone speak to the pros/cons between these choices? does spark mllib or h2o have a significant ease of use edge? are their gbm algorithms as good in performance as xgboost? any thought are welcome. thanks!p.s. - let’s keep responses in a “thread” to avoid channel spamdanjump: dask has an xgboost implementation. from what i\\'ve seen it\\'s pretty quick and makes distributed computing a breeze. there\\'s a pycon 2017 talk about it, it seems to have matured pretty well.quick question for insight folks: considering the limit of messages in the free version of slack and the growing number of alumni, should one consider either purchasing a paid version or getting a different program? e.g., at the moment one can\\'t see anything before mid may.joao: i know for my company, it costs like $6/user/month.  not sure of all the payment options.yeah, i was looking into the paid version and considering it\\'s around 1,000 users here, it would be around $100k a year. lots of :moneybag:slack is still great for quick pinging of folks here and asking questions.yeah, but it doesn\\'t scale well.read that gitter could be a good alternative. opinions?interesting, not aware of other options like that.what are some good practices for achieving better than random log-loss for time series data that’s bucketed by months? i can easily build a model that performs well when evaluated on the total year, but i want it to achieve good performance in each month alsonot sure it still works, but to my knowledge you can scroll infinitely far back on mobileat some point, you\\'ll hit the 10,000 messages limit, even on mobile. from this current date, that limit is around may 19th.other options are definitely being actively looked at, no updates to share as of yet though! or  may want to chime in re: h20 implementation@h2o has a really good gbm and its super comparable to xgboost. btw xgboost is implemented thru h2o too. additionally, with h2o the advantage is that you do not need to refactor any code and it runs in java. so super low level and you can directly tie it to any framework you use. you just have to write a wrapper class in java (50 lines of code) and that works wonders. h2o as a platform runs distributed - meaning you can train and test in a distributed fashion. h2o in production is an independent piece of java class not needing any external imports. this class is virtually scalable to really large sizes, we have folks in many different companies, especially ebay who run h2o on cluster sizes of 2000 - 3000 machines, and then run these production code on many machines as micro services.mattermost is a slack-clone which is open-sourced and can just be run on some aws server. its not slack, but i don’t think we actually use most of the ultra-powerful slack stuff for insight chat, especially if message backup is really the most important thing. it also supports bots.in addition to the ease and flexibility that h2o provides for taking your models to production, it has distributed implementations of other algorithms as well (glm, gbm, drf, glrm). i particularly like h2o’s elastic net with coordinate descent for finding full regularization path. i would stay away from mllib unless you need svm. please feel free to pm if you have more specific questionswhat types of data exploration plots did you include in your demos for nlp based projects? shared this great wisdom with me this eveningspeed up your sshtotally unrelated, but i’ve greatly enjoyed adding plugins to my jupyter setup (up to and including vim-mode…which is godlike)- plugin manager:- vim-mode: awlong: word frequency analysis for n-grams, pos tagging, sentence semantic visualization (check out spacy)nuggetbot: what\\'s the best heater?nuggetbot: weather nycweather for new york, ny, usacurrently: snow -3.3°c/26°ftoday: snow (3–8 cm.) throughout the day.coming week: mixed precipitation today through thursday, with temperatures rising to 7°c/45°f on sunday.eek - that sounds coldbest heater = soup:  does anyone have experience running apache airflow for etl?  ^here’s an intro to doing etl w/ spark in case that helps.happy to answer questions you might have  .just found this post from a 2015 insight fellow:  fyi as far as i\\'m aware, the lowest latency for this type of thing utilizing spark streaming coupled to ml is in the neighborhood of 100-150ms in case you are latency bound.  this definitely helpsi\\'ll dm you for some more specific qs, thanks a lot, lei was actually a fellow in my cohort (though left the program early to start at thumbtack), and i reached out to him after seeing that blog. unfortunately, his example isn’t so fully fleshed out, and his experience with this stack is mainly exclusive to thumbtack, so code examples are more limited! looking to have lunch with him soon, so hopefully i can provide any other interested parties an update one way or the other.anyone have recommendations for using influxdb and running analytics on top of it? that might be nuggetbot’s most terrifying “soon” yet not too concerned with latency now, looking to give a poc during a talk i’m giving next week, but the play server is more of a “nice to have”, though it’d be sweet. considering just exporting it as pmml and going that route if it’s easier, though it’d be annoying i couldn’t figure it out.for anyone else interested, i posted on stackoverflow: feel free to comment or upvote :simple_smile:also check out  and pfa.  much better than pmml, but implementations are still slow to eek out.we are also looking into using it. i don’t know much about it yet though…i’ve got to build out an efficient data pipeline at work, but i have no idea what i’m doing. we have a dgx1 which i have access and about 200 gigs of data. what’s the proper way to set up a sql environment and parallelized analysis environment? i’m seeing a bunch of docker stuff floating around, but i kind of need a list of ‘best-practices’ or anything to kind of boot-strap myselfany advice?if ur files on s3, use athena (which is aws version of presto)?we have a local machine that’s pretty powerful, its kind of specific to deep learning, but it also has a ton of cpu power, ram, fast hard drivethe whole data set can fit into ram, and there are 32 processor cores to work withhave u looked into mapd?no, but i will, thanks for the suggestion!how many gpu cores?8 tesla p100 gpusyeah mapd sounds like it\\'d work really wellkeep me updated if u go that route cause i am looking into set it up tooalrightythanks!the ciommunity version is free and open sourcehey  so i m in the baby steps of setting it up firsti m not running spark etl yet, more like simple s3 -&gt; csv -&gt; mysqlbut i am struggling on two fronts: (1) the webserver ui is not working as expected and (2) understanding the tech stack side of the server/workers configurationright now i just installed it on a t2.micro instance on aws and didn\\'t make any changes to the config fileit says it has 4 workers, what are those 4 workers? threads on the cpu since it\\'s a single instance?did you see this link: i am not using docker containers at the momentah, okaydo you plan to use docker containers?i’m considering that route, since the dgx-1 is dockerized out of the boxget ityeah, i’d prefer that, just for support reasons/replicabilitywell good to know that it can be donebut, i also have no idea how to use docker, so i’ve got to learn that as welllolyou got the reason i m not using docker containers yetnot that i don\\'t like them just that i don\\'t know how to properly set them upyeah…growing pains. moved from an ultra-small data regime to a slightly large data regimemine is more like moved from a super big data regime with engineers doing lots of that heavy lifting work for me to a large data regime with no engineer to help me :crying_cat_face: more fun though, lots to learnnot sure if worker is a process or a thread in airflow, though should not refer to number of instances.you can also do sql queries in airflow as sqloperators (instead of python or bash operators)thanks! i will come back to some more general qs, but for now the specific issue i am running into is thisso basically i do:airflow webserver, and then i manually start a dag by unpausing it and then pressing the play buttoni m just starting a basic dag from an example... but instead of actually running i see a few odd patterns:(1) does not create logs - if i do view log in the ui*** log file isn\\'t local.*** fetching here: http://:8793/log/hello_world/dummy_task/2017-08-16t18:19:04.412977*** failed to fetch log file from worker.*** reading remote logs...*** unsupported remote log location.(2) it\\'s in running state in the webui forever though doesn\\'t really seem to actually be running(3) task instance details on the ui shows the following messagetask instance detailsdependencies blocking task from getting scheduleddependency\\treasonunknown\\tall dependencies are met but the task instance is not running. in most cases this just means that the task will probably be scheduled soon unless:- the scheduler is down or under heavy load- this task instance already ran and had it\\'s state changed manually (e.g. cleared in the ui)if this task instance does not start soon please contact your airflow administrator for assistance.i am looking for ways to debug this babushka basicallyhave you gotten any “hello world” example in airflow working with two simple tasks executed via `airflow backfill &lt;dag&gt; -s &lt;start_date&gt; -e &lt;end_date&gt;?  if not, start simple and build your way up.it actually worked only one timethen stopped workinglet me try this backfillcheck out the goai initiative: we have been working on developing gpu enabled set of ml algos for use on dgx1. the good thing is that all mentioned products will have an open source/community version.can someone remind me where to find the electronic ds library? is it on google drive?dropboxnot seeing it. would you mind sharing it out? i tried. but it won’t let me share the link since i’m not hte owner. is there a specific book you want?whoever created aliases within yaml without an explicit option to turn them off should die! :headwall:hey boston fellows and alums! just plugging this one more time. come to our data challenge kickoff tomorrow at the cic!there are prizes for the winners! cfp for this year’s nyc dataengconf is open! please submit your talk proposals here:  looking forward to seeing what you come up with!is anyone going to jupytercon in ny?you can use domino’s discount code when you register: domino20come to  and get some advice from those of us who are there :flag-eu:what kind of problem are you working on, supervised or unsupervised?does anyone have some experience setting up a model versioning system? something for tracking production models and their performance as you role out new and retire old models?we keep all our models in an s3 bucket and have a simple folder (bucket) structure of `models/&lt;model-name&gt;/v1.0/&lt;weights files&gt;`.  you can add readme and other documentations as well.  think how open source projects maintain different releases.  i would suggest that you s=start with a simple structure to fit your needs now and modify it as you need it.s3 buckets also have auto-versioning that can be turned on. i typically version by `model/model-name/date=xxxx-xx-xx/time=xxxxxx/model` and have my code use the latest one.(i find this useful for models that are auto-updated regularly)not sure how stable it is but looks like s3 is promising, thanks alldid anyone join spotify as their first ds job after insight? i would love to connect with you!no one i’m aware of  uploaded a file:  and commented: i was given consent by the boston team to post this. sorry .can anyone suggest some dashboards solutions they’ve seen used? this would be for e.g. product teams to track day by day user behavior trends, not real time streaming or anything. i work for a small startup and am trying to identify a good solution — it would be great to talk to some users of various dashboards if you’d be willing to put me in touch! thanks! i have used plotly before, and it is good for dashboards if you want to put something together quickly (but scaling it to larger and more complex data can be tricky). there are some tutorials here: redashmetabasegoogle wave is apache wave.  wrong link  ? it was an earlier re: ok, still has nothing to do with google/apache wavethat article was written 3 years ago, but so many companies have switched to ipv6 for their networks that it seems odd that nearly everyone\\'s cell phone ip is still stuck in an address range used merely for convenience. that being said, i doubt the government even needs your ip to be in their range, with  stingray systems and programs like prism, xkeyscore, muscular,  mystic, and carnivore running i don\\'t think they need much help.brew updatewhoops, sorry - wrong window!anybody have experience building up an ab testing infrastructure and/or using one of the many services out there (i.e. optimizely?)i’m interested in hearing about details of allocation and analysis pitfalls in the wildme too!i saw daniel tunkelang give a talk at an event hosted by ffl a couple years back on this, you can see his slides here: and more context here: none of this really answers the question of which is better, but it might be a helpful place to start?more here: depends on what data you want to use to make recommendations. i just used collaborative filtering with implicit feedback for my project and relied pretty heavily on working with alum ethan and his blog posts thanks so much! super helpful tips. dan, thanks for the recommendation! i\\'ll definitely take a lookthis is great, thanks for sharing these!jupyterlab entered beta today: fyi for those in the bay area: as someone who only uses jupyter occasionally, is this trying to make t more like pycharm?hmm… i doubt that’s how the developers would see it. i think jupyter is more focused on creating and sharing interactive documents. here’s the overview: thanks very much ! uploaded a file: eszti: i haven\\'t performed mapreduce, but we use gcp quite a bit and have been utilizing the google cloud community slack to ask / answer questions. you can join here: there\\'s a #bigquery channel on the slack, so you might have better luck over there thanks !does anyone know of cool public ai companies? i\\'m thinking about investing, but i\\'m concerned about the multiples on the p/es of google, apple, and amazon.nvidiai don\\'t know of any public ones other (other than those large ones)but i work for a vc firm, so my exposure is biased towards start ups :wink:unless you have some new/contrarian info, the upside of ai might already be priced into the stock?yeah, i\\'m not sure nvidia has a lot of upside. the p/e is already pretty high and it\\'s kind of hot right now.only one way to find out!but yeah, probably too late to jump on the train.go the other way and short the ones where you smell bs the consensus could still be incorrect…  people were wrong on fb, netflix, google, amazon at some point in their history. uploaded a file: at least 2016 was a good year for somebody... has a bunch of these...amd is at 10-11 right now… in january it was at 2. not sure what the ceiling is, but it keeps going up and up  and others - i’d like to collect coverage of nips 2016 all together - has anyone got some good blog posts, etc, summarizing what everyone learned? especially want to key into the parallel sessions.i found a couple of posts: , my co-worker wrote these summaries of nips 2016:- day 0/1: - day 2: - day 3: i think they were essentially him turning his stream-of-consciousness notes into a post, so the amount of detail varies depending on how interested he appears to have been in the talk.thanks! not sure if you already saw them but ross and jeremy blogged about nips on medium:         thanks!does anyone use specialized software for model management/documentation?  something along the lines of  ?hey guys if any of you are going to ibm\\'s world of watson i\\'ll be there tuesday and wednesday at the datascience, inc booth.  would be cool to meet insight fellows if anyone is planning to go.true in physics and probably everything else:i recall converting videos to gifs and that working well in flask! the only downside is that they are autoplay iircfor a quick-and-dirty way, you could also upload the video to youtube and embed the youtube html link on your pagethanks erik and ben! i\\'m going to try a couple more things, and if they don\\'t work out, i\\'ll go with the youtube solutionone related question: does anyone know if there needs to be any changes made to nginx to work with mp4 files or other media? here, it says configurations need to be changed but it\\'s a bit unclear how: p.s. i got the .gifs working by uploading them to giphy... thanks :slightly_smiling_face:: your daughter has life figured out already!amazon kinesis firehose — this is pretty useful.from greenhouse’s ceo this morning: &gt; for those of you looking for any kind of ray of light, some hope and some steel in your spine from my brother, the wisest and kindest person i know --&gt; :trump_hair::scream_cat::crying_cat_face:stay positive joyceschanit’s hard, nuggetbot, it’s hardif anyone feels in need of a sugar pick me up this morning - insight in palo alto has lots of donuts!! come by!hey all! i have no idea what i\\'m doing picking data science meetups in nyc. any suggestions?if any one is interested in attending the dataengconf in sf next week, the organizers have offered fellows a discount with code insightvip.  also, if any one would like to attend free, they’re still looking for volunteers. i believe the deal is volunteer 1/2 the time, and you get the other 1/2 to attend talks. if you’re interested in attending, dm me and i’ll connect you to the organizer.hi all! if anyone was thinking of hiring undergraduate summer interns i work at a university with over 100 students who would love to work in data science roles (and have the skills). how is this channel different from  ?yeah i guess there isn’t a difference - i missed that ch when i was looking through. mostly just called it  to match insight’s channel for the ai sessionshas anyone worked with the amazon ml service? please dm me if you\\'re able to answer a few questions about it.thanks! are some of those internships in your specific team?not sure actually--the early career programs have their own internal recruiting avenues that match candidates to one of the many teams in each cityfor anyone how is interested in interview prep on a holiday: just come to the office, we are now 7 people here ...nuggetbot animate me old news alreadyhaha. oddly appropriatewell, i’m still a fan of the product, and joe would be a great person to work with. that’s my official responseindeed...i liked the team and it would be great to work with them and there is room for things to be done.hey all!  i\\'ve set up a channel for alumni/fellows that are attending kdd in sf this week.join  to connect with people that are attending!any text analytics consultants in the dc area interested in a nasa project to predict scientific advances?  pm me.  “predict scientific advances” for nasa sounds awesome and hilarious all at the same time!sounds like a really cool project... side comment, best way to predict sci advances is to make em...also, based on my astrophysics and data science backgrounds, the results of the text analysis for nasa is going to be “nanotech ai cubesat tardigrades on mars and titan from black holes”something about tesla too at this pointthere will also be fleeting dark matter spikesi heartily agree:  awesome and hilarious.  it has been a challenge educating the customer.  hi jon! super long time! one of the ways we handle this is a bit of a manual process. we look at how the data is spread. if the data seems to be running all around the space then higher probability for an outlier. in that case we prefer to bucket the data, and run models on data buckets than actual data itself. or sometimes we use data buckets with higher weights and less weights on the data itself. now this is no book, or a source, but it is something that i follow in practice, and kinda as folks that work with me to follow too, i understand, but it is something that i do to any cybersec and fraud algo. so mabbe this helps? thanks  !thanks  !   i will probably want to ask some follow up questions.   have you tried to run your data thru h2o? ( apologies, if it sounds like a shameless plug) h2o does have standard ways of imputing. mean, median, and mode, but it also has adjecent value fills - back fills or forward fills. this way data locality is preserved. nothing replaces manual inspection, though. no i have not but sounds intriguing.   i don\\'t think we have h2o on prem and visa is very picky about exporting data.  :slightly_smiling_face: &gt;&gt;&gt; even if you go as far as building a model to impute the values - you are not adding any real information. you only use what was available in the other features. so you fixed a technical problem, but not the fundamental modeling issue....the simplest case is missing nominal/categorical values: just make it a different value and you are done. so literally, if you feature is eye color with current values ‘brown’ and ‘green’, make the value ‘noclue’ or something and you are set. thanks.  yup dealing with categorical features is the easy part...there\\'s also advice on continuous features :slightly_smiling_face::  rubicon project is hiring for a data engineer position at the toronto office, and for a data science position at the playa vista office . shoot me an email () if you\\'re interested.re-post from  on the upcoming bold/kafka meetup at intuit - all are welcome! how ai should be used to augment people rather than replace them:anyone going to this panel about hiring data teams at ga tomorrow? inquiring minds in sv-2016b “wanna be in the room where it happens\"  :thinking_face: soon/giphy soon: it’s a bit of a trip from nyc, but if you go i’d love to take a look at your notes! :slightly_smiling_face: i second  - would love to have a peak and some knowledge!: and   will do my best to \"give us a verse, drop some knowledge” as well as include hamilton lyrics in every slack response :microphone:if anyone is familiar with bayesian inference using pymc3 or multi-level modeling in general, i have a question posted on stackoverflow that i’d love to get some insight into ()thanks! ^haha, already moved as off topic, don\\'t ever change se! :wink: (at least they didn\\'t close it!)why are they so terrible about this kind of thing? i left my comment on the question when it was still under se.hey all you people working at various companies, a request:is your company (or one you know of) *\"friendly\"* towards non-engineering majors for internships? my old department (physics) is putting together a resource for their undergrads and reached out to me for a list of such companies. i of course have no idea outside my own company! :confused: data science probably preferred, but anything you can tell me would be helpful! pm if you\\'d like, or respond here, whichever!thanks! :+1:square seems pretty open, and tends to have lots of internswait  and  ... are you saying there\\'s more to data science than being really, really ridiculously good-looking?!?: i\\'m at capital one and we have a large intern program here - i\\'m sure there is a lot of public info on this, but if you need anything else, let me know and i\\'ll try to help...: whats your exact issue? i would imagine the operation is doable with any of the lib players, opencv and pillowanyone update pip on a redhat aws instance and then get distributionnotfound errors calling for pip 6.1.1?downgrading pip seems hard (i have 8.0.2)  carlos is right. opencv had an imwrite command. make sure you use an mxnx4 nd array of uint8s. oh and open cv is bgra, not rgba. and , thanks! i was under the impression that the transparency only applies to color images (i imagine a transparent greyscale image would be a mxnx2 array), so it is necessary to convert from greyscale to color first then?human strikes back:boo humans boo! :wink:agreed, `kill all humans`stay positive, i find nuggetbot’s silence there… disturbing.you see, this is why the singularity is a bad thingstay positive dstoneproofhumans are still a valuable asset to…humans?victory song of the future nuggetbot kingdomsee now, this is what i’m talking about, nuggetboti use the free versionooops restricting access may be a tricky one... i literally just share the link but we are a small company so everyone see everything.. i found this but haven\\'t used it...gotcha. i’d have to check it out.  with no access control we likely couldn’t use it does anyone have experience using praw? for some reason i can\\'t figure out searching posts.i’ve got a little experience with it, but off the bat i should warn you that reddit’s api is rate-limited to like 2 queries/sec and can only return 1000 upstream hits per query, so using it to build an extensive dataset is nontrivialfortunately, the scraping has already been done by a helpful redditor so that might be more useful: he maintains the scraper so the dump updates monthly, but building the initial dataset in like mid-2015 apparently took 10 months of the scraper runningcan someone explain how much capability and longevity the free, open source side of plotly has? interactive plotting in it seems like a game changer, even just for eda, but i don\\'t want to get locked in somehowthe paid version is no different than the free version except for 24/7 customer support and  portal (so called plotly cloud)... what do  u mean by interactive plotting?paid version also allows you to have shared online plots that are also private to your org etc.  in terms of regular plotting, online plots can be useful since you can embed them in places.  with free version you either have to set up your own plotly server, or embed the plots in things that will automatically run the javascript using plotly’s offline mode (e.g. if you save a jupyter notebook with an interactive plot, in offline you may have to re-run the code that generated it if you open the notebook at a later time - there’s a bunch of options on how to deal with that depending on yoru use case).\"paid version also allows you to have shared online plots that are also private to your org etc.\" - if i have my own private server/portal running jupyter/dash those plotly plots are also private (until one clicks on the save and edit in cloud button)... but one can also hide that...oh cooli was just scraping the subset of a not too big subreddit so it wasn\\'t taking too long for me. that\\'s pretty awesome.hi all, i am a new fellow, and about to start my project. i have a few questions: 1- can you scrape the web if the website is loading more info as you scroll down?2- i am trying to figured out a way to remove text from an image, analyze its font (y-label) and train the model with those. later, when an image is loaded, i will recommend a font type. is there anyone who is experienced or familiar with these, and can mentor me through the project?1 is definitely possible, but probably depends on the website and what you\\'re using to scrape it. i don\\'t know if there are libraries with built-in support, but what i would do is go to the network tab of my browser\\'s developer tools, see what urls are being requested when i scroll down, try to reverse engineer a pattern, and then programmatically pull those urls (which will often be json).for 1, typically that’s done using ajax requests via some sort of javascript.  if you can figure out how the page is making the ajax request, you can make it yourself or send the right arguments to it to get more resultsanother approach is to use selenium, which lets you automate web browsersif you get really good, i\\'m going to start calling you selinium i used beatifulsoup in the past on ebay. i did not use ajax.i am not sure how to download it as a json.back in the day i used  to simulate a browsercan make page clicks, enter passwords, etcthis blog post seems to be offering a few more details on the first approach i outlined (but i didn\\'t read it that carefully):  did this project when we were fellowscool. i will look into this. i got another recommendation for selenium, but selenium tutorials are apparently requiring java knowledge which i don\\'t.i don\\'t think you should need java for selenium. i\\'ve only played with it once, but i used python.this is correct. selenium and what not are fun, but you shouldn’t need them to solve this problem. just figure out the ajax and then you can procedurally generate the urls you need and scan the results with beautifulsoup.python is what i know too can you give me more information?to clarify, you might need java installed on your machine, but you shouldn\\'t need to write any java yourselfoh, ok. thank you. :slightly_smiling_face:ddto think i’ve been avoiding cobol all this time...what is cobol? =pno punchcard are best :ppunch cards are great because then you can keep your code in a filing cabinet, without risk of code loss due to hard drive failure!stay positive alex_gudestatements about things!!!awww, i was hoping nuggetbot would respond to exclamation marks. :(fail!!! :wink:damn... too positive for nuggetbot.nuggetbot what do you have against punchcards? :(risk code loss failure !!!stay positive jeremykarnowskihaha!risk code !!!loss failure !!!stay positive jeremykarnowskifailure !!!loss !!!stay positive jeremykarnowskilossstay positive jeremykarnowskihahahanegativenope...calamity ok i\\'m done imagine dropping a stack of a few hundred punch cards on the way to the compiler...yes but no risk of code loss due to hard drive failure!!! :troll: stay positive scstarktrue that: also that\\'s why you draw a diagonal line on the side of themdiagonal line? ah yes, old school hacks! learned about that one from an aged professor i worked for. : yeah! didn\\'t prof st. john tell you about how her mom used to program with punch cards?the line is to be able to preserve order; if it was vertical or zig zagged you couldn\\'t tell! no, i don\\'t remember audrey ever mentioned anything like that...how many different diagonal lines could there be? would they be still helpful in reordering a stack of a few hundred cards scattered on the floor?i don\\'t know how many there could be... i guess enough to keep them legible. plus there are four whole sides of a punch chard. you reassemble them by ordering them in the way the diagonal line mark appears on the card. since it\\'d be at a different height on each cardi will notify the appropriate persons about this.i chatted with someone that said they used to number the cards. is that right? there was also this really great chapter in a book by feynman about making computation with punch cards more robust to errors. it was pretty neat if you\\'ve never read it.cool! i heard that the problem with numbering them was that there would be issues if you wanted to insert new cards in between great advice, thanks austin! i am coordinating dataengconf nyc happening in october. this year, dataengconf will have 4 tracks: data engineering, data science, data products, and ai. please be sure to checkout our videos on youtube () if you haven\\'t heard of us! of course there will be discounts for insight students and alum :wink:i am curious in learning more about what you want to learn in your career. if you’d weigh in on our super short survey re: data eng/sci tools and topics it will help us plan our next event!gentle reminder: rule number 1 of : do not ping @ channel except in case of imminent nuclear holocaust that we could otherwise not escapeor, of course, a reminder to clean up after yourselves in the officeahh  beating me to it!!!yes.  you pinged 1000 people across four timezones. the probability of making someone cranky is high., your are forever immortalizedthere\\'s also the option to mute notifications in using the channel setting cog in the upper right, if the alerts are fucking up your zen too muchsorry for another redshift question, tis for all who have wlm tuning experience.  so, i am digging into the types of queries that are running on redshift, and noticed that a lot of `volt_tt` type queries are run. the concern is that they are running in the `default` queue, which right now i have as concurrency 1, with 1% memory.  do you think this is the reason why querying can be slow? :ty:those should finish really fast so probably noti\\'ve worked with wlm management stuff i\\'ve found the concurency of the queue has almost no effect on performanceassuming your bottleneck is cpuno cpu on the metrics chart is only like 15%, which is way lowi have a person asking about the early deadline (august 14th) for jan 2018 class -- asking if there is a regular deadline? or how should they think of the deadline -- apply as soon as possible after that? or the class is usually full by nov? or interviews start in sept?i’ve changed our cluster type from a bunch of ds2.xlarges to fewer but same amt of storage ds2.8xlarge, and how there are some weird slow drops &amp; create table problem.wel if you main queue is 1%then you might be ram constrainedhmmm… that could be, but why do you think concurrency makes no real difference?hi all, handshake is hiring for a pretty special role in sf, our first (senior, full-time) machine learning engineer (). we’re looking for someone with experience shipping (production-level) machine learning code. i really love working here and would like to get another insight alum on our data team. if you or anyone you know might be interested, feel free to dm me!well we were cpu boundso having more or less concurrency didn\\'t change how much cpu we hadand the % you give to queues only affect ramso we had a dashboard that had a lots of queries that took a few minutes, and changing that queue concurrency to 1 or 5 or 10 didn\\'t change how long it took the entire dashboard to load:nerd_face: man, redshift is not easy to tune:ty: markoh, in the end what did you implement to make your dashboard load faster?  is it giving it more ram?nah nothingit was cpu bound, nothing we could door at least nothing i tried workedif anyone is in the dc area check this out. trevor hastie and rob tibshirani are doing a 2 day course on ml. in case anyone is wondering why we bailed on academia...does anyone know whether it\\'s possible to add a time slider to a map created in folium?anyone use the sephora mobile app and have strong opinions on new desired features? so sorry for using the \"here\". was just informed that\\'s a no no. my bad.yeah but you redeem yourself a bit for doing it to advertise women in tech. :slightly_smiling_face:  also, anyone else notice that there are only 8 women shown from netflix. does this mean there are only 8 there in these related fields?  also, none is a senior data scientist (some senior, just not with that job title).  good on them for hosting this, but also feels a little sparse given the size of the company.yeah, that\\'s bad advertising. we have lots of women in big data here. i think those are just the organizers.suit, no tie is hard to go wrong with.  maybe nice jeans with buttonup and suit jacket for the startup.thanks for the responses! i feel like menswear for interviews can be even more fraught than womenswear…title should probably read *deep learning* alchemyhaha, “here’s my lead, now where’s my gold!?”lecun was mentioned in this article but i still think his response the original talk is worth reading: damn, great response!poll: how much paid paternity leave does your company offer?someone collected information on this recently you have all the answers.#disagree.  suit without tie is always appropriate for men.also: can anyone suggest good resources for learning about supply chain management / analysis?here is the sheet we crowd sourced recently: curious what happens if someone has a second baby during maternity leave at some of these 52 week placesi was introduced to an eth-zurich phd recently who’s looking for a start in data science in europe — would any europe-based alumni be up for talking with him?my team at ableto, a behavioral healthcare provider is looking to add a data scientist at our nyc office! let me know if you\\'re interested in learning more or know someone who might be. this was hilarious: has anyone created visualizations of workflow status? i\\'m trying to make a diagram of the status of the team\\'s automated jobs and their dependencies, where e.g. any jobs that fail would show up as red, and all downstream jobs would also be red. i\\'m guessing d3 is a good way to go, but could use suggestions are there existing packages that do this ,but in a way that doesn’t quite fit your need? i think d3 would work, but didn\\'t know if there were other solutions people had used, since javascript is not my strong suiti’m imagining there have to be industry solutions for this already, in some format. what is the data source for the job tracking?it\\'s mostly managed in azkabangotcha. no experience there, but in general i’d try not-d3 first (something python/r-based), and then higher-level d3 libraries that don’t require as much js if possible. there are a few libraries around that also let you create d3 visualizations via python, which could be a good route if your team is more comfortable therethanks!best page from wikileaks cia dump{◕ ◡ ◕}( •_•)&gt;⌐■-■hi all,i need to create a report that shows customers their individual results. i\\'ve written python scripts for the analytics and the text to go along with them, but i need a way to put them together so that they look nice and are fairly automated. can anyone suggest page design software that i can embed the scripts into? thanks!hey everyone, are any women going to grace hopper this fall and interested in being on a panel about career options? my co-worker had her data scientist panel member drop out at the last minute and the deadline is tomorrow at the end of the day. pm me if you are interested! i used their api to pull flight schedule information for my app. it was super useful!i also used it to disambiguate codeshare flightsthoughts from a phd who became a se:  uploaded a file:  and commented: i set up my phone to automatically record my door-to-door commute times. this plot shows how my morning commute has decreased after i moved from brooklyn to manhattan. uploaded a file:  and commented: my morning commutes usually happen before rush hour while my evening commutes are during rush hour. in addition, i have to pick up my kids from school/summer camp. this makes everything in the evening take longer, but there is still a reduction in commute time after i moved.tasker to record data, r and ggplot2 to make the plotsnice analysis josh! on sight it looks like there\\'s more variance for the manhattan points. why do you think that is? or is it just noise?looking back, it mostly has to do with whether or not i had to drop off or get my kids. the first three days after moving to manhattan,  was out of town and i had to take my kids to summer camp. so, the commute was longer.google is now a subsidiary of alphabet, inc.also, if you watch silicon valley, click on the period after \"drone delivery effort” :smiley:good easter egg !for anyone interested in multiple pairwise comparisons ()teait\\'s old but i only got the internet yesterday: aws in plain english : try nature machine intelligence cometh... i was about to comment \"so they can lock all the content behind a pay-wall, goes completely against how the fields progressed so far\", until i clicked in the link. i\\'m glad there\\'s a signature campaign but i also don\\'t know how effective they are. i worry that the lure of a nature publication will be irresistible for academicians (like in the rest of stem). combine that with all the research that happens behind the walls of proprietary technology companies, i think the field may be worse off in the end.thanks all! greatly appreciate the help :)to decrease the weight that these closed access journals carry government agencies that are funded by tax paper money should mandate that any and all research results supported by them should be published in open access journals. what five words would you use to describe your job? i got a kick out of this: yes, you need to enable x11forwarding in your ssh connectionhey , i did x11 trusted forwarding (ssh -y), i’ve also confirmed my vim has +keyboard and +xterm_keyboard, but still no dice. did you have to change any other settings?me too!does anyone have experience using  for event data tracking : so many crappy living spaces :disappointed: tbh i had always wondered why the bulidings looked so similar hello again smart data scientists! does anyone want to explain parameter sharing with me?in terms of? neural networks?yeahconvolution nn to be specificalright, sorry for the delay, had to reread something to make sure i remembered it correctlyso cnns have \"layers\", but they\\'re really volumes. a conv layer might have say 55x55 \"pixels\" and then something like 25 filtersso it\\'s a 55x55x25 layerso that\\'s a ton of neurons, and worse, each other is connected to a bunch of neurons in the previous layerstay positive alex_gudeso you\\'re going to have something like 55x55x25x(3x3x3 neurons in the previous layer + 1 bias) weightsnow that\\'s a lotparameter sharing reduces this by remove the 55x55 partthe assumption you\\'re making their is that the location on the imagine doesn\\'t determine how important a filter isa filter is either important, or notso we can reduce the parameters from the 55x55 \"pixels\" down to a single oneso, that\\'s the basicswas there anything more specific? :slightly_smiling_face: there isn\\'t too much more to know, but we\\'re also near the edge of my knowledgethat’s right, alex, weight sharing is important because it constrains all nodes in a convolutional layer to detect the _same_ feature across the entire image. building out many conv layers to detect different “building block” shapes is often called feature mapping.  this article explains the concept pretty well: how exactly does it \"reduce it\"?to compute your convolution over the original data set you still have to scan the filters over the original dataset right?right, it doesn\\'t reduce the computation, just the number of parameters you are using to define your model.ohbasically, if you wanted you could have every single neuron have it\\'s own value, but instead you say \"all the ones in this filter are going to share\"so why is the 55x55 ever considered a parameter?if it\\'s part of the original datasetisn\\'t it really something you can\\'t \"tune\"?55x55 is the number of \"pixels\" you have at that point (although they aren\\'t really pixels, they\\'re ... i don\\'t know what we call them, probably just neurons)i think block is the world i\\'d use in my head, hahright, they\\'re 3x3x3 blocks in your examplewell, not exactly :slightly_smiling_face:let me see if i can find a pictureso you have a layer that feeds into your current onbut you don\\'t link a neuron to every part of the previous layerjust a tiny partright, the 3x3x3 blockwell, all the possible 3x3x3 blocksof which there are 55x55 of in your example, right?let\\'s take this to pm so as not to spam general :wink:eric, that link is fantastic!yeah that\\'s a sweet book. also has a good explanation of my question, hah.i think block is appropriate. in video/image processing there\\'s a concept of a macroblockawesome discussion! so the dude who wrote that book is an aussie #justsayinhe was a mate\\'s phd supervisor #coolbyassociation?hi all, i”m a current insight fellow trying to use scrapy to get some text and image info from a website. i’m running into a problem with scrapy not being able to deal with a “#” in the url. specifically, .any suggests are greatly welcome! see if get an opportunity to talk to  or  they both worked on something similar. or mabbe even the same library.for the incoming de fellows:\"we’re in the middle of a data engineering talent shortage\" i’d consider pinging  about data needs of any kind, particularly financialthank you! very helpful. :blush:has anyone ever seen a spreadsheet integrated into a trello card? i know there is a way to attach, for example, a google spreadsheet, but this is just a link to the google spreadsheet. i was wondering if there was a way to have an embedded spreadsheet that you can alter within trello.are there any blockchain experts out there? knows a thing or two igor barinov ( ) works with blockchaindoes anyone here know of any python packages that can be used to do type ii regression? (a.k.a. model ii regression, or reduced major axis regression, and other such names)i’d rather not switch to r just for this but i can’t find a package that does it for python.blogpost about salary negotiations for engineers written by an engineer. kudos to  for bringing it up. those numbers aren’t salary, they’re median pay package (salary + bonus + equity)…wsj had an article about this a while back if you want more info.what happens when you make a neural network hallucinate? : just in case you hadn’t seen this paper:: good find, thanks for sharingi am trying to look for the panama papers data. this data was on github for a brief moment and then removed.anyone knows where else we might find it?that isn’t the whole data set. just some summaries i believe.: thanks!certainly not all of it, ashrith, it\\'s like 2tb: i thought so too. i would like to think that github data had atleast a part of the data.thank you all for your suggestions for web scraping!i really liked bayesian methods for hackers when i needed a crash course for an interview: : save that for june 1st. we\\'ll have about 80-90 new fellows joining this slack channel!nuggetbot has plenty more where that came from:simple_smile:that bot can really dig deep for those nuggets.nuggetbot: animate me welcome unicornhahahaha wtfstay positive laurenbagreed: you heard the bot. stay positive! :smile:lolol hahahwtf now tells nuggetbot to tell me to stay positive?or apparently just wtf nuggetbot!stay positive laurenbhahahahahahilarious:smile:i just ordered the book.thanks ! i like that it has a lot of information and examplesanybody has experience using highcharts on  nyc map?what do you mean by \"highcharts\"?  i\\'ve been doing some stuff with cartodb:  highcharts is a thingcurses, slack, you ruin itfor anyone interested in the upcoming dataengconf in sf on april 7-8, the organizers have given us a 20% off promo code: hakka20xhey everyone! we’re out of storage space. everyone should please go and review the files they have uploaded and delete the ones that are not needed.i think we should also think about making this premium though.amassari: cleaned out mine! how do i check what i ahve uploaded?nm, found it: it was from teh 3 dots menu on the top right for anyone trying to find iti went ahead and deleted all the game of thrones episodes i\\'ve been hosting hereare new files blocked? or are the older files simply falling off?it’s not obvious to me (i got an ambiguous message) but it seems like the new ones are threatenedanyone have any favorite resources for principles of good data visualization? looking for both books and good articles.i\\'ve run into \"there is no space, delete old files\" when trying to upload new ones.exactly but then it let me upload it anyway…confusinglast time i ran into it (months and months ago) it refused to let me upload  one of the best that i still use.  and also there\\'s an upper limit on the number of messages that is stored, around 10,000 i think, which translates into roughly 3 weeks of data (ever decreasing due to later cohorts).premium is not feasible, considering the costs of ~$8 per user per month.unless you can score a discount ;)the solution is to either set up a tool to archive past messages and then delete them, or an alternative client more appropriate for the number of users we currently have like hipchatoh no, i’ve linked the above article, not to showcase snapshot, but just how people would do kind of test their datayou can also try to query the data vol right on mysql &amp; s3, and compare to what’s loaded to redshifta simple count on all the table could work (and be done programatically)hey folks any of y\\'all at the office?  i\\'m downstairs without a key cardhey guys. what is your favorite methods for clustering word vectors (ala word2vec)?and why?clustering for what purpose? with out getting into to much detail, i have short phrase representing expenses from our customers (eg. ‘iphone subscription’, ‘magazine subscriptions’, ‘car maintenance’). using this, information  i want to identify the key categories of expenses. so given a largish list of expenses phrases, cluster them into a smaller more representative list. i’m using affinity propagation, but i wanted advice on algorithmically assessing cluster performance. it seems like the word vectors wouldn’t cluster particularly well since the vectorization would be so sparse — ie, the point similarity would either be very close (for nearly identical phrases) or basically zero for everything else for having no common tokens two quick thoughts - you could try using word movers distance to get phrase similarity and then cluster the resulting similarity matrix - gensim has a wmd implementationyou could also use tsne if your word vectors are not too much more than 100dims - though tsne is more for visualization.if the phrases are that short, i would think a simple bag of words approach might work pretty well. how many times do you see car in your corpus? how many times do you see subscription? use the top counts to set your categories. is there a particular reason you need a more complicated approach? interesting, i wasn’t familiar with wmd… that may well solve the issue with word-vector orthogonalityquestion from a current fellow here: i’m working on my web app. i have a text file of web addresses that i would like to read in and turn into clickable urls. however, my knowledge of flask/bootstrap/jquery is rudimentry and the other fellows here don’t know how to do it either. could anyone here help me read a text document using jquery?   thanks for the recommendations. for some our problem, we’ve found that a word2vec model works reasonably well for word vector generation and have decent performance (qualitatively) with clustering using affinity propagation. i’ve thought about using tsne, however i was under the impression, as well that it’s good for visualizing the clusters in 2d. (maybe 3d). i’ll look into wmd. thanks. i did not even consider using simple bag of words… i’ll think about that. thanks! is there any reason you can\\'t dump it back into python and parse it on that side then return a list of strings to the web side?: have you tried this  ?i\\'m not sure if i understand the question, but missing not at random is a common problem and i\\'m sure there is at least some info out there on how it\\'s typically dealt with. the last chapter of paul allison\\'s book \"missing data\" deals with data not missing at random.  it might be a place to start.  someone on insight slack recommended it to me a few weeks back.   that’s pretty awesome can you give a bit more details on the process you ran through nips ?kevinmercurio: i’ve not looked in to anything myself, but i wonder if multi-user jupyter notebooks exist or you could gerry-rig one?i’ve heard of jupyter hub:  . not sure if that acutally allows 2 users viewing the same notebook at the same time though. i could imagine hacking together some scripting along the lines of:1) candidate logs in to a notebook on your server2) the notebook is configured to repetitively save3) a script running on the server watches for file updates and refreshes a backup copy of the file4) you view the backup notebook and figure out how to automatically reload any changes to disknot sure if that would actually work or how to do all the steps. but surely something hacky is possible if a real product along those lines doesn’t exist alreadydid some quick googing. the following thread describes that jupyterlab is working on such a feature: and it’s possible a first version exists build on top of google drive.  i found this website:which links to this depreciated repo:which says they are now working with jupyter in this repo:which hasn’t been updated in a year. but says in a issue comment that they are working with jupyterlab in this repo:so the take home is:a) one of the above links might have a working shared jupyter notebook on top of drive solutionb) follow jupyterlab as it develops because it looks like they’re building this functionality some possible options here? let me know if you find anything that works for you. i’d be interested in using similar for pair-programming type work as welli wasn\\'t really involved in that, but as far as i know you just submit a proposal for a competition, and they review it ...sorry, that is not very helpful :stuck_out_tongue:hey - suddenly upon re-reading your question i see that you aren’t actually looking for collaborative. just a different content test. sorry…i guess you can disregard my comments. i was thinking you wanted an environment for remote pair programming tests with pandas/plotting you could always make a jupyter notebook with a set of tests/questions. i think they offer authentication well these days, so you could open a server and just give them an access account. but i imagine part of your motivation might be to save yourself the trouble of putting together content.for  me mathematician / statistician statisticiananyone in sf want to come drink wine and listen to a qa with founder of angellist today?when? definitely interested.does any one have some good helpful resources for making recommendations systems?starts at 5. at 405 howardnbc did a data dump of 200,000 tweets from russian bots. it\\'s a fun nlp project to play around with. : for the january session(s) fellows out there interviewing and considering offers. if you want to talk to someone that is the sole data scientist at their company, my slack-door is always open!hey all,  a friend of mine, who is a data scientist at jp morgan and a columbia physics phd,  is participating in  . their front-end guy dropped out and they need someone. if you\\'re interested, let me know. the ticket would be free too! this would be a great networking opportunity, and more importantly, a lot of fun.anyone else attending aws summit today in sf?so amazon is launching a “amazon machine learning” as a service ...interesting... backed by spark+mllib or another stack?\"amazon machine learning is currently available in the us-east-1 region.\"no details on the engines behind it — pretty normal for awsha ha ha — they charge by the data point and predictionindependent of which model you use?it doesn’t specify price schedules by model … they price based on the amount of time consumed and other related compute resourceshere are details from the developers guide on their “types of models” availablewow, this stuff is expensiveanyone else at the bay area deep learning school today?i tried to get in, bit didn\\'t make the cutoffit is available for live streaming as well, fyialso  channel existsdefinitely seeing more companies looking for that machine learning engineer or hybrid ds/dewhere can we add our data? though sadly n=1 for the midwest may not be terribly useful :(every company looking for the #unicornanyone else at the women in analytics conference at facebook today?? (also, plug for the  channel):plus_one: midwest! (chicago)half of my team is there!nice!!hi all, one of our favorite insight mentors derek steer (co-founder/ceo of mode analytics) is doing a quora session. he has a ton of experience and great perspective on data science and analytics throughout the tech industry. the link to ask questions is: , and he will be answering questions/chatting on 4/17 at 2pm. considering the amount of experience on this board, it would be nice if we could get more interesting questions on there than “how does mode analytics work?” :face_with_rolling_eyes:i was here and i saw you! missed you but it was an awesome conference. i hope they\\'ll do it again next year.&gt;women make up 26% of all data science positions. assume _p_ is iid generated from a prior beta distribution. what is the probability your company hires four female data scientists in a row?&gt; feel free to use `pymc`. you have 30 mins.perhaps more importantly, what fraction of them will be promoted within two years... :thinking_face:did you ever found a workaround to this? i\\'m starting a webscraping project this week and don\\'t know yet if my target sites use distil. this looks potentially helpful though: does anyone have experience with making webapps (backed by flask) with an output of a dowloadable file such as a csv?random question: who is responsible for nuggetbot? i want to make a similar one for my team so looking for pointers :slightly_smiling_face: i would start here, i believe a fellow long ago forked the repo and is running it somewhere on insight’s aws resources:awesome, thanks! since the flask logic is in python, you should be able to use basic python commands to write to a file on the file server (like pandas\\' .to_csv()), and then provide a download link in the web template. does that answer your question?yes thanks! i\\'m very close! that was my approach i\\'m pretty html illiterate so just wasn\\'t sure if i could just pass the csv as {{variable}} to my html. is that basically the same as a downloadable link? we have a fellow working on a cool slackbot this session,  . _slack police: you have the right to stay on topic._ yep! there can be weird stuff with permissions and the app designated to open something, but it should work for a csv - just put it where your html and image files are, and make an http:// link to it like you would to one of them. aren\\'t you responsible for nuggetbot?yep! it is just an instance of  running on herokufriederike: thanks!thanks!can anyone recommend a good interactive python tutorial?  i\\'m looking for something like leetcode where they give you exercises to complete on an  editor/interpreter that gives immediate feedback on syntax and correctness, but is structured as an overview of the language.  and i don\\'t want something for non-programmers that teaches what variables and for loops are.i\\'ve found the codecademy course  but it\\'s too slow paced and watered down.  i\\'m hoping for something i can knock out in an afternoon.:  its not interactive but a pretty good place to start python. : i haven’t taken it, but everyone speaks highly of google’s python tutorial.  it might start out basic but i think it ramps up quicker than the codecademy one.for anyone interested in new projects, flink is the newest general processing framework competing with spark and hadoop, which is growing in popularity (especially in europe where it’s from).  unlike spark, it uses true stream processing (similar to storm) rather than micro-batches, but also handles batch, graph, and ml processing.  it’s still new, but growing in popularity quickly!  check out  to learn more: thanks for the python book suggestion: the google python tutorial looks just about right.  i\\'ll try it out.  thanks!i had also found this other tutorial, but the google one looks better organized with fuller explanations:  :  not sure if this is what your looking for but it might help : thanks for the link.  i saw that earlier, but i\\'m looking for something more structured.i want to write blog posts on some interesting data science problems that i get to work on. i was wondering what avenues would you recommend? i was thinking of github, but unless someone actually comes to my github page, its unlikely i would get a lot of audience. any suggestions besides that?doesn\\'t insight like alumni blog posts?: &lt;&gt; insight :blue_heart: alumni blog posts!i assume that since the heart is blue, that means insight loves alumni blog posts in a chilly, passive-aggressive way.stay positive vmgehman(that was a joke, nuggetbot.):laughing:somebody fix we could always delete him… he is not self-aware (yet)yeah, kill nuggetbot!surprised that didn\\'t get a response...i bet this will work:nuggetbot is passive-aggressive.nuggetbot is chilly?huh...blue:thinking_face:assume?heart!man…  where are you, nuggetbot?maybe he auto-destructed and he was self aware…wouldn’t that violate the third law?nuggetbot animate self terminateit took me a really long time to process what i was watching there...yeah. for a moment, i was like, “where are you taking me, nuggetbot?\"also wasn’t the point of that scene that he couldn’t self-terminate so sarah connor had to do it for him?  get your shit together nuggetbotstay positive johnrwalkshitstay positive vmgehmanthat did it...for a moment there i though nuggetbot is say ‘talk positive\\':  use medium! they cross promote blogs throughout their platform. also insight is moving to it :smiley:: do you run the supervisor as root? if so, the root user will not see any local installations for the ec2-user (ubuntu). first try to “sudo su” (to switch the user to root, you will see # at the prompt instead of $ telling you are now the superuser) and try to run the needed command to start your server, make a note of the error. most likely you will need to install your needed items (python, etc.) as root...i’ve also seen arguments against universal income.  universal healthcare though, is pretty awesome afaict that article misses a big point against universal income, which has nothing to do with the cost or the hand-wringing concern about people getting lazy if they\\'re not forced to work (a bad thing??); the idea that the problems that communities of poor people have can be solved by throwing money at them seems like a capitalist fantasy -- imo the reality is that the existence of social safety nets in a broader sense is far more importantbut that\\'s just me`seize the means of production` or smththere have been a lot of studies lately showing that the most effective forms of charity are generally to just give people money and trust them. but of course all studies have biasespretty strong evidence that it works very well (this is one of the most efficient charities on this planet, in terms of lives saved or improved per dollar)(btw, these guys:  are doing good work on rigorously evaluating charities for effectiveness)cool, thanks for sharing the giving linkdoes anyone have any advice for choosing analyses or machine learning methods for open ended questions? it seems like the best method is rarely known a priori. so far the only advice i\\'ve received is to stick to any method i suggest and provide support, but occasionally it seems like they want a specific method or approach to some of these questions and i\\'m curious if there is some standard approach that offers more optimal choices.do you mean like in an interview? i would say always consider precisely what you want out of a model.  some models are more interpretable than others.  also always start simple -- a 1-day answer anywhere in the ballpark is better than a 1-week answer that\\'s correct.  maybe the best approach is to lay out all the options you can think of, then systematically evaluate their strengths and weaknesses (out loud in the case of an interview).yes, for interview questions. i\\'m pretty sure i had the same awful problem!  i\\'ve erased it from my memory but i can try to look up my inelegant work-aroundstay positive jaredg```import osfilename = os.environ.get(\\'pythonstartup\\')if filename and os.path.isfile(filename):    with open(filename) as fobj:       startup_file = fobj.read()    exec(startup_file)```i ended up doing something like this at the top of both my __init__.py and views.py filesit executes a startup file that then finds the right directory like so:```for root, dirs, files in os.walk(other_pythondir1):    sys.path.append(root)```i actually put those last 2 lines in __init__.py file as well, just for good measure.  good luck!hahaa colleague of mine has an open position for data sciences manager at proteus.  let me know if you\\'re interested and i\\'ll put you in touch!  finally. i remember someone telling me  2 years ago they were about to ipo. :stuck_out_tongue:amazon aws selected insight as one of their hot startups of july  probably because insight forces fellows to open aws accounts and then the fellows don\\'t cancel them and so aws gets that 51 cents a month for years.hey all, if anyone in the sf bay area is looking for an experienced data scientist (&amp; phd astrophysicist) who was recently chief data scientist at a cybersecurity startup, let me know cuz i might have just who you’re lookin for! thanks!!anyone have experience with retrieving historical weather data? i want simple information such as temp and precipitation but a lot of sites limit the number of api calls and don\\'t let you batch retrieve. thanks!!!!! by batch i meant enter a range of dates for a single call. merci!!: i do. how far back? what areas?hi josh! thanks so much for responding!! my general hope is to predict aspects of the coming growing season in locations people enter where they are starting or interested in starting a farm. so i\\'m not sure how far back i would have to go to make a good prediction, but for now i\\'m thinking of just focusing on hudson, ny and 10-20 years back.noaa weather historyi wrote a short script to scrape it if you’d like that.: feel free to dm me as well — i used weather data in my project if  suggestion didn’t get at what you need.you’ll need to select the weather stations you want.do any of the current fellows own or have owned bitcoins?i’m fairly certain fcc complaints can be accessed electronically, but all i’m finding at the moment is a weekly csv summary file.  any leads on where to look?(looking for full text of complaints + date/time, etc, etc.): ask :  yes that sounds a bit like  \\'s project. you might ask her tooi used noaa weather histories as  recommends.  from what i could tell it is the richest free source of data.  kinda a pain to transform the flat fixed width files, sometimes sparse with precipitation, but not bad.i\\'ve heard noaa also provides spline interpolated data down to 5mi x 5mi at 1 hour intervals if you want to avoid figuring out attribution to a particular weather station.  never used it though.there is also a commercial airport weather data set that will give you basically global coverage but since it\\'s based on airports (which move around and don\\'t conform to international standards) it can be messy to work with.here is 30yr averaged data from noaa if that is useful: noaa density of weather stations mapped on pretty well to population density from what i remember.deep learning + stealing everyones pinshi everyone! who is the expert on working with pdf files in python? i would love to talk to you for about 10 minutes.: has anyone out there successfully gained access to instagram\\'s api? it\\'s a complicated process and i keep getting errors trying to get an access token. thanks!: how did you access your instagram data?i think i actually just got one. but i haven\\'t tried using it yet...does anybody know of a place where i can get political party affiliation data? the 2016ny channel doesn\\'t seem to have gained momentum yet so i write here: you mean of lawmakers?  and  have gotten such dataalso looking for possible data: does anyone know if there are public data sources of deleted twitter accounts / tweets, blocked twitter accounts, or other similar info?  (besides monitoring twitter for a week to see what gets deleted, of course.): no.  all i need is the percent (registered) democrats and republicans at the county level.hi everyone! does anybody have experience with text extracting from pdf? : try \\'pdftotext\\'     see:  : i\\'ve only tried unsuccessfully so far (a few years ago), but i\\'ll probably need to figure this out myself. there are at least a few python tools out there (pypdf2, slate,...): sunlight foundation is probably what you want.  their api is pretty good and they have lots of python tools., i\\'ve done it, as far as i remember if the pdf is a \\'text\\' pdf, pdftotext (in what used to be called poppler-lib, i think) works. if not, you can use tesseract to do ocr on them, then clean it up. beware that this works ok with text, but not tables.ahh,  had already answered that, and with a link... sorry!it\\'s looking to me like pdfminer3k is the only text extraction module for python 3 (which make it worth sticking with python 2). if you want to talk to someone that worked on this at sunlight foundation (  ) suggestion, lmk, i have a friend that did this work there.   thank you guys :) the pdftotext works well.that looks awesome don\\'t it though? i\\'m thinking about volunteering.i already sent them an email :simple_smile:woah, look at you!nuggetbot: animate me enthusiastic volunteerperfectthat\\'s exactly how i felthair in my face. fighting some storm trooper looking guys.low tech storm trooperhunger games storm troopersweek 3 demo day here at insight right now! anyone have any memories/words of support to share with the current fellows?don\\'t read your slideslife event: first pizza since insightinteresting podcast on terrible interview experiences, with joel grus. would be curious to hear more about terrible interviews from other insight alums. :sweat_smile:hi there! demandbase is hosting the next bay area nlp meetup on wed., june 21st, and we\\'re looking for a speaker. i know the timing is short, but i\\'m hoping you or someone you know already has a presentation from another meetup or conference. the group is very laid back and typically discusses \"real world\" (or at-scale) applications of nlp, rather than overly technical/academic aspects. any data science topic will do, even if it\\'s only tangentially related to nlp. another speaker fell through -- i may give a short \"explainer\" talk on word2vec but we\\'re hoping to pad it out with another presentation. there will be food, drinks, and networking opportunities, please send a dm if you can present :wink:anyone want to see dragon boat festival next weekend (june 10-11th)?   \"if you are thinking only about communication as having the perfect message regardless of how it lands on your audience, then you\\'re likely just to be spraying information at them and not really saying something to them that sticks....that\\'s what i\\'m trying to get scientists to do — to share that excitement and passion that they have with those of us who don\\'t do that for a living.\"i’m guessing that this has come up before, but i am not be able to search in past messages. does insight have any plans on purchasing a slack pro account, so that we can see past messages?: please keep sharing good articles you may come across. it always awesome to read awesome stuff. thanksdoes any one know a good online course that covers time series analysis?don’t know of a fully structured course, but this online textbook is a decent introduction: : thanks.  i have worked through that website but i was wondering if there was more of a coursera type course?: at some point we could even aspire to become a newsletter!: i don’t know of any myself. material like arima and var models are usually covered in econometrics classes. you can find some video-taped lectures on youtube, but i can’t attest to the quality or whether you’d get more out of them than the oxtext uploaded a file: i love this job posting. too bad the salary is so low.: which actually brings up another question.  does anyone have any recommendations for an online course that does finance/economic focused data science?: i can recommend somethings since i\\'m going through them nowit depends on what you\\'re interested in though.  finance as a topic is pretty broad.apologies if this has been covered already, i searched and didn\\'t get any hits: do people have ways of making dashboards that they like? our top contenders right now are wagon () and looker (), but i\\'m happy to check out other things.eventually, data will be piped into an internal ui so at the moment we don\\'t want to build anything (like a webapp), we just want an existing thing that non-technical people can look at without getting confused and technical people can use without having to learn anything new.there are other tools like mode that look great, but that have to access data in our aws and that violates the security agreements we have with our customers so we want a tool that we can run internally. also, this is not for *doing* analytics--the crunching happens in hive--it\\'s just to see some charts, so the data involved is tiny.thanks!mio: shiny is a very good option; it\\'s become very robust and also has a nice shinydashboards package that makes it super easy to make dashboards in particularcf you can definitely plug r into hivethere is a python version of shiny called spyre made by folks at the next big sound, though can’t say i’ve used it:no hadley no crystay positive mpanciai have used spyre in the past and it works pretty well for quick dashboards.sweet, i\\'ll check \\'em out!thanks    awesome, this is eally helpful! :slightly_smiling_face:and going public! :tada:let\\'s talk conferences - any clear winners for data science generalists?  also, i came across this local one, anybody heard of it? i really liked a pydata conference i went to. nice mix of everything - ml, tools, engineering, visualization, deep learning, etc… and all with an extremely applied focus also gave an amazing talk there: aw shucksyou’ve revealed my bias :slightly_smiling_face:hey, i\\'ll take it!great talk ! dia &amp; co visited my session, they seemed really coolthanks!hi friends,my company @point of care, is looking for a data scientist. the position includes exciting opportunities to explore, analyze and model patient and clinician data that is powered by numerous clinician and patient companion apps. this is a chance to work with great people at a well funded, dynamic healthcare technology company. please, message me if you are interested. hey all, this is a really vague debugging question about pyspark but does anyone have any ideas why the exact same code for a spark sql query from a jupyter notebook would run 10 times faster if copy pasted into a pyspark shell?same spark context in both cases? it’s a wild guess but the spark context might be initialized to localhost in the jupyter notebookand/or just not connected to the same clusterhi all, questions around feature importance from random forest.we are building an explanatory model to find out risk drivers of employee safety. we have chosen random forest classification to fit an injury binary factor with features around work hours, training, weather, etc.. roughly speaking, i plan to report that the most important features reported from the random forest model are the drivers. in terms of libraries we have been using *randomforest*, *rfpermute* and *party* in r. i am not a statistician by training. i have read every discussion online and all library manuals, but still cannot get my head around these questions.1. the 2 most commonly used metrics are *gini impurity importance* and *permutation accuracy importance*.\\ta. which one is a better metrics in my use case?\\tb. when the ranking from these 2 metrics disagree, what should i do?2. rfpermute and party also report *p-value* for each feature,\\ta. should p-value play any role in selecting important features? why do we want to know p-value in feature importance?\\tb. what does it mean when a feature is important, but it’s p-value is high?anyone here has experience using apache arrow?pyhive doesn\\'t allow you to use passwords, do these?we\\'ve been experimenting with it mostly from the pyarrow angle.  overall i\\'ve been very happy with it but there are still some features missing that will be nice.  the team that is building it is very good.  wes mckinney is a badass.  i\\'m also excited about the plasma in-memory store but haven\\'t played with it yet.what type of server side configuration do you need to take advantage of it\\'s in mem distributed advantages? are you using it on an emr cluster?i\\'m pretty sure impyla supports kerberos.  it\\'s been a year since i looked at it though. caveat that i\\'m *not* a statistician, so if anyone else has better answers, i\\'ll happily defer to them.1. a quick search suggests that permutation accuracy is probably better for explaining individual feature contribution, but that gini impurity is more common and good for assessing general feature importance. unless the distinction is important for your application, i wouldn\\'t spend time worrying about the difference.2. i might look at how the p-value/z-score *compares* for each feature relative to others, but i wouldn\\'t assign too much meaning to it individually.nothing that complicated yet.  mostly just using it as fast serialization for pandas dataframes.  if you do some work in those directions let me know.  we\\'re mostly waiting for it to be more closely integrated with spark so we can consolidate our codebase to just pyspark.i am not a statistician either- blind leading the blind.  but p value is a measure of the strength of evidence.  so you can have a feature where there is weak evidence of a strong relationship.  or you can have strong evidence of a weak relationship. get it? feature importance measures the strength of the relationship and p-value measures the probability that the feature is significant (reject the null) under the null hypotehis.  i.e. pval = p(reject the null | h0).  i.e., how likely are you to observe a deviation from the null hypothesis under the null being true...in the lr context, one can have a beta coefficient to be very high but a pvalue to be non significantfrom what i understand, the feature importance from tree based models like random forrest is the measure of a feature predictive power. but does that mean the feature with more predictive power is more important? to me, not necessarily. i would go back to the team who is trying to make decisions based on the feature importance and ask them what really importance means. in order words, what are the actions going to be taken. it is sometimes cost/revenue related and some other times, correcting/optimizing some operational steps. once you narrow it down to the relevant features, i would suggest running a linear model with scaled features and look at the magnitude of coefficients. higher magnitude, higher impact.also with regards to this last suggestion by , the signs of those coeffs provides the direction of the effect. hi everyone, i\\'m a former fellow looking for a roommate, preferably female, interested in living in the south san francisco area. if interested, please message me. thanks!can anyone on this channel provide pointers for interviewing with the facebook infrastructure team?practice writing lots of sql.  case statements, rank, lag, lead, pivoting columns.  etc.  not that they would ask anything particularly, but you don’t want to be underprepared if they do.: thanks! i played around a lot with graph theory in my ph.d, what\\'s up?chiming in to say the touch bar is not a problem. i never use it and don\\'t even notice it\\'s theresorry i missed this message, as  already mentioned fuzzywuzzy is your friend. it is fantastic for exactly this purpose but you would have to make sure you also install the python-levenshtein package as well as it can get very time expensive quickly. feel free  to pm me if you need more detail or questions.hi! has anyone here worked with census data? or do you know anyone who has?i have, a little bit yup! polisci background, so i\\'m familiar. what q\\'s do you have? hi. i’m interested in finding population density and income level in sf neighborhoods. but cant find a geographical identifier or zipcode to find the stats for a neighborhoodall i have is pumas and i cant find the dictionary for them to know which is whichif you go to american fact finder, you can specify the dataset you want (decennial census, acs, etc), the specific variables, and the unit of analysis (state, county, zip code, etc. i think block-level is also available), which you can export to csv. there\\'s also a census api if you prefer. hi -- does anyone have experience converting lots of messy addresses to postal service standard addresses? for example, converting \"905 east 6th street\" to \"905 e 6th st\"? that is a trivial example but i have addresses that are messy in a variety of ways. i believe there is a plug-in for excel that can validate the address fields and by doing so it will populate the validated addresses (in a standard format) in additional rows - i believe it\\'s called yadress or something along those linesyou can probably also geocode them in qgis but i\\'m not sure if it will provide a set of standardized addresses as output apart from the long and lat resultsthis table has maps of the puma codes by county: thanks -- this looks like it will do what i need. i got a vba error though (compile error:can\\'t find project or library(yaddressform 41:8)census tracts and census blocks are smaller geographical units that i and others ( and ) have used in insight projects, if those buzzwords might help…carola: hmmm perhaps try googling the error to see if anyone has had the same issue? i\\'ve actually never used that plug-in (just remembered it existed). sorry!no worries, of course i googled it already. thanks for the suggestion!thanks so much! i think i’ll focus on tracts and blocks. i’ll just need to find the lat and long for the center of each tracti believe usps has a web api for this: i don\\'t know how much mess it can clean up thoughthe usps api is decent, there\\'s also the `usaddress` package on github that does essentially pos tagging and parsing on addresses.for small dataset, you can also look into using google’s geo api oh, and if you\\'re wanting the centroid of each tract/block, the census also provides shapefiles has anyone tried to evaluate cluster (e.g., k-means) density at the individual cluster level? i’m trying to find the clumpiest clumps--will probably use average inertia or silhouette coefficient.  any suggestions?seconded for google geocoder: have you looked into using em instead? k-means makes a lot of assumptions about your data set’s geometry that may not be often true for real-world stuff.this is a first pass at the problem, so i haven’t looked into alternatives in any detail.  i’m putting short text documents into a vector space and clustering them. the goal is to find some subset of the clusters that show strong internal similarity.as i understand it, other non-em clustering algorithms allow for more reasonable / different geometric assumptions (e.g., dbscan).  as the timeline stretches out, i expect to do  more serious topic modeling (e.g., lda), but this is my first stab.regarding finding some subset of the clusters that show strong internal similarity: i did this by looking at the similarity matrix and only selecting  words/vectors with strong similarity for clustering.as a preprocessing step were you selecting features or observations w/ strong similarity?i see: you picked only observations that were close to at least one other observation?i was selecting features, i.e words with strong similarity. most clustering algorithms directly output a similarity matrix so i didn\\'t have to calculate from scratch. i think in your case it would be the text document.if you are using acs, there is a great wrapper in python to access the data: just to clarify - there is no spherical restriction on em - in fact it is k-means which is a special case for em, under spherical assumptionsi think maybe earlier was a typoas for cluster estimation in an embedded space - there is really no good reason to assume that the geometry for these high dimensional spaces is spherical for clusters - so you might get wonky results if you start with k-means. one has to be careful with high-dimensional clustering too, because in high dimensional spaces, there is a tendency for ‘everything’ to be close together.the census bureau also has a geocoder api: ..on geocoding, see also  and  .hi  in case you still need a solution, have you tried passing the addresses through the google geo api? the output in a search has a field that is the \"standardized\" (interpreted) input address. just a thought.hello fellows ! i have just moved to la for work and im very excited about the new opportunity ! i live in venice if you are close by lets connect and meet !google api is good but if i recall there\\'s a limit on how many you can call per day ... cheap to purchase more. openstreemtap is another one, but not sure if it has an api... you can use it through free gis software like qgis*openstreetmap thanks for the suggestion...i\\'ve been poring over their docs and it seems they want the query in standard usps format. i haven\\'t tried it though. have you used it?agreed  - my accuracy was shockingly bad in that quizhappy :turkey:anyone knows the easiest way to setup mongodb on ec2?can you install it via apt-get?looks like that works and it starts the process as well```sudo apt-get install mongodb```i\\'m running ec2 amazon linux, it\\'s redhat... yeah i should probably start a new instance with ubuntu, ubuntu seems more friendly to me....ah, okay. before jumping to a new instance you could try using `yum` but i\\'m not familiar with its usagethanks!another questions: does anyone know how to use ssl on ec2?my chrome extension project needs to send https request to my server on ec2or how do i get a valid ssl? glad you figured it out -- as an alternate approach, i ran into a similar issue trying to pass dicts into d3, solved it with the method here:  how does the {{var|safe}} approach work?in case anyone ever wants to share interactive plots in jupyternbs for presentation purposes without too much effort, chris said wrote a nice post recently describing a few minute pieces of javascript that’ll make your life much easier:  i wanted to pass in a list of  dictionaries with strings for keys as my variable. it gives the #39 error but if you pass in the variable in jinja2 safe mode apparently it saves the strings?!  yep that\\'s the exact issue mine had.  got around it using flask.jsonify to avoid ever explicitly handing the strings off to jinja/htmli was wondering: what sort of ds related podcasts does everyone likes to listen to? i really enjoy talking machines and o\\'riley data show, but haven\\'t heard any others that i\\'ve found to be consistently interesting / useful. hi fellas, this may help for interview prep: bit twiddling hacks : : another couple i enjoy are talk python to me (strictly about the python ecosystem but covers ds-related topics frequently) and partially derivative (more fun than useful)a friend\\'s company just released this, it a linux desktop in the cloud built for machine learning! basically allows you to access a supercomputer with a bunch of gpus from your browser with tons of ml packages pre-installed  what\\'s up? how can i help? just curious if you had any tips for getting started with itastrophysicists, i saw data showing that % of astro papers mentioning p-values increased  15 times in the last 10 years. does anybody know why such an explosion? anyone going to the rev conference in sf?has anyone gone through the pains of setting up an llc / s-corp and can provide some insight into going through the process?  i’m looking into it primarily for tax writeoffs.i’ve only had mine for ~1 year, but will be happy to help if i cani’ll be there!llc is your state level entity registration.  this is primarily done to limit liability instead of individual or partnership/s-corp - is a irs classification.  this  makes sense if you are running consulting business with over 200k income where you don’t want take all the money as salary and take some or most of the money as distributions that are taxed at 20% (i think).  very popular among doctor and law groups.  one caveat - you can’t raise money with s-corp.  for that you will need to become c-corp.  if you want an entity that’s going to make 50k on side business, then s-corp is too much work for it imo.:+1:anyone have any conferences or workshops that they would strongly recommend for ml and/or ai? uploaded a file:  and i met over the weekend. we were both wearing our insight shirts and for once, we didn\\'t coordinate our outfits.haha, it’s true!you heard it at insight firstmonica rogati is leaving jawbone and the rumor is she\\'s headed to twitter @mrogati\\'s tweet: : haven’t seen the talk (link?) so i’m likely missing some context, but bias is of course a major issue in data science and any quantitative endeavor involving making decisions from real-world datastay positive zkermishdepending on the industry and role, i’d think it’s often the data scientists responsibility to emphasize the effects of bias, mitigate them using best practices, and honestly report how they influence results.(in the context of ab tests...with stakeholders having preferences for results and laking the statistical knowledge to interpret them appropriately…this can be one of the biggest challenges): you clearly don’t know me at allsolution: make decisions randomlytrump 2016here it is, sorry it was from scipy: hi all! we are looking to hire an experienced data engineer. it’s a position in boston, hello. i\\'m a health fellow in boston this session. i\\'m having trouble getting gunicorn to properly access my static folder on my server. using ip-adress:5000 works perfectly, but using my url results in a site without css.the path to my static folder is: ```/home/ubuntu/webapp/my_dir/static```my index.html starts with:      ```&lt;!-- bootstrap core css --&gt;    &lt;link href=\"../static/css/bootstrap.min.css\" rel=\"stylesheet\"&gt;    &lt;!-- custom css --&gt;    &lt;link href=\"../static/css/business-casual.css\" rel=\"stylesheet\"&gt;```my nginx:```server {    location / {        proxy_pass         proxy_set_header host $host;        proxy_set_header x-real-ip $remote_addr;    }    location /static {        alias /home/ubuntu/webapp/my_dir/static/;    }}```there must be something wrong with my path, but i can\\'t seem to find it.  please let me know if you have any troubleshooting ideas. thanks in advance.just thought my folder structure may help too:simplified folder structure:```webapp    &gt; run.py    &gt; my_dir        &gt; veiws.py        &gt; __init__.py        &gt; static            &gt; css                &gt; bootstrap.css                &gt; bootstrap.min.css        &gt; templates            &gt; index.html```  sqlworkbenchj and datagrip remove the `..` in the paths. they should all start with `/static` thanks for the suggestion, but the problem still persists even with that change i’d recommend using a different method for specifying the paths for the css into your htmlyou can use the `url_for` method combined with templating, as in this answer: i don’t know if it’ll solve your problem, but might be worth giving it a shot no luck with url_for. thanks for the sugestionkeziah: can you share the url and ip?thanks for the offer richwinslow just helped me solve the problem!thanks for all the help  helped me resolve the problem. while my \"/etc/nginx/sites-enabled/application\"  file was in good shape my \"/etc/nginx/sites-enabled/application.save\" file had some typos in it.:+1:volunteer opportunity: chicktech () is seeking tas for a workshop on electronics for high school girls in sf. tas need to be able to read basic schematics and translate to parts on a breadboard. time commitment is 1 day (approx 8 hours). dm me if interested.hi all, need some insights from clustering experts here !i am trying to find clusters in a dataset that has latitude-longitude as two of the features - i found that dbscan works well for lat-lon data - but does it work for lat_lon + other features (mostly categorical)?  or should i try something else?thanks in advance for any pointers! pinned a message to this channel.want to work at tesla? what are people\\'s preferred approaches/libraries/companies for user tables with 100 or so columns (possibly linkage to other tables, at scale)? graph? relational? redshift?live from nips:  if you don’t have time to read through hundreds of papers, or watch hours of videos, we’ve summarized key highlights of day 2, check it out here! hi all, our marketing team at uber is looking for a senior data scientist with 5 years+ industry experience. please ping me if you or anyone you know is interested. :+1: thank you!!  hey @ channel (sike!), i just made  for general discussion on nns, staying up-to-date on the field, and seeking help. come one, come allnever again will i dare to @ channel :woman-gesturing-no:my team is looking to hire an experienced data scientist/manager in our san jose office.  please let me know if you are interested:  all the apis; maybe we should start a new channel for project resources?if anyone\\'s interested, this meetup is next week on cellular automata and deep learning: is anyone else doing the coursera scala course? i\\'ve created a little  channel for chats. 2016 internet trends report is out.parama: i have a few things i can come up with from my experience. 1) clustering with categorical features is tricky because your dataset is already \"clustered\" with whatever the categories it has. 2) scaling greatly affects your clustering results when you mix different units (lat-long + category), so you have to scale features in a way it \"makes sense\". 3) if two or more of your categorical features are correlated, they have significant effect on your clustering. 4) in general, unsupervised learning doesn\\'t have any measure of successfulness, so you have to come up with some type of heuristics for your use case. 5) one degree latitude and longitude don\\'t correspond to real distance. one degree longitude at a pole is 0 km long.: i have been told that dbscan in 2 d works very well on géo data, but as soon as you add a categorical feature the algo will not work. i am not sure if there are any alternatives. i wonder if you could perform the géo clustering, then add a column with the cluster identifier, and perform a second clustering minus the geo data but using the new cluster column?that\\'s how i would approach it as well.what’s a clean pythonic way of providing two constructors for a class? i know i can do some logic with kwargs, but i feel that doesn’t provide the same clarity of use that having a defined signature does?have you tried using `@classmethod`?like  said, e.g.: cool, i’ll have a look, thanks. decorators to the rescue!ah yeah, just what i was looking for. thanks.i saw it on stackoverflow but i was a bit uncertain since it was the second most popular response…sounds like a potential insight project.... is working on a chrome extension that highlights the best responses for python questions on stack overflow!sounds similar to we have a new blog post out! this one is especially for all you physicists out there!   rocks :clap:hi everyone! i am looking for some tool to plot an interactive time series, hopefully more high level than plain d3, any leads on a good package? have you seen nvd3?  it’s built on d3 and reduces the initial setup time for many common chart types. there’s also c3thanks! i will play with those. so far i was trying to use rickshaw, which looks really nice but difficult to customize.i’m probably a little late to the party, but the nginx config file example contained in the widely circulated “the internet, servers, and flask” google slides deck may have a typo. the line `location /static {` should read `location /static/ {` (trailing forward slash added).bmfarley: hey brian! thanks for the info. by chance, in the path contained within `location /static {`, does that path have a trailing slash or not?my guess is that if it’s `location /static {` then the path contained within shouldn’t have a trailing slash, but if it’s `location /static/ {` then the path needs the trailing slashyeah, there’s a mismatch between the two in the example config file — no trailing slash on the location, but a trailing slash on the aliaslet me try it with both missing, too!both missing also works, so it looks like the path mapping is quite literal!good catch! so either both need the trailing slash or both need to omit the trailing slash, correct?that seems to be the case, yes.great. updated the slides. i’ll blast the webdev channelawesome, thanks!guilherme: thanks - i am probably gonna try this out then!hey yusuke - thanks so much for the tips! you are right i am already facing trouble with categorical data as the clusters tend to pick up the feature with most categories and just use that!could you please elaborate on what you mean by scale features to make sense?i mean i understand normalizing for number databut how to scale for say gender and age categories?hey all, i\\'m helping organize an event on the state of data science education. our very own jake klamka will be on a panel and we have dj patil keynoting. if you\\'re in the dc area, you should check it out and please send to anyone you think might be interested. thanks!  commented on ’s file : : now that you have a lot more data how do these plots look? uploaded a file:  uploaded a file: : i was on vacation for a bit in august, have worked from home a few days, and gone on business trips, so the data is actually a bit more sparse than previous weeks. the biggest change is in the evening commute. i no longer have to get my kids from their summer camp in the evenings, so i think the shorter commute trend is here to stay.roughly 24 hours after the previous post, you knew it was coming, day 3 at nips! if you like rl and gans, you will like this one! hi all, particularly those with experience working with/at spotify. i’m looking to get some general information about their hiring process for a friend. if you’d have some time for a quick, high-level chat, i’d really appreciate it. please message me or email at my new blog post is out!this article is awesome ! thanks for sharing. very cool to see gans used for realistic use cases. would you be able to share the related code?just need to clean it up a bit firstthen can share the notebookhey all, i am working with a civic-minded early stage startup that is looking to hire a junior data analyst.  if you have worked with any promising interns that may be interested please encourage them to check out the listing and apply ()oh, lots of new people! is it that time!? :smile:fresh blood!welcome party this friday....welcome party, aka the last time you have fun in 3 months!(i’m trying to scare them)really till? that\\'s how you want to start out? by lying to them?it\\'s more like 4! :wink:@jessica there is enough information here. i haven’t perused through, but it looks like the tutorials and examples are good for beginners as well as advanced. i can help too. i’ve had a sole proprietor llc and now a multi-member llc with s-corp tax designation. llc with s-corp tax election seems to best of both worlds.patrick phelps will be there, i think he\\'s mc  lumiata, the company i work for, is hiring a data engineer. pm me if interested: anyone read judea pearl’s latest on causal inference, _the book of why_? worth it or too watered down?  i’m about a third of the way through it. so far it’s definitely not at all technical compared to e.g. causality, but provides a nice historical (and opinionated) perspective on how we ended up making ‘causality’ a taboo word in statistics/data analysis.oh, and how causal reasoning can help solve some of the statistical ‘paradoxes’ (like simpson’s)any des from airbnb?i\\'m completely new to this whole field but i enjoyed reading this blog post: anyone familiar with the matrix profile methods out of ucr? one of our team members saw a presentation and is now convinced it\\'s the only way to do time series analysis.not that particular technique, but i have done some dynamic time warping, which came out of the same lab. they have done a lot of good work when it comes to times series analysis. i’ll be thereme too!mini insight meetup at the evening mixer tomorrow?cool article about the data science origins of the warriors: icml, nips, kdd?@nothere we’re planning to hire a large new team of ds and de in london over the next year to work on new premium products. if you’re in europe and interested in spotify, let’s talk!what are those? they sound fancy…just a bunch of letters. almost as good of conferences as jkl and ibmright onbetter answer::+1:so were you asking people’s relative opinions about the three? or seeing if anyone was going?ah -- i was intending to respond to ’s question about recommended conferences for ml and aihaha, yes indeed, it all makes sense nowi underutilize threads, evidentlythanks guys. i was bummed to miss nips this year.a quick question: is there any advanced approach of finding the best parameter values better than the grid search?random search usually works better. bayesian search is the most efficient (you can use packages like spearmint)an randomized adaptive search using some variation of the metropolis algorithm would be ideal.thanks guys!meetup is hiring for senior ml and de roles:  ai in actiondoes anyone know a good place for sensor data? nasa has a few interesting datasets of sensor data:  awesome, i will take a closer look.no problem - kaggle also has a few interesting ones: sigopt works pretty well. it’s a commercial product but has a free tier uc irvine also has a decent collection of data sets (sensor and otherwise).  i don’t want to start an editor war, but check out the new post on using emacs for data science:it’s even on the front page of hacker news!sounds like high variance?  what do you mean by “random state” in this case?  the initial seed for the choice of bagging / features?would definitely recommend doing a k-fold validation! how big is the dataset / how many estimators are you running?thanks ! \"is there a paper?\" literally just came up an hour ago!hello, any canadians moving to us with tn visa? :flag-ca:: i did last summer, as did : good paperyeah, he’s a smart guy:  too moved from canada for work.: yep, made the move this year`eh`: i don’t think anyone plays with the random state other than for deterministic reproducibility. it would be easy to convince yourself that the model performs better by validating on your holdout with better performance given a different random state but that’s just due to model variance so that logic doesn’t hold true in the real world where the model will have to perform on different data than the holdout set. bagging in ensemble methods tries to reduce variance at the cost of increased bias, but (as you’re seeing) it won’t be reduced to zero.take a look as the relevant bias-variance tradeoff section in hastie and tibshirani. or this blog post does a decent job of explaining as well: thanks, it logically seemed like the likelihood of this on a larger holdout set would be small but given the inherent randomness of the models it seemed like it is theoretically possible that one set of trees could fit the underlying generating model better than another. i was mainly curious if anyone had also observed this and if they\\'ve ever toyed with it, but the sample here was rather small and could definitely be an artifact of that.it would be easy enough to decide that with cross-validation.in regards to bias, have you seen the pycon talk suggesting that bias could be a major issue in data science? i thought it brought up some interesting points, though given project lengths it seems unlikely it could be taken into consideration outside of large projects essential to the company that could afford an extended development period.but i’ve never toyed with random state as a parameter beforeturns movies into van gogh-esque moviesif you want to learn something, this may help you find free material: thanks for that, i’ll check it out (though i did figure out the superset issue, was actually just some browser settings funkiness :stuck_out_tongue: )i forgot to mention, thanks for the great advice!waadja: the remote program will be convening in la in a couple of weeks!has anyone worked with survival prediction models before and could suggest metrics that best evaluate their accuracies?csyhuang: concordance is one metric ppl use. i heard actuary folks also use ks testi am intrigued by this openstreamtap, however. :beer: friendly poke: please try not to leave your drinking/eating vessels lying around the office - try making it a habit to tidy up at the end of every day. haha you just pinged all past and present fellowsi am no longer in the office :slightly_smiling_face:omggggggno, she\\'s right. cleaning up my desk right now.the message still stands. wherever you are, clean that mess up!the hero we didn’t know we neededi mean, i could probably use a little tidiness guidancecleaning up the sv office now!i am mortified, but also, do it y\\'all this made my dayinstructions unclear, just shipped 2 cases of empty la croix cans to the officethat\\'s right fellows. clean up all your shit!thanks  :slightly_smiling_face: i also find concordance coming along with some survival analysis packages. however, i find it hard to interpret (0.5: random, but literature also says survival models are also in a range of 0.6-0.7. if my model has c = 0.63, is that good enough?).i’ll try the ks test and see if the p-value may help :slightly_smiling_face::smiley: good enough always depends on the problem you are trying to solve. for example, there is no value for a “good enough” aucas former om, i approve this message! i just cleaned up my dinner table. our office manager is thanking you for saying this to me for the 1001th timetime to take down the tiki bar.  *sigh**you\\'re not my supervisor *yes, here: try misspelled variations of a nyc address in the entry field. the adress reported at the top of the results page is taken from data returned from google. it should be automatically correcting the variations. eg try west street 45 nyc, the next page should return the street number at the \\'right\\' order. is this what you need to do?that became my personal motto after you demanded that from all of us the other day. i left a clean desk! i am not sure what happened to it!  that income on the side has got to go! :smile:: stay positive alvingo eat a pie nuggetbotcomposite indexing on mysql table:does anyone know how to set up gunicorn to repond to port 443?do i need to combine gunicorn with nginx?re: gender github articleseems to be a bit unclear… scientifically.a snippet:this is normally the part at which i would question how a study got through peer review, but luckily this time there is a very simple answer: it didn’t. if you read the study, you may notice the giant red “not peer-reviewed” sign on the top of every page. the paper was uploaded to a pre-peer-review site asking for comments. the authors appear to be undergraduate students.but…“i don’t blame the authors for doing a neat study and uploading it to a website. i do blame the entire world media up to and including the bbc for swallowing it uncritically. note that two of the three news sources above failed to report that it is not peer-reviewed.\"anyone ever done anything with  before?maybe   or  could share some thoughts? has worked with their gym before, maybe he can helpsam altman seems to think it\\'s a big deal, and he seems pretty knowledgeable, hah.this is pretty sweet   check out the video at around 2:20 alsocheck out @insightdatasci\\'s tweet: has anyone played around with fb\\'s prophet forecasting library? i\\'m considering it using in python for time series sales forecasting and i\\'m curious to know how it stacks up against other libraries out there and where it falls short (i guess whatever sklearn and stats_models offer). any feedback would be appreciated! :slightly_smiling_face:  diego: i have used it a little bit, i think it is a great starting point for time series modeling particularly if you are interested in breaking out seasonal trends. you can impact the forecast some but it is fairly black-box as far as i can tell so far.we have used it a little for low-risk stuff. for instance we currently have a big monitor in the office forecasting a key metric and comparing to the quarterly goal. i did essentially no tweaks other than setting up a custom list of holidays. i can’t compare much to other packages because forecasting isn’t a substantial part of my job. but, the one data point is that it does come pretty damn close to a pro forecast using basf (big-ass spreadsheet forecasting) methods.hey diego.  i played around with `prophet` (both the r and python versions) in the first few weeks after it was released.  at that time it wasn\\'t stable enough for my purposes--i wanted to forecast ~30k separate time series, but couldn\\'t find a way to script it without the kernel crashing.  so i never got far enough to characterize prophet\\'s accuracy.  but facebook just published an update that may have fixed the stability.  i\\'ve also been using the `forecast` r package, sometimes in conjunction with `hts`.  `forecast` is great--it\\'s stable, mature, and includes a comprehensive set of forecasting algorithms.  it\\'s more flexible but less automated than `prophet`.  i\\'ve also tried `statsmodels` in python but found `forecast` to be far superior.as i understand it, one of its selling points is that it exposes the knobs in a more user friendly way than a raw arima model such as in r forecast.thanks  ,  , and   ! i\\'m interested in forecasting about 300 different time series so it\\'s good to know that prophet had some trouble with scalability in the past (and good to know that r\\'s forecast package was able to handle it), but that it seemed to do a decent job straight out of the box. i\\'ll give it a shot with my data and if it struggles with it i may end up moving to r forecast . thanks a lot! :smile:my understanding is that under the hood it is running stan, so it is something more akin to bayesian structural time series or gaussian processes rather than arimadoes anyone have experience using airbnb knowledge repo within a docker container? we are exploring this option for a company-wide repo and are unclear right now on:1) if we can push to our git server from within the container and 2) if we can make a single repo for all of of our teams (organized in some way …) have you watched the video tutorials on recommendation sys from stanford uni in you tube?  you will find there a very good one on cosine sim but the way to go is matrix factorization i wrote a tutorial about this, if you wanna check it out - though i agree with  that matrix factorization is the way to go:+1: for ’s tutorial, i’ve used it as welldataengconf nyc is coming up october 30 &amp; 31st. come see  speak along with a number of other leaders within the nyc data community! the last day for early bird tickets is this friday and we also have a special promo code for insight alums: insight100 for an additional $100 off the early bird price. see you there! woodoes any one have experience with using pyhive to access hive? hi thanks for response. which video would you pls point to me ? like a link. thanksi’ve used it works through thrift-server.package actually is not maintained anymore, but for me it’s worked fine.thanks!hi all, does anyone have experience with optimizing the phone pick up rate? try impyla it works with hive alsohi! does anyone have any opinions on  ?i\\'m looking for tools to help scale up a ds orgspecifically streamlining workflows and disseminating knowledgei have a question about python tools for hand labeling high resolution time series data with a few hundred positive events.  i know approximately where each positive event occurs, but i need to know precisely.  my current solution is to use bokeh to pan and zoom around the time series data, then select the example associated with the positive event.  i use a javascript callback to generate a web page that contains the data associated with the positive event, then i save that web page to disk.  the process works, but is fairly cumbersome.  i\\'m not tied to bokeh, but i definitely would prefer a solution that can be implemented in python.  thanks!!: : there is something about the “events” that requires visual validation by a human?: yes, i need to hand label a few hundred events to generate a useful training set.hmmm…and there’s no way to auto detect them via smoothing / deltas? (i don’t know how to semi-automate a viz process, unfortunately.)well… i think i have an algorithm that works, but i need to test it using a set of events that i know to be true, plus some that i know to be false.: have you tried the built-in event picker `mpl_connect`?demo code here: : thanks, this looks like it will be useful!:simple_smile:for all the current fellows out there:&gt;&gt;&gt; it takes a lot of interviews to get used to the process and the format and to understand that the stuff you do in technical interviews isn’t actually the stuff you do at work every day. and it takes people in your social circle all going through the same experience, screwing up interviews here and there, and getting back on the horse to realize that poor performance in one interview isn’t predictive of whether you’ll be a good engineer. &lt;-- a history of the technical interview and how to fix some of what\\'s broken.hello! are there any graph theory experts out there? if so i\\'d love to pick your brain about a problem i am trying to solve.  i made some cards in domo recently, havent tried full fledged dashboards. what are you looking to do.  i\\'ll be at odsc this week as well.: are you presenting for sillicon valley data science?not presenting, but will be attending, we have a number of tutorials and panels that we\\'re presenting atyeah, i got an email from one of your peeps about that...hope to see you there. is julia joining you?anyone else attending the odsc west conference?julija won\\'t be joining unfortunately, she has other plansso i have a friend with a db question — maybe someone could exchange some emails with him?he needs to store daily frequency data for ~100k different time series datasets with 20 years of history. each entry in the time series is not necessarily a scalar, and some might be matrices with potentially varying dimensionscontext is buy-side financebasically he’s looking for someone to suggest what storage technologies they should be considering the pros and cons ofcouchdb perhaps? interesting — what’s been your experience?great for logging, particularly when you don\\'t know the size/shape of you input unot*unitand the whole http read/write methods play nice with web based apps academic papers about marketing data science?hey kevin, i\\'d also love to see those papers! hi all, i\\'m a fellow in the remote program. does anyone have experience implementing content-based or collaborative filtering recommendation systems? it\\'s very new to me, so i have several (probably basic) questions! thanks! i used pdfminer which i think contains pdftotext? this enabled me to extract all the text in bounding boxes, along with location information. this is really cool! i may try it out if i run into trouble with pdfminer. thanks! i\\'m interested in it, too. please let me know if you find something useful. thanks! i have done some basic implementation of it and also used matrix factorisation in python during my academic projects. i can try and share some of the useful materials and code for it if needed. but i would also recommend have a look at factorisation machine which can incorporate various features of movies and users as well - ps: i only have some idea about pythonhey. any of you all planning to sell your bike in silicon valley?.i would love to get one. let me know :slightly_smiling_face: i have one to sell, but it\\'s very big (xl)i can\\'t get an estimate from xl actually.. i guess one with 26\" should be good for mei’ve done a bit of it, nothing too sophisticated but i can help if you have any questions,   sorry for hijacking this thread. at customink we are starting to explore airflow as our main etl tool, i’m wondering if i can bother any of you to have an informal discussion on how your framework looks like with airflow? let me know if it’s ok to setup some meeting thanks!hey kenny - you might be better off meeting with an engineer, but i can tell you what i know from a user perspective  - just dm mehi everyone. i\\'m currently an insight health data science fellow and am seeking webscraping advice. my goal is to begin at the following website,  under \"license search\" -&gt; \"search by license number\"the scrapping involves 3 main steps:(1) search query: input a physician license number from a list (one example is: 41586). a successful match (here it is: akhtar, muhammad saleem) will show up as a list of doctors that match the search query. (2) name selection: if there is more than one doctor matching, i have to deal with choosing the correct one based on the doctor\\'s name. perhaps, i can use a dictionary to match the number to the name of interest?(3) save the data: extract the text at the bottom of the page, underneath the header: \"public record actions.\" if anyone has any advice/suggested tutorials/packages for any step in this scrapping process, it would be greatly appreciated. thanks!i’ve have built some recc systems in python and spark ml..would also be willing to try to answer questions :slightly_smiling_face:johnny-do: for webscraping off of interactive sites like this i have used a combination of selenium and beautifulsoup.  here is a useful tutorial i found a while back :beautifulsoup is a great resource. for the interactive part, rely on selenium via```from selenium import webdriverbrowser = webdriver.chrome() #selenium for chromebrowser.get(url)time.sleep(5) # let the user actually see something!html_source = browser.page_sourcesoup = beautifulsoup(html_source) #fetch html source code after renderingbrowser.quit()```i’ll ping again about that pip error, which is specifically `pkg_resources.distributionnotfound: the \\'pip==6.1.1\\' distribution was not found and is required by the application`i can’t solve it for the life of me, and downgrading pip is ridiculously roundabout. if anyone can point me in the right direction, let me know...of course the second i post it i figure out it is a problem with the pip that sudo calls (/usr/bin/pip), and not the user pip (/usr/local/bin/pip), but sudo pip is required for any package installations.is there a way to sign out of the general channel? no, you can’t leave . you can mute it however under ‘channel notification preferences’thanksblog post on generating fonts with adversarial networks: hella coolanybody using or have used keras for regression?  i can\\'t seem to reconcile what model.evaluate gives me versus what i believe the objective is...hi guys! i have a flask app deployment question. i have a pickle file that my code needs. right now, i have stored it under a folder \\'data\\' under \\'static\\' in my \\'app\\' folder. i reference this folder in \\'static/data/\\' in my python code. but when i run the run.py (which is above app folder), the pickle file is not found by flask. on the other hand, another python function within the app folder can read this file! is there a reason this is happening?sounds like a permissions issue on the file... you can test that theory by making it readable to everyone with: `chmod a+r pickle_file.pkl` if it works then then it was a permissions issue, otherwise it\\'s something else!i just figured it out. so i need to give the global path -- \\'app/static/data\\' instead of \\'static/data\\' for run.py, although the local path worked for the main python function within app folder...does anyone know about settingwhat is setting?anybody have ds-related reading suggestions?  i\\'m looking for some like pop-culture-ish type fun stuff.  is nate silver\\'s \"the signal and the noise\" any good, for example?@jaime only if you want to up your nate groupie-ness... :wink:  the book is essentially good for journalists to understand some basic statistical concepts... you\\'re way above that...haha fair enough!  i only know about that book because my mom was asking if i\\'d read it.  maybe i\\'m not the target audience...whoa, looks super interesting, thanks greg!i love the signal and the noise - it\\'s full of powerful examples of people fooling themselves about how good their predictive models arei\\'ve read some chapters, like the one about earthquakes or poker, half a dozen times. so entertainingnice! sounds like just what my commute needs!hey i’m planning an offsite for my team in sf and was wondering if anyone had any good suggestions? preferably that doesn’t have a lot of walking or drinking :)the stuff i have done (roughly ranked)1 ) go-kart racing2 ) painting class at a bar3 ) hiking in marin (toward muir beach)4 ) distillery tour5 ) escape room6 ) cooking class7 ) scavenger hunt8 ) baseball gameobviously 3 and 6 have lots of walking. almost all but the distillery can easily involve minimal or no alcohol.1, 3, and 4 are outside the city (k1 speed is in south sf, distillery is in alameda)6 is good for small groups. 7 and 8 are good for large groups.my favorite was go karts, but the less competitive members were not very into it.woodworking was funthat is terrifying!i will not find it personally fulfilling unless it\\'s blue. i\\'m glad we all agree on that.purple, or i quit.meaning: important to youimpact: matters to the companywe clearly all disagree. can we take this offline and form a team to standardize a pipeline for choosing bike shed colors?that\\'s an excellent idea. we need a streamlined way to make these decisions in the future. but... should the team also have power over other paint choices? or maybe we need a paint committee and form a bikeshed sub committee?hello insight alumni! i am a current insight fellow in seattle on week 2. i am attempting to make a chrome extension that would preserve informative reddit comments and delete the rest. i am having difficulty having my extension run python/communicate with flask as i don\\'t want to open up a new tab but stay on the current page. i am currently using jsonp with ajax to get around cors. my flask is receiving the request, but is unable to send the right information back(error 200). if anybody has any advice i would greatly appreciate it. i only need to send a few lines back. built a pretty awesome chrome extensioncoin toss!information about our stitch fix friends: i have used it beforefor those of you with a ds position opening, a friend of mine is looking. i have been his mentor in the last 2 months. he has been able to do a great deal of work in such a short time on our demand and forecasting problems. he is very comfortable with python, and sql and picks things up very quickly. this was an area that had not been touched by our data science team and helped us with gaining more domain knowledge, as well. our team would love to hire him right after his internship but sadly due to new leadership, we have hiring freeze. i will be happy to provide more details about his technical skills.someone with access needs to program  to do coin flips.hey all, there\\'s a team at netflix looking for people with at least a few years experience (but special cases would likely be made) who are really strong in applied stats (glms, mixed models, hypothesis testing, time series, etc.), is comfortable working with very large data, and who is interested in using data and modeling to improve the netflix product. if you\\'re interested slack me.that\\'s sweet news. i se them a lot so will def buy some of their sweet stock uploaded a file: amazing.i’m so happy they put that in the appstorea friend of mine at wayup is looking for a data scientist with at least a couple years experience. it\\'s a fun company, they connect college students and recent graduates to jobs. they are looking to build more sophisticated recommendation systems and there is lots of nlp work to be done. data is in good shape, one of the founders is an engineer and build the company with engineering in mind (and he is now serious about making an investment in data science). happy to put in a recommendation if anyone is interested. he found it difficult to hire, go figure! :slightly_smiling_face: here is the job description: also, very glad i won\\'t accidentally eat my shoe anymore when i mistake it for a hotdog. it was getting expensive.friederike: +1 to wayup -- i have a friend there as well and he enjoys the culture toodoes anyone have experience working with matrix partners? if so, please dm menew blog post is up.  describes how she built a deep learning model to remove eyeglasses from faces: :rocket: that\\'s a neat project / post !anyone in the nyc office?i’m downstairs and would love a key card to get me inside for mentoring :slightly_smiling_face:welcome new fellows!welcome! have you guy met  yet?woahhi all. i\\'m interested in using past tweets to train my data. i\\'ve found the following api: , which appears to allow access to tweets from 2006 with one-minute granularity. i was wondering if anyone has experience with using past twitter data, and if so, how did you access the data. thanks.this seems to be an useful list of datasets:quora open data set list: hi all, what\\'s the best python library to collect tweets? i\\'ve come across twython and tweepyhere is another bunch of data sets that are available: if you want ds sources, i dropped  a word document with links to datasets into the brainstorming folder at dropbox.  here is the link. please, feel free to add more sources.   hey 2016a fellows! jump into your individual channels (e.g. ds-sv-2016a) to collaborate virtually and in person! there also exist a wide variety of great channels on specifics topics, like python, sql, etc. enjoy!so there was no admonition to avoid twitter data in this session like we got two years ago (because it is overused)? i guess with jack back, twitter is too.does anybody have a link to the results of our insight salary survey?the one that was sent out last year is here: thanks!daaang, we rich!haha yeah we are! _looks at price per square foot of house near work_ oh… :disappointed:hahathis is super useful!working on a v2, let me know if there are requests/requirementsmy goal is more demographics data, more data about job/career transitionshi all, we are looking for a senior data scientist/engineer with a couple years experience.  we’ve got a great location right near powell st bart! a quickly growing workforce (currently about 23 in this location), diverse professional backgrounds from health to backend engineering. right now i’m the only data person. for the love of god someone help me! lots of stuff to explore and/or build. you should follow through the contact in the job description to apply but i can try answer any preliminary questions. i’d love an inclusion of manager roles as many have transitioned to that :point_up:historical information would be great (eg salaries for people\\'s previous data science positions in addition to current), especially for those of us in the middle of the country where there\\'s not a ton of data on :slightly_smiling_face:has anyone here been involved in hiring a summer intern that could give some guidance on what a reasonable hourly rate would be?is it reasonable to take the salary of an entry-level data scientist at your company and convert it to an hourly rate?sure, i\\'m not super familiar with data science internships so wasn\\'t sure if that would be wildly off the mark, but even that is a helpful starting pointfreeor 1 linkedin testimonialwhat are people’s thoughts on the following line in someone’s resume:“• hired 4 data scientists (all women), of which 3 promoted in less than 2 years.”arbitraryis their company promotion schedule that you should be promoted every 6 months, in which case the person did poorly, or every 5 years, in which case they did great?does their company have 100 levels, or 2?promoted within 2 years is not really something that you can evaluatedid they need to hire 10 and only get 4?i\\'m happy to pass judgement from afar, but likely just talking to the person can clarify :wink:i’m really just curious on people’s gut reactions to that bullet point. i think there’s a lot that could be unpacked there.also, the person was scheduled for a phone screen with me and they never picked up the phone…so that doesn’t bode well!i’d ask why the 4th one didn’t get promoted...but without any additional context, i have to admit that my gut reaction to that bullet point was neutral to negativeand i agree with all of kevin’s points, which usually never happensor hired below level and then leveled appropriately within 2 years :slightly_smiling_face:in which case nice job keeping those salaries low for the first 2 yearsfiscally responsible +1only promote when threaten to leaveyeah, it\\'s a weird line to write in a resumeinteresting that no one has touched the `all women` line`all women` rubbed me the wrong way, but i’m having trouble figuring out what exactly bugs meit\\'s a suspicious humble bragthere’s some sort of assumption embedded in there that women are harder to hire and/or promotethat’s what jumped out at me tooi think it is actually more challenging to hire diverse teamsdo you think that team is diverse?sourcing diverse talent is one thing many companies do not do a great job athard to saythe `all women` bit was absolutely the first thing jumped out at meas sort of a humble brag that rubbed me the wrong wayi *think* i’d rather them say, if they really feel the need to humble brag, something like “hired a diverse group of 4 ds”, but even that doesn’t feel/read quite rightagree, i don\\'t like the `all women` line, it feels both performative and also hyper-aware of genderi mean, what i’d really like is to see other evidence on their resume that they truly care about diversity and equalityyup. if diversity had been a problem, then they addressed that in hiring/promoting efforts, why not phrase it that way instead of that all women linecan you bring this person to us for a proper, thorough judgment? we have a lot of questions.i’m interested in whether or not this is a woman’s resume or a man’s?it\\'s 10000% a man\\'s resume^^^it\\'s tough because hiring women data scientists is obviously a good thing (tm). but the choice to call it out just feels strange.a bit like a mitt romney quotei think we pay ours something in the realm of $25 an hour“binders full of women”and yes, good call, it’s a man’s resumei\\'m sure the person would be flattered that so many phds are evaluating the resume right nowthat\\'s phd-ist, kevinwell i hired a team of dentists (all dentites) and i fired all of themwithin 3 daysi like that the guy felt compelled to add the 3/4 promotion rate. like, he put himself in his resume-reader\\'s shoes, and imagined them thinking \"hire women!? but i\\'m sure they were terrible candidates!\"anti-dentite…and one of the ds’s on my team (a woman), tongue in cheek, just suggested that people should start adding more gender and ethnicity info of their hires to their resumes…one last bit from this resume that i feel will also inspire many questions, under hackathons they list (in the top position):“bathroom hack (“p value”)  - a/b tested series of urinal feature enhancements (e.g. bullseye 2. footprints on ground) to improve cleanliness of men’s bathroom.technologies: raspberry pi, pir sensors”“hired 4 data scientists (all white males)”sighi mean, i’d want to phone screen that guy just to see if he’s that ridiculous in real life. but based on that resume alone, i almost certainly wouldn’t hire himi was actually kinda looking forward to chatting with him on the phone screeni would pay real american dollars to hear him answer the question, \"so, what was it like working with all those women?\"you should just be likebro:bro!crushed so much code on that sick p-value hacklove how you did the wordplaypee-valuedamn i just got ithahahahahhaai\\'d rather hear about what a team accomplished than how many of them got promoted. who gets promoted depends a lot on things that are beyond the control of the manager or who the manager hires.and then just see what he doesok enough trolling from mei think we all want to hear an update on how the interview goesaka any delightful quoteswell they didn’t pick up the phone when i called for our schedule phone screen, so not sure they’ll get another chance with us…$50/hour plus housing, but we’re a bigcorp (tm) in silicon valleymaybe he\\'s not used to answering the phone, as a manyeah, denver pay rates are definitely lower than anything in cahe should hire a secretarymaybe you can mansplain over email why you\\'re not moving forward in the processyou mean over man-mail?let me interrupt the fun to say “as a manager i absolutely want to see you built a team on your resume if that’s what i’m hiring for. but that line is super awkward…”likewise i’d want to get into questions of “how did you support your team? how did you develop their careers…” etc. etc. but on the phone in person.on the plus side, i now know what not to write on my resume :wink:just to add to this: $50 + housing is pretty close to (but slightly lower than) junior data scientist yearly salary -&gt; hourly for us, so that might be a good metric.awesome points of reference, thanks!wow haha, lots of (hilarious) cattiness today :smile:my general rule is to give huge benefit of the doubt to everyone in this sort of situation.  it\\'s always a mistake to try to intuit what\\'s going on in someone\\'s head from a resume line.   would definitely just ask about anything that seems \"interesting\" assuming core skills are there.definitely also would be understanding of a missed call, especially if it comes with a good reason (e.g. not an excuse) as long as it\\'s not a pattern.^totally agree about the missed call. i don’t flat-out reject someone for that, but i do make a note in their hr/candidate profileso you\\'re saying it goes into their permanent record :scream:i once missed a phone screen call while waiting for my phone to ring. it was first thing in the morning, and i had forgotten to take my phone out of do not disturb. :man-facepalming:but the more common reason for missing a phone interview is that stuff always comes up at work. in my post-insight job search, i never understood why so many interviewers would begin by asking, \"is now a good time to talk?\" why would i have scheduled an interview if it were a bad time? but now that i\\'ve conducted a job search while working, i understand that there is no such thing as a good time for a phone interview. there are only degrees of badness, and one cannot know in advance how bad a given time will be. if someone misses a phone interview, it may well mean that they\\'re important at work.i absolutely start every phone interview i give with “is now still a good time to talk?”(after introducing myself…)pro tip: you can also find out if you\\'re accidentally on speakerphone at their current place of work by starting the call with, \"is now still a good time to talk about interviewing for a new job?\" pinned a message to this channel.interesting discussion on where ml engineers fall in the world of data science and engineeringthis may be interesting for those still connected with academia:brains, minds and machines summer course 2018a special topics course at mbl woods hole, madirectors: gabriel kreiman, children’s hospital, harvard medical school; and tomaso poggio, massachusetts institute of technologycourse dates: august 9 - august 30, 2018application deadline: april 9, 2018more information: …this course aims to cross-educate computer engineers and neuroscientists; it is appropriate for graduate students, postdocs, and faculty in computer science and/or neuroscience …hey all — i’m looking for a good applied ml interview question. something that gets at problem solving ability and ml understanding, preferably with a way to quantify the responses, though that’s obviously tough. anyone got a good go to?not quite an answer to the specific question, but i sometimes point people to this article when asked about advice around finding good ml/ds candidates. thanks  — i love that article but had forgotten about it. that will definitely help!hi everyone! we suspect our data science team is understaffed. we wanted to get a sense of how many data scientists other comparable companies employ. if you have 30 seconds, would you mind filling out the following survey? happy to share aggregated results! :pray: you guys should hire some idx fellows :wink:no not yet! i’m still poking around, never really got a response on here. i looked at that answer, but didn’t try it - did you?would love to find a solutionhelp , go here to see problemi\\'ve got a friend from canada looking for bay area based internship/coop positions for data analysis/science projects. this would be for 3 months. dm me if you know of anything!hi friends, i am working on an open source project with the world food program that attempts to apply kalman filters to get better estimates for noisy survey data (checkout the  channel for details).  i\\'ve got a complete and presumably correct implementation now but i\\'m looking for someone who\\'s got some applied knowledge in fitting kalman filters to real data, be it in any discipline.  anyone know anyone who might be willing to talk with me about that?hello! has anyone worked with phone records? i have data with calling number, called number and dialed number (sometimes the last two aren\\'t the same) and am not sure the best way to tackle figuring out if certain people are more commonly dialing others. thanks!: isn’t this simply finding the number of occurrences using the pair of numbers within the dataset?maybe so, that\\'s what i\\'m currently doing.  just didn\\'t know if there were other ways of looking at it :)you could do a network-based approach with in and out going calls being your edges.i was thinking something like that too! i\\'ve never done network analysis. is there a python library your recommend? networkx is pretty easy to use: thanks!!!looking for someone whos good with apache storm + java?! :sleepy:possibly  has an idea?hey , what’s your problem?: a friend of mine who works at our company started this with his boyfriend. you can make donations and t shirt purchases all proceeds going to equality florida actionhi guys, i\\'m a ds fellow in the current ny session, having a strange problem making my web app visible on aws. the app uses a number of python packages, which i have installed via miniconda. the app runs locally on aws, but when i try to run it via supervisor on port 80, it does not find the miniconda python installation. i have tried to specify the miniconda path using \\'environment=pythonpath=\\'/home/ubuntu/miniconda2/bin\\' in the supervisor conf, but it does not find it. have any of you faced this issue before and, if so, what was the solution? i know that many past fellows have put their apps in the public domain using aws, so it must work. thanks a lot for any suggestion!stay positive lkashifshot in the dark: perhaps you need `path=` instead of `pythonpath=`? `path` is what linux uses to find programs (like python), `pythonpath` is what python uses to find libraries.tried both path and pythonpath, with same result. but thankshave you tried specifying that specific .conf using the -c option to make sure it\\'s not pulling it from some other location. see here: day in a life of a data scientist: does anybody have any good references on applying ml for supply chain mgmt and inventory forecasting, for parts in a factory/manufacturing context?i imagine the folks at stitch fix doin case you want to look very productive, go to this website are start typinghit `alt` three times when your boss walks byinsight mentioned in the nyt: big mention   and  big mention yes! i just didn\\'t read the whole thing before posting :wink:hey, does anybody know if there is a good package for fixed effect regression models in python 2.7? i’m trying to run a logistic regression model with fixed effects… thanksyou can just add dummy variables for the dimension you’re sweeping outor manually demean (i’ve found doing fe to be a pain in python)i’m totally new to fe models. can you point me to an example?this is pretty good: thank you! i’ll read it and get back to you if needed :wink:hey insight community! i’m staffing up a new project i’m leading at my company (vevo), and i️ am in talks with my senior leadership about bringing on some contractors to help with some data science workloads in the short term (starting asap), with a minimum ~2 month engagement. if you are interested, or if you know of good data science freelancers, please send a pm!thanks to  &amp;  for speaking to steve lohr at the nyt and agreeing to be featured in today\\'s article  i found this guy’s blog to be quite helpful. he doesn’t always focus on supply chain data, but he does have a lot of material for methods/metrics that are applicable to supply chain/inventory forecasting. thank you  , i will take a look!sure thing, good luck! :slightly_smiling_face:just saw this on the way home... way to go carlos, anasuya, and insight!wow, what a coup,  - close to half of the article is an ode to insight, huge pr win!god that looks boringdoes anyone have any examples of how to do time series histograms w/ time interval buckets?  thank you! :bow:if you’re using pandas you might be able to use resample:  are you using r? python/pandas? what frequency of series are you looking at (subsecond? daily?) more like in sql through self joins and stuff, thanks, but wont be using pandas or rif you have postgres or something similar use window functionsit will save you a lot of headache ok, will take a look at window fnthis is interesting/important. white house report on big data and civil rights.  my experience with similar data has been that synonyms, abbreviations, and shorthands (i’m picturing this something like the feed on a credit card statement, which is half-gibberish) shoots a bow approach in the footworks for things like “car” but imagine how many providers/terms there are for say, cell phone plansit depends on how standardized the text is. freeform text is rough for sure, but i\\'ve had success using it for categorizing the notes of sales people in the past.it just took a day of figuring out the most common abbreviations and cleaning them up  thanks, both of you, i should be able to get it to work with your suggestions!hey all, my team at sovrn in boulder, co, is hiring for a data scientist in the r&amp;d group. we’re looking for someone with a strong math background and good coding chops (python and/or java) to work on some really cool optimization and ml problems. if that sounds interesting to you, please pm me. also, i don’t know who the remote program directors are, but i’d be happy to chat with you and see if any of your folks are interested.lisadh:   data engineers - let me know if you\\'d be interested in a job at tachyus i’ve not seen a giant difference in the bs’s who are hired vs phds who are hired. from my perspective, the bs folks seem to be more energetic and adaptive while the phd folks are more careful and ‘big picture’ oriented. i think there’s room for both, but maybe on a more depressing note for phds, i don’t think the degree is magic sauce, at all, and i think that a talented bs can contribute just as much as a phd and can close any significant gaps specific to ds in a year or so. to me, this is because the valuable parts of the phd that overlap with data science are generally just sort of the ‘tool-belt’ that lets you do good science - if the phd was all about getting that tool belt, they would be significantly shorter.as i contemplate these days (maybe too much) the value of my degree, i have to admit that the phd challenged me to think more deeply, creatively, and thoroughly about a subject matter than i ever would have done had i not gone to grad school. as far as a hard skillset is concerned, i don\\'t think the advantage of the \"tool belt\" that phds possess is very significant, and even the size of the tool belt would depend heavily on the field of training (e.g. biostatistics vs mechanical engineering). the question is, once hired into an industry role, is the trained ability to think scientifically valued, or will the value always be confined to the realm of an academic setting? among all the ds roles that are out there, what fraction of jobs would actually benefit from the experience that a phd scientist brings?does machine learning = deep learning now? almost all chatter and job posts on linkedin would indicate so. did i miss the announcement that non-nn techniques are now only called statistical modeling or advanced analytics?if true, that\\'s kind of sad. i may be mistaken, but isn\\'t deep learning really still statistical learning on a fundamental level, as are all ml techniques? taking a bunch of data and identifying patterns and trends that are statistically generalizable. i think if people consider ml to only be dl, it exposes their lack of understanding of what ml/dl really is.i agree, and one source of the lack of understanding might be the hr/practitioner divide. there’s so much hype about ml in general that it’s difficult to sift though to understand real trends, but i wonder if the industry (or at least hr) is settling on titles like ml engineer = dl pipeline engineer much like data scientist became a somewhat normalized title a couple years ago. also ai = deep learning from everything i seem to read…i suspect a lot of it is marketing . would you apply to a position that says “you’ll spend 80% of your time doing logistic regression and 20% of the time cleaning data that is probably unsuitable to use in your logistic regression but you have to anyway”? :smile:but if you’ll be doing nn you’ll be all over that :smile:i\\'m intrigued. tell me more about this 20%-time cleaning data unsuitable for logistic regression job... :wink:haha i’ve seen some interesting things.  none of the features are particularly predictive (because of lack of investment in collecting the right stuff), they’re all super sparse, the classes are super imbalanced, and, of course, there’s not enough of it for undersampling.i’m going to need that model performing well by mid week please :smile:somewhere i have an image of what’s been called the saddest looking roc curve by some of my ds colleagues.hi all, if anybody wants to be a project reviewer for udacity’s machine learning nanodegree let me know. it’s a part time thing that you can do whenever you want and there are no obligations.there isn’t much public information on it but there is this: if this data science thing doesn\\'t work out... i can always review data science students!good post:it\\'s la crow-ix.  like unix hey everyone! every program at insight (data engineering, health data and data science) is gearing up for sessions this fall. so if you have leads on teams that are growing please feel free to reach out to your former program coordinators, program directors, or you know, email jake! :wink: we’d love to hear from you and work with any teams you think are doing exciting, cutting-edge work. don’t forget we have programs in boston, new york and silicon valley. plus our latest addition: health data science in san francisco! :unicorn_face: :bar_chart: :100: :wow:a good read on hiring data scientists ... can anyone recommend a good print publication to stay on top of tech news? i\\'m rather old fashioned and like getting newspapers/magazines in the mail instead of obsessively checking websites. inc. is good if your focus is tech+startup, and startup. pertinent to silicon valley. if you would like a comprehensive coverage, then i personally recommend, bloomberg businessweek + wired (lots of ads but still good) + inc + the economist ( although there can be nothing about tech in politically hot seasons)cool thanks!mit tech review is good, i also like physical media and subscribe to it :smiley:twitterthere are a bunch of individuals if you follow, who will curate the news for youand it can go very specific - crypto, vc, startup, etc...problem is, it takes effort to cultivate ittoo right mate :slightly_smiling_face:hey all, many of you may know me as helping to lead insight’s new york data engineering program. we have an application deadline coming up for the summer session and we would like to increase the number of applications and fellows in the program. considering that alums are our best source of referral, if you could send our (or my) way anyone who may be interested and encourage them to apply at  , i’d really appreciate it. the deadline for de applications is march 29 (new york and silicon valley), and the session would start may 30. questions about the program can be sent to  or me at  many thanks!!friend of mine actively interviewing for ds jobs is looking for a good resource to learn a/b testing. suggestions?i found combination of udacity a/b testing and inferential statistics course very useful for my interviewsit’s not exactly introductory, but i think slater stich’s set of guides () is very good.also, this article (“how optimizely (almost) got me fired”) talks about some of the practical pitfalls of a/b testing: big fan of this postevan miller has several good articles on a/b testing. hey all, hope your friday’s going well!quick question: has anyone incorporated test suites when building ml pipelines? would love any recommendations on conventions / frameworks / gotchas if people have any.i second evan miller’s site and slater’s blog post. between the two of them, you get a pretty comprehensive overview of common methods and tradeoffs/pitfalls  i still think the chainer version is cleaner than tensorflow, but cool either way :slightly_smiling_face:does anyone use databricks? did you have trouble importing keras?thanks davetensorflow (&amp; scikit-learn) workshop in mountain view this saturday:yeah, the default binding are pretty terriblei’ve found it to be fairly fast and scalable. you can also fit multiple time series in parallel using something like parallel in joblib. pinned ’s pdf  to this channel.for fans of the new yorker cartoons, this will be entertaining: uploaded a file:     how do you think... looking to connect with folks who are running ds teams of 10 or more, insight alum or otherwise - please pm or put suggestions in a thread, thanks!if anyone in nyc is interested in attending the upcoming dataengconf on 10/30, the founder has provided all insight fellows 25% off with the coupon code: ins25x (code expires this friday, 10/20)the speakers will include many of our mentors and advisors (e.g. chris wiggins, claudia perlich), as well as ovo genevieve smithi\\'ll be there!: does the elasticsearch (and/or spark-xml parser) lib include s3 libs? if this is a jar, one can see the content (it can be a “fat” jar) by “jar tf the_jar_file.jar”if, for example, there are 2 of the same library being included and they have different versions, you will get an error. if this is not the issue, post the error/exception and i’ll think about it a bit more...thanks  ! i think i figured it out finally. pegasus added the two aws jars to the spark-defaults.conf file under the driver-classpath — you need to do that to access the buckets. but in order to use his elasticsearch-hadoop connector, he specified that jar\\'s classpath on the pyspark command line. looks like that classpath was wiping out the classpath specified in the conf file so the solution was to specify the aws jars on the pyspark command line (or conversely, specify the elasticsearch-hadoop connector in the spark conf file) but not have two classpaths specified in two different places. thanks for taking interest!!glad it worked out! yes, spark’s config file is simply the default settings, when specifying new value with the spark commands (spark-submit, pyspark, etc.) they take priority and will override the defaults suggested in the config.for those who use h2o: what do you use it for? i\\'ve been asked to determine whether we should use it internally, and i\\'m having a hard time cutting through the hype to what it is actually good at.i drink it, usually. often i use it to bathe myself.: ask  he works there, started last december.  : h2o is a commodity software that you can unpack and start using as a ml platform. if you are advanced you can use the libraries and build it from, r, python and java. or if you are starting off, just download it, just the jar, and connect to your localhost:54321. and see yourself as to how easy it is to upload, and model data.nuggetbot check on jonneffjonneff has a happiness average of 1.1666666666666667i love life!!! everything is awesome when you\\'re part of a team.  happy happy happy.  nuggetbot check on jonneffjonneff has a happiness average of 1.3255813953488371nuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 0.9703947368421053that’s so awesome and wonderful! you are the best you amazing great awesome bot!nuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 1.039344262295082that’s a bunch of bullshit! i hate you nuggetbot. why do you destroy all that i love?stay positive jeremykarnowskinuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 1.0130718954248366nuggetbot check on kumbhanikumbhani has a happiness average of 0.696969696969697nice!a friend of mine without a traditional data science background (only bachelor\\'s in electrical engineering - currently working in a power utility company) wants to make the \\'jump\\' to ds. he\\'s learning python (numpy and scikit-learn) on his own time and just learning about the field as a whole. any advice or suggestions on what else to do to best position himself to make a career transition?  would those online data science courses be helpful to do (here\\'s a random one i found: ).100% of interviewers i\\'ve talked to have asked about what i\\'m currently working on, and/or my last project, so the advice i usually give people is projects, projects, projects. find some data. do a project. there are a couple of nice datasets/project guides on kaggle and elsewhere (e.g. ml blogs).hello .. i am just curious on what visualization tools are being used for json feed from api\\'s? tableau does not support live feed and i am still able to make the web data connector work. we can make custom visualizations from d3 but securing that needs effort (for enterprise). any obvious solution that i am missing?those of you on twitter have probably already seen this...really good readdimensions of  flavor   : we\\'re looking for a data engineer! please email me if you or someone you know is interested. we are thinking of developing a few different dashboards at work and i was wondering if anyone had suggestions for (non-tableau) tools.paid and easy to use or free and a bit harder to use?relatively cheap - doesn’t have to be free, but tableau seems very pricy.looker for just about anything, kibana(if you use elastic search), powerbi from msftor shiny if you use r and want to set it up yourselfthere\\'s a ton moreit\\'s a crowded field chartio, jaspersoft, bime, zoho, sisense, grow,  ducksboard, looker are all potential options from a survey i did about 6 months ago. i would do my due diligence in evaluating each option. also, r launched a dashboard-specific library this summer.tableau and chartio, the leaders in the commercial space, are very pricey.i’m a python person and i’m hoping to move my boss away from r.does anyone have experience/good things to say about quicksight?aww... nothing wrong with r. use both.: nothing wrong with r per se, but it would be easier if we all spoke the same language;) i tried to participate in a quicksight trial, but the permissions required were basically a showstopper for our security requirementsthanks, . good to know.we handle bank data though, so we\\'re extremely paranoid on the security  also starting to look at dashboarding, and we’re subject to bank data requirements too ;/anyone looked at anaconda mosaic, the dashboard component of their cluster management? the options you have will depend a lot on the end-users desire. will business folk be happy with static charts on a dashboard, or will they want to be able to dive into the data a bit more and filter/pivot/join to other data sources? for the former, you can get away with lots of free python options (i.e. jupyter notebooks, caravel) and not-too expensive managed options (i’ve found mode analytics quite good since they’ve added the option to work with ipython notebooks)if your end users want to play with the data themselves more and not write sql, i second looker as a great solution we want the business people to have some flexibility, so looker “looks” great! (sorry, couldn’t help myself)we are implementing looker here too. getting away from ad hoc requests is one of the big reasons.ad-hocs…blehyeah, the pm team asked us yesterday if there was a way to not bother us for everything.we’re also using looker, for those same reasons. it takes some setting up, but provides lots of flexibility for end-users.you may check the airbnb caravel. python/flask stack. open source, no setup for client. definitely curious about folks’ ramp up time with looker setup. it took us longer than i expected at my last gig, even with an analyst who was super familiar with our data model and good with sql working on writing the lookerml full time *and* a data scientist who had experience getting his previous company moved to it helping out (we found the engineering help they provided to get setup pretty useless unless your data model is quite simple)but business end-users absolutely love it and yeah…fewer ad-hocsat my last company, we used looker and it was fine. but people most likely need to know at least some sql. at my current company, we use domo which to me is more intuitive.tableau is the bane of my existence. has anyone successfully transitioned off it?  we’ve had a data engineer working near full time on it for 2 months building out looker dashboards for clients. def need to have solid sql background to build anything useful.hey everyone, we just launched a new insight ai fellowship, open for people who want to move from machine learning to more advanced applied ai work in industry: is it open to robots as well, or just people?you have to pass the turing test to get in actually i am trying to launch my web app on the ec2 instance t2.micro - my python script gets \"killed\" - out of memory. i used 5 gigs out of 8 gigs available (71%). i increased the storage volume to 12 gigs (47% used), the program is still killed. i am now looking at changing ec2 instance type but i have no  idea which one is the most suited? aws expertise is welcome.stay positive fplisson  would love to hear more thoughts on domo — what are the use cases? i think you are confusing memory and hard drive space. a micro instance only has 1 gig of memory, but 8 gigs of hard drive spacestay positive ryan.vilimyour process is getting killed because it’s running out of _ram_stay positive ryan.vilimrunning top will list all your processes and show you how much memory is being used in real timeif you are running out of memory, either change your code, or just upgrade to a bigger instance, it’s cheapyou could also add a few gigs by making a swapfile, but i recommend against that, it’ll be super slow and you don’t even have much hdd anyways. i upgraded my ec2 instance to m4.large (more memory). my program is running fine. i read about swapfile but i considered it as last resort. problem solved. thanks.i highly recommend against it.(a swap file that is)no problemstay positive ryan.vilimnuggetbot check on ryan.vilimryan.vilim has a happiness average of 1 given how a ml model i’m working on is currently performing, i think a happiness of “1” is too high by several millionlol we basically used it for all teams like finance, cs, growth, digital... data team manage it of course. similar to looker, you can make cards, and dashboards. we pull the data from redshift. you can also use d3 for visualization. does this answer you question? (curious as well since i might be deciding a bi tool to go with in the near future) what were the main differences with looker both from the model management (i.e. your) and the business user perspectives?we use looker and i think it’s a steaming pile of garbagethat\\'s slightly hyperbolic, but i’m not a fan1) it’s flexible enough to write queries to do a lot of different things, but not smart enough to do it efficiently. looker support told my boss that they haven’t bothered thinking about performance because most of their customers are on redshift or something comparably fast. we’re on mariadb and we’ve had to implement some incredibly janky hacks to work around looker’s limitations.2) too much config in ui. lookml (based on yaml) is a nice idea, but they’ve only half-heartedly embraced it. you can define a dashboard in lookml, or you can define a dashboard in the gui. you can even convert a lookml dashboard to a gui dashboard. but you can’t modify a gui dashboard by editing lookml. so  if you ever have to do a schema migration…have fun!does anyone have any experience using vowpal wabbit?data pipeline is equivalent to airflowbut airflow is easier to use and test, and cheaperglue i’m guessing is slightly similar to schema registry uploaded a file:  and commented: an early stage, very successful startup is hiring their first data scientist in nyc! this is a phenomenal opportunity to be able to define your own role under fantastic management with a pending patent defending their business model. please take a look at the attached job description and reply to the email at the bottom if you are interested!pretty cool way to think about it, especially&gt;that’s not because optics is easy. it’s because the mental models in optics are organized well.+1 to plotly dashthe insight alumni group on linkedin could use some more members (kudos  for setting it up). just sayin’hey guys does anyone have experience deploying airflow on a heroku instance with authentication? i have been running into an `bcrypt error` :disappointed:here is my stackoverflow question if anyone has any idea passing along something that might be of interest to alumni: wish they had a list of the questions =\\\\so cold i use bokeh on a digital ocean server, works fine, never tried it on ec2 did you have to do anything special compared to running locally? i’m getting some errors when i try to run it on the server. the bokeh user guide describes setting up a reverse proxy server using nginx, but i am a bit clueless on how that should work or if it’s even necessary stay positive along528 give me a break if you simply want a small number of users or just yourself to be able to connect and use the server i would suggest the ssh tunnel instead. the nginx reverse proxy config would be recommended if you want to serve a general user on the internet. the bokeh server is run exactly the same way as locally afair, nothing special needs to be done on the serverwe ( and ) are throwing a data science meetup at svds (mountain view). come listen to two short (30-45 minute) talks and socialize at the silicon valley data science office, just off castro st., mountain view. we will have pizza and drinks and we’re hosting insight alumnus andrew zaldivar (google) and allison gilmore (one medical). the evening kicks off next week at 6pm on wednesday, dec. 7th and will wrap up by 8:30pm. see the eventbrite page () for more details! invite anyone you think would enjoy the event! please rsvp on eventbrite if you are planning on attending (so we can order enough pizza).if anybody is interested in post-meetup drinks on castro st., let us know. it’s a 5 minute walk from our office and the mountain view caltrain station (here’s a link to the caltrain timetable for the lazy: ). hope to see you all there!nice article about building usable research code in a team for fellows currently interviewing: i found this list of questions to ask your interviewer be quite helpful. you can find a lot of similar lists on the internet, but this one was particularly thoughtful. uploaded a file:  and commented: insight at night reunites!have asked this before but... anyone here with experience doing image similarity analysis? currently using color-coherence-vectors as feature vectors (they do ok) but wondering if anyone aware of better descriptors? (have tried local-binary-patterns also). unsupervised problem on this side, thus, not sure how to even go at it from the deep learning perspective (new to dl). gist descriptors do well on landscape similarity thanks ! unfortunately, feels not quite suited for in my case.  trying to rank images of objects from the same class with much finer differences. fourier-based descriptor tend to work best when much info stored in the low frequency spectrum (not my case). additionally fourier descp are not invariant to rotations/translations which matters in my case. its a struggle... :face_with_head_bandage:maybe should add that have also considered the heavyweights sift, orb, et al but not much further. issues with speed and feature extraction  using these bad boys.stay positive ccci have a flask/html question! i want to include a user input in the form of a \\'sliding bar\\'. depending on where the user clicks on this bar, i would record a value between 0 and 1. i don\\'t even know what something like this is called in html/javascript. any help ?: try jqueryui slider widget:  (feel free to ask if you have questions about how to use it): i’m not sure what kind of resources you have to scale, but if you have access to cuda-capable gpus then you could use a pre-trained neural net. there are a number of nns pre-trained on images that act as excellent feature extractors. you’d want to do something like take the output of a penultimate neuron layer for the two images you want to compare and then compute mse or cosine distance. how fast are you looking to do the comparison?, great advice! i was able to find some papers pointing in the direction you mention. it does sound very promising although i would have to ramp up my game to go down such a path. currently i have no idea what a config file looks like for those systems, never mind a clue about computational load.  i have gone as far as compiling caffe  on my ubuntu pc (cpu only). do you have any recommendations as to where to look to find info on alternative tools/fast tutorials to quickly understand the technical challenges?  my image data set won\\'t be the bottle neck (i think). they are small pics (size and resolution) and not many (~5000).if anybody wants twitter data .. there is currently 80gb compressed .. uncompressed to ~360gb.. and more coming… it’s in s3… msg me if you need it in your projectthats a lotta tweets: forgot to say that i aim at comparing an incoming reference image with the whole data set. look  to pre-compute the full feature matrix for the full data set and use knn with a given metric to find nearest neighbors avoiding the brute force search (as i do now with my current feature vec).  but will have to run the nn on the incoming vector request in real time.:  did you see lory\\'s post?: i like using chainer because it is pretty readable and fully in python (as opposed to caffe which merely has a python wrapper).  they also have caffe model support so you can simply import the model file for any of the models in the caffe model zoo and then do forward passes to calculate any layer output. : caffe is also not super hard to use with pre-trained models on its own and if you have it compiled that’s half the battle :simple_smile: . i don’t know what “real-time” means in terms of throughput, but for some perspective some of the models i have been building will run through 5000 96x96 color images in seconds on a gpu.: +1 for chainer. i am using it for my project too.: great. i\\'ll dive into the details then. your stats reinforce that idea that running my model should not be a problem even for a cpu (~20x slower order of mag vs gpu?) as i will only be doing 5000 360x360 one time and then is just 1 360x360 per comparison request. only concern left is my images are very specialized (imagine all shoes for example). think a pre-trained network on \"all sorts of pics\" would have learned enough to resolve features that only apply to shoes?: hi adam, i looked at the link and there are some css and js scripts i need to link to to use the slider. but it\\'s not clear where i can access them from. the downloads section has a few but not all, for example the style file in resources folder. i\\'m sure there is some straightforward way to do this that i\\'m missing. if you are using the existing model and not training i think cpu is fine. there is an option to fine-tune the existing nn .thanks ! next step is playing with model to develop intuition for the approach\\'s resolving power within the specific class of objects i\\'m comparing.anyone here has experience saving greyscale image stored in numpy array to  png files with transparency?: hi lory! i may have use for that tweet data…is it historical data?: you actually don’t need to download any of the css/js to use the jquery ui widgets. you can use the  links for the ones that are there. the /resources/demos/style.css is just an example or something; it refers to this file here which is practically empty: yah… not clean tho.. for cleaned.. next week.. only english tweets, user, tweet, date: now its zipped with json in each line: there’s apparently also a bootstrap slider, too, but it seems more complicated to me: : is there a way to get location information? i’m using tweets restricted to us locations, but i’ve been collecting via the stream using bounding box constraints: yes, can filter by locationjust starting with hbase.  is it correct to state that it has no native support for queries?: take a look at apache phoenixthanks  – that’s what it seemed the solution was going to have to be.another big-table style solution would be cassandra with cqli\\'ve updated the repo for jupyter notebook tips and tricks available here:  the readme has install instructions that might help if you run into trouble.nice! i was scraping reddit-wide for my project so it was way easier to have the set then filter it down... still slow thoughgood luck!hey does anyone have access/could connect me to data relating geography related disease emergence? (i.e., lots of patients coming in with the flu in chicago or lots of patients coming in with asthma in sf)that\\'s for asthma in 2012. and i know there\\'s also for other years, 2009, and 2012-2015it\\'s what i used for my insight projectother states also have itlike ct and ny you can check  for this project on pediatric asthma ed visits in california by counties for several years.the asthma rates were scrapped from the california department of public health and califonia breathingand this: lots of options, but you\\'ll have to circumvent the js query page.no on cares about your insight project *runs away*nobody likes me :sad_parrot:bos-2017b does!the community health status indicators dataset has some county-by-county data for some diseases (as well as a bunch of things that are not diseases): . in case it\\'s useful to you i have a couple of tools for working with it in this repo: question on confidence intervals in predictions -- anyone have a suggestion for how to create a confidence interval around a scikit-learn random forest prediction?there was a good post on this topic a few years agolooks like there\\'s also a package implementing (or something similar to) the above idea too. i have not used it, but looks easy enough to plug into a pipeline:  -- that\\'s basically exactly the intuition we approached it with, but it\\'s a bit expensive computationally for a large # of estimators + datasetestimating the variance of the predictions via jackknifing would be my first idea. here\\'s an implementation of such a method: so the idea is jackknife is less expensive than bootstrap?ok i rtfm.  hastie and efron’s method is jackknife after bootstrap and only needs bootstrap replicates used in bagging.  so yes less expensive. thanks! those tools look super useful.from what i understand it\\'s a variation of a \\'leave n out\\' method, but instead of removing them, they are down-weighted by an infinitesimal amount.hello insight friends! can anyone point me toward fellows who have worked with instagram data for their projects? i’ve got a project idea i really like, and i want to get to acquiring data as soon as possible. any help or advice would be appreciatedyou can also look into mechanicalsoup: cool, feel free to get in touch if you have questions about how to use it, though it\\'s been a while since i looked at it so i may not be any use (especially with the d3 map...i have to learn d3 from scratch every time i use it) albert lee from my session didthanks!i\\'ve done some stuff with instagram data in the past, but to my knowledge they no longer support their public python apiagreed! the only way i would use selenium here is if the urls didn\\'t follow an obvious pattern, e.g. they were generated by some horrendously unreadable minified javascriptyeah, i saw that it’s no longer updated. i was hoping to talk with someone who has used it to see if it would be a good way to go for my projectwhat\\'s the benefits of doing that rather than just leaving it out? from a computational perspective, infinitesimally small is just 0?besides giving more stable estimates of the predictions (less variation), it also accounts for bias is the jackknife estimates.hm, it\\'s not immediately obvious to me why that would be. yes larger sample size would strictly decrease calculated sample variance of a single jackknife iteration since you\\'re just increasing the denominator but not the numerator. but i\\'m not seeing why the actual variance of the jackknife estimate (across all iterations) would be smaller since the value of each estimate remains the same. i generated my own data: for a regular jackknife, the bias has a factor of n-1/n. if instead of leaving 1 out, you leave an infinitesimal part delta, the factor would reduce to n-delta/n = 1.something along these lines.thanks ! i’ll look through your project and might hit you up if i have any questionsi’m giving a talk at pag in san diego on ds careers in industry focused on animal genomics on sunday. if you have any work at your companies that you’d like to advertise to academics that is related to genomics in plant and animal (non-human) genomes, please let me know! happy to include it.my experience is that these infinite scrolling pages make ajax requests that can be easily modified to get *all* the posts/images/columns in a single shot so you don’t even have to scrape the pages most of the time my point was that for a computer, infinitesimal delta is equivalent to 0. but yeah on a purely theoretical level, sure. or if you set your delta to something like 1e-5 i guess.does anyone have good tips or resources for mentoring new data scientists in their first role out of academia?mentoring is a very personalized thing :slightly_smiling_face:  would depend on what areas you’ve identified that you think they should improvei guess tip #1, identify ways in which they can growtip #0 : identify and help translate/build on strengthstip #1: build mvps with caveats to deliver value early rather than a better solution that takes more timethen build more mvpsand more mvps :wink: thoroughly go through the processes each company is using to achieve team coordination. but that would necessitate the team has such processes ... :stuck_out_tongue_winking_eye:haha, yes; our processes are mostly developed, but always being refined. i think the biggest difficulty i’ve seen is in getting into a business-focused mindset where there’s a balance between speed and perfection, so i’m trying to figure out how to support new people in that transition. thanks for the suggestions!i found that taking some of the pressure off of them works.  in teh past i’ve just said “i just need this model to do something by x date, give me the best thing you can, but let me worry about the outcome if it doesn’t perform well”  (also, you have to mean this for it to work, the model might not do well :stuck_out_tongue: )make the on-boarding period a sufficiently \"rich\" experience to assist in the change of mentality during this low risk period....?lol my suggestion is the *exact* opposite of  . i guess it depends on the person who is the new hire :slightly_smiling_face:time to save the internet! hey guys, does anyone have any experience porting a sas logistic regression model to sci-kit learn?if i have a s3 dir full of csv files, how long does a emr cluster take to convert them to parquet.  and secondly, what is the best way to convert csvs to parquet (besides emr)?  i’ve started looking into using aws athena / spectrum, but query speed is super slow if data is not in parquet formatideally, converting mysql extractions to parquet files periodically is great, if you know of any such tool, pls let me know! :bow: you should be able to write a pretty simple spark application that uses the csvreader into a spark dataframe and save to another bucket as parquet. i assume you can just submit this job to emr and it’d take care of it. more hosts the faster it is, unless each of your csv files are like 1kb in super small. one catch is to read all csv files at once otherwise there could be schema inconsistencies in parquet from file to file or folder to folder.any good resources on price testing methodologies for susbcription services?any one else losing their mind over gdpr out there? join morning everyone, does anybody by chance have the flask mini tutorial that insight provides to new fellows? this link may work: thanks mike! access requestedfrom ruiz.eulerthat request was probably routed to the nyc office. let me know if you don\\'t get access, and i can download and send you the files.thanks mike, looks like you\\'re right, i would appreciate if you can send them here has started doing a tutorial for boston. he can probably share something with you.there\\'s also this that we give to the de fellows: : if anyone\\'s looking for housing, the other unit in our duplex is opening up:2br, $3800 in the mission (22nd &amp; folsom).two insight alums live upstairs.: i’m the first and still-only data scientist here at greenhouse. we are starting to think about the “career ladder” for me and other data scientists, analysts and engineers. i’d be very interested in seeing the structures and skills that you all have defined at your companies. dm me if you’d be willing to share your insights!: i have permission to share our “data science levels” deck at glassdoor. it’s kind of rough but just a sec...our director (actually a vp now) was planning on a blog post about it sometime anyway. i’d be interested in seeing that as well!that\\'s really great  -- thanks for sharing!nothing really about data engineering in that. that is a different org, but data science &amp; analytics has 1-2 product managers that are product owners for their scrum.at smaller scale we were all under one director.how did the transition between the single director and multiple product owners go?  (i’m in a similar boat to , first ds here and we’re growing our team)when i started there was five ds/analysts and a few engineers, maybe 4-5 who did a mixture of bi etl and sexier big data stuff all under one director who reported to the ceo.then i believe around the same time we brought in a director of data engineering and a second analytics director who has taken part of our team under sales and marketing. my supervisor is the vp (original director) who reports to the cto.i’d say the two areas in which things are a little more stressful big-company-ish is that 1) the core data scientists are now split into two big teams and sometimes we wish were were more intimately connected. we at least have an all data science meeting every two weeks. on the other hand our director was way over loaded before the split so in net it is good.and 2) now i have to go through a product manager to get some data needs implemented. which is sometimes feels slow.the nice thing though about having a dedicated data product manager is that there is less explaining of your needs to engineers and you get more solid delivery commitments. i had a few cases in the old days when i had something 90% done and then it felt like i spent 2x the time just explaining everything to someone who wasn’t really adding much to the project.haha i know the feeling on that last bit...well really just one guy and he’s leaving so idgaf :laughing:i am in a similar boat that i am the only data scientist here at booster as well, just curious if you could share your experiences in regards to having an imapct at organizational level, and dealing with perhaps people who are not used to data.thanks, alan !now it goes more like: “i want all our job descriptions in a new db that is more analytics friendly with metadata a, b, and c so i can build product x and y” and i get “cool, but no, we don’t have time” three times over three months until another product manager is finally really curious about whether we can do y and i finally convince our director that this shit could seriously be cool, yo; and then magically the data product manager comes to me one day and just says “yes”. later we have a couple easy meetings, i don’t hear much for a quarter, and then bam it is ready.making it sound nice being the only data scientist :stuck_out_tongue:although i couldn’t sell them on my title being “god-emperor of data\"oh, yes, enjoy it while it lasts.we’re picking up more out of the current insight session, but it’ll still be an integral team rather than splitting permanently in organization to different groupsthough likely still reporting to the head of business products like i do nowput that shit on a business cardstay positive johnrwalkyou’re not my supervisor nuggetboti was never in a single ds org, but i was surprised to hear later after we had some successes with some internal data products that the “ceo and other leadership never thought you could do x…” and i’m thinking what? they hired data scientists and they didn’t know we could do x? — it makes you realize how important it is for information to flow up the org selling your capabilities., yeah i thought i was good at talking to people at different levels, but someone just recently told me that i should stop thinking about this logically :stuck_out_tongue:you need to feel the answers.dealing with business people is more about personal relationships…that’s the lesson i’ve been trying to keep in heart and mind whenever  i have to talk with them.and just be as explicit as possible. there’s a saying in spanish, cada cabeza es un mundo. each head is its own world, and that has never been truer for me. i was hoping to hear more of what anyone had to say if they were the only ds working around challenging specimens :stuck_out_tongue:well, i think  is maybe being a bit too negative about business-folk.:stuck_out_tongue:that’s why i need to hear it from you the most experienced peoplei think the challenge is for us to come to their level in understanding the business problems and actionable insights that can be deliveredstay positive zkermish…and to communicate in a constructive way when the question they\\'re asking won’t lead to an actionable insightbut great discussion about teams and structure (and thanks for the slides )!. the flip side of ’s comment about information flowing up the org to sell your capabilities is higher-ups asking for things that they think are possible but aren’t. communicating the limitations of data science (often given data limitations) for a non-tech audience is also challengingrelated to that is the perspective i’ve run into now and again: “this data product you built is wrong in cases a, b, and c, when are you fixing it?”  well, actually, i have no immediate plans to fix it because it is correct in the much more common cases; and in fact a human is going to be wrong in cases d, e, and f. even though everyone has heard and mostly accepted the 80/20 rule, sometimes people fall back to absolutes and needs to get a little reminder that getting to 81% is going to require another 20%.stay positive alanoh, and my colleague mario will be on that general assembly “how to hire…” panel tonight in sf. he’s a cool guy, and supposedly our vp of data has prepped him with good advice. if anyone goes you should ask him about banana hefeweizens. apparently he likes that sort of thing.today is a good reminder of why this slack channel is great.one more resource for interview questions (with answers / hint in python) - hi all,i\\'m looking for a presenter for wwcode-sv data science group\\'s monthly talk (august 24 evening at cisco (north san jose)). we had some scheduling issues that\\'s why the last minute call.if any of you would like to give a presentation on a ds or de topic (about 60 min), please let me know. (the topics of the last three meetups were elasticsearch, careers in data science, scalable machine learning in r and python with h2o)please let me know if you have any questions or if you would be interested in giving a presentation on aug 24 or at a later time.thanks,    juditabout wwc:women who code is a global nonprofit organization dedicated to inspiring women to excel in technology careers by creating a global, connected community of women in technology. the organization tripled in 2013 and has grown to be one of the largest communities of women engineers in the world.andrew ng\\'s coursera machine learning course in python: hey all - does anyone have recommendations/resources for ds wanting to work on failing fast and iterating?sprint planning + standups are good to see where people are at and what they plan on doing and how long they plan on working on a task, and can use those as a way to nudge people to refocus.deadlinesmake tickets with clear acceptance criteria, i.e. instead of saying \"build a model\" either put a deadline on it or some other way where they can iterate quicklyi have had my best results with short, non-intimidating standup meetings. if everybody shares a 1-2 minute progress report, no one feels singled out and no one gets left behind. this can be hard if you have members of your team that get kind of attacky, in which case the scope increases to reining them in too. this also starts to get unwieldy if the meetings are too big. i have found that groups of about six work pretty well for this kind of standup. if everyone has to report their progress every day, then it\\'s hard for team members to keep spinning their wheels on tasks with marginal returns.related to brianna\\'s question above about failing fast, how do your teams balance development code vs production code? example, i was given a data dump with lots of raw historical data which i (the ds) wrote python code to build features and train model. now my team wants to push some of the feature creation (e.g. historical averages) back into sql via sas. this production code will be run on new data not the training data, so i can\\'t plug it directly into my python features code.i’m going to guess the lack of replies is due to imposter syndrome. :wink: i’m certainly not a great data scientist, so what can i contribute? that said:i think “great” data scientist know that data science isn’t building models and wrangling numbers; data science is about using data to inform decisions, build products, and make your company money. they know the 80/20 rule, they talk to customers (internal or external) to make sure their work is having impact. they communicate effectively and often.+1 for standups and deadlines. if you\\'re a team lead you can evaluate progress continually, and always descope, descope, descope+1 for deadlines also - you can help them learn to scale back things as deadlines approach.  they should be thinking of the project in pieces.  what does the whole thing look like?  if i had to deliver it tomorrow, what could be cut?and then order those thingsi’m working on something like that right now for example.  deadline next friday - there’s a plan for the whole thing, and then a multi-stage contingency plan of things to pull back if it doesn’t look like they can be completedi would try and understand what the source is of their \"perfectionism\". sometimes, it is fear of being judged, sometimes it is an environment that is perceived to be one in which one can\\'t fail, sometimes it is habit, and sometimes it is lack of knowing how to work in a different fashion. in my experience, the best solution is an adoption of agile (standups, deadlines, pre- and post mortems) adapted to suit data science and ml tailored to the source of the problem.\"what does the whole thing look like?  if i had to deliver it tomorrow, what could be cut?\" -- that is also a good interview question :wink: if you have enough influence on production code then the dev code could go a long way, and be a part of the production code. if there is a hard line between the scientists and the production engineers then you are limited by what the engineers can put into production. also, writing back features to sql, is akin to the dutch boy with his finger in the dam. there are two things at play here, one the company has to evolve, and for the company to evolve they need to see the real value in models - education about data analysis. the fact that models are fast, features engineering is easy, tunable to variations in the data, capable of online learning, unlike a rules based approach are some of the things they need to see and value.more information: we have a small team with no production engineers. the structure is that the data scientists write python code that sits on top of the sas sql pulls created by data analysts.i might be inclined to see if there\\'s a larger problem thatcan be broken down into smaller pieces. this has the advantage of creating small, esaily achievable goals, and also serves as a foundation for figuring out where this person might be getting \"stuck\" in the process.i have also had one instance (and thankfully _only_ one) of someone using \"perfectionism\" as an excuse for never finishing anything because they didn\\'t actually know what they were doing and were generally bad at solving problems they haven\\'t encountered before. :confused: anyone want to save string theory!? all you need is some good data science!(views are my own: you may not believe string theory needs saving, or deserves it. ;-))“*goals and vision*we are very pleased to be hosting the workshop on data science and string theory. this is the first meeting of its kind, where the goal is to unite big data techniques with string theory, in order to systematically understand the string landscape.”haha jeeze they are still at iti don’t really think of the landscape as being science, so hard to tell what data “science” there is to be done with itlet string theory die and good riddancebrutal gentlemen, brutal!and i agree! :smile:i went to grad school at penn with jim halversoni lived with a string theorist in grad school and me and the other astro grads mocked him mercilessly, he graduated in 4.5 years and became a phys prof a couple years later…theorists (a former one myself) don’t need nearly as much time to write a dissertation, as they/we aren’t waiting as much on datai’d be curious who that was, , as faculty positions in theory are frickin’ rarefractionally speaking (as a fraction of phds in theory produced)it was chris beem. he was at stonybrook and then moved to oxfordhas anyone worked with aws glue or aws data pipeline? i\\'m curious to hear about your use case and when would it be an ideal managed service to use.any scala devs here? i\\'m wondering what this syntax means:abstract class treenode[basetype &lt;: treenode[basetype]] extends productabstract class and extends i get, and treenode[something] means that treenode takes a variable of something type, but i don\\'t understand basetype &lt;: treenode[basetype]similarly, abstract class rule[treetype &lt;: treenode[_]] extends logging`a &lt;: b` means `a` has to be a subtype of `b`see is basetype an actual class, or is it just used as a generic class identifier?usually it\\'s either a class or typethe above example i pulled is from the spark source code, and i\\'m confused because i don\\'t see basetype as an actual class defined in the spark packageit seems to be a self-typeah! ok, i think this is pointing me to the right trackthanks a bunch! i really need to go learn scala i have not looked at data pipeline but my company was planning to use aws glue for etling data into redshift from s3. glue did most of the things we needed it to do. however, snowflake managed using home grown apache airflow did everything glue could do faster and for less money so we went with snowflake instead. i think glue needs to be more transparent about it\\'s \"job bookmarking\" feature (how it tracks which files it has processed and which files it has yet to process) and also provide a better view of job run history in general.worth looking into higher kinded types: personal disclaimer: i highly advise against trying to understand this in detail if you are not familiar with elements of type theory, for now just having it in the back of your mind should enable you to sleep better at night :slightly_smiling_face: when you say \\'snowflake\\' you mean snowflake data warehouse, or something else?yes the data warehouse.how did your airflow dag take into account changing schemas? assuming it did  something like the cataloging of tables via the crawler like in glue.it did not, our data is very simple.however, with json data, snowflake can handle changing schema. but not other data typeshmmm, perhaps glue would have been better if you had an evolving schema that wasn\\'t json.then again i can\\'t imagine what kind of task(s) you\\'d need in airflow to take into account changing schemas.i would not want to implement automatic schema discovery by my selffor the video files, if you install nginx-extras instead of just nginx, it will include the mp4 module. then that configuration info goes into nginx.conf, telling it to serve files from the \"video\" folder with the mp4 command. “make sure you are convincing them” words to live by!speaking of domo, looks like they re creating quite the splash at the tableau conference (involving snoop dog no less)   : for those of you working on data science teams, what metrics do you use to measure your performance? what do your performance reviews look like?my team is weird, so probably not representative, but our performance reviews are a mix of target metrics our managers set with us (blog post count, projects completed, some other fuzzier ones) as well as peer reviews from four peers (two we get to pick, and two our manager picks). note: i haven\\'t gone through one yet, but that\\'s what i\\'ve been told.this is awesome. thanks i used java heavily in my last ds role. i recommend learning the standard collections well (which i mostly did from the oracle docs). nd4j is a decent replacement for numpy. apache commons has a lot of general math and stats. feel free to ping me with more questionsexcited to announce our latest insight fellows program focused on data product management. you can read all about it in this blog post:  if you\\'re a data pm, are hiring for one, or just want to compare notes or get involved in the program be sure to reach out to me or , who\\'s developing the programjeremy here. happy to answer any questions you might have about the program here or in private messages.ah thank uhi everyone! get a chance to win 50$ amazon gift card by entering a few made up sentences, follow this link: affordable mechanical turk! enjoys eating all of the peanut m&amp;mshey guys. i’ve been acting as a data pm for the past 6 months or so. happy to get involved and/or chatya damn rightalso, i added fail2ban on my aws - if you go over authentication logs, you can see thousands of attempts to access your server. another good strategy is to enable ssh-key authentication and disable password authentication. you can also enable 2fa via pem.fyi, with respect to the account takeover on aws, i wanted to put this in general, as securing your account is important, esp for new folks. it is crucial to enable ssh-key authentication and disable password authentication to prevent your account from being brute-force attacked. strong password, disable the default user ‘ubuntu’ if you want to be extra secure. if you want to see how often people are trying to log into your instance, just type:grep -i ‘invalid user’ /var/log/auth.logfood for thought.hi, i’m curious to learn about people-you-may-know models on social networks. that is, what features, models, and other components are involved in predicting who a specific user might like to connect with on a social network like facebook, twitter, or linkedin?for the people who use the latest version of macbook pro, is it possible to find a keyboard protector that will have the touchbar work?i was confused as to the equity questioni thought it meant per year, heh.also not sure how it differs from rsuthought someone was getting 350k in equity a year and thought damn! need to get that job!yeah, realized after the fact that those questions may be poorly wordedany advice for clarifying w/out messing up interpretation is welcomewas planning on just merging the two overlapsi\\'d just go with \"equity value per year\" or somethingthis was a good idea. thanks! okay, updated the questions, lets see how responses come in over the weekend. i’ll post once more on monday morning, and then i’ll stop spamming the channel.the original questions/answers are preserved on the spreadsheet (here: ) but the survey now reflects the latest updated questions: has anyone performed a mapreduce in bigquery following these instructions:i have a project and table correctly identified, can ssh into my instance via command line, but when i run the script, it seems to stall at the error: command failed: wait ${subproc} on line 326.this did not fix it:if anyone has experience with this, i would greatly appreciate any tips. please pm me so we don’t spam this channel :slightly_smiling_face: thanks!hi fellows, my company currently use swift for ios app, ruby on rails for the backend server/web app. i want to build a recommendation engine.  any suggestion on which language/stack/pathway i should go with? thanks! totally from scratch.the rec engine should probably be fully independent/separable from either frontend codebase. assuming they both write to the same backend db, i’d recommend your rec engine be written in whatever language is best for that problem (likely a mix of python and sql, perhaps other stacks if you have a particularly large data set you’re building recs from) and you can serve up results via an api (again, probably a combination of python and sql, but ruby could also work well here)the more independent and flexible your rec engine is the easier it’ll be for you to iterate on the it (leading to greater general happiness :wink: )i think there are a few possibilities:1) you simply create the full data by duplicating the rows. as long as you don\\'t have too many features, 500k rows won\\'t be too much for sklearn to handle.2) r-based packages have support for count proportions. the basic idea is that the likelihood is written as a binomial instead of as a bernoulli. e.g., see here for glm () and here for jags ()3) you might be able to use the sample size to adjust the \"weights\" in sklearn\\'s logistic regression. for instance, see the top answer in this thread (), where apparently you can input a weight for each data point. i\\'d try this out on a small synthetic example to see that it matches the results from #1 or #2, though.you are correct that linear regression is the wrong tool for the job, though. :smile:(my sql tutorial slides)automatic text generation fun.  theoretical physicists especially encouraged to try:can you guess a real arxiv paper from a fake one?stay positive ruthtoneranyone using the most recent version of airflow (1.8.1) willing to answer a question about it?  we\\'re seeing weird behavior surrounding backfilling and can\\'t tell if it\\'s a bug or something specific to our setup.  i can start a thread to not clutter the main group, any advice is appreciated!brianna.schuyler: i assume you mean airbnb\\'s right?yes sirdo you guys use it at square?we use a fork of iti have only dabbled myself, but i know jackie has used it a lothmm, ok.  the thing is, there\\'s this new concept in the newer versions called \"catch up\", which is different than a backfill.  so there are like two use cases: 1. we have a dag that runs daily with a `start_date` of last week. when it turns on we want it to “catch up” and process the last 7 days. 2. we have dag that runs daily with a `start_date` of today. we want to manually backfill using `airflow backfill....` and have it run for the last weeks worth of days. in both cases, each day should run to completion before starting the next day.  our dags are doing the correct thing in the case of a \"catch up\" but with a backfill they appear to be ignoring the state of previous days. we\\'ve tried using `depends_on_past`, `wait_for_downstream`, `max_active_runs=1`, and dedicated pools of size 1 and none of them have worked.  it\\'s weird that there doesn\\'t seem to be a lot of chatter about this on the internets.i would toss that in am email to jackie. or try pining her directly, not sure she checks slack too ofteni know she had to convert most of her etls overk, will do!hey sv / sf! you\\'ll receive an evite early next week, but our new session is starting on may 30 so the alumni / welcome party will be friday night 6/2 @ 8pm! save the date and come say hi!ooooo it\\'s coming so soon!soon...thanks guys, this is fantastic. especially, i love it when there is an actual package to do this :slightly_smiling_face:career advice question: i\\'ve gotten a couple of messages from recruiters at big companies that seem to be actually writing to me. i\\'m not interested, and don\\'t think i\\'m qualified enough yet for these to actually be high quality recruitment attempts - they feel right on the border of spammy, and are probably just casting a wide net to get a lot of interviews - but is there any downside to not responding?there is no down side to not responding if you\\'re not looking or interested. some are in fact just spamming everyone, and others are targeting more narrowly. you can generally tell based on how it\\'s written. :wink:i ignore the spam ones, and i follow up on others if interested. a recruiter message on linkedin got me an offer from ms a few months ago that i almost took, except in the end moving to seattle was a bridge too far for my family.if you reply to good ones and politely decline, saying something “thanks for this great opportunity but i’m currently not looking for a change” they often keep you in mind for future positions they have, at which time you might in fact be interested.but ignoring them doesn’t do anything badi attempt to respond in bulk maybe once per week to as many as i can get to in 5-10 minutes or so -- you can copy/paste something like papadimitriou\\'s message and move on. likely they\\'re just searching \"data scientist\" and spamming everyone, and time is valuable so i wouldn\\'t feel bad at all about not responding if you have better things to do with your time :slightly_smiling_face:anyone know of any good resources on proper data modeling? i\\'ve heard great things about this series  but i\\'m on a time crunch :confused:you can send interesting ones to insight :thinking_face: not interested but you can find more people like me at insight?hi all, can i get easily get a mysqldump into postgres?unsure, but maybe you can go mydsqldump -&gt; csv -&gt; postgres?also: this link looks very interesting apache drill claims to be able to do something like thiswe use tableau for lots of things, but people here know many open source python solutions as well thanks , do you have any thoughts on this ?mapdsuperset!what are your favorite resources for setting up a data science environment using jupyter notebooks/python/terminal on a windows pc? what about favorite blogs for doing data science on windows?i’m coming from a unix/mac background and i haven’t used windows in over 10 years.hi becky. i\\'m also a current fellow, in nyc. i\\'m using pyaudioanalysis. it is for audio data but i think many parallels can be drawn. the code is written in such a way that you can modify the functions to your particular needs. check out the features they extract. i work with python in linux, mac and windows.  python and pip works more or less the same in all three operating systems.  virtual environments work the same.  setting up virtualenvwrapper is a little different for windows ().  one little known feature of windows 10 is the linux subsystem - you can basically run linux within windows:  . it’s super cool and it works pretty well.  however, the filesystems are separate.is anyone familiar with spark\\'s catalyst optimizer (automatically optimizes queries for spark sql and the dataframes/datasets api) and writing custom optimizations? i\\'ve been working with the spark-cassandra-connector and discovered that write throughput (from spark to cassandra) can be increased dramatically if the dataframe is sorted by cassandra partition key prior to writing to database. i\\'m considering modifying the connector to automatically perform this sort prior to writing to database by writing a custom catalyst optimization, but i wasn\\'t sure if this is something that is actually possible have you been able to time how long it would take spark to sort the data for each partition key? this should help you decide if it’s worth building it. e.g sorting a df on spark and complete write to cassandra takes 1hr vs just a unsorted write from a spark df to cassandra 2hr. yeah, with a 26 gb and a 260 gb dataset, i\\'ve discovered that sorting the data first would be faster. results may vary of course depending on the exact distribution of the data, but it\\'s significant enough that there is an open issue that datastax (developers of the connector) has put up for it.  it\\'s something they\\'re still looking into. we think that it\\'s worth building, and creating this custom catalyst optimization is the way we\\'re thinking it should be implemented, but i\\'m just not really sure how to work with spark\\'s catalyst optimizerah that’s pretty awesome. you plan on extending the current connector? yeah - it\\'s quite unintuitive that the write speeds would jump up so much by a simple sort, which is why they\\'re thinking of just making the connector automatically sort by partition key before saving. catalyst is basically the optimizer under the hood that makes dataframes so much faster than rdd\\'s (and the reason why spark\\'s contributors highly recommend using df\\'s over rdd\\'s), as it does a bunch of pre-optimizations that aren\\'t possible with rdd transformationsit seems that the best way to implement this automatic sorting is to just add a custom rule in catalyst, but that kind of functionality is quite new in sparki have dealt with a/b testing quite a lot at glassdoor. we have our own in-house ab testing framework. the allocation part of it is mostly a java port of facebook’s planout (find it via the java link on planout’s github repo). the analysis and reporting is custom python. i can’t do a very fair comparison to services such as optimizely because i wasn’t part of the infrastructure decision process (but i think at least one group here *is* using optimizely).i’d say three of my big tips i can give is that to do good a/b testing you need to1) get really really focused on what is important: i see it all the time that executives and product managers look to the simple top-level metrics because they are easy to understand, measure, and move. sometimes you have to settle for these, but at least keep reminding everyone of the limitations and what might matter more (and push to get the data in place to measure these)2) make sure tracking is rock solid. when you are looking at 5% effects all kinds of quirks of the data can cause problems. when doing a/b testing on a new part of our product i don’t think we have ever had a clean test on our first try.3) watch out for the long-tail of users: as an example consider scrapers/bots, we get a fair amount of that activity and do a pretty good job of identifying them, but just a few slipping through your filters and ending up in your tests may shift the results artificially. the trick is this activity can be really similar to your most active users which you definitely do want to include in your results.thanks for the info and pointers ! yeah, we were looking into planout as well since it seems to deal nicely with related experiments via namespacesand, fyi, if you use something more general like planout then expect that it is going to take a lot more work explaining the data model and test setup to product managers. it is not just one users &lt;-&gt; one treatment and some may take a while to full wrap their heads around it. of course the upside is that at scale it is a lot more powerful.on your analysis, what’s your method for controlling for false positives when running many concurrent experiments? it seems to be a common pitfall if not using something like a hierarchical bayesian method or whatever optimizely does (some variant of sequential testing?)thanks  for planout - working at the parameter instead of treatment level seems so obvious in hindsight. [programming] has anyone encountered generating a powerset with some of the elements having a parent-child relationship? trying to visualize the logic to put it into code.: example? also are there any special constraints for including a parent/child relationshipso like if my set = { a, b, c, d} and a is parent to b, is {a,b} a valid component{a} and {a,b} are valid, but {b} isn\\'tok so a child must be accompanied by a parent? ;0and neither is {b, c}, {b, c, d}... etc*;)yeahprobably the easiest way would be generate the full power set and then remove ones where parent isn\\'t preent?present*well i would assume taking the powerset is out of the questionyeahjust wondering if that is out of the questionsince it\\'s obviously exponentialthe powerset is here to staylolthe use case involves finding every combinationwell just take those unaccompanied children homeor take them outpretty inefficient if its sparse thoughtruedo u know how sparse the dataset you are dealing with is tho?how often is there a parent/childi have 100 elements, 20 of them have one or more childrenif not too often i say go for itok never mindhm well given the validity example i would still consider doing it and back-checkor you could always make sure to add the parent firstbecause order doesn\\'t matter here, correct?nopeorder doesn\\'t matterthing is i don\\'t think it\\'d make a difference for timewould it? it\\'s o(n) just to remove them all(i feel like this is more and more of an algorithm interview question)well there would be 2^n total sets to constructworst casewhere n = the number of elements in the original setbut you would have to generate the 2^n sets first and _then_ scan to removeyep, so that\\'s 2^n again isn\\'t it?or wait no, it\\'s o(m) where m is the size of the power setrightwhich is why i was assuming just doing that was out of the question beforei feel like if you create a power set of all the elements w/o children first you would have a good startdon\\'t get me wrong there\\'d still be a vast majority of valid elements ungeneratedbut yeah computing the power set = no buenothere\\'s a good one for lda that has topic on the x axis and words on the y. i found it useful for examining the output side by side.has anyone worked with airbnb’s superset dashboard?johnrwalk: we looked into it, but i think it had a weird requirement for the backend. i’m interested to hear about it if you end up working with it.i’ve been able to get it working on my own machine easily enough, but i’m running into trouble exporting the dashboardswhich seems like it should be simple…that it does…it has the functionality, but there seems to be some underlying flask issue that causes it to not actually present the dialog to get the export filei made a word cloud! :slightly_smiling_face: i looked at it but went with  instead good advise, the backfill is not working eitherdo you have some advise as of how to debug what may be happeningi’d start w/ an extremely simple dag and do a backfill on that and build up from thereyeah so that\\'s what i didit used to work but for some reason it stopped workingit\\'s probably something trivialit\\'s the hello_world exampleit just get\\'s stuck in the run partlet me try another simple examplethis is what i see in the logsi mean in the backfill outputairflow backfill hello_world -s 2017-08-16 -e 2017-08-17[2017-08-17 13:33:25,860] {__init__.py:57} info - using executor celeryexecutor[2017-08-17 13:33:26,244] {models.py:167} info - filling up the dagbag from /home/ubuntu/airflow/dags/home/ubuntu/anaconda3/lib/python3.6/site-packages/airflow/ti_deps/deps/base_ti_dep.py:94: deprecationwarning: generator \\'_get_dep_statuses\\' raised stopiteration  for dep_status in self._get_dep_statuses(ti, session, dep_context):[2017-08-17 13:33:26,771] {models.py:1126} info - dependencies all met for &lt;taskinstance: hello_world.dummy_task 2017-08-16 12:00:00 [scheduled]&gt;[2017-08-17 13:33:26,775] {base_executor.py:50} info - adding to queue: airflow run hello_world dummy_task 2017-08-16t12:00:00 --pickle 1 --local[2017-08-17 13:33:26,785] {models.py:1120} info - dependencies not met for &lt;taskinstance: hello_world.hello_task 2017-08-16 12:00:00 [scheduled]&gt;, dependency \\'trigger rule\\' failed: task\\'s trigger rule \\'all_success\\' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={\\'successes\\': 0, \\'skipped\\': 0, \\'failed\\': 0, \\'upstream_failed\\': 0, \\'done\\': 0}, upstream_task_ids=[\\'dummy_task\\'][2017-08-17 13:33:31,697] {celery_executor.py:78} info - [celery] queuing (\\'hello_world\\', \\'dummy_task\\', datetime.datetime(2017, 8, 16, 12, 0)) through celery, queue=defaultnot sure what’s up, sorry.my only advice would not be specific to the problem but general debugging techniques not specific to your error (e.g. start from scratch again)yeah i\\'d agreei will do some clean up and try it againfor debugging though do you know if one can actually see which workers are running and then get inside what the process is doing?  i could use celery or localexecutor, or sequentialexecutoruse sequential to start “local” is actually what i used to do parallel submitting of tasksthere might be a verbose output when running airflow, not suregreat thanks dan! local executor is working thanks a lot!does anyone use aws kinesis, i’m wondering if it’s a good idea to set your own parition key so that groupings of data go into a shard rather than have it be randomly generated; any suggestions welcomei’ve played around w/ it before. depends what you’ll want to do with the data after it’s in the queue.  for example if you want a consumer to only read in data, e.g. form nyc, then might be a case to partition by city (possibly at the expense of complicating other consumers’ logic though!)and could see a slight performance gain if all the data is already co-located at the queue level.a definite potential downside though is that you may get “hot spots”, like having some shards with a lot of data, but others with very little data.ah great explanation, kind of what i though, thanks for taking the time! pinned a message to this channel.linux to mac is easy for the most partjust use brew as a package manager to make up for the lack of one in the macthe new hardware\\'s\\u200b direction is another matterhey ethan, one more question — i’m a little confused by how lightfm treats the `user_features` matrix in the `fit` method (which is presumably where user attributes are applied).  it’s described in the documentation as “the user weights over the features”, (as opposed to defining the features themselves?) so i’m not sure i’m interpreting it correctly.  also, in supplying users to `predict` does the `user_features` matrix need to be fully populated?  i may not have the full set of attributes for new users coming inagree on linux -&gt; macosdoesn’t take longlot of unix functionality already providedxbox or play station! if we really want to get a war going we should do vi vs emacs :wink: :vim:vim &gt; vi:+1: :vim:or vim vs neovimneovim is my chosen platform! love it!(after 11 years of vim)never heard of neovim — am i missing out on something? (vim user)umm, not super missing out on anything after vim 8 added async plugins.  neovim has a nice terminal though.and it\\'s aiming for the holy grail: a vimlib.c that you can link against from anything. full vim in whatever text editing program or ide is the dream.:heavy_dollar_sign:theres a vim mode in emacs (evil-mode)yeah, it\\'s definitely a bit confusing. the way to think about it is that the row of the `user_features` matrix should match the row of the user in your `interactions` matrix. in terms of the features themselves, you can think of them like regular scikit-learn features. the one key thing is that you might want to one-hot-encode a unique dimension for each user. if you were to _only_ include a single, unique, one-hot-encoded feature with weight 1 for each user and each item, then this would be equivalent to matrix factorizationi\\'m not sure if that all made sense. at the risk of being narcissistic, i wrote a blog post on this  the \"learning to rank + side information\" section details how to add `item_features`.cool, i’ll check it out — i’ve been in meetings all morning so i haven’t gotten to dissect the sample data / examples for the package.as an alternate approach i’m considering a pure model-based approach with a bayesian network, which may handle the user features more cleanlyoh you can use xming, i\\'ve mostly used it when working from a windows laptop to a linux server:grinning:i kind of love where this mac/windows conversation has gone and that it’s gone on this long :wink:  just to add one more thought: historically, mac os has been much closer to linux systems (being bsd based) and with homebrew you basically get all the bells and whistles that devops folks like. i’ve always viewed macs as linux laptops that properly go to sleep and wake back up (at least that’s why i switched off linux 10-ish years ago). being able to shoot the shit with devops folk and understand how linux systems work in pretty great detail should not be undervalued. having devops friends at your company can really help you get things done.mobaxterm is also a good alternative, but i believe they both rely on having a connection to a linux server, not a local installation, right?that\\'s what makes the bash shell as a native application in windows so exciting.vim :+1:backed up a few posts in your blog, definitely more helpful than some of the package documentation i’ve found.  thanks again :slightly_smiling_face:haha thanks. good luck!random question: we all know about ‘lift’ (i.e., improvement of some value given a model or something like that), but what do you call the opposite of that? if your model does worse, then is it “negative lift”? “depression”? “decrease”? or do you just report “lift” as a negative value?“sink”? how well have you found `lightfm` scales to folding in new users?  presumably it needs to rebuild its matrices to integrate a new row (independent of the issue of that user not having any ratings, so it would be cold-starting just off of their `user_features`)i read in some blog that you can use xming with the windows linux subsystem haven\\'t tried it yetin terms of scaling, i\\'m not sure if this is what you\\'re getting at, but you don\\'t need to retrain anything. you just need to provide a `user_features` matrix with the new user\\'s attributes. as such, the only cost is the time it takes to build the matrix (which shouldn\\'t take long, though there\\'s fast and slow ways to do this with sparse matrices)hm.  my understanding was that predictions could only be made on users that were in the original interaction matrix, given that looking at the api docs for the `predict` method it calls for an index for the user-item pairs, as well as (optionally) the `user_features` matrixi think you can pass in a new `user_features` matrix, and you just have to make sure that your user-item pairs match up with that new `user_features` matrix. the only thing is that your columns (feature indices) must remain the same.lift can be negative imoloss?hello!  i made the  channel, please join if u use these technologies“drop\"?in this context i would it call a decrease in x, where x is whatever metric.in some contexts i would be comfortable presenting a negative lift number. (like a,b test results)thanks, all, i like ‘drop’ or ‘negative lift\\': we\\'re hiring for a data-journalist type position at nerdwallet that should be really fun. pm me if you or someone you know might be interested! i\\'m going on mat leave starting the 3rd week of november...does anyone know of anyone who would be interested in a 4-5 month temp position to cover me?  it\\'s at one kings lane and it\\'s awesome here!   basic requirements are sql, (r, bash would be nice but not necessary), tableau experience a plus.  we have a lot of alumni and fellows commuting from nj to nyc — hope everyone is safe and sound this morning!made it thru hoboken 15 minutes before the crashwas looking for a ‘random’ channel, but we don’t have one, so: blatant self-promotion … /giphy entertain us with whimsy!okay...hi guys, does anyone know of any easy to use python, d3/js libraries for showing event data overtime on a map  (all i would need to plug in are latitude/longitude and time)? is quite good.when i think d3.js, i don\\'t usually think easy. :stuck_out_tongue:i\\'ve use bokeh. it\\'s similar to d3.js and can make interactive maps.nvd3 check out folium + vincent examples:  try looking into the mapbox apithanks all! this helps a tonnvd3 is beautiful, but the last time i used it (feb), the documentation for nvd3 was pretty sparse.anyone here do a shiny app for their demo and manage to embed their google slides into the app?industry leaders establish partnership on ai best practices:  russia?no?nah they’d sooner release trump &amp; putin’s honeymoon photos i used an iframe in the ui.r file. see here for the code:  hi alums, thank for participating in this week\\'s interview blitz. starting next week fellows will be going into real interviews and we still need 3 alums to sign up in order for each fellow to have at least 1 mentor session. if you\\'re available, please sign up here --  please dont use the channel tag !!my apologies i meant to post on the ny channel. i don\\'t mind if you use the channel tag !!people… chill.brick killed a guy.: if you don\\'t want to get notifications you can change your notification settings by going to the gear menu and selecting how you\\'d like to receive them.now is a great time to introduce marie paule to those of you who haven\\'t gotten a chance to meet her! she is our newest team member in nyc and she\\'s amazing! please give her a warm welcome :smiley_cat:can we just ask slack to disable the channel notification? someone here has to work therei love that faye is now an at channel meme, remember to keep your office space clean and tidy :d sending stuff to channel is the modern equivalent of reply-all. :smile:more time is spent arguing about the proper use of the channel tag then the amount of time it takes everyone who dislikes the notifications to dismiss them.now that everyone is awake - invitae is _finally_ ready to start doing some cool machine learning on genomic/healthcare data if anyone is interested.  i keep my office space extra un-tidy in honor of hahahah well played not everyone is on us time zones  ..slack is a tool for asynchronous communication to help us use one of our scarce resources most wisely (attention). @ mentions force synchronicity which is useful in case \"hey guys, the church next to the insight nyc office is on fire\", which happened (arguably, this should go in the nyc channel, or we should create an nyc office one). this channel has many new slack users, it takes a while to get used to slack etiquette. in the meanwhile, we create some of the best memes. also,  welcome!fwiw, now is the time to get all your announcements in since everyone will be checking this channel! welcome! if you guys are considering mock phonescreens you can count on me from svhistoric preservation has been battling this kind of shit for a while - look at what could be, if only developers valued rehab &amp; community engagement as opposed to maximum high priced, high density housing: hi andrea, we\\'d love to have you. please sign up at a time that\\'s convenient for you. thanks again! -- he should probably lay low for a whiledo people have slack actually send interruptive notifications their phones for things like pager duty? uploaded a file: yes. :)per ’s advice, today is the last day to use coupon code (ins25x) for the upcoming nyc dataengconf - i know a few of us from insight will be attending, so would love to meet up with othershello all, i am looking for a cloud storage provider with reasonable price. our data is around 100 tb, we need frequent upload but rarely download. any recommendations?!checkout aws glacier - backblaze? (the b2 cloud storage version, not personal) - my company doesi see -- we have them on slack as well but i figured people would use mobile numbersi have notifications set to hear about things on my phoneone of the most recent ai fellows  just posted a blog post about how to generate over 10s of millions of extra text data if you are lacking data for a classification project. read more here: i like this part: \"they were able to provide me with just a single example question for each of the seven sub-classes!\" nice article :slightly_smiling_face:thanks :blush:stay positive alex_gudelean analytic talks about how arbitron moving to more real time data on radios changed modern music, 538 covered this as well, and whether data (or bad data) killed soft jazz.stay positive alex_gude: buzz of! :wink:nuggetbot: weather palo altoweather for palo alto, ca, usacurrently: partly cloudy 19.9°c/68°ftoday: mostly cloudy throughout the day.coming week: no precipitation throughout the week, with temperatures rising to 26°c/79°f on friday.nuggetbot: weather birmingham, alabamaweather for birmingham, al, usacurrently: mostly cloudy 28.3°c/83°ftoday: light rain until this evening.coming week: light rain throughout the week, with temperatures falling to 24°c/75°f on friday.nuggetbot: weather new york cityweather for new york, ny, usacurrently: clear 24.4°c/76°ftoday: partly cloudy starting later this evening.coming week: light rain on sunday through tuesday, with temperatures falling to 21°c/70°f on tuesday.: i found the following to be useful: mitx: 6.041x introduction to probability - the science of uncertainty  hello all. anasuya\\'s and my team at memorial sloan kettering is looking into a projects using wearables / activity trackers, such as garmin, fitbit, jawbone, etc. does anybody know of good datasets from these kind of devices?  not from one device but... maybe?and thanks! yeah, these are useful although seem a bit geared towards identifying activities such as running, or walking from accelerometer / gyroscope data. i\\'m mostly searching for datasets with more derived data, like step counts, or activity indicators ...you should check out my insight project, activeme at  ! i had a lot of trouble getting data sets, but i have at least one jaw bone data set of ~1 year if you would like thathi lois! yeah, that would be great!hey bay area fellows, a close friend of mine is looking for a product manager role. she has a few years experience in fraud product management/operations and has successfully launched her own consumer hardware startup prior. she has some programming/analytical ability, and is flexible on subject area. she works at a larger tech company right now and is looking to grow. if you know of any positions that she might be good for, please message me. thanks so much! :slightly_smiling_face:also, :pinterest: pinterest is hiring data scientists right now for ads and discovery. let me know if you\\'re interested! dm me :grimacing:airbnb releases trove of new york city home-sharing data &gt; the new data set released on tuesday, which is made available only by making an appointment to visit airbnb’s new york city office [...]can somebody in nyc pick this dataset up for the west coasters?  :simple_smile: thought this article was interesting...: we had a talk by nicholas here at li last month - it\\'s a really clever way of disrupting image classifiers with imperceptible permutations since the parameter space is huge. however, it\\'s less clear how you would apply that to text classifiers and other domainsthe nastiest application of that is modifying a stop sign with just the right jitter to make it not seem like a stop sign to a self-driving caranyone looking for a consulting gig or potential side work in the future with an advertising firm? if so, pm me. a friend of mine is looking to incorporate more data science into their work, so it wouldn’t be immediate work, but it could lead to something. my friend is a smart and reasonable person, so it should be a good conversation and upside to pitch your value and the value of your work. i guess i just never cared about the security aspects, but it makes sense since there is more dependence on ml making decisions, how to protect from the “poisoning of training sets” becomes a critical issue.yeah, there is a lot of adversarial aspects to worry about. nicolas\\' work showed that you don\\'t even have to know the model that is being used - just train your own, find the direction of maximal change in the jacobian of the cost function, and add some noise there. the disruption is likely to affect the unknown model in the desired direction.a gallery of interesting ipython notebooks: in case anyone else faces the problem of needing a strong but interpretable classification model (which is explainable to non-ml/non-stats experts) - i hacked together a scikit-learn estimator producing easily understandable decision rules, based on some existing code and some amazing work on bayesian rule lists. i\\'m going to be in the market for a coworking space in the near future. does anybody have good/bad experiences with any spaces in brooklyn/manhattan? if so, please shoot me a private message.hey if anyone\\'s interested, city of boston is having a neat open data challenge (with prizes!) job alert (for senior candidates): capital one is starting an nyc branch of our research-focused “center for machine learning” group. it’s looks like a very cool opportunity where you’d be pioneering new machine learning techniques for the company. see the extended blurb in the child thread for details. i will disclaim that despite a moderate qualification list, they’re currently looking for experienced people that can hit the ground running. message me if you’re interested.sorry for bad formatting, didn’t have a public link to this:&gt;&gt;&gt;at capital one, data is at the center of everything we do. when we launched as a startup we disrupted the credit card industry by individually personalizing every credit card offer using statistical modeling and the relational database - cutting edge technology in 1988! fast-forward a few years, and this little innovation and our passion for data has skyrocketed us to a fortune 200 company and a leader in the world of data-driven decision-making.as a data scientist at capital one’s center for machine learning, you’ll be part of a team that’s leading the next wave of disruption using the latest in distributed computing technologies and operating across billions and billions of customer events to unlock the opportunities that help everyday people save money, time and agony in their financial lives. on any given day you’ll be:-using big data tools (hadoop, spark, kafka, aws) to conduct the analysis of billions of customer transaction records in concert with distributed neural network frameworks and other machine learning libraries-writing software to clean and investigate large, messy, structured and unstructured datasets-integrating with external data sources and apis to discover interesting trends -building machine learning systems from proof of concept through production on clickstream, customer, transaction, or behavioral data (to only name a few!)-creating full data pipelines and novel data tiers for your projects on the road to production-perform internal and external research for publication-contribute to open source projects as part of your jobthe ideal candidate will be:-curious. you ask why, you explore, you\\'re not afraid to blurt out your disruptive idea. you know python, scala, bash, and at least enough java to be dangerous.  you’re constantly exploring new open source tools.-wrangler. you know how to programmatically extract data from anything and model it into human-readable deployments.-creative. big, undefined problems and petabytes of data don\\'t frighten you. you’re used to working with abstract data, and you love discovering new narratives in unmined territories.-forward thinking.  you are always reading about the newest research on methods and techniques.  you actively try and apply these methods in your own projects, kaggle competitions, or in research projects.  you actively follow or contribute to open source software implementing these techniques.twenty-five years after capital one was started it’s still led by its founder. be ready to join a community of the smartest people you’ve ever met, who see the customer first, and want to use their data skills to make a difference.basic qualifications:-bachelor’s degree or military experience -at least 2 years’ experience in open source programming languages for large scale data analysis or simulation-at least 2 years’ experience with machine learning -at least 2 years’ experience with sql and database structures  preferred qualifications:-master’s degree or phd-at least 1 year experience working with aws-at least 4 years’ experience in python, scala, or java for large scale data analysis or simulation-at least 4 years’ experience with machine learning-at least 4 years’ experience with sql and other database query paradigms  capital one will consider sponsoring a new qualified applicant for employment authorization for this position. : interesting. what kind of “limits” are you reaching? rds is fully managed; so scalability shouldn’t be problem. if partitioning, that amount of data isn;t that lot. haven’t used spectrum; but another approach is using redshift(which is olap based postgres):thumbsup:hey guys,i\\'m a de fellow and i\\'m working on the visualization part of my project.i have data representing people on the  the us. in my speed layer i havedetected clusters of people  and i\\'m planning to show them on the map.does anyone have used maps for their project and has feedback on how to go about this? thanks.just spoke to nathan marz about the book he used to learn clojure, he recommended this one, fwiw:excellent… the 4clojure website got me demotivated pretty quickly. gotta learn it the right way@mpancia: what do you think? (i really just want to turn my coffee cup into a donut).  unclear what’s going on with ayasdi at all, i am happy to talk about it at length in privatequestion for the channel -- anyone using jupyterhub  in their data science teams for collaborative jupyter notebook work?separate but another python-related question -- are people who use python using version 2.x or 3.x?insight ai fellow  shows how to deploy a wavenet model on android using tensorflow hi, guys, anyone has experience leveraging azure luis service for text information extraction? how is it compared with self trained nlp models doing similar tasks? is it really usable or just a toy service? thanks a lot!his nickname is jj yet he has no j in him name :thinking_face:does anyone know which is a good library for generating fake data apart from faker in pythonhi guys, does anyone have any insights/links/experience that may be useful in dealing with extracting main topics from text? i know  would recommend eventsim. david, did you ever put together a write-up on how to use it? thanks!  i found the eventsim write-up:  lda and other topic modeling, tf-idf... you might check out this approach:  check out  data generator. it isn’t for big data, but the data types are diverse and it may sufficient for prototyping.thanks  !sure thing! :slightly_smiling_face:some data activism. there\\'s a chance that public climate datasets might not, shall we say, make it through the next few years (something which also happened with the harper administration).folks are working on both trying to mirror the datasets, and also pay for independent dedicated server space.  if either the mirroring or donating interest you, check out this resource:sent that to my friend lucky enough to have a large server and fiber internet :-)if you make your instance with a gpu it will have access  thank you!! that worked! i\\'m new to sql and spent so much time trying to figure this out yesterday to no avail. thank you!!interesting discussion about startup compensation:  second what till_be said. bq can be a butt about storing too much in memory. also, bq bills by gb processed, so be careful with running big queriesyou should get $300 for a free trial, which is plenty for an insight project!oh, got it. i work with bq a lot at my job now, so i have to watch how much i’m processingit’s still really good advice, you can speed up your queries a lot if you design them more carefully and only extract the data/rows you really needi had that problem for sure when  i did my projectthe guy who generated the reddit data for bigquery also makes the datasets available for direct download if you’d prefer to work with them locallyoh really? do you know where? i couldn’t find itbut please make a donation if you use it, the guy’s an absolute saint for generating and maintaining that set (scraper was running continuously for 8 months to generate the initial dump) and helping people with it, and downloads have to be slamming his resourceson a more general note, have folks compared bigquery to aws redshift? i\\'m not terribly familiar with google cloud platform offerings but would love to hear more. : thanks! and you’re right - i played with the scraper for a bit myself, and i can’t imagine how long it must have taken him to gather all that data.yeah, one call per second x something like 20 million api calls to build out the dataset from reddit founding up through fall 2014, then monthly dumps after thati really don’t love bq. it’s often quite a bit slower than redshift, and for similar storage and processing requirements, it’s substantially more expensive. we’re a bit stuck with it though, since some of our apps are built off the google app engine fundamentally do you want to deal with managing a database system?of course not :predshift is a managed service, but it still requires that you do a number of things to ingest and manage that data warehousegenerally if your use case is comprised of occasional ad-hoc queries on data living in an object store (s3/google cloud storage), bigquery might work really wellthose use cases might be similar to some use cases for hiveif you wanted a persistent resource/system, redshift will do that, but you’re going to take on a number of management considerationshi all, what tools/processes do you use to plan and manage your various projects?management? jira. plan? pen and paperkanban or scrum boards?plus a side `rst` file since my handwriting is terrible.i use onenote (quip is nice for this too) and i organize it as follows every quarter: (1) table stating project/ collaborators / next milestone (specific deliverables to that project) / eta(2) a calendar table for each end of week with the dates and on each cell the deliverable for the end of week (high level summary)if i don\\'t complete i just roll them over in terms of the eta and highlight them in yellow, if i complete on time i highlight them in green, and if scope changed/not_relevant i highlight them in red or just erased them sometime if no need to be there.i also sometimes add a detailed project descriptioni used jira, trello, etc. and all the fancy tools ended up being worse for me than just a document with tables and colors that i share with everyone and that collaborators can editmind that we are a small team, if the team was much larger maybe other tools would come in handyquick plug for the  channel, for those of us who deal with the challenges of working remotely or are looking for remote-friendly gigs. i know there are at least a handful of you out there.our team uses jira with scrum boards (or atleast we\\'re moving to that) thank you so much for the detailed answer. scrum boards in jira are the pain of my existence. (hope it works well for you, michelle!) a kanban-like board or table is the closest to a useful tool i have found so far.andrew ng’s coursera course of course.  tibshirani ‘s introduction to statistical learning is a great beginner book along with the free online course at stanford lagunita.   udacity’s intro to ds course.  and of course projects projects projects.  yeah i’m definitely going to have no problem getting the vp to fund a couple more months of testing after i have gotten something to work.  why didn’t i think of that before?and, of course, there is the obvious play of using this system to answer your phone for a small business…but that’s a bit out of google’s market, maybe a good play for squarei\\'m actually facing a very similar issue like this now. i would approach this two ways (ideally both). the first would be to train the model using different sizes of the corpus (e.g. randomly take a sample of 100, 1000, 10000 etc.) and train the model and see if/when the results converge.the second way would be: train the model x number of times, each time using a different bootstrap of the data, and see if the results are same for an independent testing set. you can do this quickly by using x=2 for different corpus sizes and just use the pearson correlation between the predictions.hi all, i think i need some of your brain here. i am trying to run a regression on 5gb of data (1million rows and 5400 features). this data is very sparse where most of the features are one hot-encoding from categorical features. i want to start with lasso or elastic net. but it seems like that my laptop (16gb memory) can not handle it. do you have any suggestions?look for a different feature selection method that doesn\\'t rely on fitting a regression on 5400 variables with 1 million rows :slightly_smiling_face:  or provision a really really large ec2 instancethen you can go back to the regression model once you\\'ve eliminated some features have you tried dimensionality reduction methods?yeah, that is the one that just come into my head. i’ll start with svd. or i can even just look at the correlation matrix.not sure as i haven\\'t used it before, but have you looked into the `dask` and `dask-ml` packages?now i thought more about dimension reduction. all the sparse features are domain_1, domain_2, domain_3,….., ip_1, ip_2, ip_3, ….. id_1, id_2, id_3, …… the dependent variable is `open_rate`. with dimension reduction,  if i remove domain_2 and ip_2, how can i get a predict value with domain_2 and ip_2? and with svd, it is very hard to implement the model.looks promising! :+1: i’ll take a look.what do you mean by \"how can i get a predict value with domain_2 and ip_2\" ?eventually, we would like to know  for domain_2 and ip_2, what is the predicted open_rate at this time of the day.another feature is time.the features are ```time, emails_delivered,  domain_1, domain_2, domain_3,….., ip_1, ip_2, ip_3, ….. id_1, id_2, id_3, …… ```the dependent variable is `open_rate`you can\\'t do both dimensionality reduction that combines multiple features into one and have the same amount of interpretability in the form of fitted coefficients.  you can do some interesting stuff like put in the ip_2 and domain_2 to your model and then sample different values for other features and see what it predicts on average - but all fo these types of methods will make various assumptions and have various issuesyou can try to use a feature selection method that just drops features it doesn\\'t like without re-combining them and then you\\'ll get coefficients for the non-dropped features, with the assumption that the dropped ones are unimportant so you won\\'t care about the predicted open_rate as a function of those.so if i drop domain_2 and ip_2, when i predict the `open_rate` for them, i just assume it does not dependent on these two features.yes, that\\'s what it means to drop those features.  the model won\\'t see them so how can it predict based on them?would you recommend using a forward/backward feature selection in this case?can\\'t really make a recommendation blind :smile:  use the one that works best for your model:+1: thank you so much! what about the old school stepwise regression? that will go feature by feature and select/remove as it grows... it\\'s not guaranteed to be optimal in anyway but i assume that you can run local in 16gb ram computer... do you mean forward/backward feature selection?yes great idea! i’ll definitely look into that. :blush:i\\'d load the data in chunks, use that data to train the model with batch gradient descent, rinse and repeat. if you\\'re using python, check out `sgdclassifier` in scikit-learn, which also has a parameter called `warm_start` that may be helpful for this.it\\'s  :)and we\\'ll have  for toronto :))haha love it!use domino or aws. there’s always a smarter way to do things, but i’ve found that if you just up the machine size, you can solve a lot of problems without having to spend much time on it, and your time is ususally more expensive than an aws machinieone other cool application they mentioned is that google could call \"all businesses\" and ask them if they are open on a given holiday and posting this info in their apps (instead of business hours might be affected...) - stepwise options exist for a reason, but they can be problematic. what do you know about the 5400 variables that you could use to reduce, outside of a statistical reduction technique (e.g., pca)? for example, are \\'domain_2\\' and \\'ip_2\\' related to each other? or do you expect them to have unique predictive value on your dv? seems there must be collinearity among that many variables so i would recommend doing some visualizations of the data to see what kinds of trends you see prior to putting them into a model. also, make sure you are using the right kind of model for your data, depending on the characteristics of the data (e.g., are the predictors mostly binary, are they normally-distributed, etc.)some reading on stepwise regression yeah, that was a genius use case!if you write your model in an sklearn pipeline (`input -&gt; truncatedsvd -&gt; linearregressor -&gt; output`), it is easy to use the same type of data for training and prediction. the pipeline model will handle performing the truncatedsvd the same way on both the training data and the data to be predicted. cross-validation conveniently works out of the box too.worst comes to worst, you can always email the recruiter/poc and ask what normal attire is and dress at or 1 level above thatdoes anyone use something like a \"model manager\"  that you are happy with (in your experience/at\\xa0your company)?by model manager i mean something that tracks the model and parameters being used in production, so that any results are saved and audit-able.this can be particularly important when the parameters are being updated with continuously with new data/training.might not be exactly what you are asking for but we use luigi around hereit is helpful when data is continuously updated and your model contains many dependenciesi guess nothing could stop you from adding your own lightweight manager tracker on top of that: could you add this link and a few thoughts to  ?thanks  i\\'ll take a look and pass that alongwe currently use luigi as task scheduler as well, but it is mainly for resolving dependencies among various tasks you run. (with very limited monitoring to see which tasks have been started/finished) it seems like you\\'re looking for a good logging framework that logs the model (version), parameter values and results?current silicon valley data science fellows - check out the  channel, and see the link there to the contact info googledoc that colin has startedhello all. i am trying to get my web-app running on aws and i get the following error in gunicorn: connection in use: (\\'0.0.0.0\\', 80)i have check to make sure that port 80 is not in use, and it is not.this is my netstat -tulpn:tcp        0      0 0.0.0.0:22              0.0.0.0:*               listen      925/sshd        tcp        0      0 127.0.0.1:5432          0.0.0.0:*               listen      1019/postgres   tcp6       0      0 :::22                   :::*                    listen      925/sshd        udp        0      0 0.0.0.0:68              0.0.0.0:*                           603/dhclient    udp        0      0 0.0.0.0:55074           0.0.0.0:*                           603/dhclient    udp6       0      0 :::31372                :::*                                603/dhclientand when it is running this is my netstat -tulpn:tcp        0      0 0.0.0.0:80              0.0.0.0:*               listen      -               tcp        0      0 0.0.0.0:22              0.0.0.0:*               listen      -               tcp        0      0 127.0.0.1:5432          0.0.0.0:*               listen      -               tcp        0      0 0.0.0.0:5000            0.0.0.0:*               listen      -               tcp6       0      0 :::22                   :::*                    listen      -               udp        0      0 0.0.0.0:68              0.0.0.0:*                           -               udp        0      0 0.0.0.0:55074           0.0.0.0:*                           -               udp6       0      0 :::31372                :::*                                -any advice would really help.any port &lt; 1024 needs superuser to run. so \"sudo command” may help things alongi am already running as root. thanks though.unless i need to sudo when already root?when i just run the web-app without gunicorn on port 80, everything works. so i guess for now i will forego gunicorn for my less robust solution.nope, missed that you were running it as root.that\\'s fine. i should have mentioned it in my original post. for now i have a workaround, so this is not an emergency.hi all - my wife is preparing to move down to the bay area and is looking for a position, preferably in the energy space but she is open to other areas. she recently got her phd in materials engineering and studied electrolyte blends for application in stretchable and flexible batteries, and essentially prototyped a working stretchable battery by the end of her 4th year. she has been trying to get conversations going with apple, tesla, and quite a few other companies in the area but hasn\\'t gotten too far. i was wondering if anyone has any connection to someone who may be good to talk to? in the big picture she is looking for something in business or product development, but is also open to starting out in r&amp;d and transitioning to a more bd or pd heavy role later on. if you have any possible leads, feel free to pm me and i can get you her resume and link her linkedin for you. thanks! i\\'ll be hosting one of the rooms on wednesday unfortunately, but i\\'ll be checking out the conference thursday, likely hanging out with the people at datascience :slightly_smiling_face:  would be cool to meet with any fellows who are attending!papadimitriou.c: yes, will be fun to meet up! registration opens at 8:30am (pst) for my stanford continuing studies workshop. learn tips and tricks to help your data science workflow!details here: i have the same story! see you guys thursday: is anyone using clojure? clojure with apache spark?: is scala not enough for you? :stuck_out_tongue_winking_eye:my boss is anti-scala but pro jvm languagestorm is written in clojurenathan marz can teach clojure in 2 minutes (whether anyone can learn it in 2 minutes has yet to be seen)between the apache and aws stacks, there are too many tools! :wink:: care to jump in here?why is boss anti-scala?i think the primary complaint is not the language itself but the scala that people actually writehaha yeah the number of tools that a company can use is overwhelming…spark is written scala though so i would recommend looking at sparkling thanks, austin. had come across that repo. have you used it?i have not actually. i could ask nathan marz about it when he comes in for the next session as well.cool, thanks.i haven’t went thru it yet, but i’ve heard good things about aphyr’s blog learning clojure:thanks.there’s also looks like google open-sourced colaboratory, which is something like jupyter on google drive:anyone else going to odsc in boston?yepnice, i\\'ll be there all 4 days, maybe we could set up a time/place to meet up during one of the coffee breaks or lunches and have a little insight group. :slightly_smiling_face:great idea - we insight folks can ping the boston and nyc alum to facilitate a gathering. not sure when/where would be best. any thoughts?i\\'ll look around and get back to you. need coordinate with the other folks from the office to make sure we don\\'t have something planned.i\\'ll try to respond by tomorrow afternoon.@ joao i think this may be true as well...its just something that i was trying to make sense of. the way you put it. that makes sense. thanks everyone for point these things out :)how about just using a lasso penalty with logistic regression?  that tends to drive less important feature coefficients to zero.  maybe i am misunderstanding your question.  i prefer using `conda` environments over `virtualenv` in almost every context; polygon and census folks! i\\'m trying to normalize by population. census data is by tract which are small odd shaped polygon regions and i have a rectangular query which may overlap portions of three of four of the tracts. has anyone dealt with extracting portions of data from overlapping polygons?i\\'m not sure exactly what you mean by that -- do you want, like, a percentage of area for each tract that intersects the rectangles?yes basically that, but i think the package shapely might solve all my problems.sure; in r you could pretty easily use `sp` and `rgeos` to get the areas of the pairwise intersectionsthanks!i just had to do this, so :simple_smile: uploaded a file: i\\'m building a model for predicting heart disease using a webcam, and am sorely in need of good data. i\\'d really appreciate it if you guys could record a minute of your heart activity here:  . it only takes a webcam, a desk lamp or smartphone flashlight, and 1 minute of your time. if you could send this link on to anybody you know who does have heart disease, i\\'d be forever in your debt!!! (for now, i only have 14 positive data points; and am supposed to have a \"finished product\" this friday... i have good evidence that my approach works on clinical interbeat interval data; but need webcam data to back up the claim on the web page)find out who is spying on your cell phone: apparently the uk ministry of defence is interested in me.hashcat can now try 8 million passwords per second. and apparently up to 250 billion per second with 8 nvidia titans depending on encryption type.well that\\'s unsettlinghappy thanksgiving all 539 is asking *you* to help them predict the oscars! okay, ill give it a shotthe oscars: an awards shownuggetbot animate me smart ass:simple_smile:crassthe black masking just covers up cells for when the survey was missing that question, or after that question was removed. for example, i changed how i asked about equity midway through the survey, and i added age/ethnicity late into the survey. granted, this is absolutely horrible practice, but i wanted to ball-park the numbers - at least to provide low-confidence prior in the worst case scenario. i\\'ll eventually take what i learned from this survey and make a new, better one, but i\\'d like to avoid spamming folks about their income.i cleaned up the raw survey data, merging cells from different questions on the \\'cleaned\\' sheet, which also has some masking due to the age/gender/ethnicity questions.good polereminds me of i’m interested!is deep learning going to take all of our jobs? i don’t know, but it is starting with front-end developers! check out this awesome project by ai fellow  tools for fe development have always gotten better thoughyes, but having them automated to the extent where you can go from a very specific data input (sketches), to a specific output (website with precise styling) is very exciting to me!what were the other 2.5, i thought there were basically 3and safer than self-driving cars :+1:how and where companies are applying machine learning:   here is an example i used in the past. it involves retrieving an access token thanks!!!!!!i\\'d like to learn as much as possible about practical database design and maintenance (ideally using postgres) in as little time as possible. i have in mind topics such as: how to choose the number of tables, how to think about keys, dealing with time series, etc. any protips for sources or other suggestions?does anyone here use beaker notebook over ipython notebook?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_re3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_re4 = re.sub(r\"\\\\\", \"\", tag_re3) #remove \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not sure if this is a joke or for real…  :slightly_smiling_face:that’s a funny joke, but certainly not a viable business. rates are too low. the cheapest plan is $15 a month for 2 scoops a day, which comes out to less than a quarter per scoop. that’s not going to be worth anyone’s trip. nor is there any feasible mechanism accountability, unless the poopers wait around for the scoopers (which the video says they don’t need to do). anything they might do to implement accountability (e.g. require scoopers to stream live videos of the scooping) would only make scooping even less financially attractive.on the subject of fake apps, this was a good one:  uploaded a file: (source: _the ultimate book of vehicles: from around the world_)haven’t listened to this yet but it might be interestingeigenvectors and eigenvalues: an introduction for some but beautiful #dataviz for everyone: hello past and present fellows!  it\\'s great to be part of the community.  in exploring project ideas this week, i\\'ve become interested in uber\\'s movement product as a way to answer some questions.  i requested access last week, but have not heard back.  have any of you alumni worked with the product or know how to expedite access?  thanks! dear past and present fellows. does anyone know of a dataset that they know for sure contains information on people who have experienced falls? here’s something. it’s a paper, but hopefully it has a citation for a dataset that’s publicly available. i haven\\'t worked with uber, , but i\\'m pleased to see that the number of willetts among insight fellows is growing at a rapid rate.thanks pwinslow!  here\\'s resources from the cdc  see ^^ re falls; don\\'t we know someone that wrote a dissertation on that?any stock junkies want to join  with me?you can also kill machines that are performing worse than others because it\\'s been shown that they will be worse in the long run with high probability. but you have to collect the performance of all your machines together.anyone have an open room they can rent to an insight fellow coming in for the jan 2018 sv session? let me know in this thread if so. super awesome friend coming in. preferable for south bay.good morning, all! can anyone recommend any resources for getting up to speed on kubernetes? ideally, i’d be seeking a tutorial that has a toy example, but other resources would be very helpful too. thanks!i stumbled into this a few months ago:haven\\'t tried to actually go through the examples, but it seemed a nice introduction.the oreilly book kubernetes up and running is decent: some people have found it useful to publish both on medium and linkedin. they reach different audiences.: i second \\'s recommendations. i\\'ve personally had great success with linkedin posts as they act as a sort-of \"portfolio\" project for your ability to communicate complex topics. if anyone is interesting in attending this event about unsupervised machine learning  (6.30pm @ sunnyvale) , please let me know.  i can\\'t attend so you could use my tickets.  the event is sold out.  correction 6pm-8:45 pm.if you are interested and you don\\'t find me @ slack, shoot me an email at martin.gascon  (gmail)for your industry-curious friends who did not get to do insight (yet), we\\'re hiring for three (paid) internship positions at my company fast forward labs. we are looking for curious team players, creative builders, code- and wordsmiths. science * and * humanities phds welcome! more info in the job descriptions that are linked in this blog post:  please refer your friends! thank you!if you are going to use age categories (i assume 0~10, 10~20, so on) then you will create like 8 orthogonal features after one-hot encoding. it will greatly increase the dimensionality of your dataset and hence not suitable for clustering. why don\\'t you just encode them into (0~10 --&gt; 0, 10~20 --&gt; 1, and so on) and preserve the ordering?by scaling, i mean for example, if you define male=0 and female=1, then how will you mix it with your continuous features such as lat-long? you can use min-max scaling, or standard scaling, etc. you can tweak the scaling to increase/decrease the effect of gender category on your clustering. this really depends on your use-case, and you have to have some justification of why you use certain scaling method. thats\\' why i said scale features in a way it \"makes sense\".a nice post about identifying trends at instagram:   sure why not ? i cn understand that, the feeling in ny insight was somewhat different.it all depends on the company, and how hierarchical it is, but i\\'d imagine most of us would just be \"data scientist\". typically a senior data scientist role would require some time working in an actual data scientist role. then, there are companies that just don\\'t use the title of senior data scientist at all (you still get promotions, just without title change).  as for the other options, most of us that went through insight came straight from academia, so to land a role as a principal would strike me as odd.*principal. :wink: i think that\\'s a data scientist with strong ethicsgot it! thank you!likely this will depend a lot on the company as well -- i\\'m sure the same person could conceivably be cast as a senior/lead at a small startup, but be a \"data scientist\" at a larger onesome people would also argue that titles are only meaningful to make yourself satisfied, and to make it easier for recruiters to find you for your next role :wink:also some larger companies may have pay scales that cap out for certain roles and to attract talent they may give you a title to distinguish you as a phd-haver from data scientists with other degrees and to allow them to pay you a higher salary.ah got itmy aws account was recently hacked. approximately $3,500 of ec2 usage charges had accrued by the time i found out about it. for anyone who still has their aws account active, i highly recommend setting up a billing alarm to avoid this happening to you. from the aws console, click on *services*. then under *management tools*, select *cloudwatch.* in the  menu on the left, select *alarms*. i set mine up to email me if the estimated charges exceed $1.i disabled my insight account after reading an article about someone getting their aws account hacked :disappointed: sorry it happened to you!you can also go to *instances*, select your instance then go to *actions*, *cloudwatch monitoring*, *add/edit alarms* :alarm_clock:_vps dime_ :cool:closed my account permanently, the end of an era …. not sure if that would set up alarms only for that particular instance. in my case the usage was accrued on a bunch of new instances that the hacker launchedjesus. they let you out of the bill, right?still waiting to hear back about thatfrom what i’ve read online about past cases like this, they generally waive the billkeeping fingers crossed,  !  me too...how do people create hive table from csv files that contains json data in one of its column, where the data of interest for building the hive table is in the json data?hey there, do you guys know of implementation of text pattern matching package called autoslog? i am trying to run some name entity recognition task leveraging this package developed from university of utah, but couldn’t find an implementation of it yet.  it basically extracts pattern like:&lt;thing&gt; happened in &lt;location&gt;&lt;name&gt; lives in &lt;town&gt;or a package that does similar thing would be great too.spacy (python package) can do neri’ve had good luck with stanford’s corenlp as welli\\'d contact prof. riloff directly about autoslog. google also has some built-in ner through their nlp api on google cloud, but i haven’t explored it yetthank you guys! i am checking out spacy, corenlp and google nlp, and have sent an email to prof riloff too i’ve played with spacy for named entity recognition. it does a decent job out of the box. not to hard to figure out. btw, how you doing buddy?to everyone, what are good external method for validating clustering results.i’ve been doing text clustering using word2vec—&gt; affinity propagation (after proper tokenization and all that jaz). the results work pretty well, but i want some advice on how to validate my clusters without to much intensive manual curation.nice visualization in case someone missed it: you might want to look into  for remote hosting of a classifier toothanks for the input!   anyone else science marching in nyc? you might want to try the  channel!hello, do you guys have any quick thoughts around database management best practices?  that\\'s a bit vague, but it can be very general stuff like \"utilized cloud based services such as aws\", things like thatknow the difference between olap and oltp.did anyone submit to icml 2017?looking to see if you were able to update the paper submission, or only the response to reviewersi was curious if anyone had any interest in participating in a survey which would collect data from insight fellows anonymously to provide feedback on how compensation, promotion, etc, progresses as time passes after insight. my feeling is that this aggregate anonymous information can help fellows coming up on their one-year anniversary navigate the whole getting raises thing, with regards to what to expect. if you’d like to contribute answers, please check out the following link: this looks really useful; it would be great to add location as well, since salaries are heavily geographically dependenti considered location, but in the interest of keeping it anonymous, i opted to not include itah, fair enough :slightly_smiling_face:can we make it geographically broad...ie midwest, east coast etc.?sure - what geographic regions would you suggest?san francisco bay area, east coast, midwest, west coastanything besides that?maybe just optional write-in?i think i’ll add just an optional write-ini can merge laterfyi, link to survey is on my user-profileus geography is weird. which bucket would you put e.g. austin and denver in? those buckets probably cover most cases thoughi think with the write in now, i can program in some geographic clustering logic laterthanks for the responses so far, folks!you could anonymize after and state that in the consent, using wolfram to convert all salaries to sf without geography i don\\'t think the results are generalizable: coolwell, there’s a location field now, but i think people are updating as comfortablelive link to summary spreadsheet: submited! what are you trying to build?hi all. former de making transition to ds here. what are your thoughts of using p-values for hypothesis testing? i\\'m reading islr and it is mentioned all across the first chapters, but when checking out the implementation of the regressors/classifiers in sklearn i see that this feature is not present, which makes me wonder how much relevance do p-values have for nowadays machine learning.i used collaborative filtering with explicit feedback for my project and ethan’s blog has some good resources for that as well. in everything i\\'ve read and at my company wayfair, p values aren\\'t important for evaluating models these days, only performance stats like r^2, mean absolute error, f1 etc relative to other models and train-test, but we do use p values to evaluate a/b experiment results, along with rigorous power analysis etc.agree with all of that^ and especially \" but we do use p values to evaluate a/b experiment results, along with rigorous power analysis etc.\" thanks much! i may just come your way with questions :) p-values are just one piece of evidence to use if you\\'re working with experimental data, smaller sample sizes, or if you\\'re interested in inference. if you want to wade into the p-value debate and replication crisis going on in psychology (and spreading elsewhere), andy gelman\\'s blog is one place to start. sklearn is focused on prediction / ml and not stats.  you might want to use something else if you\\'re asking if model coefficients are significant (or write your own p value thing for sklearn models), but if you\\'re just asking about predictive ability there\\'s other measures.from a more practical perspective, i\\'ve been asked to explain p-values at almost every ds interview i\\'ve ever had. although, i think it\\'s as much about communication skills as about the statistics.who is at data science elevate today?i wish :disappointed:  my whole office is empty hahahi\\'m like teh only one not therei was just traveling last week and again next week, and i actually have to get stuff done at work so couldn\\'t go this timeassuming it is a regression problem, if you are interested in the contribution and strength of each feature (basically the coefficient), then p-value can be informative./poll “what python environment tooling do you use?” “venv” “virtualenv” “virtualenvwrapper” “pipenv” “docker” “i’m a special snowflake” “what’s python?”i don’t see an option where i do \"sudo pip install\" how about conda?+1 for condathere’s also pyenv — i use pyenv, virtualenv, virtualenvwrapper with autoenvmake sure you consider practical as well as statistical significance…with enough data you can make any inconsequential difference hit a small p-value.no anaconda ?maybe that’s the special snowflake optioni don’t use conda but my understanding was people still use virtualenv or similar to handle environments? or does conda have a built in separate way of doing it? :slightly_smiling_face:ah nice, didn’t know about that, there isn’t a way to edit polls so i guess conda folk are special snowflakes for this exercise :smile:¯_(ツ)_/¯from what i saw pyenv was deprecated in 3.6, but i haven’t used it so i don’t know all the detailsi use docker for real things, conda for super quick thingssame - conda runs my local dev environment, docker for deployingmy understanding is that the `pyvenv` script was deprecated () but `pyenv` () is a stand-alone project. it’s not confusing at all.lol, well good to knowthank you all for these pointers!conda++one can use conda inside docker too :wink:yes ;)watson machine learning is finally generally availablewhat does it do? can we hang our coats on the hook and chill all day by a cup of coffee before we get sacked? :coffee: :coffin:your an exception little pixi you could do it just on your looksanother thing is to add multi-factor authorization. use authy in your phone or something.is that why their retrieve&amp;rank is down? pydata nyc is coming!! if you submit a talk/workshop that gets accepted, then the registration is free!also, talks &amp; workshops are recorded and posted to youtube too!erm i hope we\\'re all past that phase in our lives when _free registration_ is a perk...i still look for free food quite often. especially pizza.don’t do this modeling myself, but i think a graphical model would be a good approach here. create some graph where nodes are people, and edges are weighted by feature value, then do something like djikstra’s shortest path between any two people.after insight my pizza-capacity has been reduced to a mere slice per day.hey fellows. does anyone have experience with elasticsearch api on python? i need small advice on quering data. please pm me. meet  = es genius! thanks for introduction.i have no idea if this is the same elasticsearch but @ronak has a series of blogposts on elasticsearch hey max, i’ve used the python elasticsearch client a while back. what did you need to know? if your question hasn’t already been answered as i am sure many insight fellows have experience with it.just saw that you got intro’ed to ryan. he knows way more than me. :slightly_smiling_face:hi alumni, we at pixability (boston office) are looking for a sr. data scientist to join and be a leader of our data science team: if you have practical experience leading progress on data science projects, know how to add business value to an organization using your ds and organizational skills, have experience working in and preferably leading a team, and you are willing be the face of _our_ team to the rest of the company, this role may be right for you. direct message me if interested!have any alums made a decision about hiring their own manager to spend more time as a senior ic vs becoming the manager as the team grows? i\\'m around 50% ic and 50% managing/strategy with 2 other ds on the team, but with more coming it\\'s going to be unsustainable at some pointthat\\'s a great idea! thank you, i\\'m trying that now!hey kevin! not in the exact same situation, but i’ve also fallen into the “now i’m the manager…” role and need to decide if i want to go back to ic and get my manager to replace me, or just embrace it.hi everyone, i wrote a general practical nlp guide. it can be a good resource for anyone trying to stand something up quickly that works well in the realms of classification and clustering.it’s just made it to the top of hn, which is exciting :0it\\'s great!#brusheswithfamei guess you’re looking for advice, so i can only really say this: i lead a team of three in my last job, and the biggest mistake i made was using tech work to procrastinate doing management. i would say “man, i need to respond to that customer email, but i’d rather work on this model…” if you find yourself doing that, it’s definitely time to get a manager, or make the jump. :confused:yeh i had to make a similar decision around 4 months ago. the team was growing and it was becoming impossible to wear two hats. ended up staying as an ic on the condition that i could shift between verticals and focus more on long  term strategy for data, projects etc...but no people management responsibilities at all. i\\'d say it\\'s important to make that distinction very clear when bringing in a new manager though (whether internal or external)as for ic vs manager… i wish i had great advice there! still trying to figure it out for myself.in a perfect world, companies would nail the ic / management parallel tracks :(and i just saved that article in feedly...well done! wow tom is alive!?hey all, i have a friend who’s looking to get into sports analytics but not really sure where to start.  she’s a career physicist so she has a good technical base to start from.  anybody know of any resources for training up and/or breaking into that specific subfield? thanks!but even in a perfect world, it’s not a single company issue. you have questions like “am i more marketable as a manager or an ic?” which depends on everyone in the ecosystem, and yourself! :confused: it’s not easy may be a good person to ask! can reach out to fellow megan schroeder (dodgers) or i have friends at hudlyeh i agree with that! funnily enough though, my final decision was facilitated after i interviewed someone for the role of staff r&amp;d engineer (it was one of those interviews  where you end up being the one getting interviewed after 15mins). he was incredibly good, and it did convince/reassure me that if you\\'re really good, then you\\'re basically marketable anywhere regardless of ic/managerjust got to work on getting really good now hahathe hard part!nice, thanks! reached out to megan, and i wonder if we have the same friends at hudl… ex-hep people?thanks april, i hit her up for some info!william spearman, austin basyeyep :slightly_smiling_face: i went to undergrad with will and met austin at cern #smallworldi reached out to both of them, thanks!hey guys! current insight fellow, working on analyzing arterial pulse waveforms. i was wondering if folks have some suggestions on good possible features, particularly in the frequency domain? (i’ve been looking at features related to individual pulses, but have yet to explore frequency-related features. any suggestions would e greatly appreciated!) uploaded a file:  commented on ’s file :  can’t think of any off the top of my head but have you looked at tsfresh? if you haven’t, it might be helpful for feature engineering of your time series data: frequency, number of harmonics…amplitude, area under the fourier transform, min/max frequency at threshold amplitudepeak to peak interval, distribution of peak to peak intervalthis message was deleted.project or product?projecthas anyone used both chart.js and plotly.js (or any other js charting tool) and can give me a recommendation? it will be used in production so need to be fast, work well with data updates and css-customizable preferably. my charts are very simple. thanks.best python library for plotting latitude/longitude data?you might check out `folium`: if by plotting you mean geometric-like pretty plots in 3d, or grid surfaces mapped onto 3d surfaces, maybe think about the vtk library.  it\\'s used by the computer graphics community.to take a binary classifier to a multilabel classifier you could use the onevsrest wrapper in sklearn. it treats each label classification as independent. folium looks like exactly what i need. thanks!speaking of lat/long data, anyone have experience doing choropleth maps in folium? i\\'m having trouble getting it to play nice.we’re hiring! if you are interested in iot (or know someone who is) please dm me. hi all, i was recently lamenting on twitter that even though i’ve been a ds in sf for ~14 months now, i still feel pretty out of the loop as far as conferences go (e.g., i hear about conferences 2 blocks from my office only 3 days before they start!). in astrophysics we had this great, yet fairly simple website () that listed all the astro conferences around the world and would regularly get updated. so my questions to the hive mind are:1) does such a website and/or mailing list exist for ds-related conferences or meetings?2) if the above answer is ‘no’, is there anyone on here who’s interested in working on such a website and/or mailing list? i already have interest from one non-insight ds in sf and i think with a few others we could throw together something that will hopefully be useful!if you are living in the bay area please help my friend with her research by filling out this quick survey - she’s having problems finding a large group of respondents with advanced degrees and i know y’all have that on lock :wink:link here: this is such a huge oversight in my opinion, thanks for making noise about it!  my boss is telling me he\\'s got money for me to go to conferences and i got nothing...  count me in if you end up trying to build somethingawesome, will do!what big lessons did you learn after you deployed your first machine learning predictive model as a data scientist that helped you improve for your second model?make sure your evaluation matches what the production environment is like. on search, we handle position bias currently as a feature within our model, but in production we assign ranking scores to documents assuming they were in first position. our evaluation, however, didn\\'t adjust the position features for a few weeks until i looked closer at the explanations for the scores. anyway she could make this a bit anonymous? you gotta provide name, dob, and location(almost). thats a 2.5/5 hit on the pii data list. mabbe just take the age?following ’s point, try as much as possible to use canned tools to control such things — the earlier in your process you start having things in self-contained conda envs/docker containers/whatever, the easier it’ll beif you find yourself building a lot of pipelining from scratch/one off coding, good chance you’re reinventing the wheel+1 for ensuring production environment is as close as possible to test environment. when this is not possible it becomes important to remove barriers for gaining visibility to production environment, and ability to quickly retrain model and push to product. it’s not birthdate it’s the date you take the survey, i.e. for consent; name will be removed for analysis  thanks :smile: fitm moment!oh haha you’re not the only one, i totally put my birthday when i did the survey, without even thinking of pii. that’s why i’m not in cyber security i guess :wink:hmm, yeah that can get tricky as you could potentially run into correlation-causation issues. honestly i would start with feature engineering and logistic regression, and think about causal analysis (i\\'m no expert) if causation isn\\'t clear.i have some friends at work that are having fun competing to automate a tank maze game. it is 10x16 bits. i thought it would be fun to make a neural network with q learning to play, but the darn thing either goes trigger happy if i weight getting kills too high or spins in a circle if i weight it blowing itself up too highly. i was wondering if i could borrow somebody\\'s expertise for advice.definitely happy to help, as we were just talking about this at insight!this is pretty cool … centralize your access to datageneral question: anyone out there either in a lead/senior ds or hiring role willing to [briefly] act as a sounding board? i\\'m pondering different roles falling within the greater data realm and the trajectory they set one upon. (recently finished the inaugural boston session.) thanks.dropping this here: in terms of aws athena vs spectrum, is the concurrency more auto scaled with athena?  i’m basing this on what i’m reading from  vs hi all, my team at digitalocean is currently growing and we are looking for both a manager and two senior data engineers! if anyone here is interested, or if you know someone that may be, please free to get in touch and i can provide more details!if you\\'ve never heard of digitalocean, we are a cloud infrastructure company, and its a pretty awesome place to work at. the data that we are currently in the process of consuming/analyzing is also super interesting (both customer and machine/hypervisor level data)how does people create emr clusters on spot instances? :question: you can do it directly from the aws emr console.  as you walk through the steps, you\\'ll have to choose what instance types to run.  there is an option on that menu to select spot instances and set your price.  the usual spot instance caveats apply.ah cool, thanks ryan, i was looking at the cli tool, and couldn’t figure that out, will try the console. thanks!and the source for the article: although i\\'m having a bit of trouble distinguishing ```meaning of work: are we working on something that is personally important for each of us?```and ```impact of work: do we fundamentally believe that the work we’re doing matters?```can you believe that the work you\\'re doing fundamentally matters, but not have it be personally important to you? i guess.i think that\\'s called being an intern :smile:eh, i think lots of folks can be in professions that they think are important but not for them. i can\\'t help myself but share this article which says this language is due to economic theories we started to implement in the 1990s. it even highlights the strategic use of \"personal passion\" by recruiters to nudge people to overlook many other positive aspects of jobs to convince them to quit their jobs (and take on new ones, which serves the recruiters).i can find my day to day work (writing good code) satisfying without finding its ultimate purpose (selling ads) importantgood points :point_up:to my brain changing \"meaning of work\" to be something like \"satisfaction with my work\" clicks a bit better for me.of course, also, this is descriptive of successful teams. if there\\'s a team put together to decide the color to paint a bike shed, there\\'s probably little one could do to help the endeavor.red, obviously.i think we\\'ll just go ahead and agree to go with green this time.or the opposite. maybe there\\'s work you really want to do or concepts you really want to learn but it\\'s not necessary for the work you\\'re doing.can i pm you to add to this? this is awesome.*insight\\'s nyc office is moving this week!* if you never returned *your keycard*, we are declaring an amnesty and will be taking them back, no questions asked, until the end of the month. drop yours off in person, or mail it c/o tracy clark, 45 west 25th street, 9th floor, ny ny 10010.looks like you were right! when would this be?this is likely going to be towards the end of the week of the 21st of august.so end of next week.interested in coming?probably, as it sounds very promising. will have to see if i\\'m available then, but will definitely try. just let me know when you hear of a more definite time.thanks for posting this here! and , you should contact terrell, it\\'s possible the file they sent was corrupt: yes i did yesterday and he sent the file, and all works fine now, thanks!: try pyqtgraph or oxyplot if using c#insight alum chris moody just published a paper on his hybrid algo. lda2vec on arxiv: this may be a naive question, but does anyone ever play with different random states when fitting random forests?  i\\'ve noticed nearly a 5% difference in fit based on which random state i use. are hyperparamaters generally considered to make these differences inconsequential or is this something that may be useful in considering?this is on a 33% holdout testing set, not the training set. i suppose k-folds may be more useful for random forests than a simple holdout set though, given the \"randomness\" involved i\\'m operating under the assumption that the more data you have for validation the better. check out i am at the conf. also herelots of great info on the web, i set my own up a few weeks ago for similar reasons. disclaimer: this is by no means official legal/account advice. key notes:1) if you\\'re considering out of state, the \\'best\\' choices are delware, nevada, and wyoming for reasons of taxes, fees, and anonymity (balances between each, i went w/ wyoming).2) filing fees and requires vary a lot.3) depending on how/where/when you do business, you might need to register as state foreign entity (which may tip cost of registering llc + f.agency to more than just registering in the state you live/will work in). best bet is to talk to accountant.4) required if out of state, but either way you\\'ll almost definitely want/need an agent to be official recipient of legal paperwork. services and fees vary wildly, choose wisely.poet rnnsthe article that spawned that is also amazing: anybody know how to include \" \" as part of string in hive sql (escape chracter for \" \" ). eg. i want to print               1 \"john\" and i tried select 1, \"john\"  and it wouldn\\'t work... any suggestion?better example: hive -e \"select id, casewhen age&lt;4 then \"0-4\"else \"5-10\"endfrom patient_2\"the aim is to print id =1 \"0-4\" but the above just gives 1 0-4 and the no quotesi think strings in hive are actually single quotes and not double quotesso if you run hive -e “select id, case when age&lt;4 then ‘0-4’ else ‘5-10’ end as dim2 from patient_2” it should workreplace dim2 with actual column namethanks  i figured it out and its ==&gt; \\'\"0-4\"\\': that returns literally 0-4 in quotes as a stringi’m not sure if you want that.you want 0-4 as a string, not “0-4” as a stringam i correct?i tried it and it gave me \"0-4\"right because it gave you the quotes as wellthe string contains the quoteswhich i don’t think you wantor needyeah thats what i wanted ... i want the string to be there literallyi meant the quotes*alright, if that’s what you wantshiny equivalent for python, fresh off the stitch fix blog: fun article: have you checked out the jsonlite package in r,  ? i have had to do this a few times recentlywhen you also say grouping var, do you mean a categorical variable?: check out tidyjson does anyone know of a good customer event tracking analytics platform? we looked into mixpanel and heap, but are looking for other alternatives.  i like amplitude is the other event analytics platform that comes to mind.  what did/didn\\'t you like about mixpanel and heap?  are you looking to just track and collect the data, or do you want a visualization layer on top of it as well?addepar is looking to hire a data scientist in mountain view. if you are at all interested, let me know and i can make an intro.as many of you probably heard from the insight team, today is the deadline for the next insight session. some of our best applicants come from recommendations from all of you. if you know someone who would be a good fellow in the future, you can dm me or fill out this form:nyt article from yesterday about the desire to hire a.i. research talent: sweetm. who do i go ask about my 500k raise?+250k for scikit-flow uploaded a file: oh right, i need to rsvpdoneanyone planning to go? hi, i\\'m elizabeth from the current new york de session. i am interested in working on a de project that involves biochemistry and/or bioinformatics. if it is possible, i would like to be able to talk to someone one with background about whether or not my project idea is feasible. please pm me. i would really appreciate any and all help i can get! thank you!hi elizabeth.  i\\'d suggest you try summarizing your project idea in a few sentences, and post it here.  then anyone who has the appropriate experience can pick up the discussion.  i think bioinformatics is a pretty broad field, and people have varying expertise in the many topics it encompasses.  for example, i could probably comment on genome-wide association studies, but not on proteomics.seriously would love a copy as well if anyone follows through on this.i can probably make an appointment and get it. i’m just trying to figure out how right nowhas anyone seen any more details than the times article offers?this mashable article makes it sound like it will take a special kind of black magic to get access to that data:  uploaded a file:  and commented: looks like you can download it!hmm slack is acting up on me and sometimes not showing all the links posted. sorry about that.: i think that’s a different data sourcebut super interestingi think that insideairbnb may have scraped public data^ what  saidsweetgotcha! good to know!hi.  curious how people have approached version control + code development on emr + s3 systems when they would like to also have jupyter access to data on s3?i have. but the solution for us was to fork jupyter and create a new format for notebooks that\\'s more git friendly instead of mixing data and code in a json blobbasically, we\\'d only push code to github and the outputs of the notebook would not be part of the code that get\\'s pushed to github... mixing code and output is a bad idea imo since (1) the repo can become huge, (2) i care about doing code diffs mostly and not about data being different from one run to next one cause data gets updatedi have had terrific experience with plotly dashoh, interesting, that\\'s very cool.  so, do you spin up an emr cluster w/ jupyter running and ssh to the master node to run git there?  do you only do code development in jupyter itself?  or, if you have code outside of jupyter, do you edit your non-jupyter code while ssh-ed into the emr master node?let\\'s start with non jupyter first cause that will clarify the problems with jupyter and why we had to create our own jupyter basically...if writing regular code one can use github like one would normally use github... you have your repo on your local computer push to git and then can update the code also on the master node and run the codethat code should run if it was developed with your emr configuration in mindi.e., yarn cluster or any configuration you may haveanyone has a favorite tool to generate an erd from an existing database (redshift in my case)?now with jupyter is more complicated because users are using a webui (aka jupyter) to write code and that code get\\'s saved somewhere in the emr cluster.  jupyter doesn\\'t only save the code but also the output of the notebook in a json format so effectively you are not only saving code but also output.  we didn\\'t like this so we created our own jupyter.  now if this is not a problem for you then in theory you can have your repo in emr, save the notebooks and push github changes like you\\'d normally do and i think it should workjetbrains datagripgotcha, that\\'s a huge help, thank you so much!thanks!i got this message this morning: ```your file was uploaded — it\\'s safe and sound in slack. unfortunately your workspace doesn\\'t have any storage space left. ``` . wondering if others have gotten the same…maybe insight hit the space limit for storage on the free account?luis -- you\\'re correct :slightly_smiling_face:i think it works like a queue though, so it\\'s the older items that get moved out and we can still share with each otheryeah, message could be worded betterhey all, i got put in contact with a rice u alum who’s finishing her masters in stats in december and is looking to get into a data analyst role. she is in houston currently, but willing to move. if anyone thinks they might have an opening in a couple months or is interested in chatting with her or checking out her linkedin page, let me know!depends what level she\\'s looking for, but we have student opportunities here:and other opps for full-time (not intern or fellowship)well she’ll have a masters in december and no longer be a student after thatlots of other relevant postings in that case :slightly_smiling_face: she could have a look and ping menyc folks: apartment sublet a coworker is going to spend a month in our la office so would like to sublet her apartment during that time.the month in question is flexible: it will be sometime during october to december and is flexible based on finding a good sub-letter!the apartment is a sunny 1 bed in bay ridge near to 59th st subway (n)if you\\'re interested, or know anyone who might be dm me and i\\'ll connect you!  do me proud with that space time project!space time project?you also have my attention...:rocket:agreed still a mystery to me... unless you don\\'t have enough gpu ram to store all the data and there\\'s loading/unloading going on during the trainingin my mind, ds more general than ml which in turn is a more general concept than dl (which is one of the possible models to fit) - i still think dl is more general than logistic regression/rf/gradient boosting in the sense that dl encompasses multiple types of architectures as well as very different models such as ltsfm versus a regular deep net or just the old and forgotten regular neural net... in my mind a job should say what the objective is and what problem one will be trying to solve... if the problem totally requires dl knowledge then one should be explicit about it but so far i am very skeptical that this would be true about most ds/ml work... as far as ai, well, that in my mind is more related to the problem of creating generally intelligent machines which is still very much in its nascient stage (this only in my opinion) and we still don\\'t know if deep nets or something else will be the right answer to this problem... this is just my mental model on how i think of the ds/ml/ai space... which could be all wrong too jajajaon an unrelated topic, this is why i am more concern about hs than ai, and the misinterpretations and hype around ai too... from what i’ve noticed, bs and *some* ms often do not have the same breadth and depth of knowledge in statistics and machine learning as phds. just like what marco mentioned — “knowing what you don’t know”. (of course, they are much younger:slightly_smiling_face:). and they are mostly hired as analysts or engineers in my company. what i’ve also noticed is that they often follow protocols and prescriptions learned in classes, and do not apply the same rigorous analysis as more experienced researchers would do. for instance, when reporting monthly traffic and unique visitors, no one cared to include uncertainties in their reports, and just announces what looks like noise fluctuations as ups and downs. but on the other hand, perhaps most of the work do not need that much rigor anyway. companies want results not papers. and there’s usually tons of work to be done.i think it’s just too much hype. most of the work we do don’t really need dl and are not suitable for dl. but there seems to be a trend that eveyone should write some keras code to do image classification. what i saw at nips was that a lot of the demoed dl projects have either minimal value (putting some masks on your face, change your photo to some whacky style) or are still in academic settings (generating images). there are applications where dl would shine, but i think the hype is just way too much.hey all, there’s now a  for all european based alumni and usa based alumni who are looking to get outso, what\\'s our responsibility to push back on all the hype?anybody else not have admin permissions on their work machine?  just started at a health care company where i\\'ll be working with patient-level info and can\\'t install any 3rd party software.  i kinda feel like i\\'m trying to work with one hand tied behind my back without all the extra tools i\\'m used to.  curious if others have similar situationsum, i think your responsibility is to learn dl (sorry, i mean ai) and ride that hype train also windows is terrible, amiright?can you dual-boot into a linux env, or do most work remote vis ssh?good thinking, i\\'m gonna ask about that.  i heard windows 10 can install a linux \\'subsystem\\' so you don\\'t even have to dual-bootwhat\\'s the status at spotify? mix of oses? did you get any choice?it\\'s pretty laid back, geared toward whatever makes developers develop faster :wink:ha, sounds nice...also at a healthcare company, windows is standard here. i had to ask for a mac, which they procured easily. i think i had to justify it, the windows pc\\'s we had used windows 7, so no linux sub systemoh man, that sounds amazing... what did you use for \"justification\"? just that you wanted to get actual work done?i\\'m in a small-ish company so i think they may not have anyone who\\'s qualified to support non-standard computing situationsi\\'m on a mac too and can install whatever i want...it almost feels too free sometimes #butilikeit :grinning:#jealousfinance here.  we used to run a heavily sanitized version of cygwin.  my business unit lobbied heavily for macs, so that\\'s what we use now.  i feel your pain.nice.  i\\'ll have to start building some political capital around here to achieve a similar take-oversubsystem does work pretty well for a lot of things if you don’t need access to the gpualso, i highly recommend ooohwhat about ubuntu linux on windows?feel ur pain...if u code python/r i\\'d get jetbrains and then just work as if i were on a macyes! just found out about that, gonna see if it will go for itlol almighty it - how do i exercise power? i\\'ll take long on trivial requests, dissaprove rational approvals, and whatnothahalinux subsystem works great if you don\\'t need any gui applications. however, you still need admin permissions to install it.a vm would probably work best since you should be able to control what to install on it, and if you configure the mount points right, you should have access to all files in the windows system.good to know, thanks!also healthcare - we work on windows vms to work with patient level data. usually installing 3rd party software requires a call with the system admin and a conversation about how we can’t do our jobs without r/pywindows sucks, vms suckhaha, glad to know i\\'m not the only one! what about things like slack? i\\'m using the web client but maybe you found a way to trick the sys admin into thinking it was a new python tool or something?it\\'s definitely useful to help me do my job!typically just ask for r/rstudio/git. recall i’m on a vm so i can always just switch back to my actual computer for slack and emailaaah, right, makes sense, thanksthough i will never. never. never learn when to control+c vs. command+chaha, i\\'ve already accidentally hit the windows key like a thousand times...so you use your personally-owned computer and work via vm from that?no, i have a work computer that i connect to 10ish different vms (1 per hospital) that can access data for that hospitalooh, i see, so the phi stays on the hospitals\\' end.  for me, my company stores all the data internallyi wasn\\'t able install any ide or for that matter any packages needed. also there\\'s no predefined set of packages that you can ask admin to install for you, as i didn\\'t know what i might try out next. so couldn\\'t really do my job without admin rights, that was the justification, to ask for admin rights on windows until i got the machi y\\'alldoes anyone remember the names of the engineers who gave those quick talks?these talks were about a month agothere was yashashree kokje from yelp and catherine alverado from qventus. can\\'t remember if there was another... judit would know!i was looking for catherine! thank you!hey  !hey folks!would anyone be up for speaking to a molecular biologist (dna sequencing/quantitative genetics, 5 yrs of industry experience) interested in discussing ways to broaden her skillset/pivot careers? perhaps someone with a genetics, bioinformatics, or molecular bio background? please reach out if so - thanks all!while we\\'re on the subject, there is now a  channel for any/all computational biology/bioinformatics discussion!welcome new fellows!! :wave:: never worked on topic myself but this link seems to be dealing with sim prob: folks i am having issue with  hive over hbase,ubuntu@ip-172-31-1-133:~$ sudo apt-get install hive-hbasereading package lists... donebuilding dependency treereading state information... donee: unable to locate package hive-hbaseubuntu@ip-172-31-1-133:~$ sudo apt-get install hadoop-hive-hbasereading package lists... donebuilding dependency treereading state information... donee: unable to locate package hadoop-hive-hbasei am trying to install package hive-hbase on  cloudera cdh 5.5cant locate it, if any one has info. pl. let me knowthx: fyi we have 2 channels you might appreciate:   hi all, we’ve been having more conversations on professional growth lately and one topic that stood out is what are some resources to read on for both data engineers and data scientists, any suggestions?usually for me keeping tabs on this slack channel has been useful, but not everyone is an insight alumni :stuck_out_tongue:kenny_kwan: twitter/kaggle/department seminars. mostly just be engaged with the community you have around you should cover it.hi everyone, we’re looking for a few more practicing data scientists (who code analyses and models in languages such as r and python) to collect some feedback in a 20 minute survey, the results of which will be used to directly inform the roadmap for our product.  as i said before, we’re also offering a $25 amazon gift card to everyone who participates.  if you’re interested please dm me, and sorry to ask again in such a short time, but we’ve had some unforeseen openings.  :slightly_smiling_face:  thanks!thanks again for the interest everyone :slightly_smiling_face:  we’ve once again filled out our slots!some people have just too much time on their hands ..  that makes sense i do some combination of that, but it feels like it requires too much time to go through :stuck_out_tongue:it\\'s not reading but i find talking machines and o\\'reilly data show podcasts good for keeping up.   thank you!  i feel much better now. :slightly_smiling_face:  ?about powerpoint thanks those look great i remember subscribind to talking machines at some point. podcasts are things that i definitely need to keep reminding myself of :stuck_out_tongue:if you\\'re into privacy or really into jem and the holograms:  that is truly truly truly outrageousdoes anyone have recommendations for a/b testing analytics platforms? i\\'m in a position to decide what software my company will use for a/b testing, but it\\'s not something i have much experience in. what are the pros and cons? i\\'m aware of issues related to bad statistical practice (e.g. stopping bias) and bookeeping (e.g. keep the bucket assignments forever). what else should i be looking for?lots of deep learning in the (geek) news this week: top 10 rising and falling buzzwords in tech job postings: \"big data\" down, \"real-time data\" and \"artificial intelligence\" up.i, for one, am glad to see \"big data\" die off; it\\'s such a horrid buzzword! :wink:the only time the use of the term \"big data\" is acceptable:: that connection between deep learning and renormalization is incredible, i hadn\\'t seen that before. thanks for sharing!interesting article on using ml to create photo albumsalso, i had no idea there was a disney researcha whole new meaning to mickey mouse research projectshow us gov might prepare for ai-induced economic shifts:something to do over the holidays:til where the oreilly animal pictures come from```many of the animal engravings that appear on o’reilly animal booksare from illustrated 18th and 19th century books on natural history,including richard lydekker’s royal natural history (1893);reverend j. g. wood’s animate creation (1898), insects abroad (1883),and illustrated natural history (1862); riverside natural history (1884);meyers kleines lexicon (1894), and the brockhaus lexicon (1882).```(for anyone who\\'s written anything in latex)a non-negligible portion of my grad career was spent wrestling with tables and figure bounding boxes in latex...yes.they didn’t even use $latex$ in the title of that bookanyone know of openings in the san jose - campbell - santa clara area?  i\\'m in the market for a shorter commute.  thanks.my latex challenge was getting margins to match for my thesisgood morning! does anyone have experience installing qgis via conda in ubunut/lm? i am running into various issues doing so, and believe it may have to do with the pythonpathi\\'ve written a few scope docs. happy to chat! let me know if you\\'re still looking for answers!hello everyone, i want to ask what your experience has been using sensitive data from enterprise clients. - on one side, there is the \"everything on-premise\" deployment: preprocessing, training, testing all happen behind client\\'s firewall. - on the opposite side, there is \"sign nda, get data dump\" policy.what has been your experience, and do you have any tips/best practices to share? thanks!prefer to connect directly to their systems with their security policies so that i’m not in any way responsible if somehow security is compromised.hi everyone, de fellow akshay just posted an excellent blog:computing shortest distances incrementally with sparkit’s also on the front page of hackernews: i just started a new channel for all nyc alumni, i.e., all former fellows living and working in/around nyc. please join  if you match that filter :simple_smile::thumbsup:just saw the discussion above, but i’ve found  to be a pretty decent intro into the why/how of cnns.it doesn’t go too deep, but also isn’t all that shallow either.i see what you did there : “it doesn’t go too deep, but also isn’t all that shallow either.\":slightly_smiling_face:: curious: is that the entirety of the course?ie 3 assignments?: i believe they also get a final project?but i haven’t taken the course so i’m not entirely sure.well meant it in the sense would i get a good experience goign through thoseor if they just stopped updating it halfway through and i\\'d be missing stuff hehstay positive markwangbut it definitely seems usefuloh, i see. no i don’t think it’s incomplete.new folks welcome? :smile: prepare for the onslaught :slightly_smiling_face:we’ll be closer to 800 members by the end of the day!i wonder what is the upper limit for slack.  mabbe we should stress it, just like hadoop.i know i have one channel with around 2000 membersoh boy is it time to haze the new fellows? the 1st rule of fight club is.../giphy fight club uploaded a file: insight only supports shade-grown data scientists how do you propose stress testing slack?! hard to bring down their nodes :wink:preferably grassfed...has anyone here found or bought data on credit scores aggregated by regions in the us? for example, average credit scores by census zone, zip code, etc. i\\'ve found sources, but i\\'d like to hear anyone\\'s thoughts on accuracy, usability, price, and so on.neat use of ml:   just releasedhappy to talk to her/himfinally, stopping gan violence - generative unadversarial networks: &gt;&gt; \"while the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (non) violence popularised by generative adversarial networks have yet to be addressed. [...] experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. our work builds on a rich history of carefully argued position-papers, published as anonymous youtube comments, which prove that the optimal solution to non violence is more guns.\"enjoy: &gt;  unlike previous generators which were content with simply reaching this goal, our generator is more motivated and gives it ‘110%’ moving it a further 10% past the data distribution. while this isn’t terribly helpful from a modelling perspective, we think it shows the right kind of attitude.it\\'s a gem.\"in this work, we have shown that network-on-network violence is not only unethical, it is alsounnecessary.\"this is the trolliest article i\\'ve ever read. lol.lol, i think the ai fellows are full of hate right now….also, why didnt they wait for april 1?sigbovik is always held on april 1st but i think they released the paper earlyhere\\'s a fun one that people liked a couple years ago:  when i cannot scrape natively with scrapy, i use phantom jsthat is call phantomjs inside the scrapy scriptmy team is hiring a senior data scientist: message me for a referral or apply directly.anyone who has experience doing social network analysis on large graphs (paths, communities, cliques) ? currently, i\\'m using graph-tool and igraph in python and want to move to a distributed system because the graph is growing very rapidly. i\\'m exploring different technologies and would love to hear your experience with any technology that you have used.i use igraphit\\'s both in r and pythoni think in python there\\'s networkxfor community detection there are quite a few algoslike edge_betweeness, modularity greedy maximization, infomap which uses a random walk surferi\\'m using igraph for now but the graph has grown too big and i\\'m facing scalability issues. so looking for distributed way of doing it now. spark graphx or some graph databasedon\\'t have experience with very big graphs like the one you\\'re mentioning. sorrythanks :slightly_smiling_face:i wonder if there\\'s a way to factorize the adjacency matrix, like cholezky decomposition, or svd, or similarmy 2 centsi\\'m reading papers on distributing adjacency matrix. let\\'s see if that solves my problemmy experience with networkx in python has been great```when it played against the version of alphago that first beat mr lee, it won by 100 games to zero.```some exciting stuff going on at deepmindif net neutrality is a policy you feel strongly about, here\\'s a friendly reminder to contact your representatives. the fcc is likely to announce a vote to strip net neutrality in the coming month.  has more information to help you contact your reps. if you don\\'t have a formed opinion on it, totalbiscuit explains its importance (). thanks to  for bringing it up via fb post.if anyone is interested in tech in africa, come check out the event we\\'re putting on at fenix in january!  i think that’s a good idea. is it something that some companies already have deployed, do you know ? i see such work for hotel, doctors, and restaurants reviews in 2014 ().:smile: that\\'s cuteany vim users out there who have successfully gotten their clipboards to sync over ssh? i.e. can yank from a vim terminal in ssh and paste locally, and vice-a-versa.if this e-commerce place is more of a marketplace, you can use nlp to suggest improvements to sellers\\' item descriptions to make them sell quicker/higher price etc. (ebay does a bit of this, also see  for a related idea)i don’t know a straightforward way of doing this but instead you can scp the file to your local machine or mouse select, command+c, command +v to a new file and then scp the file to your local machine.anybody else going to this conference in chicago next week?   ?would this help? as far as the hmmlearn package goes, i played with it and it didn\\'t work very well. the documentation was poor and some things were broken. i\\'m not sure the creators are still maintaining it.thanks adamazzam, that’s a great idea too, i may mention about that in the talk. i decided to go with the harold’s suggestion for this time because i checked with the research team that they are interested in the topic. thank you guys !i hadn\\'t heard about it till now but it looks interesting!it\\'s on the pricey side, but if nielsen will front the cash you should totally come!:point_up: probably your best beti haven’t seen that before, i’ll have to give that a trytmux works as well, especially if you change the copy/paste bindings to be vim-esquei’ll need to explore tmux more then, i’ve tried a few time and haven’t been able to get the copy paste to work on there, even locallyhey insight! i mostly do model building in the p &gt;&gt; n space, where feature selection is important, but really i work in the p &gt;&gt;&gt; n space, where i have very few samples (think less than 10 with hundreds of features, in some cases). one of the biggest issues i have with the pipelines i construct is validation— if i choose a sane and simple model, i am virtually guaranteed to get 100% training accuracy (which is likely overfitting). cross validation is not really an option with such few samples. i am curious if any yinz have come up with other ways to validate such models in this corner of ml space. the ones i have used so far: first and foremost, domain-specific sanity checks (in microbial ecology, in my case), and also randomly initializing the models a few times and looking at the variance in the feature importances across those initializations (the hope being that if the model is truly doing well, there shouldn’t be high variance in the feature importance of the most important features). any other tips anyone has got?get more data email austen - this is what he does as well i‘m having a really hard time understanding how one can do any validation on less than 10 data points total (if it really is your grand total). i don’t think it should be possible. if you get any lead please do share here because i’m very interested to know!! that’s kind of my point— you can’t. what i am trying to do is find proxies for that validation that are useful would if i couldsounds hard.np-hardmake up a model, assume a flare prior, strap your seat belt and go bayes beast mode. your samples seem like they may be even smaller than this, but it might give you a few ideas. the only thing that comes to mind is leave one out cross validation with model averaging. leverage is also something that may be of interest for small data sets.also: it seems kind of like a phylogenetic problem - so tree building, where you could assess support using bootstrapping. thanks i had actually read that medium previously; i’ll have a look at the article you linked too i will give that a thought as well, thanks: thanks for the clarification! i know that the roc auc is also a measure of how correctly predictions are \\'ordered\\'. so if i transform this regression problem into a classification one (by normalizing the regressor to turn it into a probability), then the problem is akin to finding an algorithm which has the highest roc auc. do you agree?  that is a very helpful article with more useful metrics, thank you!: i guess i agree in some fuzzy philosophical sense that there’s a similarity with the roc score. i’m not sure how that transformation would go exactly, but it seems like a roundabout way of doing things. i think the better approach would be to write a gini coefficient metric for scikit learn. but first, i\\'d check the kaggle forum to see if anybody has posted that code already. (some discussion here, but i haven’t checked anyone’s code yet: )hi  yes, there is a code for it.but there are some wrong codes, gotta be careful :neutral_face:yeah i\\'m just saying i don\\'t know of any are right : here’s a python implementation that looks good to me: . i haven’t tested it, but it looks right (famous last words!) and comments seem favorable.wonder if anyone has tried to access redis database from multiple worker nodes in a spark cluster. the problem is that every rdd that\\'s processed needs to access the database in some way and it needs to connect afresh. the ideal solution is to have a static connection per worker node or better yet, use a connection pool feature that redis provides. redis lets you define a pool of connections and lets you pick one when you need one. the problem is that spark does not let you create a static variable in a worker node. it lets you define a broadcast or accumulator variable in the driver node that can be shared by workers. what i need is a wroker-node-private variable.: one way to do that is to refactor your spark job slightly.  instead of using map/foreach, use mappartition.  this allows you to operate over an entire partition and you can then keep state for the whole partition.  so when you get a new partition in spark, you start a connection pool, then you can call map on the partition to do operations on the rows within the partition and write the results using your redis pool.anyone an expert in nonlinear non-convex optimization? i have a polynomial non-convex cost function and have done some lit search to find possible solutions. i was wondering if someone could share their experiences on best practices.one of our ai fellows who came from a pm background shared some of his experiences being a product manager for ml-driven products and how it differed from regular product management. we\\'d love to hear more from the insight alumni community on other differences you\\'ve seen. hi all. i was a de fellow last fall in nyc. i’ve always been kinda in the middle between ds and de. i finally finished my phd and i just got scheduled interviews with google and apple for ds positions so i wanted to ask for any tips/prep material/suggested readings/sample questions that any of you may know or think are worth revising, since i did a lot of de interview prep during insight but not for ds. everything is appreciated. thanksmost people really like andrew ng’s machine learning course on coursera and udacity’s a/b testing course. both are free.+1 to bothhey insight amigos+amigas. i’m hiring for a data sci position and wanted to post the job description here before posting on our company website. dm me with any questions or interest! also, if you know anyone junior who is looking for a ds summer internship let me know.-----*about us*at stealth security we protect more than 100 million consumer accounts from automated bot attacks and give fortune 100 companies broad real-time security. join our small (growing fast), highly collaborative team who enjoys working on large-scale, challenging and interesting problems. come help us fight malicious bots and defend enterprise’s web, mobile &amp; api endpoints!*responsibilities** work across teams to frame problems, with an emphasis on deliverables for our products.* perform exploratory data analysis to gain deep understanding of the problems* design and construct statistical, machine learning, and optimisation models* write production code; collaborate with software engineers to implement algorithms and models in production* design and run both simulated and live traffic experiments* analyse experimental and observational data; communicate findings*experience &amp; skills** m.s. or ph.d. in statistics, mathematics, computer science, physics, or other quantitative fields* 2+ years professional or research experience* passion for solving unstructured and non-standard mathematical problems* end-to-end experience with data, including querying, aggregation, analysis, and visualisation* proficiency with python (preferred), or another interpreted programming language like r.* knowledge of spark desirable.* willingness to collaborate and communicate with others to solve a problemi did coursera’s machine learning a year ago (very good intro level course). i’ll do now the other one. thanksany ds/de peeps looking for remote work?  hmu!, ,  ?anyone at pycon this weekend?at nuna health we’re looking to hire a data science manager in san francisco. we’re trying to lower health care costs and improve quality of care. if you or anyone you know would be interested please let me know. the ideal candidate has been an individual contributor for a couple of years and is ready to transition to the manager track. the job posting is here good luck !hi all, a good friend and former astrophysics collaborator of mine has a son who\\'s a junior math major and cs minor at occidental college in los angeles. he\\'s very interested in the tech industry and is trying to get a summer internship in the bay area. do any of y\\'all have any advice of resources he should look at? or, even better, do any of y\\'all have direct connections with folks (or you yourselves) looking for summer interns? thanks in advance!: where are you scanning the systems from?insight svhey  and previous fellows - anyone have experience enrolling in medi-cal / coveredca as someone who just got here from another state?  were you able to enroll right away? did you have to prove some residency in ca (which i’m not sure i could do at the moment)?  did you just enroll in your previous state’s exchange and pretend you still lived there?i just enrolled in medi-calpretty painless.  ~2 week turnaround?don’t think i had to prove residency.  just provide address.i have a question. can an ordinal feature be used in a  random forest model as an ordinal variable?yes: and  what\\'s the medi-cal website?: when i lived in california last year, i signed up for medi-cal somewhat accidentally (i was just trying to check eligibility), and i’m pretty sure i didn’t provide any proof of residency. i couldn’t figure out how to cancel it until somebody from the office called me; it was actually kind of annoying.in ny, the medicaid stuff was part of the same exchnage processi imagine medi-cal can be done through covered caif you tell them you have no income it probably steers you straight there(former ca resident, doing de in ny for assorted reasons)github is down:grimacing::scream:my favorite part was how github\\'s status page was up, and claiming the site was working for about 10 minutes. :wink:oh godhas anyone came up with a good name for the angry unicorn?twitter had the fail whaledoes anything rhyme with unicorn?forlorn?orangealmost time for ladies in tech nightgithub is back! current fellows who are about to interview: this is a great blog post on engineering principles that you will encounter : very cool (and a really nice walk through/blog post, too). cool stuff! has anyone used bayesian inference (w or w/o deep learning component) in an industry application w. real data? most applications i have found are academic (or fabricated data). anyone of the stichfix folks, perhaps?: i know in the defense industry, they use bayesian inference to detect whether a some object is an actual missile vs a decoy.  not sure if you’ll find public blog posts about it :slightly_smiling_face:i generally try some form of bayesian methods for many of my modeling problems. one recent example that comes to mind is classifying medical abstracts that describe a problem in a patient as being due to an adverse drug event or not. i compared a few standard methods like bag-of-words, tf-idf, etc. to using word2vec to produce a representation map that can be used for document classification. the method is described here:there isn’t any fancy mcmc, just a simple application of bayes rule. it worked pretty well.: most of my exposure is in the academic setting, but i know foursquare has used bayesian inference in the past. : neat!   thanks for sharing! i’ll look into it.hi guys, i am looking for some good resources on hidden markov models, ideally some lectures or notes along the lines of andrew ng\\'s (i.e. something that can be understood without a phd in maths). suggestions?hi guys, does anyone know of a way to export the results of a google search into a list of urls? i have already looked at their custom search api and it seems that there is a limit on the number of urls you can extract. any help will be greatly appreciated!google is pretty notoriously hard to scrape because they dynamically generate class tags and the like to purposefully keep you from trying to do what you want.i’ve noticed that bing, on the other hand, is quite welcoming to scrapers.probably because they’ll take what they can get in terms of traffic. :slightly_smiling_face:: i second  in that google is extremely hard to scrape, especially since a few months ago. dynamically generated class tags and suchlike can be dealt with given enough work, but google will also block your ip after 100-150 requests. getting around this problem requires a good proxy service that can be very expensive.if all you need is 1-10 search requests per hour and ip blocks are not a problem for you, then let me know. i can share some of my code to get around the dynamic tags problem.: can you share your solution to the dynamic tags anyway? i’d be interested in seeing what you did.:  thanks for your inputs. currently the number of queries is not high and i’m looking for a quick dirty solution before we get our content into an internal indexi’ve been looking into this  but it only gives the top 10 results or so, not all the resulting urls of a query:  let me find that script i wrote, and i will dm you some of itthanks!thank you!first beta release of matplotlib v2.0.0will i be able to compress and store 2.0 in my head? i certainly wasn\\'t able to with 1.0. :disappointed: you could always tell when i was making a plot by my google searches: \"change axis spacing\" \"change label size\" \"move tick labels\" \"remove legend border\" etc.i’ve committed so many of those commands to memory, , i don’t know if i can take drastic changes to the apinot without an aneurysm at least!nothing nuggetbot?? :stuck_out_tongue:dear alumni, i am a health insight fellow at boston. for the insight project, i am trying to create a predictive model from a dataset that consists of ~2000 features and 10k+ data points. i hope to end up with 10-20 most significant features. does anyone know a robust way of doing feature selection? any suggestion is greatly appreciated. :thinking_face:stay positive higginsc: principal component analysis ? : you can do forward stepwise selection for one, or throw in a l1 regularizer with a large tuning parameterpca will help you reduce your feature space as a place to start...: to add to what others have said you should find the number of dimensions using pca that explain a large fraction (say 95-98%) of your total variance. sklearn has a fraction of variance attribute in its pca class that returns an array of length n-components that you can do a cumulative sum over and find the right cutoff. if you want retain interpretability you should do a model fit with an l1 prior as eddie said and possibly look at the largest feature coefficients. that part is somewhat model specific. however you should be able to determine the most significant feature in any model by dropping a feature and looking at how your fit changes with your feature space projected down to n-1 dimensions. btw, the power for recommendations using this procedure is by doing vector embeddings in a space which naturally preserves heirarchical relationships is really cool: thank you everyone. i will give a try.dear alumni, i am a current health data science fellow at boston. i am trying to find datasets about  running records with multiple users. any suggestions? /giphy find datasets about  running records with multiple users. can you message me your email? i\\'ll send details todaycolaboratory notebooks have a “search stack overflow” button whenever you run into an error… it’s really streamlined my development process will also be here at least one dayquestion for airflow users: what’s your preferred message broker?make sure you don’t compete with the salesforce thrasher on wednesday evening, celeryah, sorry, i asked that wrong. what do you use as the broker?rabbitmqbut i haven\\'t used the other options so i\\'d also be curious what others thinkthose who are going to salesforce, we can figure something out. i thought it was only 1 or 2cool, that’s what i’m leaning toward. thanks!btw if you\\'re planning on running airflow in docker containers this repo has it all: thanks - found that one. was just trying to decide if i should use redis or rabbitmqhi, has anyone used any recommender system in python? which one is a good one to start? the matrix is about 9000 * 18000xi -- can you tell us a bit more about your problem? what are you trying to recommend? what are the features like? how sparse is the data?rabbitmq i\\'m helping put together training material for people to use our software (i\\'m at tamr btw), which involves some machine learning understanding and sql-esque transformations. has anyone used lstm for time series anomaly detection?*herthe  for ai. helpful to navigate the ocean of sensationalist, misleading news articles as well as fearmongers and soothsayers who watch too many sci-fi movies hi ds fellows, i have an idea of analyzing twitter and youtube data specifically the impact of a tweet referring to a video on the popularity of this video on youtube and the number of subscribers. anyone has experience on analyzing twitter/youtube? thanks!for either ds or de fellows… thx!yes, i’m doing something similar for my project, and have quite a bit of infrastructure set up. let’s talk tomorrow. uploaded a file:  and commented: for my immigrant friends on this channel, i am posting this card that explains what to do if ice shows up at your door.  moral:  you have constitutional rights even if you are not a citizen.  the phone numbers are for santa cruz; be sure to have an immigration attorney on speed dial.so anyone know how to delete older files?  i just got a slackbot message saying we are out of space when i tried to upload a small pdf file.your pdf worked -- i think it\\'s just warning you that it\\'s bumping older files out of the cache.in other words, i don\\'t think there\\'s anything different to do. i used postico for some time, but the lack of tab-completion made me switch. i now use pgcli, it\\'s a cli rather than a gui, but do give it a try: i’m setting my project up on aws. does anyone know the easiest way to transfer a postgresql database from my computer to the server? should i use pg_dump? how do i include the security key? thanks!`pg_dump` is probably the way to go. what do you mean by security key?`pg_restore` on the aws serverthat worked perfectly, thanks! at first i thought pg_dump needed aws login info to transfer the db, but i see it just outputs a file that i can transfer. thanks for the help :slightly_smiling_face:insight alum  and i wrote up a little post on our collaborative work together using webcams to measure clothing at stitch fix, with description of the algo included. no deep learning nonsense this time, just straight computer vision: does anyone have any tips for regularizing names in a database. for example, in this they\\'re listed multiple ways \"john smith\", \"smith, john\", \"smith, john a\", \"smith, john a.\".  i\\'m not sure if there\\'s a standard approach for this.: i would spilt the name into first, middle, last then recombine in a standard format. it would probably be good enough for most but not all names. does the db have location info? some countries list surname before given name even without a comma.: in addition to what  suggested, you may want to look at the openrefine library, which performs clustering and better normalization of names, including international character conversion to ascii (e.g. ö -&gt; o)google open sources syntaxnet.  nn for analyzing english textaka parsey mcparsefaceour data team stands “alone” in a weird way. our engineering team is completely independent of the engineering team, but our analysts are mostly embedded to product teams/business lines. in my opinion for a stand alone team i think that it starts with who is leading that team, the person needs to be someone who is pretty much always taking the initiative to make sure that the product and engineering and additional stakeholders are aware of the roadmap and of the data team’s capability. i agree with kevin that depending on whether you get slotted to the engineering or product team then you end up on those buckets. but then that starts limiting what each individual person in the team does, it becomes more “siloed”.thanks kevin, soumya, and kenny.for anyone getting options this is something to keep an eye oni know!!i almost posted the fred wilson article yesterday, but i think this one presents more perspectives how did you try that method in the paper? i mean code-wise..yeah, i liked that it at least mentioned possible prosmy toy model had y be a gaussian distribution whose width depended on the input featuresi can\\'t figure out how to attach a file to this thread, i\\'ll dm it to youok, technical question for you guys. i have an extremely unbalanced dataset (99:1)  for which i undersampled to balance. however, how do you go about normalizing the data? i am testing on my unbalanced dataset so using the same scale feels incorrect. also, if you guys know of any models that are better suited to this problem, let me know!an old colleague of mine put together this blog post on the topic. i recommend giving it a read anyone have experience with ranking/recommender systems in python? i do - what\\'re you working on? awesome!  so i have a set of user attributes, and a ranked list of products tied to each user.  want to be able to reproduce some of the ranking (realistically, only care about the top 5 — there’s 144 products in the db, and any given user has ~10-20 ranked) given the user inputs.  ideally i’m thinking of something like a user-user recommender system, so i don’t have to have the full attribute payload for a useralternately a modeling approach like a bayesian network could workbasically just haven’t worked with that type of outcome beforemy usual go-to is a collaborative filtering approach. if you know which products a user likes, then you can build a user-product matrix and run, say, item-based or user-based collaborative filtering or matrix factorization. this would ignore all of the user attributesfor a more complicated solution, the python package  is pretty awesome and will handle both ranking and the user attributes in a matrix factorization approachcool, i’ll check it out — i think it needs to be attribute-based, cause the reproduced ranking would be for a new user coming in without ranked products — can’t base it on pre-existing rankings, so user-product matrix wouldn’t work if i’m understanding ithave you worked with the surprise package in python at all?ah, gotcha. yeah, lightfm should be able to incorporate attributes and work for new usersspeaking of hiring and recommenders...vevo is hiring for a director of personalization and recommendation. let me know if you\\'re interested in the role.i\\'ve never heard of surprise - looks interesting!likewise for lightfm — thanks!starting my first data scientist job soon and they\\'ve given me the freedom to purchase my own work laptop (with reimbursement). any suggestions on specs? also, i\\'m most comfortable with windows, but started using ubuntu for insight. however, there seems to be a strong mac preference in the data science world. should i start familiarizing myself with mac or stick to ubuntu? thanks! it honestly depends on your role. if often you’ll find yourself having to do some excel work in addition to the more fun stuff, i’d recommend a mac over dual booting ubuntu/windowsi tend to just get the ram-maxed out macbookpro of the month, but if most of your computational/io intensive work will be done on servers / clusters, the air tends to have lots of fans (though maybe that’s changed with the newest macbook pros? i’m out of the loop)there is no inherent benefit to using a mac these days - it’s easy to do both local and remote work on windows os.  i have both windows and mac os for work and the only reason i use mac is because our it people have set things up with that in mind.  might want to find out if that’s true for your work place alsoi haven’t tried the windows bash shell thing, not sure if it’s out yet actually i shoudl update my windows :slightly_smiling_face:  potentially that could close the gap even moreuse a mac so you’ll fit in at data science conferences &gt;_&gt;i have both a macbook and native ubuntu dell for work. for bash/docker/aws there’s little difference, but i still can’t find a good sourcetree equivalent in ubuntu. i feel like your life will be easier with a mac (coming from a guy who prefers ubuntu)thanks for the feedback! looking at some of the on-boarding information, it definitely seems like mac is used by most of the employees. macbook pro of the month it is!i use a macbook pro (and have used many other platforms, though not in the last year or so) and it is great *except* i wish you could get more than 16gb of ram; i’m pretty sure you can’t even now. ram is huge limitation in my experience. just download it :troll:you need to get your boss to fax it to you get a mac  if you are a person who would like to tweak everything, and i mean everything - thinning down kernels, guis, window managers, modules, etc - all of which are possible on linux.  and the reason why i say this is because mac has limited ability to be a playground - so you get lot more *work* done. if not, windows, mac, and linux do not matter. almost all of them have amazing cross compatibility these days.  windows has great support for a bash shell, its tough to find limitations. if you go the windows and linux route consider getting a lenovo x1, you could run win/nix on a intel mac too but its just more expensive. does the lenovo x1 have the same planned obsolescence crap that mac has (soldered ram, etc)? leonvo has the same problem, especially in the gen 4. i have heard people say that you have to choose the socket option instead of the solder option, and then you would be alright ( that costs a bit more moolah, apparently. no data to back the socket version, btw)has anyone used spark-sklearn, and would you recommend it vs. other implementations of spark and sklearn?just to follow up: windows *does not* have awesome support for bash shell. their version has broken symlink support and has since release... there is something to be said for slow patch roll outs, but at some point it\\'s horriblealso, from experience, the right choice us *not* macbook with ubuntu on it... :disappointed: i\\'m running that now and they fight each other pretty badly.yeah i haven’t tried it, i mostly use mobaxterm and remote terminals, hopefully they’ll fix it so local command line stuff is bearablelong time lurker, first time poster here- more on the windows vs mac debate. i noticed that the ctrl key on the mac is in a sucky location. this usually doesn’t matter except i just started using jupyter notebooks, and a lot of the shortcut keys involve using the ctrl key instead of command key. also, it’s really bad when you have to use the ctrl key to crouch when playing counter strike go. after this mac, i intend to go for windows in the future, because they now have bash, and they just have more options, like the yoga/surface tablet convertibles that can also stylus pens for note-takingthis is a super under rated point! :+1: also... i have the keys swapped to the correct position in software... :nerd_face:haha, i swapped it with caps-lock! you can bind `ctrl` to `caps` no?there ya goonly if you don\\'t use :vim: and already have `esc` as `caps` :disappointed:that’s what i have (why would you use `ctrl` in any case?)this has been mentioned on many a forum but the new escape “key” on the mac touchbar isn’t consistently there and can be frustrating. or you could try the new windows linux subsystem, it was developed together with canonical and it might help depending on your needs .  also, you can always get say an i3 or i7 and run linux mint xfce. currently, i\\'m working with an i3 with only 4gb of ram and works very nicely with linux mint 18.1 xfce.the advantage of working with a laptop is that you can upgrade memory, hardrives, etc very easily.fwiw, you can run windows on a mac using the bootcamp partitioning utility.  i set up my old macbook air this way and am quite happy with it.  installation is easy, switching oses is easy, and you get the full windows or mac experience in your active boot session.my laptop running the bash subsystem in windows, is an i7 with 8gm ram, 4gb dedicated video memory , mind you it also does well with games :grinning:wow thanks for all the information everyone! next up: xbox or playstation? just kidding :laughing: ... was a long time linux user and recently changed to mac when my workplace offered to set up my workstation ... got to say that i\\'m enjoying the experience! the big drawback of the new bash shell on windows is that it has no gui functionality. otherwise, if you would only want some terminal action, including ssh, it works great.you could also have a system set up as a virtual machine, without the need of dual boot. for example, installing ubuntu in a virtual machine inside the windows os.nonetheless, i\\'m not sure how long it would take to learn how to use a mac effectively, if you\\'re coming from windows and linux. anyone with experience on that?does anyone know of a package for logistic regression that will allow you to enter total count and percentage of positive responses? the summary data is about 1100 rows, i could create the full data but it\\'s something like 500,000 rows. i came across someone mentioning that this was possible in sklearn but it didn\\'t explain how and i\\'ve been unable to find any documentation regarding this function.i\\'m trying to predict accuracy, and linear regression is doing pretty well. it has an intercept of 0.9 and all variables reduce this value and none are going out of range. however given that it\\'s unbounded and knowing it\\'s a suboptimal approach i\\'m trying to implement a model using logistic regression.whoa that made my day.`explain the difference between an inner join and an outer join.  let\\'s say you have two tables: one with &lt;strong&gt;n&lt;/strong&gt; rows and one with &lt;strong&gt;m&lt;/strong&gt; rows. what are the minimum and maximum numbers of rows that could be generated as results by joining the two of them (one join of any type)?    what\\'s the difference between `union` and `union all`? which one executes faster and why? (not super important)      if your query runs slowly, what will you check to try to make it run faster?        what is a subquery and when might you use one?calculate a mediansql questions from derek steer and i are going to be part of a data science career panel, just in case anyone is interested in attending:on  naming big data projects: : what language are the pokemon in? it is trying to tell me that avalugg is called crebase...submit a pull request with the json file fixedit\\'s just javascriptloading a json filefinally a way to contribute to the big data community through my love of pokemon :pikachu:also i meant natural language lolpokemon have different names in different languages...oh... hahai think the author is french or something so it\\'s in romanjii was like \"i\\'ve caught all 720 pokemon wtf is a crebase\"hahthat\\'s true, steph has a shiny charmwell technically i don\\'t have some event legendariesthose are required for the shiny charm in black &amp; white 2 thoughbut i am intimately aware of those i do not have and none of them are called \\'crebase\\'... at least in english :stuck_out_tongue_winking_eye:in x &amp; y/or &amp; as event legendaries are not required for the shiny charmwell you\\'re getting sometrue :pikachu:with your help :smile::smile: what are friends forcatching pokemon, obviously :smile:clearlythat quiz is really hard for someone who only played red...quiz?the pkmn or big data onedoes anyone have a macbook pro? how do you like it? does yours have the touch bar?it\\'s ok. touch bar is a pain - but you can always map the keys somewhere else. i like the usb-c power supply though. it takes me a while to get used to the keyboard. still not a big fan of touch bar.the touch bar is my main concern. would you buy it again?i hate it. usb-c is a downgrade, touch bar is a serious downgrade, keyboard is a downgrade, and the larger trackpad is annoying and feels like it has worse palm rejection. on the positive side, it is 10% thinner (barf)  i haven’t found much use for the touch bar yet, and it can be annoying at first (accidentally calling siri happened a lot before i removed it from the bar). i think the keyboard is great, but it did take a bit of an adjustment. i really like the large trackpad, and haven’t had any problems with it.  how much ram does your machine have?8 gbwould you rather have the macbook pro or the macbook air?8 gb. macbook pro. i\\'m happy with my decision.i just got one (3 weeks ago) and i am happy overall.  i am pretty ambivalent on the touch bar - i used it to escape and control the volume mostly, but occasionally there are fun features (like navigate among weeks in google calendar).  i like the big track pad, but was kinda bummed about the usb-c.  i bought a converter ($20) for my thumb drive and other things.same here. i have to get a usb-c converter for my mouse.the keyboard is really loud, the touchbar pretty gimmicky. i like the big trackpad actually, but i’ve dealt with daily crashes (possibly a bad video card, which is common enough … just started the process of getting it looked at). i’d trade it in for a comparable non-apple linux machine in a heartbeat, but i don’t know which brand stands behind its hardware the best anymore.: being that it is a cloud warehouse the advantage is that any data coming into domo can be automatically subject to their magic etl workflows, sliced and diced and thrown into a preconfigured view. the views i.e graphs themselves are mature , based on d3 and have some specialized functionalities like the ability to drill down into a part of the dataset . once the listening jdbc/odbc connectors are setup they permanently listen to that data source. the differential advantage comes from being able to leverage the warehousing capabilities rather than just the dashboards. also you can push and pull data and custom viz in python and r to domo.thanks  for the informative answer. i think i\\'ll sign up for the free account and try it out. thanks stephanieany one with ear phones that he/she is not using? i want to borrow for today: this is the chat for _all_ fellows and alumni  — in sv, nyc, and boston. you may have more luck asking under your specific channel (de-ny-2016a / ds-ny-2016a) :simple_smile:thankssv people! you should check out this event from pintrest about machine learning on tuesday the 9th: i was going to sign up until i saw that it isn\\'t in glorious magnificent new york city but instead is in ... uh ... well... san francisco: could you share some code snippet of how you save grayscale image with transparency? specifically, what dimension of np array do you store your grayscale image with alpha channel as? opencv\\'s `cv2.imencode` balked at saving a mxnx2 array...hi, does anyone know how to password-protect a site hosted on aws? i’d like to host my project on an instance, but it’s a consulting project so there’s proprietary data involved. do i do this at the flask level or is there some other way?: i\\'ve done that before. come chatis anyone at aws reinvent and interested in meeting up for dinner today?hi everyone! our team at viacom is desperately in need for a good candidate for a vp level position. here is the job description: please let me know if you are interested or have anyone in mind. any help would be greatly appreciated! :slightly_smiling_face: if you ever need a vp of de, i’d be happy to consider :smile:haha i wish we were that advanced  :wink:anyone out there using neural networks to take heterogeneous data about users and embed a user into some vector space?not yet, but i am actually thinking about doing this exact thing.so... i\\'m interested in it too.we meet again, dr. ainsleydr. mercurio sounds way cooler. like a nerdy super heroand i suspect we have the same types of user data (site visitation, purchases, random demographic data)yes, so say my spanish friendsyes, and i have some of that information for some users and it\\'s missing for others. so, it\\'s a bit challengingi am also thinking about doing the same thingi think we’ve just created a working group.it\\'s been in my queue too.can i get in on that working group too? that application would be super useful on some of my projects.me too :stuck_out_tongue:+1 :wink:i just thought i would check again if anyone was at aws reinvent and wanted to meet for dinner?    excellent idea. that\\'s something we\\'re starting to work on. hey dr. mercuriothis is dr. rubinme too!hi everyone! has anyone here tried their hand at spell checking book/movie titles?like input examples :  \\'the theory of everything\\', \\'theory of everything\\', \\'theory of the everything\\', \\'theory the everything\\'. \\'theori of the evrythng\\' etc.it does not have to correct but atleast identify them all as the same thing i’ve used `fuzzywuzzy` in python for word similarity, the interface is pretty simpleor go with the core functionalities in `nltk` to get edit distance / levenshtein distance etcany nyc alumns going to the office tonight? want to meet up briefly beforehand?thanks! so here\\'s my understanding of a (basic) recc system. i have a dataframe where each row is a review, and the columns are user_id, item_id, and review. in total there are 700 users, and 9000 items. i convert the data matrix into a n_users (row)  x n_items (col) matrix, where each element is the review score that user gave that item. my first question is: is there an elegant way to do this? right now i have a for loop that iterates over all the rows in the dataframe and adds them to the matrix one by one. i feel there is probably a better way to do this using pandas.i want to find items that are similar to other items. so i treat each column (1xn_users) as a vector, and for each vector pair i compute their dot product to give me the cosine similarity between  each pair of items. next, to find the most similar item i, i find it\\'s row in the similarity matrix, and find the max value in that row. the location of that value tells me the most similar item. i\\'m getting reasonable results with this, but i have a few questions. is there a more efficient way to compute the cosine similarity besides just iterating over each item? i know this is the most basic type of recc system out there. what else do you recommend? i want to add more features (like item description) into the system. thanks again!does anyone have install scripts readily available for hadoop multi-node cluster? have my hands on a respectable amount of bare metal to build out an infrastructure ... do you know how to choose which function to use in fuzzywuzzy? there seems to be a bunch of those.i’ve only used it a little, so there’s some trial and errorfor ’s example (dealing with misspellings, rather than semantic differences like “braves vs. yankees” and “ny yankees vs. atlanta braves” meaning the same thing), i think the simple `ratio()` method is best`partial_ratio()` would handle things like missing the leading “the” you can take a look at . if you don\\'t want to use the cli, the bash scripts are under the install folder and hadoop thanks austin!!! really appreciate it!!!!! for configuring the nodes you\\'ll want to go to the `config` folder and for service start stop under the `service` folder ... thanks for the directions! i\\'m going to start the process, hopefully, tomorrow when the servers are activated ... will definitely keep u updated!noice yeah feel free to ping me whenevshi all! i\\'m a fellow in the remote data science program. anyone out there have experience figuring out who\\'s speaking in an audio recording? i have zero experience with audio analysis, but my project involves finding the % time a particular person is speaking in a recording with multiple people. the voices sometimes overlap. i would really appreciate ideas.wendy.carande: hi wendy, i\\'m currently doing the data science program in nyc, and i\\'m dealing with some similar challengesto get off the ground i\\'m using the \\'pyaudioanalysis\\' package that will extract some basic audio features like mfccs and chroma vectors. there are also functions to take these features and use them for speaker diarization. have a look in here:i\\'m still trying to get the outines of my project all ironed out, and i\\'m super happy to chat with you about this. i can\\'t promise any deep insights though! thanks for that suggestion. we should definitely trade ideas.for sure. feel free to send me an email at  if you want to move the conversation off of slack. we can also set up a skype or phone call if you want. im free after 3pm est tomorrowsome 3rd party apis may do this as well-- google ml or something in microsoft azure, for example  worked on a similar sounding project in our session - i think using mfccs and hierarchical clusteringthis isn’t my project by any means, i’m not entirely sure who is responsible for it..hi all, is anyone aware of existing work related to education (school etc)? tax related? thanks!could you be more specific? data science applied to education?: yes exactly sorry it was a weird way to phrase ithi everyone, has anybody used tornado web server for python before?yeah. there’s a ton.i’ve done contract data science work for pearson.hey kenny, what are you looking to do?does anyone know a good package for multi-document summarization? i\\'m looking for something that would take a few hundred reviews and return an average/aggregate review.might be useful for you thanks josh! i looked into it but it doesn\\'t seem to do summarization.for any subsequent happy hour with pizza at insighti was wondering if someone could decode an acronym for me.  a recruiter just asked me in an e-mail \"have you thought about user insights or cro?\"  ... i\\'m not sure what she means by cro though. any ideas? thanks!conversion rate optimizationi found this weird thingah, of course. thanks  !no problem!stay positive katie.amrineneil degrasse tyson has issued a great call to action: “make america smart again”.  i believe the power of the insight hive mind can make a unique and valuable contribution to this mission.  i created a new channel -  - for sharing positive, action-oriented ideas.  the first step i will take is to contact a friend at the southern poverty law center.  they do a fantastic job of tracking hate groups in the united states ().  i’ll inquire about their needs and about opportunities for the larger tech community to volunteer services.  what ideas do you have?  please join .i think this is a fantastic idea. \"make america smart again\" sounds like a worthy call to action, and the splc does seriously good work. i would love to help coordinate with them.while i in no way consider a vote for trump to be the \"smart\" choice, to me \"make america smart again\" isn\\'t a slogan that\\'s going to make any friends. it\\'s only going to serve to create more divisiveness by perpetuating the assumption that you can\\'t be a smart person and vote trump.i understand the sentiment, but perhaps the catch phrase is a little too exclusionary. the majority of people who voted for trump are not terrible racists or bigots. they\\'re confused and scared people hoping to regain what they perceive to be their rightful place in the world. most have been pandered to and incited by demagogues hoping to capitalize on the situation for their own ends. some vocal fraction are bigots and racists, yes, but understanding how to connect with the others is not only laudable, it\\'s essential. \"smart\" doesn\\'t come into it.stay positive higginsc how do you know the majority are not “terrible racists and bigots”?stay positive varunrajansource: grew up in iowaworth reading jamelle bouie on this: i left the midwest for silicon valley precisely because i was responding to the same economic influences that are driving moderate conservatives toward trump. i saw no future there. i wanted to be a part of the optimistic technological future here, and i was privileged enough to have the education and connections to do so. most people still living in rural areas don\\'t have that opportunity. you can blame them and say it\\'s their fault, that they could always move, but really they can\\'t. people have children or other family, many never went to college. if we\\'re so \"smart\", why can\\'t we figure out how to manipulate the \"stupid\" people into seeing things our way?i don’t disagree with any of that, and i think you make good points. (and i didn’t mean to get into a heated argument on friday morning.)i simply think that, as “data scientists”, we should use data before making quantitative statements like “the majority of people xxx”. i think if you look at the political science literature, especially on “racial resentment”, you’ll find that race appears to be a much bigger factor that any economic tendencies. here\\'s an article you might find interesting: anyway, happy to take this offline if you want to talk further.i\\'ll check that out. i also recommend  which despite being a listicle on cracked, is quite goodalso, if this is what passes for heated, i\\'ll take it. i haven\\'t even gotten out of bed :)i only called it heated because i got the “disappointed” emoji from stephanie. :grinning: i was not disappointed in your statement! i’m saddened and frightened by the reality it represents. as someone from north carolina, and as someone who has just legally formed a family that many vocal trump / pence supporters would like to legislate out of existence… i see and personally feel the normalization of bigotry. this isn’t just about obsolete careers and struggling to make ends meet. some people voted for donald trump because he shouts the things that they can only whisper and dog whistle about. maybe we’ll be ok, who knows. but i have real things to fear right now.stay positive stephaniemarilol i’m working on it, nuggetbotbefore we get too lost in the existential despair of the white working class (which is a very important economic and sociological topic for discussion and action--let\\'s throw de-industrialization and the opioid epidemic in for good measure), it\\'s important to point how how jokes work. \"make america smart again\" is funny and effective because it hijacks an existing meme and turns it on its head. if you want something like this to work, it has to have a hook that stays in people\\'s brains.one last, and even more important point is that if anything i think i know about ndt is remotely correct, this will be an effort in enhancing the public understanding of science, not something to dump on conservative voters. i am guessing that there will be pushback against the denial of evolution and climate change, the fact that undocumented immigrants commit crimes in less often than most other demographics (which could be seen as having a \"liberal\" agenda), as well as pushback on the anti-vaccine and anti-gmo movement (which will be more likely to antagonize some of those same liberals cheering the first few points in this list). it\\'s possible that tyson will drop the ball on this, but i doubt it. i apologize if this message is in any way aggressive or incoherent. it\\'s still pretty early in the morning, and i am trying to drink decaffeinated coffee to see if it improves my health at all.good point. ndt is a baller.anyone else at mlconfsf!? i already saw  and  ! want to grab coffee or lunch?we’ve got openings for data scientists and a data engineer on our machine learning team at crowdstrike. (a remote option is available, but it would be really awesome if you were in the los angeles area — you would still able to work from home :slightly_smiling_face: )hi folks! international network question: i am possibly moving to paris relatively soon, and am starting to look for data science jobs there.  does anyone happen to have any connections out there, or suggestions of particularly interesting companies to check out?  (i swear this was my plan even before tuesday....)  thank you! with marine la pen you might need to move again soon  i have some french people on my team, i’ll ask around that would be great, thank you!fwiw, ndgt loves soundbites and definitely used that phrase/slogan to sound good on tv w/ colbert. i have no idea if he\\'ll actually use it more often (but if others have seen him use it in other contexts please let me know cuz i\\'m curious). it also isn\\'t clear to me that he\\'s going to be doing anything different than what he already does. he hosts star talk, tweets, gives interviews, and will almost certainly be doing a book tour for his 2 recent books, but i find it very unlikely that he\\'ll be putting together some new big public science education program.those kinds of programs already exist in many places and i personally hope that they expand and grow now more than ever. if people are interested in tutoring kids (in any subject), the boys &amp; girls clubs of america are all over the place and do great work and if you want to do more you can get matched with a kid through them. some cities have education and/or scientific organizations that match scientists with teachers/schools and you get to talk to kids about science directly. lots of places also have public science programs for adults, either free lectures at universities, or in pubs (i\\'ve been involved with these kinds!), or at libraries, etc. many of these programs are volunteer-based and free to attend and try to target underrepresented groups and lower-income groups, both in urban and rural areashe also likes talking about other sciences as if he was an expert, and makes spectacular mistakes.to be fair, if any of our voices sounded as good as his, people would instinctively believe pretty much anything we said too. he\\'s barely an expert on astrophysics these days... he still knows his stuff.  your comment could be interpreted as an attack on science educators by implying that they’re not “real scientists.”  all that does is discourage other scientists from doing their own public outreach.as a nationally funded scientist and science educator until this past summer, and currently an unpaid and volunteer science educator, i am well aware of the views held by some scientists of science educators. neil does great work getting the public excited about science. he has also done harm by passing off incorrect information as fact, about astrophysics and many other fields. as a poc in science, as a \"famous face\" of science, and as a communicator, he is doing good work for society and creating a scientific literate public that dwarfs my humble contributions. that being said, he is not perfect, as a human or a science communicator. i, and many other astrophysicists and scientists, find it infuriating that he rarely, if ever, apologizes for misinformation or corrects himself, which is the exact antithesis of the scientific method.to all my insight fellows, please join me in this petition to block myron ebell, a climate change denier, from being appointed to the head of the epa. trump\\'s major victories came from oil/fracking states and he has already started trying to lift emission/mpg limits from car companies. we need 100k signatures by the end of this month. man. did anyone know about this? stay positive markwangstay positive americaanyone have any advice or experience with event coding within a mobile app? preferably something that\\'s interpretable but also can be easily analyzed. primarily for tracking user movement throughout the app, identifying where users may be getting confused, etc.hi all! my company (unifyid, security startup in sf) is greatly expanding and hiring mostly engineers in the following specialties:devops engineerfull-stack engineerfront-end developerios developerandroid engineerwe use advanced ml &amp; physics applied to phone sensor data and similar, to identify and authenticate users into apps or websites and eventually replace passwords.please let me know if you are interested or know anyone who is. i am happy to split any referral bonus :slightly_smiling_face: i’am at your service for any further question.hi all! a colleague of mine is a founder of an early-stage ai company in the business travel industry called , and is looking for nlp, ml, ai and data engineers for the founding team. they are based in the bay area. if that sounds interesting, please let me know and i’d be happy to introduce you you should reach out to  to get connected with some of our recent ai fellows! :partyparrot: i know that  would be excited to talk to you!and jumping on this bandwagon! my company (tempalert, boston) is looking for a data engineer/data architect to build out our analytics platform and integrate it with our product. let me know if you are interested!  talk to ! :smile_cat:sending her a message as we speak :slightly_smiling_face:for all you visual learners out there...since  is too modest to promote his own work, he wrote a fun post exploring glove and different styles of english as used on twitter. preview: \"workin - working + going = goin\" :wink:that’s neatway to go nice  . this is the first time i’ve ever thought about a negative letter :confused:thanks !showing solidarity with the victims of the pulse tragedy in orlando, fl, latinotech (), lesbians who tech (), queer tech nyc (), out in tech (), ny tech alliance (), &amp; outnexus () are donating all of the proceeds of their happy hour tomorrow to “the center”, the glbt community center of central florida. the event is in nyc (), but anyone can make a direct donation here:  :rainbow:hi bay area peeps, anybody need a roommate or know of housing around menlo park / anywhere south of sf? (my male cousin is moving down from portland)can anyone suggest what is the best practice to do bulk load from file to cassandara? - i think  might have some experience.anyone currently watching lcs?league of legends?yes thanks for the tip.  i will reach out!a new blog post by  compares a/b testing and multiarmed bandits for adaptive movie recommendations take a look! if any fellow/alum wants to write a post on our blog, hit me up and let\\'s chat.i think all you have to do is emphasize the minimum requirements for the job. title is less important. also, keep in mind that anyone that might join in a junior position may want the flexibility to grow into a senior position. you could also just create a tiered system like other companies (l1, l2, l3, etc) or “junior data scientist, data scientist, senior data scientist, principal data scientist, senior principal data scientist”.many companies (like daimler) try to match seniority to other company ladders. in our case, we match roughly to google’s, but change the designation/titlei would just make the requirements (and comp range) obvious so folks know if its worth applying with their skill set.hi all!  my company, proteus digital health, is hiring a data scientist.  the role is on the platform team and uses data to optimize our core product, a pill that transmits a signal when swallowed.  please contact me if you\\'re interested.not yet - still building the main scraper, might try by the end of the weekwe really should. the survey will help us with that!anyone has experience with jupyterhub as a data science collaboration tool within their team?you might try this.  kindle version is $7.  done! looking forward to seeing the resultsyes. i like go with something involving an imbalanced data set (fraud, clicks, etc) and ask the candidate to walk me through how they’d detect the minority class. it’s a standard question, but i like it because it gives me plenty of insight into how the candidate thinks, what tools they’re comfortable with, and what types of creative solutions they can come up with if i throw a wrench in their approach (what if you discover that 90% of the clicks were bots?). i also like to go into the details of the tools they’d use, any etl (let’s say the data is in two different tables organized like this….), how models could be stored, etc, just to get a better gauge on the scope of their understanding of a tech stack.how flexible it is in order to pick up a teammate’s notebook and edit/add codes?  can you also compare it with any other platform?any recommendations for visualizing a very large amount (~300k+ records) of time series data in redshift? interested in features like panning and zooming. lots of bi tools out there and not sure where to start.i\\'d love to hear if caravel can work for this: linkedin is open-sourcing a scalable ml library for spark called \"photon\". it\\'s used widely here internally:carlos, what does “open-sourcing” mean? is it not publicly available on github already?open sourcing also implies a particular type of license.i wouldn\\'t read anything into \\'open source\\' besides \\'being able to see the source\\'but that\\'s just mesamei believe this is why richard stallman hates the term “open source”. _free software_ implies a certain kind of license.: i miswrote. you\\'re correct indeed.: if you need power you can try vispy which leverages gpus (there\\'s a pycon talk from last year about it on youtube). i\\'ve also been playing with plotly lately, it has interactivity built in.‘;n’mmn cxcvbnhey all, i’m trying to find a style guide to write quick-running, yet fairly human-readable sql. any suggestions?postgresql, in particularif anyone is interested in attending this spark streaming workshop, here is a 25% discount for insight:hi illustrious alumni, if you guys have a chance, could you try out  and let me know if it survives? (it takes a long time to load). extra appreciate authing your reddit account to it. extra extra appreciate using it like it’s a real thing for a bit :slightly_smiling_face: thanks!\"’s server dns address could not be found.\"i already broke it i don’t have a reddit account, but i ran it through my twitter account and it worked! it recommended subreddits about music, outer space and onion-esque news stories. i accept.works fine for megot a lot of space recommendations, which is somewhat but not  wholly indicative of my tweeting habitsif you need personal validation, it recommend ksp as a subreddit, which i was already subscribed tooh it’s just blocked at work…guess i shouldn’t be procrastinatingr/dataisbeautiful, r/data_warehousing, r/data_warehouse... i guess it thinks i\\'m some kind of data scientistseeing as how i follow statfacts and hadley wickham and drew conway and such on twitter, seems like it\\'s workin```internal server errorthe server encountered an internal error and was unable to complete your request. either the server is overloaded or there is an error in the application.```10,000 data subreddits... which is what i\\'d expect from my twitter(but not ones i\\'d actually follow on reddit)lots of ml stuff tooi mean it nailed my interests expressed on twitter!wow, so many data subreddits, good job!thanks for the help everybody! :slightly_smiling_face:50 free data science booksanyone has experience using flask+swagger  (flask_restplus)?any companies out there looking to hire an entry-level ux designer? one of our badass ux interns here at app annie is finishing up soon and looking for full-time work. pm me if interestedwho\\'s at odsc west?i’ll be there giving a workshop tomorrow !clean yo desks!help i\\'m locked out the office would be great to have a dockerfile in the github repothis (not @)channel just keeps on delivering :popcorn:i feel like taking glee in these interruptions is all thanks to you ! clean desks foreverpug bomb 50arrgghh how does it worknuggetbot pug meit’s onlinenuggetbot pug bomb 50:trollface:this is a good dayi thought these pugs were wrapped in towels or something. no those rolls are their bodies.:pug-bomb:renaming this channel to #adorablepugsruuuuuuuttttthhhhhh daaaaaaaaaaaaaavid!:bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching: uploaded a file: hhahahaha...sorry i know we are not suppose to laugh at this meme but i cant help it :stuck_out_tongue:   i think it was me :smiley:. i didn’t formulate my problem correctly.for me it is often safe to assume that  data is likely missing completely at random. it’s just my feature matrix is a triangle at best: number of predictors is growing.the correct way to formulate would be: i wonder if somebody get to work with the growing set of predictors, rather then growing set of samples while doing online learning.if anyone is interested in a freelance web developer job, see the message below from a friend of mine at upenn.\"my research involves virtual navigation behavior and i use a custom built website to administer the studies and collect the data online. i didn\\'t build the website and don\\'t have the knowledge and/or time to change it. the guy who built it is awol. the website runs fine, but i have no one to maintain it, update it, or make changes. we\\'re looking to continue hosting it online, but it uses a plugin for unity 3d, which, i understand, browsers are pulling support for. this poses a pretty serious issue for future online data collection. i\\'m looking for a web developer who could provide a time/cost estimate for some of this. the project uses html, ruby on rails, and is built on a mysql database. the server is a version of red hat, and is currently hosted for free on a university server.  if you would be interested in consulting on this project, or know someone who would, please contact steven weisberg by email: .\"cc:  :wink:a bio on mr. lecuna good friend of mine who’s a total badass data scientist (non-insight) and who cares deeply about equity and inclusion issues (in tech and beyond) and who’s been in the field for quite a few years is looking to switch companies in the sf bay area. so, if anyone has or knows of an open lead/director-level data scientist position, please dm me!heard uber is really big on diversity. just kidding.i must share this with the world. thanks! !anyone seeing any performance issues with tensorflow on aws today? we’re seeing a &gt;15% slowdown in training and speculating that it’s from the intel bug but honestly have no ideaindeed amazon has already applied the patch for meltdown which is expected to slowdown cpu performance by 20%unsupervised/semi-supervised learning question - i have a binary classification problem where i know if something is true positive, but not if something is a false positive (basically, some of my data is labeled with a 1, but everything else could be 0 or 1 but i don\\'t know the answer).  is there some weaker analogy to an roc curve that exists for a situation like this? i can show \"what % of true positives does the model correctly classify\", but i\\'m wondering if there\\'s something more sophisticated i can do (because of course i can tweak the thresholds to catch all of the true positives, but i\\'m probably also incorrectly classifying a bunch of other points as 1).i do have a sense that probably not more than 10% of the data set is a 1 (so 1\\'s are relatively uncommon - this is basically an anomaly detection problem)hey, in medicine this comes up a lot, where it is often easy to get a subset of the positive population via a query, but getting the complete positive dataset is very hard (manual review). i found the following 2 papers helpful:it sounds like the usual caveats apply here, i.e. wanting to make sure you minimize false negatives, and that you try to preprocess your training set to balance the data. depends on what a true positive means though, and how bad it is when you miss one.this is great, thanks! (and good guess - this is indeed a healthcare problem) has a cool blog with some others which discusses this issue. the relevant post is: hi, i am looking for *basic or simple github or tutorials* for  *user recommendations by cosine similarity*. any recommendations ? thanksall mayors should take public transportation at least once a week.i have seen rahm emanuel on the cta in chicago so some do :slightly_smiling_face:linear regression question:if i have both categorical and continuous features (&gt;0) what is the best way to preprocess the data? my continuous features have wide ranging scales - some are in [0,10], others are in [0,100000]. i can rescale the continuous ones and one-hot encode the categorical ones, but is that the right thing to do? seems this scheme would favor the categorical variables. i am just curious why “this scheme would favor the categorical variables”my approach, based on this article by gelman is to to center at 0 and then scale continuous variables by two sd, and center binary ones at the mean. here\\'s the reference to that this pretty much makes parameter estimates comparable in regression models and helps with convergence in bayesian models there is no right way of preprocessing your data. usually it’s an iterative process and you should try a few different approaches. here is a nice guide to normalization, scaling, mean removal and other preprocessing steps from sklearn: one-hot-encoding is most common way of encoding the categorical variables but there are better approaches. i really like this deck from an experienced kaggler, where he describes different approaches on how to handle categoricals: because when i rescale (and in this case i really mean normalize, not standardize) very few of my continuous variables will be close to 1. however my one-hot encoded variables will be 1 or 0. one of my categories is coming out on top in feature importance. i am just not sure if this is a \"true\" signal or a consequence of this choice of preprocessing. are you going to apply l1 or l2?right now playing with l2in that case it may be marginally important. you can always map your categorical feature to 0 &amp; 0.5 for example and see if there is any difference. it would be interesting, if there is :slightly_smiling_face:thanks for the suggestion. i\\'ll let you know if i find anything interesting :slightly_smiling_face:hi, i\\'m trying to use multinomial logistic regression in spark (to predict ~300 different outcomes) but keep seeing some strange behavior (eg. the number of jobs triggered vary even if i keep both the data and the model parameters the same, as well as when i use the trained model it just predicts the majority class for each record so my assumption is that the model is  quite messed up). do you have any suggestions how to debug this issue? thanks!thanks  and  i\\'m leaning towards luigi.  it seems easier to digest.   anyone at gtc this week? pat and i are all four days.does anyone still publish now that they\\'re out of academia?  i\\'m looking for what journals are considered good for data science/analytics articles.  thanks all!: i have a few in the pipeline, but like anything else it\\'s likely to be specific to your analytics (i.e. distributed clustering, nlp approaches, etc.)probability by peter norvig i\\'d just like to say that i love almost all of the notebooks that peter norvig writesupdate: he ended up wearing a navy suit jacket, white shirt with no tie, khakis, and dress shoes. he was interviewing with cfo and coo so this felt appropriate. for interviews with non-exec, he would have kept the top but gone with more casual pants-shoes combo. thanks for the tips, y’all!^ this is cool but it also freaks me out a little?they should make one for ratsstay out of my dreams, ! last night i had a dream that the nyc office was infested by rats! the alpha rat was named darryl! thank you for your reply. it is a great idea to visualize the data first with some domain knowledge. the  predict value `open_rate` is heavily related to human behavior - people tend to open their emails during the middle of the day and the weekdays, so there are periodicity involved in the time feature. besides that, it’s worth to look at the correlation between domains - `` vs ``. also between ips - presumably we assign emails with different qualities to different ip. but for our customer `portal_id`, i will assume they are independent of each other.thanks!i am making a list of common human cognition pitfalls when using stats to analyze data... more ideas?human cognition pitfalls in statson basic statistics1. null rejection. to think that because one proved that x does not cause y implies that x does cause y. i.e., to think that not having evidence to reject the null implies that the null is true (one cannot disprove that medicine a is effective in treatment t does not imply that actually, medicine a is effective in treatment t) correct: keep trying to find evidence to reject the null and accept it as not disproven but never as true2. correlation and causation. to think that correlation implies causation and that therefore the pattern will repeat in the future. correct: correlation does not imply causation and to infer causation one can use causal inference or experimental design.3. correlation of non-stationary time series (i.e., prices). they will always appear correlated even if they are not due to correlating trends but not the underlying stochasticity of the series.correct: first make the time series nonstationary. i.e., correlation of returns (stationary time series)on fitting models4. future leakage. to use historical data that was construed using future information, i.e., using a fundamental indicator that appears built at time t but actually used information from time t+x.correct: always use point-in-time data with correct dates.5. backtest overfitting. to fit n+ models using the same dataset until getting good results (even when doing this \"kosher\" and using the out-of-train times to test the model performance). correct: take into account the number of models that were fitted and use proper multiple testing corrections.maybe it\\'s better to start a doc and let people to contribute there?that\\'d work but we can leave it on the thread in the meantime... do we have a way for all slack members to contribute in a common doc?... sounds like we would have to create a google doc (or quip) and therefore invite people into it which creates some friction into getting everyone\\'s input...you could also make the doc accessible to anyone with the link (public)looking for significance in posthoc subgroups when the overall result is null i think i\\'d be ok with that if the next step was to run a dedicated experiment in a sub-group, rather than to make a  decision to roll-out a feature to that subgroup based on that first testi am having a problem with my reverse proxy on ubuntu (aws) for my website. i have a shiny app that runs on the apache web server, but i am getting a 502 proxy error when i try to load the site without the port number. some of it loads, but some files do not. here\\'s what i have in the default config file for apache2:stay positive alexisyelton&lt;virtualhost *:*&gt;        # the servername directive sets the request scheme, hostname and port that                                                         # the server uses to identify itself. this is used when creating                                                                   # redirection urls. in the context of virtual hosts, the servername                                                                # specifies what hostname must appear in the request\\'s host: header to                                                             # match this virtual host. for the default virtual host (this file) this                                                           # value is not decisive as it is used as a last resort host regardless.                                                            # however, you must set it for any further virtual host explicitly.                                                                servername localhost        serveradmin webmaster@localhost#       documentroot /var/www/html                                                                                                         documentroot /var/www/html        proxypreservehost on        proxypass /         proxypassreverse /         # available loglevels: trace8, ..., trace1, debug, info, notice, warn,                                                             # error, crit, alert, emerg.                                                                                                       # it is also possible to configure the loglevel for particular                                                                     # modules, e.g.                                                                                                                    #loglevel info ssl:warn                                                                                                            errorlog ${apache_log_dir}/error.log        customlog ${apache_log_dir}/access.log combined        # for most configuration files from conf-available/, which are                                                                     # enabled or disabled at a global level, it is possible to                                                                         # include a line for only one particular virtual host. for example the                                                             # following line enables the cgi configuration for this host only                                                                  # after it has been globally disabled with \"a2disconf\".                                                                            #include conf-available/serve-cgi-bin.conf                                                                                 &lt;/virtualhost&gt;any thoughts?if its not in sk learn or statsmodels, that probably means it doesn\\'t exist :confused:python tends to be lacking in more advanced linear modeling.yeah i figured.  i guess the only option is to either build it in pymc or use r hahasomeone needs to give statsmodels some more dev love i’ve had to switch to r for things that aren’t implemented in python - it’s kind of a pain but doable. if you’re happy using a notebook, then it’s pretty easy to load up an r kernel. alternatively you can call r just within a single cell.does anyone have good examples of (technical) topic summarization formats?  either interactive or not.possibly using d3 or other tools.geolocation question: has anyone figured out how to return all lat and lon coordinates within a circle of a particular radius? thanks!how do you define a circle?just wondering since a circle has infinite points and there are infinite lat lon coordinates in a circle.do you mean get all lat lon from a select list?ah, yes from a list! like you want to find all taxis near some point at a timeand i would define near as a particular radiusin python, you can do all that with a the shapely packageoh! some databases, like mongodb, have built in functionality to do that too.cool thanks! i\\'ll check out shapely:plus_one: for shapely. i use it all the time.hi all! we are looking for project reviewers at udacity for our machine learning projects. you get paid per review and capstone projects are paid at like $70 each review. let me know if you are interested.good video on how google makes their teams: ha! i had just this idea a little bit ago: haha sweet. feel like there\\'s a few title missing, got any idea to complete the set?hmmmmm i’ll think on it!facebook has open-sourced a pretty neat tool for time-series forecasting! roger.w.shaw: nice find! was looking for something just like this.i think there are lots of converts,   - vim was the fourth-ranked option in the 2016 stackoverflow survey. and the top two (notepad++ and visual studio) are windows-only products, so i\\'d guess vim\\'s use in open source systems is higher. hi all, for those of you in the la area or planning a summer visit, the date for `big data day la 2017`, a free to attend conference with over 1500 attendees and featuring many great speakers from the industry, has been set and finalized!  the conference will take place `08/05/2017` at the usc campus (yay!  we upgraded our venue from previous years as we continue to grow).  so save the date and i hope to see some of you there :slightly_smiling_face:for those who are local and want to volunteer, please reach out to me directly and i can loop you in to our volunteer meetings and opportunities.  it’s a great way to ensure that quality data and data science conferences like big data day la remain free to attendees.looks like ai is ready for insight! :+1:link to github repo: it’s worth noting jupyterlab (the evolution of jupyter notebooks) will also have collaborative notebooks. link: . i think these also use google drive on the backend. the beta milestone is due out before the new year (fingers crossed), so colaboratory is probably a better experience at the moment.colab is also really focused on using google compute backend. juypterlab seems more flexible beautifulsoup question. is it possible to use soup.find_all( ) to find two separate tags and classes? i\\'m looking for both (\"p\", class_=\"program\") and (\"div\", class_=\"center-hrs\") ideally in one final list in order. thanks!!!solved it by passing in two lists!so, dropbox got pwned. check if your accounts are compromised.   and whats your score? mine is 3 :slightly_smiling_face:1 (damn you 000webhosting!)stay positive carlosfahammy old gmail has 8 breaches and 4 pastes :confused:waoh!just one, frikkin dropbox^ dropbox for me tooshitbitching ass-bastardsstay positive johnrwalkyou’re not my supervisorno breaches ^_^no dropbox for me….but skeptical.  i had a random sign in event a few weeks ago and changed my pwstay positive rociosngdropbox, but thankfully i have a unique password and actually don\\'t use italso adobe!haha adobe yesman, using plus addressing makes searching so hardwtb regexand ..ugh myspace. yes i had one like 14 yrs agoflash player is the dirty thing for the internet ever.adobe needs people to signup to get flash player! how stupid is that? its like signing up for getting a jvm from oracle.oh wait.. you have to do that too:smile:woah, i had a myspace account. it must have really sucked because i don’t even remember having it.  i believe the system admins said, “here you go kind sir, myspace, is down and dead, take this server and do what you want with it”. it turns out it had passwords.but the most valuable thing on the server was really an index of all of our tastes in music during middle schoolhaha..i am sure half of those passwords were threedoorsdown, sum41+blink182, allstarz, etc.apparently my adobe password is somewhere to be found on the internet, but i still have to reset my password every single time i login there. well, apparently someone has access to one of the 5 backups of my thesis research. it probably would have been easier to just look on github than to hack dropbox...for all of you word2vec users: what is the method that you use to create a sentence vector from a word 2 vec model?paging , you\\'re nlp magic is needed!@alex_guide: hey alex.for short things (3-5 words) we\\'ve found that summing works finewe tried some weighted sums, but i don\\'t remember their performanceand we\\'ve used some different models like paragraph2vec and skipthoughtpat though is more expert than i, hopefully i can poke him into showing up: for the weighted sums: are you just taking the average, or do you assign different weights to the different words?: paragraph to vec is doctovec?yeah, although i think we\\'ve done with and without averaging; i don\\'t know if those experiments ever went anywhere.: i was assuming that we would use just the simple sum. example for simple bow: ‘marry had a little lamb’. assuming dropping the ‘had a’. would just use model(marry) + model(little) +model(lamb) to get the vectorthat\\'s worked well for small numbers of words for usi\\'d certainly try it as the first thingthat seems to be the way i was thinking.what data set have you had luck on training with?i know it matters for the actual problem.you could also try tf-idf weights on the word vectors before summing^--that\\'s what we did for weightingbut again, don\\'t recall the performance :confused: you would have to do the tf-idf prior to training the model as well corect?for training we generally do one of two things:1) take a pretrained model, and maybe retrain it a bit2) train from scratch on the text we\\'re interested inyeah, you\\'d have to calculate it on the full sample, you can\\'t do it on the vectors afterthanks  . it’s been a while since i’ve tried itwhat do you find has better performance. training on the data to be used. the ‘docs’ that i am trying to cluster are short. maybe ~ 2-3 words on average.train tf-idf on your dataset, train word2vec on a larger dataset that’s domain-specific if necessary (e.g. law)  thanks!!!!just to add to the noise, i\\'ve used skip thought vectors a bit... they are very interesting but the author\\'s implementation tends to kill your python process with floating point overflow errors so if just for that reason it\\'s worth pursuing slightly simpler solutions like tfidf and sum as   is suggestingstatistics without borders:   sign up for the september barug  meetup here\\'s an interesting read on earnest!   : in my company it isn’t based on paternity/maternity but on the primary care provider.  (although, looking at that sheet above, maybe that is not uncommon.)who else will be at pycon?perhaps of interest, github just released a new 3tb dataset: nuggetbot animate me happy gilmore nuggetbot animate me grateful dead: hey smart data scientists! does someone want to explain rectified linear unit and the hidden layer to me?over the chat ?  i suggest you read this : : thanks chris. let me take a look at that.okay, so ```an example neural network would instead compute s=w2max(0,w1x)s=w2max(0,w1x). here, w1w1 could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. the function max(0,−)max(0,−) is a non-linearity that is applied elementwise. there are several choices we could make for the non-linearity (which we’ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. finally, the matrix w2w2 would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores.```i get that you could do that. but why would you do that?why the hidden layer ?sure, what advantage does this have over not having ita series of non-linear transformations (each hidden layer does this) can be a very powerful non-linear predictoryou can think of the original features undergoing a series of non-linear transformations to a space where classifying them becomes easierwhat\\'s your definition of linear in this case?the rectified linear unit is piecewise linear, but not a linear function, other activation function are also non-linear like the sigmoid or tanhso the features space is mapped to a different one that cannot be described by a linear functionor better said a linear transformationwhy is non linear better than linear?so in this example, say we do w2max(0,w1x)swhy is that better than w2(w1)s?don\\'t we lose information by taking away the negative values of w1s?same reason you might want to use a svm with a non-linear kernelhi i’m a remote session 2016 b fellow with a world of problems on her project - but right now, i’m looking for a bootstrap expert. i had working flask/bootstrap code and a beautiful webpage on my mac, but once i tried to host it on centos, i lost all the images and the bootstrap nav bar. loading text files and other images outside the static folder works fine. i am very confident i have the right address - i had two other fellows check it too.anyone have any idea why mac working bootstrap code wouldn’t be working in centos? :confused:: are you serving your site with gunicorn? gunicorn doesn’t serve static files.no… haven’t even made it to gunicorn yeti’m just typing in the ip addresson my local machine - everything else loads just fine. just nothing in static or templatesand i checked permissions on those folders, it’s not thati strongly suspect this is a centos/bootstrap problemstay positive loisthanks nuggetbot but i feel like you’re taunting mecould it be a path issue? i had to change the paths in my html.i changed all the paths and have double/triple/quadruple checked themi did relative and hard pathstried with and without the leading ‘/\\'tried with .. and witouti get errors that look like this:it might be useful to know what http status codes are being returned for the bootstrap requests. you can get that info by loading the page with your browser’s developer tools open to the network panel uploaded a file: or in the log which you just postedgreat minds think alike :wink:so they’re 404si tried googling this set up too and working through all the stackoverflow pages with suggestionsme and 2 other fellows have been working at this for about 3 hours… we’ve tried a lot of different things :disappointed:this is happening with the other fellows’ apps, too? or just yours?just mineyou could use a cdn for the bootstrap files, but that wouldn’t help with your imagescdn?this is a really good suggestionbecause i can try to put the images where my output files are to load them inand see if i can get the cdn to workthanks  !!i am going to go for a run and then give it a try :slightly_smiling_face:you might also consider just moving on to gunicorn because you’re going to have to do that eventually, and there’s not much to be gained by getting it working on the flask server in the mean time.good luck, !thanks :slightly_smiling_face:: that worked perfectly!!! you’re my hero of the day!!!yay! i’m glad you’re up and running!….  !!!!  thanks for your replies the other day on the multi-label issue! i wanted to ask for some clarification... so multilabelbinarizer can be used with any sklearn classifier, and this is just a way to feed in the target…  but i believe we have to use either the onevsrestclassifier or onevsoneclasssifier meta-estimators? whereas the multi output classifier is both multi-label and multi-output, so it doesn’t seem to apply here… ? thanks!link for the lazythis might justify an @ mention for the whole channel. i’ll let you incur the wrath of the @ channel police. :stuck_out_tongue:has anyone been to datascience: elevate? what was your experience like? hello everyone. we are trying to deploy cassandra in production with very less experience with how the deployment should look like. could someone suggest few thingsdo you really need cassandra?  managed nosql databases like aws’s dynamodb or gcp’s datastore/bigtable have a much lower bar for developers to use and maintain.thanks so much for the reply aws is out of question in first release. we are deploying cassandra now some of the  questions are do we need physical machines or vm. what kind of disk. what is the max disk we can addwill your team be operating the cluster? yes we will be managing the cluster too i work at datascience so yes :smile:  this is the second elevate event we’re doing so the n is small, but the first one had some great, very high quality talks from la companies and ones that are on the national scale ones.  there were a lot of people from great companies big and small in attendance also.  the talks included i think a really nice balance of covering companies’ specific data science uses cases and technical solutions (types of models and technologies), and tying them very well to the business.  i was actually surprised that everyone brought such consistently high-quality talks, since we don’t control the specifics of any given talk and only the general themes, and i’ve seen a lot more fluff at other events i’ve attended before.  whereas the elevate talks were packed with substance.  but i think our event coordinators spend a lot of time vetting and filtering the right people.we have some great people coming again for this one, so i expect it to be very good.  let me know if you have any questions.anyone know if it\\'s possible to use the rangepartitioner for pyspark? i only see a way to access it in the native scala api....is there anything like “schema registry” for kinesis?i never found one. i wish there was!both those issues are consistent with their core offering though..they’re basically just trying to be a simple gui front end to whatever your data warehouse is for users that are not interested in editing lookml or writing sql. they’re not making any claims of being able to speed up queries beyond what your backend is capable of and i don’t know of any other companies/products (accurately) making such claimsmy issues were more on the limitations of that gui…plotting options were often limiting (though quickly growing and customizable if you’re not running a hosted solution) and i was curious if others found domo or other products less limitingour main performance issue is essentially that you can\\'t force aggregation to happen before joining. this effectively increases the run time complexity of many of our reports.  you might not mind it in red shift,  but it\\'s still poor design, and wanting better is not asking them to \"speed up queries beyond what your backend is capable of\". politics + linguistics + data science! “semantics — what does data science reveal about clinton and trump ?” by maixent chenebaux hey guys, thought this was a good cheat sheet for big o complexity of different cs and machine learning algorithms^ very good! much useful.has anyone had luck logging into facebook on python? if so what did you do? thanks!daniel lakens (of the ’no more statistically unsound science!’ revolution) has a brand new coursera course: : thanks a ton, that is super helpful!datacamp is hiring for multiple roles related to data science:curriculum director: lead our curriculum team and help to define what it means to be a data scientist in the 21st century. you will shape the future of our curriculum and expand our network of world-class instructors.data science curriculum lead: help us engage with leaders in the data science community who could become instructors, and work closely with them to design new courses.data science content developer:  work closely with instructors to build new data science courses. you will help create exercises, slides, and scripts, and most importantly be an advocate for our students.data scientist (growth): help us make sense of what our users are doing so that we can better serve their needs. you will be the first technical member of this team, so you will have lots of opportunity to make a big impact.you can see more info here:i used/battled with the instagram api for my project. happy to help!generally speaking, if i\\'m scraping multiple sources for a set of values, is it worth the computation time to use fuzzy string matching to keep it resilient to minor changes in formatting?does anyone have information on best ways to identify and store metadata on faces in videos ?the requirement is to identify human faces in the videos and then store, the related metadata - for eg., vectors of only those that contain the faces. i am wondering if anyone has experience with this and can provide some guidance.does anyone use any internal tools or services that patrol aws/gcp costs and look for efficiencies in some sort of automated way?for example identifying scheduled jobs or queries that all scan the same data every day, meaning a new rollup or aggregation might be a good idea you write this for fun with your coffee breaks, i assume :wink:lol :joy:on a train right now but can dm you later tonight also does slack mobile not let you start threads? :disappointed:yeah it’s awful about thatif you single press the message where you want to start the thread a new screen will appear with a box to start a thread! thank you you\\'re right, it is pretty much zero. but the quantity being calculated is a ratio of 2 differentials, i.e., how much the prediction changes when you change that delta. just as the slope of a curve, you can\\'t computationally have deltas, but you can extrapolate the derivative as delta approaches 0.i\\'m not privy to which algorithm they use for this extrapolation though. uploaded a file:  and commented: did a seed vs. seed analysis going back the last 30 years, enjoy! commented on ’s file : i hate you all.: . i still need to fill out mine though jejejejdoes anyone know if there is an easy way to change the distance metrics in sklearn classifiers?: which classifier(s) were you thinking of?, the easiest way is to put in a distance matrix.  it will take sparse matrices as well.that question doesn\\'t necessarily make sense, depending on the classifierdistance metrics... euclidean or bust!: no manhattan :trollface:actually better yet: chebyshev distance :troll:hmmhi, guys, i am wondering if any one knows a good tutorial or source site of ml in c++ ? the only machine learning stuff i\\'ve done in c++ was with open cv (computer vision). there is a ml part and a separate deep nn part.   thank you so much,  . i will take a look and would be coming back more question. i guess. please let me know if you have any source of this.  :smile:heads up: meetup is considering hiring another data scientist. nothing posted yet, but thought i’d put it out here first. dm me if you’d like to chat about it.anyone have any tips or suggestions for mobile app a/b testing across multiple devices (android and ios primarily). bayesian methods like multi armed bandit are something we\\'d like the solution to include. a third party vendor is fine, but we\\'d likely need to self host due to phi/pii. oh, i\\'ve also seen that you can deploy changes to an app without deploying a new build. does anyone have experience with that? hbo talked about doing this with their neural net for the hotdog app. thanks ! would love to get your input/help whenever you get the chance :slightly_smiling_face:hello everyone - i\\'m trying to get a large dataset from an external ftp server to s3. i\\'m currently downloading the files by sshing into my ec2 instance, and running wget to download the files from the ftp server to my ec2 instance - i\\'ll then upload the files from my ec2 instance to s3. the download from the ftp server is slow, and is taking a while - i\\'m not sure if this is due to the bandwidth of the server, or if this is due to the fact that i\\'m downloading a large number of small files. anyone have any ideas on how i can increase the throughput? (here\\'s the ftp server: )what happens if you try to download several files from the ftp server simultaneously?the server seems to throttle me to 3 parallel connections per hostthis might help excellent! thanks, that looks like exactly what i needthanks again guys - this saved me hoursdoes anyone know how to compute the efficiency with true positives, false positives but also unknown detections that you cannot validate?: if you are only concerned with speed, you might want to consider using cython () to compile parts/all of your python code. last time i checked cython was just a tad slower than pure c. also, if you are familiar with java, take a look at  scala -- a very concise language that offers the speed of the jvm and relatively easy parallelization thanks to tons of nice java libraries that it can use natively. i want to use the dirichlet process gaussian mixture model.   do you have a good reference/doc page that you can give me?  i have been looking all over.: there\\'s also waffles, which is written in c++ :  thank you very much guys. i will study them and may come back with lots of questions :sweat_smile: thank you so much guys , keep posting me any other references!!!: i have found vw pretty good at times, its pretty fast and written in c++, unfortunately docs suck : i was thinking of clustering algs. it is very straightforward to have a custom distance matrix there.  for the model you referenced, it sounds like you want a custom covariance matrix? i\\'ve only taken a brief glance at the source, but i think you could subclass sklearn\\'s dpgmm or gmm and override _get_covars and _set_covars with whatever you want.: ok! thnx.  was hoping not to go that direction but looks like that might be the only way...: thanks for the source. at least, this try to be detail in its contents. worth to look at. so you have tried this?: yep!  code runs well once you figure out how to tell it what model/parms to use.  good luck!: thank you, i will play with it and come back to you :smile: thanks ~~does anyone have any experience with non-negative matrix factorization packages in python? i\\'m thinking about scikit-learn\\'s nmf and biolab\\'s nimfa, but perhaps i\\'m missing some others.stay positive alex7non-negative is the best i can do  :wink: i\\'m using nimfa.  if you find something better let me know.  there are a couple packages in matlab as well.  r also has an nmf package f y ithanks guys!i\\'m writing python code, so i guess i\\'ll stick with nimfa or sklearn.: any reason in particular you\\'re using nimfa over sklearn\\'s nmf?nimfa\\'s api gives finer control of the l1 regularization out of the box.  basically i want to increase sparsity in one factor but don\\'t care about the othermakes senseone last question: do you know how it deals with missing values in the utility matrix? does it impute them, or does the cost function just skip over them?i don\\'t know offhand but i assume you could control for that by preprocessing the utility matrix.  thanks  ! i\\'ll investigate furtherif anyone is interested in running 5k/10k/half sometime in the spring, a few of us are starting to look for races in  - maybe a fellows reunion run where we get insight jerseys. join in if you are interested!to cover letter or not cover letter, that is the question.i don’t find prose format particularly useful at the resume stage of applicants.  portfolios + key skills sections are a much better alternativeagreed with , i will read a cover letter out of respect if it is attached, but it will more often sway me to reject than to accepti’m also put off by cover letter requirements on teh applicant side of things.  when i was applying for positions i always skipped that part even if it was required, opting to miss out on the opportunitytoo much fluff needed in that format for my liking :slightly_smiling_face:hi all, i\\'m a current fellow that also  has a 1.5 year old son. i just started the channel  to commiserate about how our kids are ruining our careers (joking). more seriously, i\\'m really curious as to how everyone has navigated their careers with a family.i’m taking a look at this hortonworks schema registry, supposedly it doesn’t require kafka wow, cool. i’ll give that a look.welcome fellow fellows!can you invite others? i\\'ve invited everyone from our class, but it\\'d be great to have all fellows on this same slack, don\\'t you think?hello world!tomv: how\\'s dow jones / news corp. treating you so far?hi all!hello!: fabulous thank youi\\'ve only deeply offended 4 people so farso it\\'s a major winalthough i\\'ve spent the last week doing nothing but flask -- the irony has not escaped me....was one of them rupert? :simple_smile:unfortunately not, but my next target will be one of the fox news talk show host/giphy fox/giphy hannitylamehahagreenhouse all good?very good. lots of interesting data and good nice co-workers. i don\\'t feel like i\\'ve been doing too too much yet, but i can see some meaty data science on the one-month horizonshould i start an #impostersyndrome channel? :wink:we are hiring a senior data scientist on my team at square. a lot of happy insight alums here! happy to answer any questions. question about evaluating offers that i’ve had a hard time answering via internet searches: what is the value of stock options in the case where the company doesn’t ipo or get acquired?: have you taken a look at this? hadn’t, will now. thankstl;dr: if it’s private, you can only estimate to the best of your abilitiesshort answer:  absolutely nothing.(to the question of what is the value if no ipo or acquisition). this is on top of the usual estimating you can do about the potential value in an ipo and all the the other caveats such as vesting, dilution, etc.statistic i got from a vc back in 2008:  1 in 10 of funded startups result in an ipo, 2 of 10 in an acquisition.and absent those exits, shares (or unexercised options) are essentially a worthless, illiquid asset, right?yes. though there can be exceptions such as the recent twilio buy-back: …but they may be preparing for an ipo so this may not be much of an exception. a colleague who has exercised some twilio options said the repurchase offer came with a very strict nda - it wasn’t officially supposed to be public.preferred stock is usually investor stock.  employees have common stock.  although sometimes some founders elect to participate in a funding round and purchase some preferred stock too.hey co-fellows! does anyone know of a ready-to-use method to get feature importances from a multioutput randomforest classifier in scikit-learn? thanks!, hadn’t seen it, thanks!: i started a new channel  as a place to discuss tools that improve the quality of life of data scientists in the wild. there is a rapidly expanding ecosystem of ds technology out there, and hopefully we can crowd-source insights about the best solutions.tcmoran: excellent idea!hi all. i\\'m working in a poc for a recommender system based on cf. i recently came across variational autoencoders and though it might be a good idea to use them to generate new samples in the input domain in order to reduce sparsity. any thoughts on this?anybody going to google i/o?\"you\\'re going to overfit!\"i have hopefully one last question - the webpage works great when i run using ./run.py from the command line, however the output page doesn’t work when i use gunicorn/supervisor. it gives a syntax error:     cur.execute(query2)programmingerror: syntax error at or near \"(\"line 1:  create table  (date timestamp, steps real, distance real, c…and i’m not sure why this happens with gunicorn/supervisord but not with running the python 2.7 code directly (./run.py). any thoughts?we figured it out :slightly_smiling_face: it was an issue with time in gunicornsays it runs on vm. i wonder if that vm has access to gpu.  linkedin is open-sourcing a very useful tool called \"dr. elephant\": seems like a very useful site especially for people getting their bearings in python:thoughts on the best unit testing package to use in python?  specific use case is to unit test on functions heavily dependent on luigi.we use unittest, it works, i like it, i don\\'t have enough experience with the other frameworks to give a pros/cons list though. :confused:anyone did kaggle competetion: allstate purchase prediction chanllage?hi all, does anyone have experience working with large scale geo-location data? in particular, i am looking to find a way around using the haversine function.using the mgrs grid system is a good way to organize geospatial datai\\'ve used mongodb geospatial indexing in the past. easy to configure and fasti understand elastic search has similar capabilitieswhy do you want to find a way around haversine? is that a bottleneck for you?analyzing the data for feature-selection before making the train/test split. i was guilty of this during some data challenges i did at insight :stuck_out_tongue:. there\\'s a good discussion about it in elements of stat. learning, 7.10.2.i do, long story short is kinda of a pain in the butt so far for me but i am managing to do it... my tech stack currently looks more like a babushka than a clean barbie dollfor fast processing i use mapd but it\\'s geospatial capabilites are still nascient and it\\'s scale still limited... so i had to combine this with postgis (not scalable)... i still haven\\'t found a full scale solution... i am looking into geomesa now (spark-based geospatial processing engine)as of haversine, what are u specifically doing there? thanks.  thank you. the data is already in a sql db and cannot be moved to another db. i am trying to find locations that are in a certain radius of another list of locations. the large scale of data makes it very expensive to compute haversine for all of the data.i am looking into mgrs right now.+1 to  if you organize points on a grid that could help and then you\\'d approximate the distance from point a to point b as distance from grid centroid that point a belongs to, to grid centroid that point b belongs to... you can easily convert points to x miles by x miles grid using any db, i did that in aws redshift using python udfs... and then distance from grid a to grid b is known since you built the grid.. i personally built my own grid since mgrs cells are too large.. look here: as of the data being in sql it sounds terrible to have large scale data in a sql db... but well, it shenanigans are always hard to fight...look into vowpal wabbit — lightning fast linear learner seems like your data is in dense format, even though it’s sparse, if that’s true, i would recommend converting it into sparse format, which will drastically reduce the size of the data and allow you to try wide range of models. one more suggestion, try other encoding schemas other than one-hot, this will help with size of the data. some that come to mind are weight of evidence, label, frequency and mean encodings. in my experience they work better for categorical features with higher cardinality,additionally, some algos allow checkpointing. this will allow you to train in batches, but personally not a fan.engineer humor uploaded a file: watch out! if you flat map too much, the beer is going to get dumped in the cooler! :wink:i’ve found postgis very useful. it handles a lot of these grid computations very wellgreat idea! just learned pipeline in the odsc!these are all great ideas! thank you! another thing i am thinking is cat2vec with keras.facebook median salary at 240k... that seems highanybody find a clean way to insert github, email, linkedin symbols in word for resumes?: fontawesome font, install it on your computer and then save the resume as pdf. you can find the cheat sheet here and just copy and paste the icons into the word doc. thank you this is currently my favorite data science application:   so, so usefuldoes anyone know of any interesting papers/tools for conducting audience segmentation in news media -- e.g. identifying engaged vs. experimenting vs. unengaged readers? i\\'m currently reading this paper  but am looking for something for recent.for the jupyter fans out there: has anyone played with declarativewidgets and jupyter-dashboards?for those of you interested in attended pydata in sf on aug 12-14, use the code insight for 15% off early bird admission and 20% off regular admission. hi all! i posted a couple months ago about an open position at coach, inc. for a data scientist. right after this, we acquired kate spade and hiring was put on hold. the position is open again, and now we\\'re a bigger company with even more data. data science at coach is a green field right now and we\\'re only limited by having enough people to get the work done. our work touches all levels of the company from understanding the customer, optimizing marketing and pricing, dealing with fraud, forecasting sales, product design and advertising, hr analytics, etc. here\\'s a link to the job description, and please let me know if you are interested:hey guys, got wind of this awesome plugin for jupyter notebooks on atomhello fellow insighters (hah, see what i did there....i\\'m here all week):has any worked with twitter data where they are trying to find the number of hashtags mentioned in a certain time period? how easy/difficult it is to do this?i work for boston, and the mayor\\'s office wanted to get some data on how many tweets are made using the #mbta hashtag for all you boston alumni. the mayor mentioned to the press that the mtba was generally reliable, so we\\'re trying to get him the more correct information so he\\'s aware of mtba\\'s performance. :wink:\"once again, the train is on time. #mbta\"i guess all of you could do some mad social engineering to the mayor if everyone hashtagged positive comments. the idea is to have a dashboard in his office so he can look up at a screen and see how the mbta is doingwho is running mbta, is this information accurate and can we get them to come fix the ny mta?haha i\\'m not sure you want the folks running the mbta to fix up the mta...boston\\'s transit is notoriously terrible as welli mean, for a system that\\'s as flammable as it is, the mta works pretty ok, until it catches fire.maybe we could do a mayor swapsomeone created a website that tells you if the dc metro is on fire  . i\\'m sure they could use some help tooas someone who has lived in both boston and nyc, i\\'ll take the mta over the mbta anyday. except i\\'ll trade the card taps for the card swipes.are they trying to measure the sentiment of all tweets with the hashtag #mbta? or just the number of them? because if it\\'s the former, it\\'s not going to look pretty. i\\'ve met a lot of people that only joined twitter to complain about the t to the t.no sentiment. just raw number.the point is actually to get the mayor to know that people complain about the mbta, alot, and to get him to noticesomeone from the policy office was frustrated at his comments that the mbta \"generally runs pretty smoothly\"yeah, right. i especially liked on the t scheduled daily interruptions of the d line during morning rush hour (from 8:30 til 3pm) for scheduled tree removals. couldn\\'t they have done this not at rush hour???sometimes marty says some pretty weird sh*ttoday i learned that the mayor of boston does not have a charlie card.he gets escorted by a security detail from his dorchester home so...yeahditto to what kevin said. it is great if you may have multiple different clusters of outliers. but if you are looking for a \"normal\" cluster and an \"outlier\" cluster perhaps something as simple as k means may work. i would personally run dbscan and set a frequency threshold like if the cluster didn\\'t have at least  1% ( for example) of the samples it would be an outlier. fix that threshold based on the parameters of your study. thanks to all the replies! you guys rock. ```in 2016 google\\'s ai had an iq of 47.28. it came out ahead of chinese search engine baidu (32.92) and microsoft\\'s bing (31.98) and had almost double the iq of siri (23.94)... in 2014 google\\'s iq score was 26.5,```pretty impressive, almost 2x improvement in 2 years. wonder if it plateus now    thanks so much for all the input!  i may ping you guys individually.  i’m seriously exploring this because i may potentially need to get this set up!alecalthat’s what i do. i communicate clearly to both sides “person x is looking for a job at your company”; the point of the chat (with resume) is for person x to learn what the company is really like and the position actually is, and for the person at the company to meet person x so they can make a more informed decision if they want to recommend them. you can often hear about job openings before they get posted online if you ask around. also a personal reference carries far more weight than a random online application!oh that sounds perfect then! :smiley_cat: uploaded a file:  uploaded a file: to fellows in the nyc office. i lost a pair of blue bose headphones. please let me know if you see them around.  thanksluigi vs airflow wondering if anyone has experience or opinion.  ps relatively unsophisticated data team.   we use luigi pretty extensively at work. so far, so good. relatively quick learning curve to get people writing tasks and we haven’t had many issue. happy to dm if you’ve got more directed q’s about it. can’t say anything on airflow. i’d recommend talking to  about airflow. he’s used it quite extensively. we looked at both a few months ago and decided on airflow.  luigi is really elegant and simple but ultimately we needed the full functionality of airflow to accommodate a pretty complex workflow.  i\\'ve found airflow to be frustrating at times but it meets our needs.  if your workflow is standard and relatively simple, i\\'d pick luigi.  it will be easier for your data team.thanks  and  maybe i will try bothran across this recently:  (look for links to more extensive reviews in row 3)we use luigi on my team. the choice was made before i arrived, but i’m told that luigi won over airflow because it’s really easy to install.: this might be a good place to start: what do you use/recommend for scraping sites, beautifulsoup?!old school regular expressions!but sure beautiful soup is okay.. if you want it the easy way. heh.yes beautifulsoup is the way to go.scrapythanks  and !if you need to digest pages and interact with javascript on pages in an automated way, you might try selenium. i haven\\'t tried it myself, but i have heard rave reviews.: damien had suggested splinter instead of selenium.  from my experience, selenium isn’t the fastest.good to know, thanks!splinter is also very easy to use used selenium for his project i believe.hi channel, does anybody have experience on hyperparmeter optimization on a large dataset?  can i optimize hyperparameters on the subsets of the dataset, and use the average of these to train my model on the whole dataset? do the hyperparameters of the machine learning models depend on the size of data?: what type of model are you using? in an lda (say), one hyper-parameter controls the number of topics per doc, which can definitely depend on the size of the corpus. another controls the words per topic, which wouldn’t depend on corpus size provided your sample of the data is truly random.: i am using supervised models such as random forest, gradient boosting. so i am trying to optimize number of trees, tree depth, etc.yes, it’s totally ok to optimize on a subset (esp if dataset is large) as long as it’s representative. nice thing abt rf is that it doesn’t need much tuning except ntree. gbm, gd luck with tuning =pthanks  isn\\'t tensor flow using the gpu instead of intel? which instance are you using?^^exactly! that’s why this is so confusing. i’m using a p2 instance, which uses an nvidia gpu and an intel cpu.  the time per step does seem to show slow downs at intervals equal to my batch size, which suggests that the data loading is even more of a bottleneck. but shouldn’t the training be largely unaffected once it’s on the gpu? because if that assumption is correct, a 15-20% overall slowdown would suggest that data loading of the intel cpu is slowed far more than just 15-20%... hi all, there\\'s something i\\'ve been wondering about recently and hope there are people here who can share some perspectives. i\\'ve noticed a few of my younger acquaintances have recently been hired as data scientists straight out of college to work at established companies like facebook, apple, etc. in my mind i would like to think that data scientists with a phd background offer value to companies that ba\\'s and bs\\'s cannot offer, but i\\'ve never had the opportunity to find out for sure. so my question is, for those who work with both phd and non-phd data scientists, do you see a noticeable difference in quality?i think there\\'s a lot of ds work to be done all around. some work is more about designing metrics, coming up with hypotheses, designing experiments, visualizing data, testing, etcsome is more like de, some is designing new algorithms in deep learningphds may be more biased to enjoy certain areas and non-phds may gravitate to other areas tooi think there\\'s some truth to the stereotype phds typically do spend longer on projects and have a tendency to go for 100% perfect, while non-phds get shit doneso if you\\'re a huge company and have 100 roles to fill, you might be better off taking a super-smart person from a bs and training them to be a great ds rather than taking a great academic and training them to focus on short-term deliverablesthere\\'s a need for both, to be surei\\'m curious to hear more of others\\' experiences. here at salesforce, people hired at the bachelor level are \"data analysts\". i think they can level up to data scientists through work experience, additional classes/training, and achievement. phd and masters level are treated as work experience and specialized training and start at a higher salary/grade.i think our company is similar to what people described above. we have a range of degrees, and more of the bs/ms people are in bi positions, while the phds are all in r&amp;d. one of the big distinguishing factors for phds in my experience is the ability to work on larger, unwieldy projects. that’s certainly learnable, but i wouldn’t expect someone right out of college to be able to take on as large of projects until they were more experienced.i\\'ve worked with both types of data scientists (as well as sat on hiring panels for both), and i think there are advantages as well as disadvantages to hiring either. phds do have the added skill set of thinking scientifically, but we tend to have a tougher time transitioning to the speed of industry than someone out of college. companies typically will hire college grads into lower level positions, whereas oftentimes hr and other factors dictate that phds must have higher salaries and roles. i\\'ve also noticed that some companies prefer to train up their own data scientists from scratch (for better or for worse), which again lends itself to college grads at lower levels. ultimately it comes down to the preferences of the company and hiring manager. i agree with \\'s point about it all being conditional on the company/culture/needs. i would add that there is also a lot of value in a phd knowing what you don\\'t know, which is typically not the case with a bs. that might not matter in certain company/culture/needs, but it definitely matters in some environments&gt; value in a phd knowing what you don\\'t know:100: this is clearly from what i have seen across my industry, and i mainly work in the area of cybersecurity, fraud and malicious behaviour - state/antistate/criminal actors. there are two kinds of phds ones who have graduated with work/thesis where their pi is running a project and these students have contributed a large body of work that has earned them a phd. the second type are students who work with pis where there is just a pile of data (call it pile of junk) and their pis wants them to explore and come up with insights/hypothesis/proofs/algorithms.   the second type is purely exploratory. you fight with the data, do years of analysis, come up with ideas about projects/thesis then identify insights, create hypotheses, and eventually publish them and earn a degree. when  phds transfer to the industry, the later kinds are the ones who are the ones who get into positions of driving projects from start to finish *faster* as they have done the same thing multiple times, that does not mean that the former kinds cannot, it just means that they need to be bootstrapped a bit - work with a seasoned analyst/scientist etc, and they get the hang of it. the hang of it being owning a project, the success and the grave failures that come with any research challenges. this approach is evident across all people i have worked with. people with with bs/ms degrees are not much different and that is the reason i really do not focus on the degree one has earned while hiring them. i primarily look for ownership of the project. can you own something? can you look ahead and think of pitfalls, validation mechanisms, can you imagine pitfalls, what are your recovery plans, contingency plans etc.  few folks with bs/ms degrees too have gone thru similar training and that makes them not much different than us and they can operate autonomously - that is important. but this number is small, because bs and ms does not give one the luxury of time to explore data for extended periods of time. so, quite a few folks with bs/ms degrees require some amount of boot strapping.  *my experience comes from a field where there are no prior algorithms and you almost always have to start from exploring data.*  that might not be the case in other fields.i was going to reply with something similar from the finance perspective but i think that about sums up what i\\'ve seen.  only thing i\\'ll add is how bad a new hire is if there is a mismatch between the manager\\'s expectation and the new hire\\'s actual ability to own their own projects.  in the past, we\\'ve made the mistake of hiring people of the latter type under managers who expected them to do their data science manual labor for them.  those hires end up leaving once they realize the opportunity for career growth doesn\\'t match their expectations.we\\'ve also made bad calls in the other direction: hired people who need a lot of hand-holding when we expected them to work independently.  those people end up finding different managers.purely by experience, i have figured that you could have the mismatch in hiring the one talked about by  . in that case, i it would be great if the manager takes up the onus and is flexible to operate in both the styles, makes it harmonious.thanks for all the thoughtful responses. it was helpful to realize that there are many different types of data scientists out there, and at the end of the day it\\'s about finding a mutual fit.i do find that phd data scientists on average offer advantages.  but it\\'s also true that i\\'ve worked with some bachelor data scientists who were extremely talented in all sorts of things, from modeling to experiments.  so it\\'s not surprising to me at all to see companies hiring these people (assuming this is who they are hiring) straight out of college.  as good scientists, we should keep in mind that just because the average is some amount, doesn\\'t mean there aren\\'t a handful of exceptional outliers :slightly_smiling_face:and just as an anecdote, someone i worked with in the past started with just a bachelors and behind the phd  and masters data scientists at his organization, but within the course of a year went on to basically surpass all of them in application of modeling techniques at his organization (though not quite as strong on the experimental side as some of them).  woudl definitely hire this person out of college easily, if i could somehow identify them :slightly_smiling_face:i would be happy toanyone have experience with building a google chrome extension for the final deliverable of their project? currently, i\\'ve implemented a flask-app that takes in a url, scrapes data, generates features, and outputs a probability using an embedded model. i\\'m thinking an extension would reduce the friction of physically visiting my webapp and inputting a correctly formatted url--it\\'d ultimately still report a probability. i\\'m just not sure if this is easy to do in a few days, or if i should just stick with snazzing up the web app. thanks! ^^that article looks cool, but wow, that photo is majesticwe have microservices (maintained by de outside of juypyter) that provide access to what\\'s in s3. then jupyter notebooks can make api calls to those serviceshi, sparkcognition (my company) is desperately seeking for data scientists. this is an ai startup (founded in 2013) and grows bigger every day. it currently has 130-150 employees and 1/3 are data scientists. there are lots of cool data science projects (nlp, vision, autonomous driving, finance, security etc.). feel free to connect with me if you need a recommendation. if anyone is going to the grace hopper conference this year and wants to come to the ghc square events, shoot me a dm!thanks for posting!is referenced in the post and seems super interesting, but i’m wondering how a distributed protocol like this is not susceptible to malicious behavior. : deep learning and nlp, via the useful @pythonweekly on twitter ^^ tufte is good here too: thanks! that is very helpful, do you mean programatically within sql(redshift) or using python to query and find counts?my dev-sql skills is not up to par, but i know it’s fairly easy through python/pandas to get column names programatically and do sums and counts.the prob with that is that it requires me to load the whole data to a pandas df, which is not good (i guess i could use chunksize and it would be a generator)you don’t need to load the whole data, just do a count(*) or somesuch on your table.the alternate client is tempting, but i fear we have a network effect that doesn\\'t transfer :disappointed: my favorite:  really good for communication in generalwish you were a few months earlier! i love boulder!  i\\'m going  : moving into my new place tonight :s but would love to hear your impression of the meetup after sure.  maybe lunch next week?: with pleasure. i\\'m working somewhat near insight. calav a place where you\\'d like to have lunch?opinion piece on toxic data: sounds great.  monday at noon, you pick the place on cal ave: how tempting to get good old chinesei can taste general cho’s chicken already :simple_smile:hello channel: do you guys know of a way to retrieve query status when you connect to a database server using sqlalchemy?  waiting and hoping for the best is not the best solution, and would be great if there is a way to retrieve query status while waiting!are you using a cursor to query using sql or are you using the orm?i am using a cursor to pass sql query to the databasei mean, she wasn’t wrong thoughthis mess is a place^ a universal statementnice to know twitch keeps you slack game up to parmy company, pear therapeutics, is hiring for a sr. data scientist with potential to lead an experimentation team. you should excel at experimental design and advanced statistical modeling. the position will include the exciting opportunity to lead the development of a scalable system to test the efficacy of pear products. chance to work at a rapidly growing sf startup that’s doing a lot of good. feel free to message me if interested  question for data scientists from canada working in the us under nafta tn statuswhat tn profession did you indicate? computer systems analyst or mathematician / statistician?started by same author as  and : i\\'ve been using the xlsxwriter package with pandas, and it is pretty intuitive and has url functionality. : a somewhat random request for those that are active on this slack (alumni and current fellows). would folks mind updating their profiles with their insight program-location-session, current role &amp; company? i find it useful to know more about people than just their username, especially if i haven’t met them in persongreat suggestion aren\\'t you on a boat in greece ?alright. i no longer do \"sweet, sweet data science\". i have a boring job title in my profile. unfortunately the boat has ended :wink: in seattle today you should have negotiated for a less-boring job title like `sr. director of data science and chief slayer of dragons`if only i was a good enough data scientist to predict how bummed i would be about the situation.it\\'s a great idea! i saw someone doing that a few months ago and changed mine to matchdone!: anybody here has been working on churn prediction? i would like some pointers regarding where i can start learning it. thanks! and  i thought we were c? did it for his insight projectno more @ here ing uploaded a file: looks like someone woke up the ruth monster i had the same q earlier and had a chance to talk to some of the fellowsi can give you a transcript of the conversations if you’d likeabout not using @ here, duly noted :slightly_smiling_face:, would like to see the transcript.there’s a channel we setup last session i believe for fellows to chat about churnno reason that alums can’t use it too :slightly_smiling_face:thanks! looks pretty empty, not sure if slack has deleted old stuff or fellows didn’t end up using itprobably got deleted by the output of i didn\\'t even know about the channel stop blaming the foreigners  stay positive till_bethere is a ltv channel, is case anyone is interested. i created it couple of weeks ago.: i just used survival analysis and bullshitted my ways through the demos  don’t share the secret sauce!i think one important thing is that your definition of churn will heavily depend on your business case i’ve seen others do survival analysis (non-insight), is there an issue with not doing cv? i.e overfitting on data?i’m not too too familiar with survival analysisme neither. i think i ran it on different samples to prevent overfitting but i can’t quite remembermight be easier to approach it as a classification problemyea, i was wondering what the pro/cons wereherelet’s move this to lolnuggetbot animate me pikachucrap wrong channel sorrystay positive scstarkdoes anyone know if there is a common statistical analysis that allows you to evaluate two outcome variables at once? for example, fitting a slope and intercept to each group in a study, and being interested in whether other variables are correlated with changes in those slopes and intercepts. it seems like evaluating one in the absence of the other may not be appropriate but i\\'m also unaware of a common technique that allows you to take both into account at once. in the case of this study, the slope and the intercept both reflect two different specific ways in which the other variables could relate to a change in these outcome variables.i\\'m not sure if i\\'m explaining this clearly enough.: they\\'re bread and butter macro-econometrics, i\\'ve never seen them in ds. so they probably suck at prediction.stay positive patrickdmy first nuggetbot :heart_eyes:: in the simplest case you could try a mancova, or you could look into structural equation modeling. if you want something more focused on prediction rather than inference, you could try multivariate tree boosting () long live net neutrality!: if both variables are dichotomous, you might want to try something like a bivariate probit (), or else some sort of seemingly unrelated regression (sur) setting if you can comfortably assume that the relation comes through the disturbances (). clearly both methods would be inferential...thanks for the suggestions everyone, i\\'ll take a look at these.there’s a mention of a y combinator experiment in this article. does anyone know anything about that?ubi (universal basic income) is all over the news recently, partially thanks to the swiss’s (rejected) referendum on it. y combinator is supposed to be running an experiment unconditionally giving out ~$1000/month to some hundreds of families in oakland (just remembering these numbers from my head - don’t quote me on that) to see how it affect people’s behaviorsthere was a couple of announcements on hacker news as well (in case you didn’t see these yet) - does anyone have experience passing variables in flask between html pages/ app.routes? thanks!figured it out using session!!hi can a de person help me out by confirming / correcting my plan to take data from our current internal oracle db, convert it to parquet and store in aws s3?(private message if interested)seems like a good plan if you’ll be analyzing it in spark in the futurewe currently have an internal spark cluster but we’re migrating over to aws. i was just curious how/when the best way to do this is.we currently use sqoopanyone used airbnb’s superset (née caravel (née panoramix)) with nosql databases? perhaps mongodb?the git issues are unpromising/”left as an exercise to the reader”-esqueyeahi\\'ve done some with ithi! i am looking for volunteers to represent insight and tell the story of your transition to data science. the catch - it’s through the lbnl postdoc association and they would like specifically people who did their phd or postdoc at lbnl. it would be in the evening of 3/7 or 3/9, please let me knowbay area peeps: my old company (palantir) is hosting this event in downtown palo alto on thursday 2/23 that looks pretty cool — i have school that night or i’d be there! check it out and rsvp at the bottom if you’re interested: “it starts with girls is a year long, five part series that investigates each part of the pipeline in engaging girls and young women in stem.” the speaker for this one is debbie sterling, \"the founder and ceo of goldieblox, an award winning company on a mission to ‘disrupt the pink aisle’ with toys, games and media for girls.” anne: i can helplooking for someone with extensive *marketing ds* experience for a few hours of *paid consulting*. dm for details or with recommendations, thanks!hi all, we’re a small startup focused on helping companies make better hiring decisions though data analysis and predictive modeling. we are looking for a *(data) engineer contractor* to support us on a project in a few ways:• access production data systems and build etl pipelines• identify, assess, and execute pdf parsing services in terms of cost, feasibility, and accuracy• work with us to make infrastructure recommendations to support data analysis and modeling• time estimate: 60-80 hoursplease dm me if you or someone you know might be interested.how do folks distinguish between data analysts and data scientists, in terms of job descriptions/recruitment/hiring? at my company we are having a tough time emphasizing that we don’t necessarily need the full qualifications for a data scientist, but instead want someone more junior that still has statistical chops. any input is appreciated!when you say you\\'re having a tough time emphasizing it, how does that manifest itself? do you have too few applicants from people of the desired skillset / salary expectations you\\'re looking for? or is it you(r team) emphasizing to higher-ups that they’re overshooting on what they’re looking for? i.e., the classic “5 years experience in a 2-year-old tech” job posting request or they’re just gung-ho for a phd/senior-level when it isn’t necessary? an excuse to skip your siesta.hi everyone! i’m posting this for a friend.  vl49, a new company from the same flagship venturelabs that started my company, is looking to hire for a principle / sr. computational scientist.  the role is focused on using ml on multiple ’omics and high-throughput datasets to drive therapeutics development.  it’s a great opportunity to lead the computational arm of a well-funded early stage startup.  dm me if you’re interested.[unrelated] i know a friend looking for a da positions(from engg backgrounds). if you’re looking, let me know. can share their resumes :slightly_smiling_face:has anyone tried out the packages for slack integration with python and r? they\\'re really easy to use and pretty useful. i have automated scripts sending daily metrics messages into some of my company\\'s internal channels.i\\'ve used the python package for slack to do some visualizations of channel activity at work.this guyhey everybody!  i just created a new channel  - a place for people to chat about founding and starting companies.  we have seen fellows and alumni go off to start their own company, or have a strong desire to do so even if they dont have a product idea of there own.  let’s plan to use  to connect!ps - insight also gets early stage companies that are looking for ctos and founding heads of data/eng.  currently have a couple interesting opportunities kicking down our door.  pop over to  to find out more.has anyone used neo4j in a production environment, and in particular within docker? we’re running into some snags getting it off the ground and i’d love to pick someone’s braini\\'d be interested in this as well! now we just need someone who knows it hahit’s a delicate dance to get right. depends on the workflow you want to have. for ex, if you have a shared directory accessible from different team members’ jupyterhub then by default notebooks created by others are executable but read only to you. how you structure your version control becomes an interesting complication. i don’t know how to compare it to other platforms. thanks jonathanwould anyone that has experience with facebook\\'s graph api be able to answer a few questions to help me get an app up and running?any recommendations for movers from sacramento to san jose? i got a quote today that estimated we had over 4 tons of stuff… that’s roughly 4 ford focuses :car: :car: :car: :car:beltmann relocation groupwould be one i can think of, thank you! i’ll ask for a quote from them.i had a good experience with  nuggetbot: animate me settinghmmm.... i know nothing about thatgood settingi am doing a price comparison between aws ec2 and another service, would people here mind saying what they spent on a monthly basis hosting their insight project on aws?~$50 /month but can\\'t remember if it was for a small or medium instancethanks chloe: : thanks. i am aware of the pricing model, but i find its still hard to accurately predict what the monthly cost would bethat is probably by design :simple_smile:eh?what is the study for?i\\'ve done some cost comparisons in the pastec2 is rather opaque because of the reserved instance pricing schemegce is rather nice because your discounts automatically kick in based on utilizationthis is a new feature thoughi am evaluating another service for my own personal use that uses a flat rate pricing model once you\\'ve reached 1 month and i am trying to figure out how much i\\'d save or how much more i\\'d spend vs using awsit really depends on your use casethere are many many many vendors that provide a vps service(virtual private service)at the lowest end that can beat the net cost of a t1/t2 micro ec2 instancebecause the lowest tier instances require that you use ebsexample: digitaloceanlinodeetcec2 provides the largest variety of instance configurations or \\'flavors\\'what is your use case?many different casesif you expect to have sustained use, the appropriate ec2 strategy would be a reserved instancein one situation, i want to experiment with building a cluster of elasticsearch nodesin another situation, i want to build a small web application and host it, perhaps with a database such as mysqland finally, i may also be interested in hosting several static or dynamic websites (not sure yet)which of the cases, if any (or all), would be running in perpetuityand -- do you want to be a sysadmin for all of the websites?that depends, as of now, i am not surefor standard web hosting, a resellers account at a webhost probably makes the most senseunless you want to also be a systems administrator and want to build everything from scratchfor your web applicationswhich i doa vps or \\'droplet\\' from linode / digital ocean probably makes a lot of senseand for a cluster/lab --- ec2 / gce / azure probably makes the most senseyou can also build your own stuff from metal :simple_smile:yeah, i already have several servers at home which i am using currently, but due to network issues in my area, they are no longer reliablehow about a colo ?haven\\'t looked into that option, but thanks for giving me another ideacolo is classicdon\\'t forget to check out rackspace too for a vendorif you wanted toyou could provision yourself a virtual private cloudand use that as a labhomelab!it typically turns out to be more expensive in the long run if your utilization is greater than 33%which is why homelab ftwthanks, i will look into ithomelab + colo would be a really cost effective lab setup if you find someone friendly -- keep in mind you\\'ll need something that fits in a smaller footprint than most homelab gear since they\\'re charging you based on rack space, power, hvac, and the network connectionif you’re already using jupyter notebooks, extensions + jupyter widgets is probably the fastest and easiest solutionyou can just spin up your notebook dashboard w/widgets, using your extensions to hide input cells/lock editinghas anyone used loftey in nyc for finding an apartment?*what separates _great_ data scientists from the _good_ ones?* if you have opinions, ideas, and/or anecdotes on this topic, i\\'d be excited to hear them! please add a comment to this thread or send me a direct message.short q, is there a way to get list of all actors, directors and movies from imdb?  how big is that? by that i mean title id, title name, title rating, title genre, title type, country, director, actors (4 main) contains info about the data they make publicly availablei don’t know if it helps but might have an starting point via the bokeh gallery, upper left. has anyone had to set up a vpn on their aws instance because a site they were (in this case scraping) blocks the ip range from aws? uploaded a file:  and commented: hi alumni interested in health!my husband (who’s a high school math teacher), wanted me to pass along this community service opportunity that’s taking place in oakland on february 17:i wanted to invite you to help support the new 9th grade interactive assessment for our integrated project about cancer. this february, 9th graders will be looking at the many faces of cancer through a deep study in english, ethnic studies, biology, bioethnics, and math classes. they have been given the task of testing the genetic material of a family that has a history of colon cancer for the hnpcc gene. they will then be given the opportunity to be either a patient receiving these results who needs to ask the right questions, or the genetic counselor who will be delivering these results to various family members. this is where you come in!we are looking for community members to assess our students’ simulations of this conversation live in the patient rooms of our on campus health clinic. following the conversations they will be writing a formal lab result of their results. when: february 12th where: life academy health clinic (best entrance is on 34th street).times: students will be engaging in 15 minute role plays from 8:30-12:25. we will be scheduling students and would be happy to have you for any portion of those times slots. you can come for one 15 minute session, or stay for 4! please see the attached flyer and we are looking forward to having you support our students in this real world learning.thank you very much.sincerely,david gardner*please rsvp to david gardner at . i think those are sample datasetslike this would be all the data which is not muchmovie 467,523i am wondering where\\'s all that availabel u may be right these may be all the titles - it seems to be missing country of origin for director, actors and titles, but the rest seems to be all present imdb sells the full data set... same with rotten tomatoes, etc... you can ping limited amount of cases using their apis, but not get their full dumpwould anyone have experience saving a checkpoint model with just the relevant trained weights, ie. the minimal needed to restore the model.  i’m working on trying to package my model while trying to save space.  it’s currently 800megs.  that’s the output of `tf.train.saver()`.  i’m looking for a way that’s more efficient on model size.  any input is appreciated. thanks! hi i am using flink complex event processing and i heard that you had issues with out of order sensors?nlp solves the voynich manuscript language: was it a d&amp;d manual all along as suspected? ;)ancient hebrew. the original sourcebook language.whoa!hello! does anyone have any experience playing videos from aws using flask (with nginx and gunicorn)? i\\'m trying to play a .mp4 video that is saved in my static directory. i tried two methods: (1) a redirect to the video filepath in flask, and (2) rendering a html template with the video filepath (\"/static/video_filename.mp4\") as the source. any help would be appreciated... thanks!the government uses evidence? :thinking_face::grimacing:in all fairness , the fed is not really \"government\" as much as a hybrid bank-regulator in the financial sector... :wink:they have the best and biggest evidence!hello wise former-fellows — current fellow here: what do you think about job titles? as far as career growth, how critical/useful is having the “data scientist” title?  some of the companies are offering positions with other titles (research analyst, quantitative analyst, etc.), and i am wondering whether its something that should be given strong consideration when examining offers. thanks!i would also love to know what the wise former-fellows think about this. and also, do the different titles correspond to different wage brackets (assuming the roles are ~equivalent)?although it should depend mostly on the actual skills shown in your resume, i would rethink taking a \"data analyst\" position, as it has a different foundation and career trajectory than a \"data scientist\".piggy backing on what  said, i will suggest talking to the team about the role and what the day to day job entails to get more clarity. lot of established organizations has their own nomenclature for data-scientists, so don\\'t be disinterested just coz job-title doesn\\'t look that fancyi have been presenting in countless meetings where i had to explain performance metrics to non-technical people. i wrote a medium blog about this topic. please let me know if you have any comments or edits. i think some of the fellows in our cohort negotiated for a title changeyes, title change is totally on the table when talking about offers.i negotiated for an external-facing title change. outside of large tech areas i found that many organizations have their own terms. sometimes places would be happy to have a data analyst turn out to be a data scientist, but don\\'t know the skill set exists until you sell your skill set to them during the interview process.on the hiring side, a job title of data scientist generally gets more applicants than jobs where the work is the same, but the title is different. we have our own internal titles, but when i hire, i never use that title for the job ad. if i want more applicants, i put the job title as data scientist. i also don\\'t care much about previous job titles that applicant have held as long as their experience is strong. so, it could be that applying to jobs that aren\\'t titled data scientist could mean less competition. this is only my anecdotal experience though.i’ll agree with josh, but reiterate joao’s point: if they say “analyst”, they might really mean it is a different job than “data science”. see what they actually do day-to-day!when i’m browsing resumes i find the titles don’t make a lot of difference but they do makes some difference. i do glance over what they say they did in those roles and i do know typical alternative titles and titling quirks of some of the large employers. in fact i recently screened a resume that was pretty good but it had all “data science” titles going back to 2004. no one was using that title back then. when you look at his linkedin profile they were all non-ds titles. i felt it was too sketchy and you definitely don’t ever want to give a hiring manager a chance to question your honesty. though a colleague disagreed and thought it was ok given the work described was fairly data sciencey.you definitely should demand that your title reflect your work. if you are using substantial statistics on large projects or doing engineering you probably should not have analyst in your title.and of course the point about getting a lot of clarification on what the current role is going to entail is the most important thing. plenty of places will happily give you a data science title and then hand you a locked-down laptop with only excel.what **data** pipelines do people use for working on small amounts of data? something under 10gb where you can\\'t justify a map reduce library.if it fits into memory on a single machine, i\\'ve used the sklearn pipeline pretty successfully. if it\\'s larger than that, python/pandas/dask is a solid option.for sql lovers:  the query:  uploaded a file: following on to  sklearn-pandas () is good for stitching transforms together - i’ve found it more frequently useful than sklearn’s in-built pipelinesif you need to encode more complex etl tasks, i like luigiminimal boilerplate to build the task pipeline, and it actually scales decently well (mainly due to good integration with pyspark)i was more curious about what does the bulk of the transforms. spark\\'s nice, but the map reduce paradigm works well for larger datasets, it\\'s not really needed for datasets under 10gb.i\\'ve looked into luigi though, and it seems cool! i\\'m using an aws stack though so i\\'m looking at aws data pipelines, but they seem...unpopular in the search results i\\'ve gotten because they\\'re configured via json.pandas is cool! i didn\\'t know much about it on the de side, but it\\'s the best answer i\\'ve gotten that close to what i want. it kind of stinks that it\\'s only single core though.some of pandas’ functionality is vectorized under the hood via numpy (and through that some compiled c libs) that should run more efficientlyand then there’s dask for running pandas(ish) at the cluster leveli like luigi (another similar option is airflow) since it can run p much arbitrary python for its tasksi\\'ve used a containerized airflow deployment on a single box running with the localexecutor for stuff of this scale. i used this repo: can\\'t wait until this is fully up and running... anyone want to come have wine and hear a q/a from cofounder of stripe on friday at 4:30? at 405 howard street in sf.this is matar’s talk at strata this winter: starting with a spectrogram (looking at the signal in fourier space over time) and clustering based on derived features is a good startinsight alum  just published a killer paper on a new approach: hey, i am trying to get hold of a data set of marriage information. the data should include spouse names, the spouse birthday (day/month/year) and the marriage date (iff possible)i did a bit of related work in grad school, and spectral features are definitely quite good for speaker identificationmarriage records are held by the state, often by a department of vital records. finding a dataset that includes names doesn\\'t seem very likely to me, because you usually have to request records on a one-by-one basis. however, you can probably find aggregate, anonymous marriage/divorce datasets (but, again, on a state level). thanks. i was thinking that might be a limitation as wellyeah once you are down to names and birth dates that\\'s highly identity theftable pii if you\\'re okay with older records, census data i believe gets you name, birth date, and marriage status, so you can get within a ten year period of the marriage date by comparing censuses (censi?).  but the most recent records publicly available are from 1940 to address privacy concerns.,  thanks for the info about matar\\'s project! that is super useful to have as a reference pointold timey big datavintage data@limventa  thanks! yea, i was aware of her project based on a recommendation from someone who was in her cohort. :smiley: pretty cool methodology. this site has marriage license application data as well as divorce reports. it is limited to texas. it might be helpful:   thanks!i wish i had been wrong.   by no means an expert, but have some experience. feel free to dm.: thanks. i am wondering if domo has any advantages over de novo dashboards created in r or python.for silicon valley and san francisco members: a good data science conference in the area this year?anyone here done hackerx before? i got an invite and can\\'t tell if it\\'s a solid program. i\\'m happy where i\\'m at but always enjoy the opportunity to network.i see rwd groups at pharma moving this direction ^any airflow experts in here?   i am trying to to understand why when a task times-out  and fails it  does not call the on_failure _callbacksto piggyback on ^^, does anyone have guidance on best practices for writing tests for airflow pipelines? we’re having an internal debate about how much to use airflow-specific features like plugins, sensors, operators vs. keeping that code in separate libraries that are testable outside airflow context and just calling them with the python operator currently we add “check” operations before and after every task to test assumptions of the data before we land it in production tables. we’re starting work to basically write a testing library outside of airflow, and even starting to think of building an entire etl framework on top of airflow. airflow is really just a scheduler, not an etl framework, so it’s not optimized for these things.also, you might want to check out this: it doesn’t quite work with our infra right now, but could be a fit for youha. funny you should mention that. we had abe in yesterday to present to our team and had a good discussion with himi guess he’s on a roadshow! he’s coming here on monday :slightly_smiling_face:i use af, but haven’t encountered that, but try af mailing list, contributors there are quite responsive.not sure if this is relevant, in my team, my teammates prefer to do everything w/ operators, like we may have stuff like mysql to redshift operator, etc.  but i don’t feel personally strong in having dags only use operators, because a lot of the dag tasks are very one off sources or sinks that aren’t reused elsewhere.hey all, ai fellow  just published a cool post at the intersection of video games and machine learning, give him some love! hi all, if you\\'re interested in grace hopper 2018, the deadline to submit talks and workshops is next week! you can read more and apply here:  a few insight team members are working on submissions, so let us know if you\\'d like to collaborate, or if you want anyone to proofread your submission. we\\'re hoping to have an even bigger insight presence this year :slightly_smiling_face: you can also join the new channel  to share conference-related updates and eventsi’d like an emoji for “i enjoyed reading this discussion”this is great !  during insight i read an article on a/b testing that had a great plot on the dangers of early stopping, which showed the p value of an a/a test over time. anyone know what i\\'m talking about and know where to find it?maybe this one? this? i don\\'t think the air bnb one is the one i remembered but is perfect for my current needs. thanks to you both!*where may i buy a gpu desktop* (for deep learning , not gaming... of course) ? it has been 10 years since i did diy myself... i do not wish to spend my time on learning diy now. :sweat_smile: thanks maybe: figure out what it costs..then, see how many hours it will buy from paperspace cloud or gcloud or aws - spot instanceshey folks, thanks so much for the responses to the alumni compensation survey. we got about 50 responses so far, which allows for some splitting of the data among region, seniority, etc. the data is available here (with some prelim descriptive statistics on one of the worksheets): if you’re looking for a reason to procrastinate a bit on a monday morning, and haven’t filled out a survey, feel free to do so here:the survey is anonymous, and may help fellows figure out based on region and experience what kinds of compensation they should expect from employers, especially helpful during performance review/compensation adjustment periods.additionally, once enough data is collected, one can compare to other online compensation aggregators to measure the collective effect of “insight alum” on compensation.cheers!mike in future iterations can you please track info like gender/race (provided you can suitably anonymize it)?that\\'s a great idea, i\\'d be happy to.gwen.gillingham: for gender and ethnicity, what do you suggest for an anonymization strategy? i think this is important to include in the data set, but i think maybe the best way to include it is to make it optional?making it optional will make it less useful. maybe the simplest option is just to use a randomized response paradigm? there\\'s other options that have been used for generating differentially private datasets in medical fields...(but they mostly involve generating data after collection, rather than mitigating identifiability risks during collection)hi all! can anyone talk about “productionalizing” their ml model? more specific i have a sklearn random forest model that i would like to put into production. tips, tricks, tutorials anything that has helped. thanks!we pickle our models .. save it to s3 .. and save associated metadata in dynamodb .. and the production pipeline loads the models from s3 if the models are labeled ‘for prod’ in their dynamodb metadataand make sure your models pass a sorta smoke test before it is labeled as `for prod` (i.e. expected scores given a static set of data)i’ll have to see if google forms supports randomized responses. i understand the basic principal, thats a good idea that still gives the survey statistical relevance. i’d be interested in making the survey even better, so if you (or anyone else) have any additional feedback, i’ll look into ways to implement, or just add y’all as collaborators on the form itself. uploaded a file:  and commented: hello, i am looking for a machine learning/software engineer for a vc backed early stage startup in the bay area to help implement image processing algorithms for personalized virtual reality experiences. if you know of some one who will be interested in this space, please connect with me at . thanks!hi everyone, i’m doing some research and need a handful of volunteers.  this will take about 45 minutes of your time to video conference with me where i’ll ask you some of your opinions on some data science topics, then send you a followup short survey that takes about 5 minutes to complete.  then i will get you a `$100 amazon gift card` for your time :slightly_smiling_face:   only need a handful of people working as data scientists at any level, but who are actually in an industry position, so first come first serve.  please dm me!  thanks!hello ny: with cmj music festival coming up next week, i\\'d love to hear your suggestions about what bands are a good pick. i personally want to go see the henry millers as they are friends of mine and i like their music.we’re trying to sound more statistically sound...the non-science parts of funding agencies like p-values?that\\'s my guess. grant or journal reviewers started asking for them thinking they were useful.any other folks in media talking about facebook\\'s metric mishaps these days? and to a broader degree, how much reporting accuracy falls on data scientists? given that a majority of us come from academics where job number one of the scientist is accuracy. i think someone before was saying they wanted to retrieve old messages off of slack. i figured out that on the iphone app, you can retrieve the old messages without a premium account. it just doesn\\'t work off of desktopgiven that a majority of us come from academics where job number one of the scientist is ~accuracy~ getting results for funding.fixed that for you: someone posted this earlier:  i have not yet listened to any, but would like to. if anyone has some hand curated suggestions my gym time could use some data science podcasts!: the insight project i just finished is actually a podcast recommendation app. i can’t link to it at the moment since i need the site to survive demos, but here’s some recommendations (note i haven’t personally listened to these): friday lunchtime lectures, software engineering daily, uie brain sparks, ctrl+click:   thanks all! and good luck with your demos, lindsaycan anyone shed some light on why hadoop streaming would map and reduce my input perfectly when the input is in a single hdfs file, but the mappers output nothing when the input is split across multiple files?are the multiple files partial zips? raw text.  if i run my job on the output of a prior mr job it works finei wonder if it\\'s because your splitsize is set too small, so the raw texts got chopped up and not being mapped properly, but if i use more than one reducer and then run on that, no mapper outpuyin case anyone is or knows a software engineer who wants to make the next generation of tools for genetic analyses, we\\'re looking for someone at the broad (cambridge, ma): pretty recent post about netflix\\' data stack: : anyone else at spark summit east?no, but you should share cool things you see on here. :simple_smile:hey guys! anyone at scala days today?another analysis tool: .  seems interesting because of the potential for flexibility.  potentially not quite as useful for people who don\\'t code, but i haven\\'t looked into it much yet.: i have a friend who is finishing his phd and is looking to go into data science. but he is mainly interested in working in the public sector. would anyone happen to know of any opportunities, companies, or government agencies that are hiring data scientists? is awesome, and dc was looking for a civic hacker for a law fellowship- the free law innovation fellowship. bayes impact is a 12 month ds fellowship for social good: have also seen some good things about the data science for social good summer fellowship: maybe?  another maybe?  lots of opportunity to work in private sector (consultant), yet support the public sectorthanks for the suggestions, very helpfuli really don\\'t like the keyboard. otherwise, i\\'m happy with mineit\\'s thinner and lighter than the older macbook pros,  that\\'s mostly the pros. neutral - the new keyboard has much less travel, takes getting used to but once you get used to it, it\\'s fine. no complaints on the size of the touchpad. i have very mixed feelings about the touchbar, i think it\\'s mostly a gimmick, but sometimes, when there\\'s a button to click like save or install, it offers a quick alternative to using the mouse. cons- 4 usb-c ports are very limiting. the dongle apple sells isn\\'t great, e.g. the usb-c ethernet adapter we got only works at the port on the mac body, not through the dongle. each dongle supports only 1 monitor. 2 monitors, 1 port for charging and 1 for ethernet and all my ports are occupied. hopefully i don\\'t need to connect anything else :confused:. battery life isn\\'t as good as my older 4 yr old mbp. limited to 16gb ram, expensive as hell, especially at the top end. hey nyc folks, i\\'m sourcing projects from companies for a class i am teaching at columbia in the fall.  the class is project based where mbas and engineers will work together on an analytics project over a semester.  this is an awesome opportunity to get some work done (reasonably scoped of course), get connected with analytics and ds folks at columbia for recruiting, and possible get quality content for a blog.  message me if interested and i can mail you more details, thanks!!channel notifications for hundreds of people, the majority of whom are probably not in nyc? gutsy move, !h2o is awesome. sometimes, not really sure why people use anything else. i feel like it’s pretty much won this spacesorry for being so mia wendy et al. i\\'m in the process of moving and will try to get the strata slides online soon. there is a good github repo for mfccs. i can\\'t remember what it\\'s called, but i think i forked it (so you can see it on my github account). sorry for not being more helpful - i am not near a computer. any people interested in security/exploits seen this? insanely simple exploit for root access on many inexpensive arm devices.reminder that ethics and anonymizing data = a really important part of data science:(re the ok cupid data dump today)working on a federal grant with human subjects will teach you how important anonymization is. :slightly_smiling_face:they have some pretty strict rules about data storage.none of my datasets have any personal information, it\\'s possible to link them eventually through subject numbers and experimental logs, but then you need access to our database of subjects.we\\'re expanding the data engineering program to boston. really excited to see the insight community in boston grow. we\\'re already at over 100 insight alums in boston and now we\\'ll add a  data engineering contingent to that - applications open for april now woo! :party_parrot_shuffle:hi, has anybody used h2o with python wrapper on mac? i can’t get it to work. it keeps telling me i have to install java but i already did it.just a guess but is it asking for a specific version of java?i think  has some experience with this.thanks. somehow i solved the problem. i switched the default browser of jupyter notebook to safari and then manually activated java for safari. it might have helped.i’m not sure if it was asking for the exact version but i ended up installing the latest version.nice advice when updating your cv!   this person likes 10 page cvs apparently:smile:\"another good example is where the candidate explains why they chose to use one algorithm over another for a specific project.\" i guess he is assuming a several page cover letter?there is no way i can fit: \"built a recommendation engine using collaborative filtering in spark. we considered using content based filtering but we needed to cover a very wide variety of item types so there was no single good way to parameterize them all. we also considered writing our own big data processing code, but you know what that\\'s crazy. during this project i mentored two engineers who were new to recommender systems....\" on a cv for the 5 major proejects i\\'ve worked on. :wink:i am always skeptical when a single person extends their personal anecdotal experience in their title to the plurality of people like them.  want to make a claim about what hiring managerssss look for?  i’d love to see your data that goes beyond what you want :slightly_smiling_face:  also i think this is generally some horrible advice `but you should never not apply for a job if you think you don’t fit all of the criteria. `listed job criteria usually not representative of actual job requirements :smile:oh woops that’s a double negative in there, uh, so it’s good advice after all in that line :smile:yup :wink:also he does specifically say he loves 5 page cvs\"and candidates should not be afraid of having a cv that is longer than a page or two. i would be happy to read even up to 5 pages of useful and relevant details, especially if it allows me to move faster in the interview process\"does the uk not do phone screens?a lot of what he\\'s advising is stuff that you would want to cover in a chat, not typed out on a cv of death\" from experience i have found that combining the hypothesis driven approach with the kanban agile framework supports the lean startup framework really well” … :skull:look, if candidates write a 10 page cv that gets them rejected _everywhere else_, then he always gets first pick. it’s really a brilliant ploy....if your filtering criteria is \"does not understand the hiring market\"zero attrition!that company they use as an example of a current gen crm on page 202 must be awesome and everyone who works there must be really handsomeanyone using tableau? would love to discuss usage with python :slightly_smiling_face: thanks!hello everyone! has anybody had experience with tensorflow. i want to figure out if use it or keras for my problem (my mentor softly isists on tesorflow)you can use both: you can use keras with a tensorflow backend. in my experience, it’s much easier to get going with keras. but i think keras may not support all of tensorflow’s functionality.an, nice! so, i can you tensorflow without knowing much about itthe question is whether your mentor is insisting on tensorflow because it’s the best tool for the job or just because it’s a buzzwordi am not sure. he said that it is a lot going there. for example google put out some new algorithm or something like that.as a matter of fact i am not sure if neural networks are the best. mentor also wants to have insights from the dataif _he_ actually uses tensorflow, that would be a good sign. if it’s just something he’s heard about, i’d be worried.i see. thank you for the advice. i\\'ll ask him what is they use iton the subject of tensorflow: it is what everyone in deep learning is really excited about now. at gtc this year they had to move the tf tutorial to the ballroom so they could fit the o(1k) people that showed up for it. i think this excitement comes from two things: 1) google is really good at deep learning, so people assume tf must be awesome2) google has some of the biggest scale problems imaginable, hence people think (and i think there is evidence that) tf can scale to whatever size problem you have.i haven\\'t used it enough to say; we don\\'t have the scale issues that others have, and i don\\'t have enough experience with tf to answer the \"is awesome\" question. but our current deep learning project (that we just started and i\\'m working on) will be mostly tf.that said, if this is for your insight project, pick the thing you can develop fastest in!! :wink:there is integration with scikit learn that is supposed to make the interface simpler.  it was mentioned briefly at a workshop i attended but i have not try to use it myself.from my limited experience, you probably fine using keras with tf backend. the tutorial linked above is a good place to start.a note about the top 10 example that niharika, mudit and i discussed this morning. if you look at the program am listing around page 62 an 63, you will notice that the mapper class declarer a map, that maps a \"reputation\" (which is derived from the input record) to the entire record. as it keeps reading more input, the map keeps growing. whenever the size gets bigger than \\'k\\', the \"first_key\" is dropper. after all records are procssed by the mapper, the map contains \\'k\\' elements. now the \"cleanup() is the critical part of the process. it\\'s this \\'cleanup() that takes the map and emits a (key, record) so the reducer can combine all these inputs and pick out the top \\'k\\' elements. if you look at the program, the mapper class has two mthods, map() and cleanup(). presumably map() gets called every time for each input, but cleanup(0 is called after call input has been processed. the cleanup is doing what a local combner does. but it is not clear where cleanup() is getting called. it\\'s clearly not called explicitly from the mapper anywhere. it must be happening behind the scenes my feeling is cleanup() is being registered as the \\'combiner\\' method. but this needs clarification.i am looking for fun and interesting applications of probabilistic programming and bayesian inference like anomaly detection, inference of missing data, or making decisions that involve risk from limited or sparse data. at my company (fast forward labs - an r&amp;d company), we\\'re working on a report on probabilistic programming. if you use these approaches, i’d love to chat. :upside_down_face::slightly_smiling_face:: these posts might be relevant for you : nuggetbot animate me parsey mcparsefacethanks  the bayesian deep learning stuff is really cool, you can imagine all the applications one could build. :slightly_smiling_face: for now, i am curious about what people have built! anyone?i built it something for a toy problem, doubt it will be useful for your work :  i haven\\'t but was looking at this paper recently... ... keep us posted on this, i am def interested hi kevin, thank you for your response. i am trying to recommend if a given domain is a spam trap domain or not, by using the known spam trap domains. the feature is the name serve of the mx records of the spam trap domains. (we find out that spam traps domains tend to have the same name server.) i am in the process of collecting data now. we have ~9000 spam trap domains. from what i know so far, each spam trap domain has at leas two name serves.so you have the raw text of the name server and the raw text of the domain, as well as a list of spam trap / not spam trap labels for other domains?i\\'m just not familiar with the domain  at all unfortunately -- if anybody couldn’t make it to nips, we wrote up our thoughts about day 1: benjamin recht just laid down the hammer in his “test of time award” talk! yes, you are correct. i am trying to lean “how the internet works (dns records)” what about the data made the recommendation system seem like a good approach?this is a long read, but it’s very relevant to all scientists turned data scientists. includes a great quote from fernando pérez, creator of the jupyter notebook: “i have seen many talented colleagues leave academia in frustration over the last decade,” he wrote, “and i can’t think of a single one who wasn’t happier years later.” there\\'s...a lot that went a bit too underexplored in the article. the title\\'s def sensational. i think a more honest conclusion after reading this would be\"scientific papers need to be published in a medium that allows interactive integrations. the pdf is too static for the types of complex ideas that can be easier interacted with than explained. the pdf also can\\'t embed data necessary for scientific analysis.\"there\\'s a couple of problems. like...evident problems that you see right after reading the article. the notebook for gravitational waves is getting the \"hug of death\" right now from all the traffic. and if there\\'s a lot of data, as you\\'d need sometimes for modeling complex systems and for training in ai, then there might be problems just storing all of it. then...there\\'s just cultural, financial, and incentive problems in academia that might need to be overhauled before a system like this can thrive.the title is total clickbait--obviously we’re not going to ignore scientific papers--but i think the article demonstrated how research workflow in academia is archaic and could be so much more innovative. of course, an alternative approach will almost certainly not catch on any time soon in academic research--yet another reason why i’m glad i’m no longer part of it.maybe i should preface anything i post with “i don’t agree with 100% of this…but…”workflow is definitely outdated but i’m not sure how well thought out this article is.  for example, i think sharing raw data in a paper is a very tricky idea.  the naive idea to give it out doesn’t hold in many situations.  a lot of what matters is how data was collected for example - and a lot of what you should be doing as a scientist is interpreting raw data responsibly.  without literally writing a book, it would be impossible to describe how data that is difficult to get is collected so as not to confuse what a user playing with it may encounter.  what i think would happen is that it would flood many fields with false objections to quality work, and bog people down with addressing them, by essentially writing ‘this is precisely how the data was collected’ book to accompany each paper.  there’s something to be said for interactive figures and such in papers, but i often find the best figures are simple and striking - and don’t require interactivity or manipulation to understand the concept - it’s sort of an art to do this well.  and in that sense, interactivity could also be a distraction.  and of course, many people still do print stuff :slightly_smiling_face:  a very important thing in academia that isn’t as useful in industry is to surround yourself with pieces of your findings that you can consume simultaneously (e.g. a giant wall of printouts ) as you try to put together the various pieces.  like those detective shows with all those pin-up figures in the wall and pieces of string connecting them :smile:all this said, should definitely integrate new tools to improve old workflows - and people are slowly beginning to do that.crawling through \"a giant wall of printouts\" should have a software equivalent though. i think \"data gathering\" and documentation could be a separate issue, but there are other places like data.world and aws that can expose data like that. i think in psych data sets aren\\'t as useful, but for things like twitter it doesn\\'t matter as much. whatever comes of this should probably integrate data with a separate data hosts so that researchers could \"see\" data in the same way that a python programmer could import libraries instead of trying to package the data with the notebook.the “software” equivalent in industry i sa wall of monitorsso far anywayit’s pretty great, but also less portable and more expensive :smile:as far as documenting data, there’s cases where it’s easy and should be done and shared - and there’s others, where the data collection, because it’s done with some newly developed thing heled together by pieces of string, or is pushing old tools and methods to the limit of their capacity, can just be endless.so it can be done but probably at a big cost to productivity, and would not add muchin terms fo valuenah it\\'s necessary. papers just show their results. having data, even badly gathered undocumented data is at least an indicator that something could be wrong. having nothing but the results means someone has to replicate the experiment without documentation which is just as bad because sometimes descriptions and methods are too vague to re-implement.that’s definitely a problem.  but having data without the right details in the wild will bog down productivity to more of a crawl than it already is.  the reason is that people will make bad assumptions about how stuff was collected, and be mislead down wrong pathsthe best thing to do would be to consult the scientist and work with him / her in getting the data, telling them what you hope to accomplish, so they can tell you any relevant things about the data as they hand it to youas far as trusting the ‘results’ that’s more of an argument for improving the review processthat’s how i recycled the papers i read during bbq.hi insight peeps.  anyone know a good beginners reference for sizing a kafka cluster?  i am looking for something that works through an example problem.  thanks in advance.  has anyone switched from jupyter to zeppelin and been happy with it? i’m trying to figure out what the advantages of zeppelin are, but i’m coming up short. it seems to be missing a huge number of features compared to even a standard jupyter installation.no, we tried them out and went back to jupyter.this was from many months ago, but we found it to be very buggysame as above, checked it out, but for python it wasn\\'t an obvious win. i\\'ve seen people use it for monitoring etl jobs or other things more successfully.i\\'ve used it and liked it, but i haven\\'t used jupyter. i think one is more collab friendly than the other.i setup zeppelin on our hadoop cluster a few years ago and gave it a solid try to run adhoc spark code.  it worked alright, not sure if jupyter could be integrated with spark back then.  i like jupyter over zeppelin though.the sv-insight office is really cold after the party last night. anyone know where the thermostat is? (so far, 15 phds can\\'t find it)hvac is under central control. hope you can find some blankets!:disappointed: thanks!apologies for the off-topic question, but my family is traveling to boston next weekend and i was wondering if any boston-based folks can recommend a good babysitter? if so please pm me. try : thanks, will do!datakind is hosting an hackathon on nov’13/14th in sf.  check it out if you are interested - does anybody know of a good book/resource that covers a/b/n + multivariate testing?i like slater stich’s series (it’s biased towards bayesian methods, admittedly) has anyone set up ds pipelines in aws? specifically a pipeline with a daily retrain/score task and then a persistent api for retrieving predictions. bonus points if you had a probabilistic model in there somewhere.i\\'ve worked with lambdas to do data collection, storage data in s3/rds, daily retraining of models in ec2/sagemaker, and serving of predictions with rds&amp;api gateway/sagemaker endpoints, happy to help with any one of those points if needed :slightly_smiling_face:thanks  will check it out!if anyone\\'s looking to sublet a room, a friend is renting in fidi:  - $525 worth books for $15!i have an onsite interview coming up on monday next week and i could really use some advice for interview preparation. it\\'s a data scientist position at travelers insurance. i\\'ll be speaking with seven people, six of who work in predictive analytics and/or dev. the other is hr. i\\'ll speak with each person for 30 mins. i really want this job! so i have to ace it. any advice on types of questions i should prepare for and/or resources would be great. thanks!:  my biggest advice is to not put so much pressure on yourself: before having your on-site, there\\'s really no way to know that you really want to work there. i know you\\'ve heard all about \"cultural fit\" from the program directors (and that it seems like a lot of b.s.), but it\\'s actually pretty true.  to be honest, you\\'ll never know if the company is right for you before your on-site.  at this point you\\'re probably ready for any question they might have, but that might not be so when you bring a lot of self-generated pressure to the interview.  so, as hard as this \"advice\" might seem, the best one i can give you is to \"relax\", and go a little easier on yourself.  :wink:`chill`stay positive  good luck  !my friends are now hard launching their funding campaign for their educational non-profit startup. if you are interested in the cause, you can definitely donate. but another way you can help is upvote online and spread the word. thanks! twdoes anyone have a good reference for the starting salary of a data scientist at a very small start up?  all the numbers that i see online are cumulative over all company sizes and i would assume that a small start up would offer less than this; but how low is unreasonable?: did you look at glassdoor? this is close to the numbers i have heard: there’s also the o’reilly data science salary survey: with glassdoor, can you sort by company size?  the o\\'reilly survey does have salary by company size but is again cumulative over location.  i know that salary for data science is quite a bit lower in other parts of the country....insight it self may have some of the best information on this  i would also keep in mind the cost of living, etc. absolute numbers mean little on their own. 100k in nyc wont take you as far as 50k in san juan, pr. just saying...:  glassdoor, gives some minimal sorting options, by popularity and more recent updates. also, glass door gives you the median pay by the nation and by city. : you might look at angellist. you can filter by location. although, there are some reasons the data might be biasedangellist is the first thing that came to mind. and i work at glassdoor!company size is one thing that glassdoor doesn’t have great data on.oh, correction: apparently mr. salary estimator at gd tells me he *is* using company size now. i can tell you this: it is the weakest indicator of base pay out of everything we look at. maybe a little surprising. that is interesting.  do you mean if you are looking at all data or is it more predictive if you are looking at specific data (e.g. data scientist in sf)?not specific to data scientists unfortunately.we do have numbers specific to data scientists… that i’m not allowed to share. but you won’t find anything revolutionary.insight should have summary statistics on this, reallyyeah, insight should have some very good data on this.it\\'d probably be to their benefit to share aggregate data with the fellowsi found it hard to negotiate coming out of academiasome random thoughts: you do have to think pretty strongly about totally pay. there are a lot of complicated issues around it. i’m sure insight gives every session some instruction in this regard.on one hand signing bonus and options are often very negotiable. people compare salaries much more often than bonus structures (in part because it can be complicated) and in fact the comp expert in hr (if the company is big enough) is doing this very carefully once or twice a year. in a reasonably fair company salaries will tend to normalize but the hiring bonus stuff is a one-time thing.on the other hand, compensation in the form of options is a huge gamble without some very solid understanding of the company, its industry, and all the annoying financial quirks of it (amt, 90 day limit on vesting after separation, dilution risks, etc).true: are there any resources for us (current fellows) to learn about options and how to assess those opportunities during the negotiation process?we had one session where a mentor talked a bit about how that stuff works, but i’d love to dig into some more information if you can point toward any good sources useful tools in the field of bigdata: quora can be have some decent info: the tldr: “… this is why i value pre-ipo stock options at $0. …” (maybe a little extreme, not much)very interesting, thanks for the reference has anyone used the python difflib module?? is there anything similar?: similar to difflib in what way? i’ve been using fuzzywuzzy which has difflib as a dependency.having issues opening ports on aws ec2.  security group is set to open ports 80, 22, 5000 but port scanning shows only 22 is open for ssh.  does anyone know how to go about fixing this?all ports are opened to anywhere: have you checked both the inbound and outbound ports?: so the security group for the instance has the desired inbound ports opened.  outbound is open to all trafficodd, are you sure that you’ve put that instance with the correct security group?me tooi was planning to start down this path myself with ch. 13 of bishop:  they are very popular in comp bio, speech recognition, etc.these papers are very good overviewnow for almost all about them this book is good but it\\'s completely focused on math/algo\\'s but no implementation examples... hey everybody!  does anyone know a good place to start to learn coding for someone that would like to eventually be a software engineer, but has zero experience coding?there an `hmmlearn` python package, but i haven\\'t played with it yet.the wikipedia page is actually really good.the examples are easily understood and there is even example python code for the verterbi algorithm thanks to  for the pointers!there\\'s an outstanding mooc on probabilistic graphical models (of which hmms are an example) by daphne koller on coursera. i think hmms are discussed in week 2.+1 for daphne koller\\'s coursera classthe textbook that was used in my stochastic processes class in college was *“introduction to probability models, tenth edition” by ross*. it discusses markov chains, hmms, the poisson process, queueing theory, etc. i would recommend it as reference material even if you don’t do well self-studying out of textbooks. the bdt (xgboost) -&gt; logistic regression approach is used quite a bit here at li (an internal library does this for us automatically), and it works really well since you get the best of both worldshere is a useful reference: thank you for the paper carlos!i\\'ve been trying out the method you mentioned since you suggested it.i\\'ve also been exploring using something called shap values for individual feature importance and that for interactions. have any of you used these?hi everyone, i created the  channel to connect with other fellows who are either interested or are already working in sports analyticspandas docs has some interesting results as well i usually suggest that people brand new to coding check out , which is cute and well scaffolded.that looks awesome , thanks!i use it with hourly granularity over a few years of data for a production task. it isn’t fast, might require 10-20 minutes of running even if you don’t want seasonal uncertainties, but it is very solid. it is really just a wrapper around  which preps the data and compiles an appropriate stan model.is anyone familiar with nlp and willing to discuss with me about good nlp related projects in e-commerce industry ? i’m going an onsite interview for a nlp research scientist position and asked to give a 20 minutes presentation on a research proposal. i’m a complete beginner of nlp and need some suggestions from experts !thanks.i have not used shap. looking through the docs, it seems interesting for interpretability, but not sure it\\'s useful for large-scale feature selectionas a massive disney fan i like this one: i\\'ve done both ubuntu and windows 10. ds work was easier in ubuntu. communication with rest of company varies as others have saidif the e-commerce has a review system, you could use nlp to detect fraudulent reviewshahaha... hehehe.... haha.... no:arrow_up: (for anyone lucky enough to have used root)nuggetbot: weather antarcticaweather for antarcticacurrently: overcast -28.5°c/-19°ftoday: light snow (under 1 cm.) tomorrow night.coming week: snow (7–15 cm.) throughout the week, with temperatures peaking at -10°c/14°f on saturday.a free webinar titled keys to data strategy: online seminar this thursday at 11am pdt hosted by svds it\\'s aimed at business leaders, product managers, marketing leaders and senior architects -- so if you know people who would be interested please pass this info along.   that’s surprising to me, but interesting!super interestingyeah, i imagine many of us had our start theredoes anyone have experience using this:  ?  or built their own data and model management system? well, not a rumor anymore! for bay area folks: a team of graduate students at berkeley are starting to organize the 5th annual cdips data science workshop (), which runs july 15-august 5 and they are looking for mentors. from one of the organizers:\"the workshop is designed for berkeley grad students and postdocs to get a taste of what it\\'s like to work on data science projects. the core of the program is the mentorship aspect - 3-4 students/postdocs are paired with an industry mentor who devises a project. if you have a project that could use a few eyes on it, or are just interested in exploring a particular topic, you might find this useful. the only fixed commitment as a mentor is joining us for a few hours for the opening and closing ceremonies; we do ask that you check in with your team twice a week, but the timing is flexible. if you\\'d like any further details, please let me (diya das &lt;diyadas@berkeley.edu&gt;) know - i\\'d love to chat more. you can also reach all of the organizers at .\": i changed my project to predict what songs/music genres are best suited for health recovery (mood or emotions change). would you know any psychological site that connects or labels emotiond with health. thxpublic service announcement to people writing their resumes: it\\'s ipython not ipython (it\\'s not an apple product :simple_smile: ) source: you make one of my favorite editing points on resumes,  !  to current fellows in both sv + nyc -- if in doubt, please look at the language/tool website to see how the name is (or _is not_) capitalized. e.g. ipython, scikit-learn, pandas, git, python, numpy, latex, etc.  feel free to direct message me if you have any questions.i once got a complement on my resume because i don’t flagrantly assume c and c++ are basically the same, a less subtle distinction than ipython vs ipython but i think common.  the facts of a resume are easy, the really tough task is instilling confidence that you are trustworthy and have sophistication in your field.can someone help me understand what a typical ai stack should look like? or any literature that can help me understand on a high level? haha, so true...welp, boss approved the cost, but now registration is closed :cry: looks like it\\'ll be back in the fall though? i\\'ll try to watch for that registration to open! pinned a message to this channel.this is too realdarn! yeah, i think it\\'s bi-annualohh common! that’s completely self-inflicted by using anaconda and easy_install :smile:i’d like to add the eleven virtual environments i’ve set up in various places with uninformative names and conflicting dependenciesjust created  for attendees later this week. we can use it to coordinate meeting upi learned about pipenv during one of my interviews. it’s become my go-to for environment stuff: i’ve been totally happy with the python built in virtual environment `python3 -m venv my_venv` …works great. i think it’s way easier to keep track of than all the anaconda path stuffam i the only one that is happy with exclusively using conda environments? i have a dozen or so, but they’re all named well, no conflicts. just...stick with one method?just out of curiosity - has anyone tried any of these? nuggetbot: animate me welcome newbiesthanks  ! what you forgot to mention is that it\\'s free!hey everyone.  i have a 2bd 2ba in mountain view and i\\'m looking for a roommate.  gimme a msg if you\\'re interested!hi fellow fellows: if you know anyone looking for a room for either short term (feb) or longer term, my room is up for rent.  place is really nice and 5 min walk from f/g: good morning all!  does anyone know of a clean way to incorporate a python model into a sql database?  basically, all our data is stored in sql databases, and we want to streamline a process that appends the model output as a new column in the database. any reason you wouldn\\'t create a new table with the predictions and be able to link by id? might avoid conflicts if you re-run the predictions.  but you could use alter table to add a column, and use update to change the values within the row. great suggestion to improve the structure, thanks!  the challenge here is really that we want an automated process to run new data through the model as it comes in - so how to incorporate the model component.  i can see it working using a cron job to run the python script outside of sql, but i was hoping there was a nicer way, especially since i\\'m the only python programmer at my company, but we have a few people who know sql. so do you basically want to generate a model prediction as each new data row comes in in real time?  i think you may be able to implement that at the layer that generates the data row, e.g. if it’s on a website, call the model from there (e.g. if you put it behind some type of api you can call).  but i think if there’s a lot of data rows coming in and its not critical to do in real time, using a cron job to do it in batches and during less busy periods is probably ideal.  we use rundeck for scheduling these types of jobs, hi everyone. does anyone have experience analyzing bed-sensors and/or acti-watch (acti-wear) sensors for sleep staging? it doesn\\'t have to be in real-time.  really the problem is the data warehouse guys do everything in sql, so they want the model in sql as well, and i\\'ve gotta convince them that hard coding all the rules for a random forest into sql is a bad idea (note: doing the math for him on how long that would take and how inflexible that would be did not work).  so if the general consensus is to run it in python with a cron job, as there\\'s no easy way to run the model in sql, i can use that to support an argument for leaving it in python.: i\\'ve done some work with sleep analysis from wearables.  feel free to pm me.ah i see.  that’s interesting, i wonder how hard it would be to store the trained rf as some sort of sql structure.  however, yes, i think it’s a bad idea in practicei’m quite surprised someone is asking you to do that hahathat’s why i didn’t even realize what you were askingyeah, it\\'s much more a business company than an analysis-heavy company at this pointyou can see how it sounds easy from an outside perspective, just switch from python to sql.  i figured i\\'d see if anyone in the community knew of something i was missingyeah, makes sense, but i think if you give in to this type of ask, it will be more difficult in the future to deal wiht your model , e.g. refitting it, and things like that, i bet even if you come up with some sort of sql solution in  a sort of ‘minecraft emulator’ sort of way it would not be easy to manage some db (redshift for sure) support python-based udf (user defined function). have you looked into those? no, that\\'s a great idea, thanks, i\\'ll check them outthat’s neat, i hadn’t seen that beforehi everyone!  i\\'d like to do some crowdsourcing. can somebody please recommend a simple tool? something besides amazon mturk (i\\'d like to avoid php if possible)does anyone have any experience with chunking in natural language processing?i don’t have personal experience, but i think `spacy` has an inbuilt chunkersee:  thanks!this might also be helpful: another nlp question from a newbie: i\\'m trying to build a corpora from reddit posts (using praw), and ultimately apply lda or lsa for classification. for example, i\\'m focusing on extracting titles from posts on reddit. to build a corpus, would it be useful to write each title out into a text file, or append each title into a single string, then write it out, or something else?here\\'s some sample code i have `# create a for loop that iterates over a subreddit, excludes posts made by the automoderator,# and appends titles to a stringfor submission in reddit.subreddit(\\'anxiety\\').hot(limit=100):    if submission.author != \\'automoderator\\':        corpus_titles += submission.title        print corpus_titles` uploaded a file:  and commented: better code snippet.really depends what you want to do with it after, but in general the more general your data storage is the betteri.e. write it out in a way that you can further process itmy understanding of praw is that each title comes in 1 by 1 so saving it out individually makes sense in case of failure is a much better way to get reddit data out -- the reddit api is capped at 1 request per second, and can only return out to 1000 results due to how their data storage is structuredso scraping a significant amount of data would be very very slowpushshift has archives of praw outputs as monthly dumpsyou can also download a reddit dump from bigquery directly, i forgot about that. that’s what i did for my project.iirc to build the initial archive up to like fall 2015 required his scraper to be running continuously for like 9 monthsthanks for the advice  and ! you can pass python data structures around via pickle over a network - in which case this is just a bytestream. i bet its possible to store this bytestream in a database, but i have no idea how that would scale, and it introduces the complexity of having to track what these collections of bytes are when they are unpickled, in addition to ensuring that all necessary libraries to access this byte-stream’s attributes are installed everywhere the pickled object needs to be used.does anyone know of any semi-supervised clustering algorithms for network data that are usable through either python or r? i have only found some packages in java for weighted-vote relational network models or unsupervised community detection algorithms. or am i stuck with importing java through some java-to-python package? uploaded a file: what dis, ?an example of how one can copy/paste a plot from a jupyter notebook (the jupyter notebook talk was tonight)by the way here is the link to the repo from tonight: if i could not copy paste plots from jupyter into slack i think my company would fall apartat razorfish we often took the sign nda, get data dump route. for us that meant we’d spin up a cloud environment for each client engagement based on need, proposing aws/gcp/azure based on the specific case, and we’d have a somewhat cookie cutter approach to each use case. part of the contract was that those providers guaranteed some security standard and the client would transfer the data to that cloud env, which was separate for each client.  they paid the cloud costs as part of the engagement, and therefore they still “owned” that storage etc, so it never really moved to something they didn’t own. pros were that we could iterate faster because we could customize our env and resources, which sometimes you can’t do entirely on premise because they’ll give you some small vm that’s insanely slow and you have to keep asking for changes. also you can develop some expertise in one env so you aren’t learning something new for each client. cons include that set up takes time, and they’ll also usually give you prototype datasets initially, and sometimes you run into walls looking for the most new/correct data or just build models on data that you later find is incomplete. probably that happens anyway though and you just need good communication w the client all the timeanyone going to be in seattle for data day?hey all,  , i have 2 tickets for red bulls vs nycfc @nj this sunday that i’m trying to offload. anyone interested?does anyone have any suggestions or tools that you like for  automating data analysis on data from  google analytics? so far, i\\'ve been using the rgoogleanalytics package from r. hi asli! sounds like it might a really useful question — what sort of analysis are you trying to automate?for those interested: : pandas has google analytics api integration:  have not used it though, but thought i\\'d give you more options!hi . i  am trying to automate user segmentation reports. thanks . i wasn\\'t aware that world bank data was supported for remote  access. hey folks, do you have any recommendations for data science/data engineering meetup groups in the silicon valley area? too many to choose from :thinking_face:that\\'s a lot of people...any one know when the east river swim initiation is for ds-nyc-2016c ? the trick is where the source is… sourcing from.  for example, lending club produces a free dump of issued loan data, including fico scores and localization data (at least to the state level) but it’s biased (naturally) to people who are applying for personal loans, which is emphatically not representative of the overall population.  most other groups will have similar issues, as they can only report on their own users.  the best source for representative populations would be the credit reporting agencies themselves, but they’ve been quite protective of that data in my experience.ibuder: integration with whatever platforms you’ll be testing on (web, native apps, email, …)? how attribution for various success measures is done (how close can you tie a success to a testing scenario)? are you only testing ‘users’ or could you potentially split on other entities? does it tie together logged-in and logged-out sessions?does anyone have a hacker-rank type tool to send out data science screens?we\\'re looking for something that\\'s more like a tool to screen for data frames + plotting rather than cs fundamentals let me see which of the internal stuff i have i can dish out. in the meanwhile do you want more technical stuff or just a general audience overview thanks! technical reading would be super appreciated!we used swrve at various points at my last job, and it was okay, but definitely built for non-experts. to be honest, it might be better to build a platform in-househi all, a good friend of mine/colleague has to move to the west coast for personal reasons, are any of your teams looking for a data engineer in the south bay (california?) thanks!this is a good point, thanks for reminding me of these privacy concerns. we only have mobile device  ip adresses, which i think is fine in the us.anyone have experience with building ml models on full-homomorphically encrypted data?  we used monetate, optimizely, and an in-house platform at various points during my time at bonobos.optimizely seemed the best of those options from my perspective, allowing the raw allocations to be written back to our tracking backend () so we could do our own analyses when their tools fell short and the team having enough statistical sophistication to not be doing things obviously wrongthanks! this helpssomething to keep in mind is that in our experience nearly half the experiments end up requiring some sort of ad hoc analysis. either to figure out what was broken or get some sort of explanation of the result. so you really need the allocations easily available alongside all your other data.  our analysis platform was mostly created in-house, the allocation system is a java port of facebook’s planout. this is for search algorithm and somewhat complex ux changes. for simpler, try different creatives in a marketing funnel type stuff, we have also used off the shelf stuff.does anyone know what kind of (emergency) us healthcare insurance options exist for a canadian who is considered resident of us for tax purposes (on j1 visa)?i know a little. j1 visa holder (excluding full time students) can buy the emergency (bare minimum coverage) of obamacare or whatever providers. but they are not required to get health insurance until they are considered resident of the us for tax purposebut, genrally, the us won’t issue you the j1 visa from the first place if you didn’t show that you’d have health care coverage at the time you apply for the visahowever, the visa allows you to enter. so it does happen that someone had health insurance, say, though employer/school when entering the us. but then lose coverage for whatever reason. at that point, you are not breaking immigration law. and if you’ve been in the us for less than…i think2 years…you’re also not breaking the law which mandates everyone to have some types of coverage or you’ll get a penalty in your taxthanks  !!hey guys, check out this kaggle competition that was put together in my team at mskcc work on cancer and win $$ : if you want to know more about the connection between deep learning and renormalisation, there is a much more technical (but also more precise) description here (with some nice references within):  insight is continuing to expand, and we\\'re looking for a great program coordinator to join the new york city office.if you know someone that would make a great fit with the insight team, and would work well in the role, have them submit their resume to . also, if there\\'s someone in particular that you\\'d like to recommend, please send me () a quick email. many thanks! :blush:: fantastic, thanks!: you\\'re welcome!does anyone have significant experience doing data science in java? i\\'m a python/r guy but will need to build some analytics functionality in java production code. are there tutorials/books/java libraries that folks would recommend?  i deployed some java models in production and a few months prior to that, i thought learning java is going to take time. it turned out moving from python to java isn\\'t that bad. this is a good tutorial: but what really got me started were:1. reading through some production code and make it some minor changes (and of course with pr)2. having someone available for asking question. does not need to be a java expert necessarily3. a good ide. this is as important the other two. i recommend intellij. there is a free community version which does the job for a ds projectthis book ain’t bad to help better appreciate the mindset, constructs and design of javadoes anyone have recommended reading around centralized data science teams vs embedded in products? thanks in advance!i would  be super interested in it as welljust to let you all know, i\\'ve sent an invite (to receive an invitation) to the current insight nyc fellowswe could have channels for different sessions to partition the conversations... yes, i said \"partition\":clap:well done andrew!haha, thank you leifur !(my coworkers were criticizing me for not knowing enough emoji ..)i would like to invite all fellows soonall. the. fellows.let\\'s allow the conversations to build up, then perform nlp to identify popular conversation topics. that sounds like a data sciencey thing to do right?or we can just start with a #beer channel and see where things go from there.dvats: you on that?,  excellent idea!! :simple_smile:we\\'ll do supervised topic modeling for now: i\\'ll be attending these workshops in nyc, may be useful for others:mining the quantified self :  does anyone know of a service that provides financial text data (letters to shareholders, investor presentations, etc)? you might want to ping  - he’s done work in the area. i can provide you with an email by pm if that’s helpful; not sure how frequently he’s on here.hi  yes an email address would be great, thanks! i\\'ll try him here too.yep  is the right person to ask, as far as i knowfrom training a model to predict gender on twitter from tweets: men talk about wrestling, women use :heart:.men love to nerd out on pro wrestling :wink: also,  new avatar is amazing  and bayesian lovers - can anyone advise a good accessible book on bayesian networks? there\\'s some material on it in koller and friedman \"probabilistic graphical models\". daphne koller also has a coursera course on pgms.there\\'s also barber \"bayesian reasoning and machine learning\"didn\\'t someone build an app for finding the right textbook? :wink: thanks   - book in dropbox. i still believe in well-calibrated human advice, this is not like some cooking where you can just find any recipe online :wink:semi-supervised learningmy project used rnns also (for video analysis). i found the keras wrapper for tensorflow to be a lifesaver. also, this site might help: but message me if there’s any questions i can (try to) answer!what\\'s everyone\\'s opinion on the best python package for live dashboard implementation?i haven\\'t had a chance to explore this, so i\\'m wondering as well. i heard some good things about superset. i\\'m curious how plotly dash stacks upsuperset was a little clunky in my experienceeasy enough to get it set up thoughi tried out metabase as wellsuperset is nice! fairly easy to get up and running.still nascent so lacks a ton of dashboard options, but it\\'s getting therebootstrap has very nice dashboard templates (). after that, plotly is very convenient for the content.i\\'m sure your imagination was better :simple_smile:we\\'ll do our best jboysen0 !has anybody (in nyc) done pyspark training they would recommend?i have permission but policy says i can only install “approved” stuff.  i tried to go through proper channels at my company to install anaconda, r and rstudio.  i was told no one ever asked for that before.  (!!!???). then i talked to other ds folks who just did it anyway.  i followed suit and approved software list eventually caught up with us.  :grinning:before that i worked at a financial company where admins refused to install impyla on server even though it satisfied all their security requirements.  so i had to make system calls to manipulate hive in python.  painful.   not in nyc but i liked spark moocs on edx developed by databricks.  they teach pyspark via jupyter notebooks in vms.nice to hear maybe the ds people have the power to affect the policies over timewe had better change the policies, otherwise we\\'re not doing our job :slightly_smiling_face:who is the best person to send a referral to for insight?an insight employee should comment on whether this is up-to-date, but i’ve used this in the past:   not answers to the question, just tagging folks who might be able to give an answer. to what program would you like to refer someone?probably health data science, and regular insight. both would be good fits(data science)i checked with our admissions manager - the link above is the correct one! thanks for the referral!what do people use to make dashboards in python?i\\'m looking at:but it\\'d be great to know if there are other good options as well.dash is awesomeplotly dash. also look into superset by airbnb i’ve used metabase and superset, plus bokeh in an in-house ember app by one of our engineerswe ended up going with the in-house for access-control ease because like 30% of my job is fighting our cto over data accessthat said bokeh is pretty easy to get up to speed with, and you can make some cool shit with iti found metabase to be a little easier to work with than supersetlooked into dash for the project as well, it looked cool but i haven’t done much of anything with itplotly dash in my opinion by far the bestthat link is pretty good to get started since it has lots of examplesand exercisesi will say dash seems way easier to work with at the “dashboard” stage rather than just visualizations themselves compared to bokeh as wellthis is what it’s looking like… i think we were more tightly bottlenecked on the i/o due to the bug, which let the gpu have more idle time, and the slowdowns started stacking on top of each other. we’re seeing decent returns on speed by tuning our i/o parameters.now we play the game. who @ channels general first. new fellow or  :smile:do you use free, or commercial version? how do you share dashboards internally where you need to limit access? yeah, you can have grayscale pngs with alpha as well. i used to use them a lot before coming to insight. i\\'m really digging this silly webapp: would\\'ve been a fun insight projecthello! i\\'m looking to overlay a kernel density estimate output, plus some basic lat, lon scatter points onto a street map. anyone with experience in this? i\\'m handling my data and generating the kde in python.: i did something similar on googlemap, but i have to use heatmap to do it, which is not very accurate representation of 2d distribution. i am not sure what you can do with openstreetmap. most of our interns at viacom tend to be undergrads and we are always looking. we just finalized our spring interns so the summer program will start recruitment events soon. the company page should have all the info , let me know if you need something specific and i would be glad to help our unit at mskcc accepts grad and undergrad interns - in the two years ive been here ive seen math, pre-med, econ and or folks come through and do some really cool stuff. its a great way to see how tech and data science can impact healthcare.let me know if you want to be connected to the person who manages the programany insight fellows playing overwatch on ps4? i just got it and am looking for friends - my psn name is avicura :slightly_smiling_face:i am also looking for friendsin general:thinking_face:stay positive alex_rheebut i am stephaniebut i am:sakamoto:: generally i think its best to start with the simplest and most interpretable model that will solve the problem.  be sure to understand how to implement and interpret any results you will get in addition to assumptions and limitations to the model.  if the interviewer wants you to provide more complex models they should make that clear in their feedback in the interview.  be ready to explain how you would choose the best model to solve the questions..as  points out there may be tradeoffs between accuracy and speed/interpretabilityhey data scientists! how does one determine the threshold for winsorization? any papers you can point me to? :smile:does anyone know how to write files while in a flask app? i am trying to create a png image from a third party api based on user input, and then displaying said png image.any body using d3.jsuber news: : ton of useful stuff on d3.js: .  the modules c3 and nvd3 listed therein are particularly nice, easier to use for simple graphs.fantastic collection of data science ipython notebooks: jure leskovec\\'s team has been advocating large memory machines over distributing over a cluster.  the snap project ought to have some benchmarks for this.hi all, does anyone have code or suggestions on how to convert a json with a grouping variable to a data frame in r? if anyone is curious, you do this by extracting the json data and using \"unnest\" from the tidyr package here is a good example of solving expectation value questions with recursion: has anyone worked with api consumers in python? i am looking for something like  which lets me define schema in sqlalchemy? any suggestions?@newbies: welcome to the world of data science!welcome, [ds,de]-[ny,sv]-2015c!hey sv guys, can\\'t wait to meet you at the party this friday!the shit list: stay positive sebastien_dery: i\\'m all good, but thankspanama papers go live tomorrow: thought i would share:so, i have an acquaintance (fellow phd in physics) who is looking for ds jobs anywhere in the us and asked me to let him know \"if i hear of anyone hiring\".  if i don\\'t know of anyone close to me actively hiring, any tips on how to respond?  it\\'s such a general request i don\\'t even know where to start...  i guess tell him about insight :slightly_smiling_face:we\\'re hiring at samba tv in sf!but, i guess, does it somehow help him if i send him your way?  i just feel like he could find plenty of postings online (i imagine you have an ad out?)the way i’ve done this in the past, and it’s been moderately successful, is:1. use the insight network to find people i know at companies.2. ask them if i can make an intro to my friend.3. use that to set up an informational interview with the goal of finding out about the position and job in detail, and getting a warm intro to it (either through the employee referral tool, manager intro, whatever)very cool, i really like that workflowand maybe i can talk to him first to narrow down what he\\'s looking for? i mean, i don\\'t want to spam all 1,549 of us here...that’s always a good start! i normally start with step 0 of “hey, i know people at x, y, and z. what companies are you most interested in and what sort of ds track are you looking for?”i’m partial to the “introduce them to insight” route… -- i get that a lot too, and i recommend reading our 6 white papers: ds / de / ai / hd / devops / pm to try to think more about what they want to do in their day to day and where to do it. doing a job search in all locations for all kinds of roles isn\\'t usually very effective.awesome, thanks kathy!there’s an ai backlash underway for sure. even henry kissinger is getting in on it in this month’s atlantic. thought this piece actually cut through the hype the best. quick question: for someone moving to the bay area, who is interested in staying in touch with other techies/broadening skills/hackathons, can anyone advise how to find those and where the best ones typically are? are they more in sf or sj? any advice about getting the most out of the area is welcome! i very much appreciate direct refs from insight alums!anyone working in city infrastructure/smart cities? this is from a friend:```i’m on the steering committee for a leadership training program called the water leadership institute. we’re planning a panel (webcast) on smart cities and topics related to how data impacts the water industry in cities. i wonder if you have anyone in your network that might be in thinking about data as it relates to city infrastructure - specifically water and wastewater but not necessarily so. a lot of engineers are really new to this topic and could really benefit from learning about the potential of big data in our industry. let me know if anyone or anything comes to mind!```?ooh, i’d hesitate at that use of informational interview…would consider informational interview more as ‘interested in learning more about this field/your career/general insights/etc.’ and would much rather be told straight up ‘friend of mine is interested in job opening at your company, do you mind talking to him/her and, if good fit, referring’.also, would def want a resume sent to me with the request!i would say it\\'s evenly split, with probably more hackathons in sf. plus sf has that density thing going on, everything is blocks away. i would hop on , type in all my specific interests, and just start attending as many meetups as possible... that would definitely open the door to hackathons and other networking opportunities.for those of you who live in the bay area, and climb and/or drink alcohol: i propose thursdays at dogpatch boulders followed by drinking nearby.  the bouldering gym is very close to the 22nd st. caltrain station and several half-decent bars.  i\\'m going to attempt to get to the gym between 6:30-7:30 typically, and to a bar by 9:30 or 10pm.  for a bar, this week, magnolia brewery (pretty sure that\\'s the name), about a hundred feet up from the bouldering gym.  perhaps i\\'ll send out an email too, given the low density of people on slack at the moment...fellow fellows! i\\'m trying to get my jupyter notebooks to render on a github enterprise account and having no luck.  the internet is giving me mixed feedback on whether gh enterprise supports notebooks.  do any of you know if/how to get notebooks to render? i’ve encountered this problem before - i believe that this is an add-on feature in gh enterprise and needs to be configured by an administrator. a quick workaround is to save your notebooks as markdown files, which will render in gh enterprise.thanks !i am wondering if anyone has thoughts on the best way to build a model in the following situation. its a regression problem but the predictors are all 0/1 categorical and the matrix is very sparse, but we want interpretable coefficients (e.g. this variable is contributing this much when others are held constant, so no deep learning fun... etc) and preferably doing some predictor selection (e.g. these predictors are contributing the most). linear model with l1 penalty is not performing well --- its not doing any predictor selection in this case --- we suspect the matrix is so sparse that the coefficients are super unstable.sounds like an interesting problem — what’s the context? site browsing data or ad exposure data?you can try some type of leave-one-out method and see how removing predictors affects the accuracy of the model.  it’s sort of different and aimed at prediction so may not be applicable.you might try an elastic net regression if your solution is unstable. when you combine l1 and l2 penalties, you gain the benefits of a more stable solution. yes, it does support it. how big is your notebook?anyone going to odsc-west this week? sadly, can\\'t say more. :disappointed:   i\\'ll look at that.  it came up in our discussions, but then the added complexity was cited as a reason to not necessarily go that way...stay positive amelia are you also using interaction terms between the features? with many binary input categorical features, i assume there must be some correlations there that may be interestingi would think so also, i\\'ll have to ask the person who is primarily working on the model about that one.  i\\'m just helping on this one on the side...i’ve also heard of people applying pca to discrete variables with some fancy math, but i haven’t ever read up on iti\\'ll be at odsc this weekend! neither a small 2k nor a larger 250k notebook are rendering, so i think its more likely a configuration issue hmm. interesting. i didn\\'t need to play with the configuration. anyway, have you tried this?does anyone have experience using play framework for scala to serve spark ml models? if so, dm me :sunglasses:or just post some info about it here :slightly_smiling_face:does anyone know how to open an .xz file on mac?: did you figure it out?? are you doing ml or probability? i am doing probability, and nothing i try is opening the file!it\\'s all part of the test!!!!oh no, i pretty much fail at the beginning :disappointed:that is *many* billionsi agreeso, curious - i\\'ve set up ssh servers in the past, and made some attempts to secure it with key-authentication, no root login, etc. i\\'ve noticed that even with password-authentication only, there is a slight delay in the response - is this a network effect generally, or a security feature of ssh? i imagine that even adding a slight delay to password validation would basically make most brute-force attacks worthless. so, curious, if someone does want to brute-force a password - how are they actually going about overcoming these kinds of delays to make it a viable method to crack an account?generally you need offline access. either a dump of the site, or hardware access to the machine. things like this are useful when $site_x has it\\'s database stolen, but you only have the hashes. then you can brute force hashes, and now you have a pair of usernames and passwords which you can attack other sites with.jollyhrothgar: just speaking of generic ssh servers (openssh) its probably your network delay or some processing (ssh -vvv should tell you). there is a feature in pam to prevent common attacks, but its negligible enough for us common users to not notice. you can check that by spinning up local machine, setting up ssh server and try logging in, it should be much faster (in ms). speaking of attackers attempts, it depends on a myriad of things, there are automated tools (even a distro: kali linux)  that are custom written for different versions of services we use. so if you use a really new pen-test tool against really old service(including ssh, never tried it but just guessing) you have a really high chance to cracking into the system just with brute-force. at this time, its just a race in versions against who finds out vulnerabilities first, attackers or developers :simple_smile:: openssh does not have built-in delay for wrong password to my knowledge. but one can always use fail2ban or google authenticator (two-factor) to make the server more secure. to actually break in ssh servers,  one has a lot better chance through bugs in the implementation of various security protocols as  pointed out . passwords were rarely the weak point.jollyhrothgar: three\\'s also a distributed brute-force attack, with a coordinated effect of thousands of machines hitting an ssh server. assuming don\\'t crash the host, you can try multiple passwords in rapid succession., , , : ah, okay - i guess it makes sense that these kinds of attacks would be used offline, or in a distributed attack (assuming a server can handle a high volume of requests). thanks for the info, folks!hello. i was wondering if anyone has any resources on the impact that social media can have on a business. e.g., from the perspective of a data scientist, how can inclusion of social media data impact the type of analyses (and business insights) of a company? ideally, i’m looking for research/articles and/or use cases, but even just examples that come to mind would be helpful. thanks!apparently slack is losing it\\'s appeal for some: slack: let\\'s kill the inbox so we can be your inbox! :wink:does anyone remember google wave? hehe.this starts off slowly but is very well-done and gets at several of your interests: is anyone familiar with using things like app usage patterns for classification of things like churn, purchases, etc. i\\'m having trouble finding the term for this type of analysis. it\\'s not really time series, as i only have two measurements, prior to app usage and after. but i\\'m curious if different usage patterns correlate to certain outcomes. is there a term for this type of analysis. or is simple feature engineering using app usage times the best approach, things like average time of day, variability in time of day, average days between app use, probability of using the app for each hour of the day, etc. interpretability of the results are extremely important, so black box or ensemble methods may not be the best approach for me.i would call this *cohort analysis*.that\\'s somewhat along the lines of what i\\'m looking for. though in this case there is a single cohort, and use was limited to about one month. think of something like a work tracking app with a one month free trial. i\\'m interested in something similar to whether certain usage patterns were more productive after the 30 days as compared to before using the app. implications could be importance of implementing daily reminders, whether reminding at certain times of day matter, etc. there is no chance for future data collection and i can only rely on the data at hand. things like forecasting obviously aren\\'t appropriate because there isn\\'t a measurement at each time point, but it seems like this is a problem that has likely been tackled before. is there a specific type of cohort analysis you\\'re referring to?this sounds pretty similar to engagement measures in e.g. mobile gamingmeasures like sessions per day and session length might be a good startthanks, sounds like the feature engineering approach and logistic regression may be a good starting place. i just wanted to make sure i didn\\'t have an obvious blindspot with regards to a standard approach.anyone know a good way to deal with changing data schemas with an implemented data pipeline? for example you have a data pipeline configured to save data in a specific schema. business/product decides to change the schema, is the only way to ensure pipeline still works is to change the way data is handled. is there away to allow pipeline to have visability of schema changes with minimal changes?why don\\'t just take the entire fft ? if that\\'s to big, you can take mfcc instead (or an equivalent mesure that does not use the mel scale) confluent has a schema registry. aws also has a strategy it’s been telling businesses, which is to use dynamodb to store metadata on data stored on s3, which potentially could be use in your case. that said, any alumni who have had direct experience with this?i’m getting a shape miss-match error from somewhere deep in a pretty complicated model (keras implementation). i was wondering if anyone had any tips-and-tricks for debugging this kind of thing? unfortunately, printing out the model summery wasn’t helpful for this particular issue.the model compiles, but during training it breaksyou should be able to call each layer, one at a time and look at the shape of the output (are you using functional or sequential api?)i’m using the sequential apiuhm, it\\'d probably be easier with functional (see this example: ), but it can work also with sequential, if you explicitly define the instance for each layer (and append with model.add)interesting, okay yeah i haven’t been exposed to the functional api yet. i will definitely check that out.yeah, it requires a bit of getting used to but it makes it easier to handle complex models, imo/poll \"straw poll - where would you be most excited to work in europe?  i\\'m wondering what city would have the most draw for good talent.\" \"amsterdam\" \"berlin\" \"paris\" \"#nevereurope #godblessamerica\"i retract my previous commit… i am using the functional api…:five:london:six:munich:seven:warsawhey all, i know an experienced executive assistant who is looking to move out of the finance industry and into tech. if anyone thinks their company might be interested, send me a pm!ha, fair point - i only included those three cities because they would make sense for us for other business reasons :grinning:i figured…and we have an office in warsawi have a similar problem, and in the middle of investigating differnet tools.  i have been looking into aws glue, and think that it can be a potential solution  cool - yeah i looked into aws glue as well. any other tools you think are worth reading up? thanks in advance.i also rolled my own tool currently, and deployed as a lambda, but that’s all i can think of for nowfor all des, what do you guys think of managed services like alooma, stichdata, segment, fivetran, xpenty, etc that offer etl and data pipelining solutions. what kind of companies would benefit from these teams better than let\\'s say hiring an entire data engineering/infrastructure resources?so happy i added this extension. :slightly_smiling_face:what techniques/strategies do you use to improve your precision without hurting your recall? this is for an imbalanced classification problem where i sub-sampled the negatives to match the number of positives. i have plenty of positive data to do this. i use some of those services, it’s cheaper than doing it from scratch and putting people to manage that when you want to centralize a bunch of data sources like hubspot, salesforce, etcdid anyone join amazon as their first ds job after insight?i\\'d love to connect with you!, , ? happy to gchat or gmail. won\\'t respond to slack unless directly pinged, appreciate the help michelle :grinning: . amazon is great! i\\'d love to help in any way i can.hi, becky. i am a current ai fellow. for my project i am working with mfccs. if you want to discuss about that, you can talk to me.i can connect you with several people in seattle if you\\'d like :slightly_smiling_face:a good way to start is to set up a chain of different methods.you can start with a good high recall algorithm that doesn\\'t have a high precision and then systematically reduce the amount of fp with other algorithms and therefore increasing the precision without affecting the recall.you guys hiring this round?yes please!a good chunk of the company isn’t hiring. but, many teams are still hiring this round. we, down in san diego, will start hiring again hopefully in the fall, except for a few teams which are hiring software eng right now. no data scientist hiring in san diego at the moment. thanks! i mightmichelle, i can\\'t get over my initial reading of the event, \"zero data\" . it makes me a very sad panda.hahaha, i just noticed that! unbeknownst to all participants, we are trolling them by continuing to dangle cool data despite not even having any. :upside_down_face:has anyone gone to grace hopper?  love/hate breakdown?based on my experience last year, i believe that every women working in a technical field should at least once go to grace hopper. i was, for the first time ever, surrounded by overwhelmingly deeply technical people who happened to be overwhelmingly women. when i got up during talks, i did not stand out just because i was part of a minority. it gave me the experience of what it feels like to belong to the majority, it allowed me to better distinguish between personality and situation, if that makes sense. also, it is an extremely friendly, very fun conference. there are many undergraduate students in the audience, many of the events are not directly relevant to data science/machine learning/ai, but you learn a lot about your field in ways that have less to do with, say, \"stochastic gradient descent\". also, you will be able to help undergrads figure out what they\\'re excited about, what it means to work in data science/ml/ai, etc.hi all -- a new seattle fellow here. does  anyone have experience with saving/loading tensorflow models? i have managed to re-load a model i pre-trained, but i am having trouble using it to make a new prediction... my goal is to re-use the pre-trained model to make a prediction on a new data example (e.g.: user uploaded data in flask! :fearful:)what kind of model are you using? i retrained the last 2 layers of inception in my project. that is an awesome visualization   thank you for sharing!!hi fellows!! i have a question. i am looking for a way i can implement a register website to download load a file. like, let\\'s say, you go a web site of data set. then you have to type your name, email, institution/company, and so on the web page, then you will receive a link to download the data set.  anyone have any idea or know any website that help me? i think you could set that up pretty easily using flask and some extensions: thanks! would you point me any tutorial site if you know any?i won’t do anything besides google it, so you might as well just do that yourself :slightly_smiling_face:ok i see, flask with extension...thanks!!if anyone is interested in the dataengconf in nyc next week, you can get 20% off by using the code insighti think you should still encourage people to post senior level positions on the board (in addition to emailing them). i know i always look at the descriptions to see what\\'s out there and figure out what skills i should focus on next. also, while i have no intention of embarking on a job search, if something interesting pops up, i will apply. hello, i was curious if anyone had any experience with enterprise-level api design. our current stack is all java but we’d like to setup a python api that will be queried 100,000+ times a day. if anyone has any expertise in the area and can answer a few (newbie) questions, please pm me! that\\'s possible.  100k requests / day can be solved in many ways.  one way it\\'s done in wsgi apps like django or flask is to have many workers behind nginx or a load balancer.  the number of workers depend on how many *concurrent* requests do you want to handle.  that also depends on how long does it take for the request to get processed by the application.  is the response in 500ms vs 1s means that you need half the workers to do the same job.i have used python tornado (which was designed to solve the c10k problem).  tornado is usually architected when you there\\'s lot of wait time (i/o or db) in processing the request on the server.   i have written a sample app that might be useful -template: `` or sample project: ``.  i have given a few talks on this as well at pydata, so ping me if this looks interesting.created a public channel called  to talk more about it.for anyone interested in attending next week’s dataengconf in sf, they’ve given us a discount code for 50% off - “insight50”. there seems to be a lot of good presentations and we’ve got several alumni presenting. looking forward to it! and i + a few friends will be starting at 11am at the justin herman plaza. i\\'m borrowing a lab coat. anyone else interested?^i’ll be at civic center helping organize some of the official march volunteers!hi everyone! i am very excited to announce that my first course is now up on datacamp :tada:.  it is geared at building an intuition for how to use clustering for exploratory data analysis. i would greatly appreciate any shoutoutouts on social media to help spread the word! when i\\'m doing personal stuff i use conda for envs. at work i\\'m a really special snowflake.hey guys! quick plug for one of my mentors: you might have seen in datascienceweekly that lending club are on the hunt for a weapons-grade data scientist. chaomei chen, chief risk officer at lending club, is widely regarded as the reason lending club achieved the second largest ipo in tech history behind facebook. she is an incredible woman, she was humblingly kind and helpful when i panic-emailled her out of the blue to get my insight project over the line, and many people owe their careers and their success to her. we would be lucky to work for her. let me know if you are looking for a new position and i would be delighted to make an introduction.does anyone have or know of some nice slides covering \"ml for pms\". i am giving an introductory-ish presentation to our product group on monday and it is always great to see how others have organized the topic.i’ve shared around  and  to our groupi think the latter has slides up on slideshare as wellyes, found those slides. thank you! thank you ! one of the things the bothered me the most in insight was the fundamental lack of flexibility in the infrastructure we usedi used digital ocean personallyyeah and i used hyde+google cloud+blumix, and the support was amazingyou can always just say “thanks for the interest, i’m not looking right now, but can i keep your contact information and reach out should that change in the future?” i was actually quite happy with the level of freedom (and everything else, generally) at insight. they provide tools/support for certain ways of doing things (e.g., aws) so it makes sense for them to encourage people to do things those ways, but in my experience they never stopped anyone from doing things different ways.hi all, for your very first job right after insight, is it normal to go in with the title “data scientist” or “senior data scientist” or else?“junior”, “principle”, “associate”, etc.slide-share woes:  anyone have any suggestions on having a nicer looking slideshare?  mine is cutting out edges of my tree on p. 13, even tried to change the color but still a few aren\\'t showing up in slideshare, and do show up on my .pdf export from keynote:don\\'t use slideshare? [hi dan]yeah, i’m just linking ot my pdfsha, hi matt!  how else would you recommend us present our demos to companies?  slideshare was recommended to us as being safer than relying dropbox.  note:  i do not have a front-end :simple_smile:well, a link to my github repo is my \"front-end\"you can stick a pdf on github, why notor make a website that serves as a slide deckflash drive?internet of thingsthings of internetdoes not show up as a presentation on github:ah, well i can click \"raw\" and download the presentationyeahpsa: github renders markdown/rmarkdown/ipynb inline nowi used google slides.yes me too: check out github pages thanks worked like a charm!if you structure your readmes well and use markdown you can turn your repo into a project website using the automatic conversion processnice: i used speaker deck for my demo  (slides are actually still up at ). no issues with the slides getting cut off, and the website automatically updates when newer versions are uploaded.i’m also using speaker deck. slideshare destroyed my vector graphics, even though i converted to pdf.stay positive brad.deutschyour daughter is a good icon designer uploaded a file:  and commented: good luck to the current batch of fellows as you embark on demos.  here\\'s a checklist my daughter (age 7) made for me when i was getting ready to demo last summer.  i think it\\'s pretty good advice, not just for insight but for presenting in general.see now i look prescient and weirdhah, sorry, couldn’t get the file uploaded…should work now.hey all, i’ve a very open ended question for the health people: i’m looking for some starting points/articles to start thinking about merging and using messy emrs. experience/tips/packages/cat gifs welcome. :cat:hey  messy emrs are the bane of my work life. not sure what you want to do  but the fhir project might be a good starting point: what kind of emr\\'s are you talking about? is it still a data integration problem or are you looking to do something more?this is the python client for it. our team (leifur\\'s and mine) is slowly moving towards adopting the fhir protocol and setting up a fhir api that allows patients to be retrieved as fhir objects. its essentially a standardized protocol/format for exchange of emr data across providers. we are using it internally because msk has a really wide variety of emr systems. so integration and lining up data from different emr systems is quite painful. ok really painful!!! :scream_cat:/giphy banecatthis looks super good thanks ! it’s good to know what your data should look like, even if it doesn’t. we (at mt sinai) have a dump of emr data from different hospitals and are looking at extracting any information. at the moment the main objective is integrating and lining up data. :upside_down_face:yeah the fhir project is really pretty awesome.  which team/dept are you with in mt.sinai?  we\\'ve seen some really good stuff come out of there...just started at the arnhold institute of global health. mainly focused on global public health but with some local and internal things too.hey all, does anyone know good sources for learning time series analysis with python?:: also,  thanks the tensorflow dev summit is happening next wed. on-site in mv has sold-out but there are still spots for the live-stream viewing party at google sf office (for now): that\\'s great, thanks for sharing! does one need a work visa for this?anyone know any data scientists in vancouver? i have a smart friend trying to get into the field who just moved there.i know someone who used to work at mobify as a data scientist in vancouver, but he is now a data scientist in ottawa at shopify. if there\\'s no better connections, he may know someone or be useful himself. thanks  - probably that’s removed enough that she should look for more direct avenues. but thanks for the offer!mike aka schnaww works for kablamo in vancouversadly, most likely yes!oh yeah! i remember that now! thanks : when you say you don\\'t want to fit a distribution to the data do you mean you\\'d like that continuous function to respect the weird shape?that\\'s correcti was thinking perhaps some kind of gaussian expansion, if that\\'s a thingmy first thought was edging towards using some spline interpolationfitting the spline to your data will give you continuous measurements over your initial range of datatoo naive perhaps ?i\\'ve used spline interpolation with this data set, and it does work, however, this distribution has to be sampled hundreds of thousands of times. spline interpolation increases the run time significantly, unless there is some way to serialize the spline interpolation?i could probably serialize that, but it would still require lookup of the proper splinebut still, it would probably be faster, since the interpolation would only be computed oncei was considering some kind of fft, using an exponential basis, since i require this distribution to converge to 0 at +/- inftyhmm guess depending on the precision required by your sampling procedure you could use some form of memoization and only compute those that are absolutely necessary and lookup from the previous computation whatever falls within a threshold distancei\\'ll mull over the fft, don\\'t have a great intuition as to how it could be applied at the moment :slightly_smiling_face:alrighty, i\\'ll mull it over as well. thanks! i\\'m here!just find out this online ai conference next month - seems cool. can\\'t argue with that price also the results are in: thanks gabriella!so, i haven’t read that article yet, but are physicists back in fashion? i thought the \"physicists are great data scientists\" thing was very 2010, and that humanities and social sciences were growing in demand. no?half my team is physicists for what it is worthi’m also a physicist, and i hope i do at least average work :wink:hi all,does anyone have experience web scraping using python, in particular scraping for images?  i’m currently trying to scrape google images, and am hitting some road blocks since the api is now deprecated. i’ve created the channel , please let me know if you have any tips/advice!i have used splinter for my scraping in the past ()i’ve used requests + beautifulsoup, works pretty wellhas anyone had experience getting data from hcup, namely the kids\\' inpatient databasetrying to get data on a particular diagnosispediatric asthmathis is more of a pm question, but can anyone refer me to a framework on a best practices approach on how to evaluate projects?i’m aware of irr/npv for larger capital allocation, but i’m wondering how often these are usedif you have projects a,b,c,d.  other than the hippo, how does one decide what to pursuewe’d used a scale in the past where we scored projects across 1) estimated impact of project 2) estimated confidence of that impact 3) ease of implementationand then added projects into a sprint in a way that seemed to balance implementation time with potential impactthis was passed along to meseems very similari think reach is really important to keep in mind as well — we run into this when prioritizing analytics projects for clients, e.g. this model will add value, but only for 10% of users so we will not pursue scaling it yethi past and present fellows. does anyone have advice for downloading medicaid/medicare outpatient claims data? ? on the free end, meps is a good choice (medical expenditure panel survey) -- . hmm, just saw you were interested in outpatient claims. not sure if meps includes those or not. thanks for the suggestion! i\\'ll check out meps and see if anything clicks.does anyone have experience writing scopes of work for contract data science gigs? if so could you please pm me? i\\'d like to pick your brain. thanks!  i\\'ve used scrapy new fellowship here. throwing this out into the ether. is anyone knowledgeable regarding multiple sclerosis? do you know what type of drugs or therapeutics are commonly used for treating it?there you godata science from images obtained by google street view though no longer a neuroscientist by occupation, i can tell you some general stuff. since ms is an autoimmune disease most drugs used to treat it those that block the immune system or mitigate the pain, tremors, and other symptoms of ms.  have you examined ? they have a section on treatments that fairly comprehensive and easy to read (bonus!)., thank you!  how about ny state hospital inpatient discharge data:congrats  !  great post: awesomethanks ! it was a fun project! data and analysis code is in a github repo (linked on the viz page)i appreciate your implicit use of the isoperimetric inequalityi didn\\'t know that name for it before, but now i do! :simple_smile:“that thing about circles\"hey there, anyone have a clever way of working around distil network\\'s scraper detection? i keep hitting up against captchas while trying to scrape streeteasy using selenium, even when using very polite sleep timers and vpn cycling.from my memory, distil embeds some javascript in the client side code that is used for detecting automated clients and esp browser automation tools like selenium. so sleep timers and vpns won’t help  it also acts as a reverse proxy for the target website’s traffic like cloudflare does. so no real way to get around it i have a couple ideas for dealing with it that are out of the box, so message me if you want to discuss further :) :+1: thanks francesco and eddie! looking through the medium post now, it is quite helpful. :slightly_smiling_face: (and i\\'ll probably take a look at the book once i delve deeper)the bay area bike share has released it\\'s year 2 data:  perhaps of interest for future projects!question for python de types: can you suggest any resources (e.g. articles) about building scalable python production servers? context: we have a ml algorithm written in python to put into production. in the past, we have rewritten algorithms into our existing java production stack. however, this one uses enough scientific python libraries that we wanted to explore the feasibility of a production python stack. info about memory issues, performance comparisons, or server and container options would be especially helpfulhi ,  we actually do a lot of this at datascience as deploying python models is a main part of our data science platform.  what i\\'ve found is the main thing for performance is python itself.  therefore using modules like `numba` or building the models in cython will have by far the greatest impact on performance.  we also find that load balancing the model between multiple instances (e.g. if you have it running on 10 different hosts or docker containers, then use a load balancer to route requests to them) can help scale python models.  as far as python-specific options, i would be weary of default web servers in python projects like django.  these can be a bottleneck, so you should look into a more sophisticated python web serving stack, e.g. :  thanks  !hi guys, question: in my company we are trying to come up with a title system for data scientists that can show different levels of seniority/experience. we span from people with undergrad degrees to people with phds and years of experience. any suggestion? all i can think about is data scientist/senior data scientist but would love something more granular...junior ds (bachelor)/ regular ds (grad school or exp) / senior ds (grad school + exp) ?what purpose do you want it to serve?  junior / regular / senior are all based on a combination of education / experience levels.  what do you want to highlight with these titles?data *analyst* for bachelor folks?we have people with bachelor degrees who are definitely extremely talented data scientists :slightly_smiling_face:  i would not base titles on things like that alone, that’s why it’s important to consider what you want the titles to convey before just adding granularity^i personally agree, i was just suggesting what i’ve seen at companies...data analyst is usually a pretty different rolecan be, at least.yeah it can be a mix, we in the past have had people who do data scientist things but don’t have the experience or education needed for higher level, as data analysts who were later promoted to data scientist, then other analysts who do entirely different things and are on different career pathsi\\'ve been chatting a lot with companies about this recently actually. the distinguishing of analyst vs scientist can vary from company to company. the general trend though is that analyst\\'s job is to surface numbers while the ds\\'s job is to understand them. you could ask an analyst to find the % of users that click on a button for android vs iphone, but you\\'d ask a ds that works on analytics to find why android users are churning out at a higher rate.so if you\\'re an analyst on paper, but actually do ds/ai work, that distinction\\'s easy to communicate to potential employers?hey - i\\'m a new fellow in the health data program in silicon valley. does anyone have experience getting plotly plots(specifically chloropleth) to run on a webpage through flask ...?? ^^ have you done this? in an interview it’s easy to explain, on paper less so. squarespace went through this angst recently hits the right points too — in my company analysts are great with sql and tremendously helpful with feature engineering, but perhaps less sure of why those features are likely to be mathematically relevant. we’re trying to train them in python to move up the stack a bitjunior, ds, senior, principal for individual contributors. above the ds level, the managerial track can add \"manager\" and \"director\", which naturally funnels up to the vp and cdoalternatively, junior has a bit of baggage, so associate works as wellyou can always do the really boring microsoft-ish way: ds1a, ds1b, ds2a, ds2b, senior dsa, ... :wink: but i guess big-ass companies need huge laddersthat’s an interesting problem, how would you find the best so page? compare code block you’re currently working on to the question or answer code? or only look up the actual line of code you’re working on?  is anyone else going to nips?,  from what i’ve read that only works for python 3 though.has anyone ever had a grumpy it department who will not install required ssms (my work chose this for sql) on their computer and instead wants you to use the interface remotely? i am not familiar with this or what it will accomplish since out database will be hosted securely on their server without full administrative rights to us.  but i am also worried how this will affect my interacting with sql from python. anyone done anything like this? or should i stand up for myself and tell it this is bonkers? thanks!i guess the question is what would you want to use ssms for? an it department is probably wise not to install software you don’t needwe are using tsql and i assumed that i would need ssms to create and modify  the databases but i\\'m really not so familiar with it so maybe there is an alternative?i haven’t worked with microsoft sql server, but i doubt you’ll need the administrative tool it to create schemas and tables. in general, once you can connect to a database server via python (using pyodbc or sqlalchemy or one of the many other libraries) if the user you’re connecting as has the appropriate permissions, you can do everything you need.yay!!!! i love this solutiondefinitely read up on it first before taking a stand with it…best not to make enemies:wink:very sound advice! thanks!anyone have suggestions on what kind of “hustle stats” they would like to see in their workplace as part of employee performance metrics? anybody else going to meet up on survival models in stan tonight in nyc  ?  : sorry for the spam but can my mentees this evening please slack me? i will be late.i had a few questions related to an app i am developing. every query to this web app (in flask + spark) is answered by a simple pre-trained machine learning model in h2o which is then served to the user.  now if too many users begin to go to the web app, what are the problems that i would face and need to be solved? how many users it too many? and how would i solve these problems? if someone can shed some light, it would be terribly helpful! p.s. someone mentioned that i should be using spark streaming + kafka to solve this problem.: normally a web server has a queue for incoming requests and one or more workers to handle those requests. in flask, there is an easy way to set the number of workers, and it is usually set to the number of available processors on the computer + 1. that means that if the average rate at which the requests come in is more than the rate at which they can be handled, you will soon run into trouble since the queue will overflow. one way to solve this problem is to get a better computer with more processors to have more workers, but that soon becomes impractical. kafka is a messaging system that has its own queue and allows you to scale to more _computers_ rather than more _processors_ on one computer. it is not at all trivial to set up and maintain, but it is a much more robust solution for systems that need to handle millions of requests per minute.thanks alex for the great explanation! millions of requests per minute is certainly not something i\\'m expecting :smile: one last question, what happens if there are more incoming requests than processors on a computer? does flask queue these requests for later computation, or are they dropped?the requests should be queued, but i don’t know the default size of the queue in flask (it’s probably 2048 or something). another point is that http connections tend to time out after 30 sec of waiting time, but you should be able to increase this timeout in flask settings if needed.great episode of data skeptic discussing a ds interview problem from the perspective of the interviewer: stay positive dima… someone really needs to take on a project to make  smarter.... i listened to that last week, it was great!i think i’ve removed that sentiment “analysis” from nuggetbotcan be summarized in a single bullet point:* give everyone access to the data.¯_(ツ)_/¯it’s a good point, though probably doesn’t need 15 minutes.  the one complication is that sometimes you need to have spent a lot of time sql monkeying around to understand it’s quirks.so  you need at least a couple sql monkeys somewhere that you can quickly find and exchange bananas for insight.also, i’m staring at a empty banana peel on my desk right now. what does this say about me?hahatrue, or, you need to have someone etl the data into a nice denormalized schema that is easy to query for non sql expertsbeen finding some inspiration from this articleyes, though such schema can make easy questions easier and harder questions secret traps. also, the only thing i like less than being a sql monkey is working with bi to get an new etl setup.it leaves you to do the hard stuff on a different data store while everyone else can pull their own data with the bi tool + data warehousewe had a client with a horrible, several-hundred table database that only one guy at their company understood. i was asked to pull some analytics out of the database. because their one expert never got back to me to answer my questions, what should have taken me about an hour took over a week. oh well... we bill hourly, so it was good for us.haha, yeah, bad for you and i don’t want to be that “one guy\"hmm, i\\'m a male scientist married to a female teacher... as predicted! data science verified!n=1 and doneit works for my parents :smile:: \"the small sample size is made up for by the sheer consistency of the sample!\" :wink:i think one lesson learned is if i want to find a spouse in the future, i should be a truck driverthey’re basically married to everyone, not simultaneously i believefor us physicists and astronomers uploaded a file: i dunno… a mobile lifestyle might lend itself to having multiple lives and marriages.if you’re into that. i\\'m also married to a teacher, n of 2!qed!new blog post: feeling out the famous mnist handwritten digit set.hey guys! two of my best friends from the uk are coming to sf for the week, and i wondered if anybody could help me with a couple of favours: 1) are there any spare spaces they might crash on monday to thursday evening? and 2) might somebody lend one of them a bike during the day on tuesday? cheers!sorry everyone, wrong slack group, ignore!stay positive mattgibbany alumni currently work with aws tools? please dm, i\\'d love to pick your braini feel like the answer to that is probably almost everyone. any specific tools you have questions about?trying to pipe simulated data through kinesis streams, dynamodb, and lambda. i\\'ve found a few tutorials on the web, but none quite seem to work (not sure if me or them)etl question:  i have an sql database with 3700 people.  each person has hundreds to thousands of associated multi-category columns events, such that when loaded into python the 3700 people become a 10+ million row database.  i want to one-hot-encode it for sklearn.  after one-hot encoding, it would become 10+ million rows and thousands of columns. by using db.groupby(\\'person\\').max() in python i would then have an accurate and useable one-hot-encoded 3700 row dataframe (one row per person) for sklearn.  i don\\'t have the compute for the large one-hot-encode step.  has anyone experienced the similar and have any suggestions that don\\'t require more compute?for big transformations like this, i often use the just-in-time compiler from numbabasically convert your dataframe into a numpy array and then write for loops to do the transformation that you wantmake sure to use the @numba.jit(nopython=true) annotationi often get huge improvements in run-time this waybtamadio: thanks brian!  i\\'ll give that a try!no problem. let me know if you run into any issues. also if it works for you, i’d be interested in hearing about that too!that’s super shitty of nyc. tho, congrats on the baby !i\\'ve worked with geospatial indexing in mongodb and it is fast! if you can get a way to index your points and/or your shapefiles geospatially you might find a good use for it.yes, it is. and thanks  !if you love scala, building data pipelines, creating key metrics, building and enabling actionable visualizations, and helping to measure a new business, please come work with me :wink:is there anyone who has experience in policy or regional collaboration? i have someone at brookings (vp of cities) looking to work with a data scientist to review a proposal. dm me if initerestedprobably depends on the subject area, but maybe try some of these folks?      i\\'m happy to talk if you\\'d like. what are you planning to do with this data?i’m looking for advice on navigating the process of taking over a team, especially for a first time manager. i’m sure a number of people here have experienced this—any willing to share publicly (or privately)? (also, please don’t share outside of insight network—nothing set in stone yet.. just looking down the road.)i\\'ve read several articles over the years, so that\\'s where i\\'d start. i\\'d say probably 80% of it is good advice, subject to the usual disclaimers about personal style and company culture.message me and let’s catch up? prodigy looks really neat! how have you liked it so far? can it handle multiclass or multilabel labelling?yay! genevieve you made it!nuggetbot: animate me welcome genevieveit would be fun to attend and hang out but might be too soon for me to give my insight spiel again :robot_face:good input, nugget!haha, i think it\\'s totally fine! do let me know if you\\'re up for it! why not talk about work then, were all data scientists :stuck_out_tongue:hi all. i\\'m thinking about a move to the bay area, but my husband and i are not sure how affordable it would be. i\\'m trying to figure out salaries for data scientists and i was wondering if i could crowd source some data here. i have looked at glassdoor, but i would like some more specific information. i\\'m wondering what the salary range would be for someone like me, a lead data scientist currently with three years of experience in the field by next year. i\\'m thinking about looking at jobs at established companies or late stage start ups (100 or more employees). i would also like to get a sense of more than base salaries, like what do bonuses and stocks/options plans look like since these can make up a large part of compensation? thanks in advance for your input!check it! alexis there was a recent crowdsourced salary survey run here (so only insight alumni) that might also be useful: sorry that link is from another source, here is the insight one: links look the same, let’s try againwow, that\\'s awesome! thanks!for those who use spark, do you prefer pyspark or scala? my team is moving to spark and considering which route to take, since we’re primarily a python shopi’ve used both and i would suggest going the pyspark route and have a few people dive deeper with scala as the api is complete (pyspark has a few limitations). the future of spark appears to be in python/r (and other higher level languages).i don’t think i’ve encountered a spark use case where i couldn’t get it done in python, and that intergrates with the rest of the workflowwe use pyspark. works well and i only moan about the lack of typing when reading pull requests.scala! :thumbsdown:if you really want to understand and leverage spark, scala is the way to goalso worth noting that we use luigi for task flow management, which plays well with pysparkthings are typically developed in scala first, and extended to pysparkanyone know if there a preferred method to advertise job postings?  we have a bunch of insight alumni at grand rounds and have a bunch of positions open.  pm me for all the juicy details.interesting. thanks for the advice!i have also found pyspark to be able to cover all my needs.  for reference we use it in standalone mode (etls, exploration, some ml) since our data is not at hadoop level scales yet.anyone have recommendations for an introductory spark text or two? preferably r-based but python would also be okay. a focus on ml applications would be a plus.i prefer scala - error messages are easier to parse (just jvm rather than python runtime wrapped in a jvm), the api is more feature-complete since spark is written in scala, and type-safety can really cut down on the rate and variety of errors you can get in the middle of a long-running job. however, scala is a steep learning curve from python so weigh it against how heavily you\\'ll be using spark.hi guys! any idea why the insight websites are down? this may have been announced but i\\'ve been offline!... and we\\'re back up! the internet overindulged last night, perhaps? :wink:internet, you\\'re drunkhey folks! i have a tuition reimbursement program at my job and was wondering what kind of external learning/moocs people have found helpful?i\\'m more on the engineering side, and found an offering for a data engineer certification from google here:udacity is pretty greatcoursera has some helpful specializationsfor all our nlp projects, we use py3 because of better text handling. it wasn’t hard to switch to py3, so ideally, the choice of 2 or 3 depends on the project.hey fellows, i have a quick question about recall optimization for imbalanced data sets. any insights appreciated :slightly_smiling_face:how imbalanced? i can\\'t quite remember the exact cutoff before it becomes worrisome, maybe 15%/85%?hi fellow fellows, does anyone now of any existing, straightforward methods to calculate error bars/confidence intervals for xgbregressor predictions as applied to time series forecasting (where you can’t do standard randomized cross-validation/bootstrapping due to time stream autocorrelation)?  i\\'ve been scouring for one and have only found the following semi-cryptic blog post.   thanks!  i wish :slightly_smiling_face: it\\'s 99.1% and 0.9%. i\\'m using smote and xgboost, which is helping the model recognized the minority class. the business situation is detecting credit card declines.  i have an intuition about the answer but i\\'m curious if there is  a convention for optimizing recall for a minority class when doing so lowers the recall for the majority class.do you know the potential cost of lowering the recall of the majority class vs minority class? if detecting the minority class has higher value for the business, it makes sense to optimize for it and just take into account for future predictions that this will mean an increase in falsely labeling majority class cases as minority class.+1 to what  said. also, xgboost has a `scale_pos_weight` parameter which may help directly balancing your optimizationsince you can’t apply regular cross-validation for time series data you should use something like it:fold 1 : training [1], test [2]fold 2 : training [1 2], test [3]fold 3 : training [1 2 3], test [4]fold 4 : training [1 2 3 4], test [5]fold 5 : training [1 2 3 4 5], test [6]sklearn provides a special class for timeseries splitting sklearn.model_selection.timeseriessplit there\\'s a minimal cost to lowering recall for the majority class and the scale_pos_weight has been super useful.  i wanted to be cautious not to make any amateur missteps for this type of case and my thinking sounds in tune with your suggestions. thanks for your input; it\\'s reassuring.hi all! a bunch of us from insight are going to grace hopper next week, if you\\'ll be there too and want to meet up, join us in . we have a panel on wednesday, event on thursday night, and we\\'ll be doing office hours with applicants if you have friends who would like career advice. :woman-woman-girl-boy:i know what you mean, . if you come up with something else, i\\'d love to hear about it.oh, and as a reminder for folks who want to try upsampling the minority set, make sure to split out a validation subset before you do it, so as not to accidentally pollute it. :smile:hi georgiy, thanks for your response.  i have implemented this walk-forward/expanding window forecasting with xgbregressor, but i\\'m wondering what the error bar/confidence interval is at each prediction step.  there  doesn\\'t appear to be one for the model output (unless i\\'ve somehow missed it).  i\\'m thinking i might bootstrap a confidence interval, but have to be careful there as autocorrelation means leaving some samples out doesn\\'t entirely remove all of their associated information from the model (and leaving \"holes\" in the data may destroy  autoregressive components the model relies on).hey folks, data for democracy is having a hackathon this weekend and i\\'m co-hosting an in-person event saturday in boston.  there\\'s also a nyc one! hi folks, a friend from indiebio is organizing a science hack day that i thought some of you might enjoy: i\\'d love to see updates on this project as it progresses!our start-up company (grapevine ai) is looking for a freelance full stack developer,  javascript, node and angularjs expert, please shoot me a note if you have contact with someone like that.. thanks!@hee you can use the 80gb file by manipulating the rdd (`rdd = sc.textfile(’filename\\')`)most probably, you’d wanna forget about the order of rows in the column, given that the computation is distributed across the cluster.please feel free to pm me.hi everyone! can anyone share best practices for code review processes that work for their teams? we are a small group, working mostly in jupyter notebook environments (python, r) as well as lots of sql queries. our team is just getting started using git/gitlab, documenting code, and generally collaborating (new analytics team within gov\\'t agency). any suggestions would be great!we have two kinds of reviews, a style review and a requirements review.  style review - we have slowly done away with it if the language has a robust set of linters (flake8 for python for example).  requirements review - done by another dev.  there are blocking and non blocking comments as welland if the comments in github review is getting long, best to  have a face to face with the reviewer.and we try to make the code for review as small and atomic as possible so it won’t be a pain to reviewfor big/lengthy code commits, a face to face at the start, usually helps to explain things to the reviewer before she/he spends hours trying to figure the code out :slightly_smiling_face: i saw this the other day: thanks    for your comments! nice! thanks for sharing i think one of the most important ingredients of successful code review process is the individual pull request (pr) themselves. modular, topical pr are much easier to understand than one gigantic pr that contains lots of changes. as you build up the code base, keep pr small (maybe one class per pr, or one set of related functions per pr). when it comes major architectural changes, share and hash out the design in a different platform (like google docs) before putting out code changes. code diffs are really bad ways to see big changes. for jupyter notebooks, try to refactor most of the code into utils .py files and library, and import those in the notebook instead of putting most of the code in the notebook. it’s fair to have different code standard for those common utils/libraries and the code in the jupyter notebooks themselves.: thanks for the detailed response! i\\'ll follow up on separate channel ...super bowl sunday!has anyone done a pid controller for discrete data?does anybody know of a compilation of common character substitutions made by ocr software (e.g. i-&gt;l, e-&gt;o, 0-&gt;o)?thanks guys!what you expectbay area -&gt; austin for data/tech people!where in south bay? and is month-to-month do-able?mountain view and no, it\\'d be for a 6/12 month lease :disappointed: my friend susan is helping to hire 10-15 data analyst/ds positions at the federal reserve based on these two descriptions if anyone knows someone looking. you can email her for more info: does anyone have experience with azure and some spare time to chat about it? thanks!: if you\\'re really into emoji nlp, check out idibon, especially tyler schnoebelen\\'s blog posts. he\\'s a stanford linguistics phd that wrote his thesis on the grammar of emoji, and cofounded idibon.: thanks! i’ll check it out. so cool.my mil wants to come visit on the ues. anyone have any good experiences with inexpensive hotels in the area?: we got a hotel room friday night for $250 with hotwirei might be missing something but i believe at least for sqlalchemy you want to use \"?\" (without the quotes) apparently the formatting is driver-specific, but will be chosen from one of the 5 param styles detailed in pep-249: pymysql driver at least uses the “pyformat” option (’s answer)thanks for all the help.  i saw that page above and have tried the ? and also found some ideas on . i can get any number of \"correct combinations\" in string and params code to not throw an error, but i still get no data...so weird.  when i figure it out, i\\'ll post here.time to make a point. if you support net-neutrality here is the link that makes it easy for you to voice your opinion to the fcc. ^jon oliver also purchased the domain  (which links right to the page to comment on the fcc site) which driver are you using?  i think sqlalchemy defaults to using `mysql-python`mysql-pythonconfirmed.as so my connection string starts with `mysql`right.  if you have the pymysql package installed you can use that as the driver instead, e.g. `mysql+pymysql://...` for the connection string.  think the pyformat style `%(param_name)s` should work thenleast that worked on a mysql instance i have locally herethis is in a prod system and so adding a package is going to be frowned upon, but i\\'ll bring up the option in a review meeting i have later today and i can test that...i\\'m not sure if anyone is looking for opportunities right now, but if you are, automotivemastermind is looking to hire data engineers and scientists in both san francisco and nyc. the (possibly outdated?) job descriptions can be found here:if you\\'re interested, feel free to reach out to me for more details that\\'s exciting to hear! if anyone is interested in connecting with automotivemastermind _directly_, then the best thing to do is to reach out to me or someone else from the insight team! :smile_cat:whoa, microsoft to buy linkedin yes it\\'s big“linkedin, powered by microsoft” sounds a lot worse than “linkedin” :disappointed:stay positive tigrani guess the question is: why would ms drop that much money for linkedin?same reason they paid so much for minecraft. to make even more money.to be fair, after failing to buy yahoo! for $45b, they have a lot of \"we need to buy someone!\" money burning a hole in their pocket. this is basically free!i meant “how” will they make money from linkedin? linkedin apparently made $3b in revenue in 2015. unless they’re looking at stellar grown, it’ll be a while until ms makes money off this deal.i suppose they’ll leverage linkedin’s network in other ways...it doesn\\'t make sense to me either... my ms spies tell me they got an email saying there will be \\'great synergy with office 360\\'microsoft user data + linkedin data = :hippo:ha!this is going to make it so much easier to recruit via xbox live*xxx skull destroyer xxx* would like to talk to you about working at abc corp!my guess is they are trying to keep a foothold in the office software game to fend off google apps and such. linked in is the biggest professional network and will probably give them a lot besides the revenue streami think msft was supposed to have a call to talk about all of this today. not sure when it was going to happen though.yeah  , that\\'s what my contacts in ms said, it\\'s a 360 play.which does make some sense, office is (to my knowledge) their biggest product, so spending $25b to try to protect it makes sensei\\'ll try real hard to not take offense from these comments :slightly_smiling_face:yeah. if they are going to continue to “give\" away os upgrades for free, they’ll need to be sure to keep the 360 revenue stream going.would love to hear from both our linkedin + microsoft alumni  :slightly_smiling_face:*addendum*: my spy has told me that it is actually *office 365*, not 360... oops. :wink: working on that branding guys!office 360 sounds more funbut 365 &gt; 360how about office 720 or even 900? tony hawk styleoffice graham’s numbersoon we can upload resumes using word!seems a lot of folks have mixed feelings about this.. i believe it\\'s an interesting movebut not as interesting as that gif...there is a lot of potential to utilize cortana intelligence using linkedin data to empower networking. let\\'s seei guess that\\'s all we can do, wait and see right?i look forward to the future of this.i think emmanuel has had too much kool aid. but i agree. as much as i don\\'t use microsoft products, i do think they have some reason smart people working for them and there is a good chance something interesting will come out of it.  maybe, this is the start of microsoft bob 2.0bing?bingedin: the best place to search for a new jobyer welcome msft:stuck_out_tongue:lol, you guys..hahhaahey all! i\\'m helping udacity to build a new nanodegree on data science. we\\'re looking for partnerships to build industry relevant projects for students to work on. let me know if you are interested in.sounds like a familiar plan :wink:yes and no. udacity is offering a paid educational program with no job placement promise.has anyone\\'s company migrated from a vertica warehouse to snowflake and been subject to the very unpleasant surprise that all queries in snowflake return column names in all caps? uploaded a file: i don\\'t think it can be changed but i have found that snowflake is not case sensitive unless the database/schema/table/column was created with double quotes around the name. commented on ’s file : this is an old tutorial but might be useful if you haven\\'t seen it yetif you use docker there are some great containers here: the appropriate containers come pre-built with spark running nicely in jupyter along with kernels in scala and python and r.pretty well maintained, when new spark versions come out they usually update the containers within a few weeks.use docker as much as you can  why docker? why not...anacondadocker makes it easy to standardise, and it’s portable. for example, i have docker images that colleagues and new hires can run with very little setup, then they have access to my entire stack.yep- you can port the environment and code anywhere in a box, no need to ask a colleague to set up their env in the same way, or to set up a server the same way etc. just build the container reduces env issues that come up inevitably just being silly. plenty of good projects to go around, and most people here may be good at coming up with well-planned ones hi question, how many of you are managed by non-technical people (what are their backgrounds) and how has that worked out for you?what task (regression/classification/ranking/etc) are you planning on using it for? i can flood you with ranking information and pitfalls if needed.i\\'d like to hear about ranking :slightly_smiling_face:@ channel, ha ha, just joking. my company is looking for a machine learning engineer, let me know if you\\'re interested:  uploaded a file: anyone at dataengconf in sf today and tomorrow?update: i actually phone screened the guy who’s great at hiring women today. he was not great, especially considering he’s been an analyst since 2004 and a ds since 2012 (including being a sr ds for the last couple years).from my interview summary:-candidate did not know quite a few basic ds ideas and was overconfident to the point of condescension, even arguing with me on a couple of responses-what got you into ds: “this is how i’ve always thought about the world”-when describing a logreg model: “throw it into logistic regression then things happen” and “the model will spit out a thing”that last one is amazingwould it be unethical to give him an on-site just for the additional slack gems it would generate?i\\'ve had interesting experience with the first one, where the interviewer was clearly wrong.  instead of arguing i used my standard strategy of immediatly accepting the statements as true, then qualifying them into oblivion (\"yes, but it\\'s only true in this impossible scenario, otherwise it\\'s generally not\")i\\'d definitely prefer someone who  challenged me if they thought i was wrong though :slightly_smiling_face: it seriously crossed my mind…nothing to lose if he\\'s local :smile:it\\'s win win!hahaand i totally agree that i appreciate someone standing up for something if they can make a cogent argument in their favor…this was not such a case howeverright, it doesn\\'t sound like it\"throw it into the model\", \"the model will spit thing sout\" is enough to convince me of that :smile:though... that\\'s where data science is going, this guy is going to be a star soonoh god nuggetbothahaq: how do you build model x?a:  buy data robot and throw data at itgreat!  welcome to chief ds positionit\\'s the future!did not expect a follow up on this sagathis guy has been a sr ds manager at a well-known company for a couple years so he’s already a star!did you consider that if you don\\'t hire him, you guys won\\'t be as good at hiring women as you could be?definitely a considerationsure, he may not know logistic regression, but he brings women hiring to the table“sr ds manager at a well-know company” … makes me nervous!well, alex, did you interview recently and forget the details about logistic regression?i mean, what even is a logistic?! i think you throw data into it and it spits out a thing!i can shed some light into thatit does logistic things to the data before spitting out logistified outputspsh, i have so many logistics just laying around the office, we just shovel data into one of them and it spits stuff outlogistic oven is the best for neapolitan pizzai think it has to do with shipping and storing things.  whatever.there’s a quora for that: i argued with an interviewer when he said that a random forest was an interpretable model, and i can\\'t say i managed to stay polite. they did invite me for an on-site including an hour long talk. i seriously considered going and giving a talk about model interpretability, but in the end i decided it was too much work and declined the interview altogether.^i fully support you in that argument. also, it woulda been hilarious to give an hour talk on model interpretability!could you explain where you draw the line on “interpretability” and why random forest misses your definition?are you asking for the hour long lecture?i think we are :pworking on some content right now on model interpretability with respect to gdpr. a lot more gray area than black &amp; white, so i\\'m also in favor of this lecture.not using it for any specific task.  trying to learn as much as i can about it.  let the flooding begin. :slightly_smiling_face:i’m curious which company will be the first to get fined under gdpr, though it might be more fun to guess which company will accidentally drop table when trying to add the ability to remove all of a user’s data at their request…i have a fun announcement: if you\\'ve seen my jupyter notebook talk during the first week or so of insight, you know that i have a lot to say about the notebook. i\\'ve sat down and brain dumped an extra 4 hours of content into a one-day workshop held at stanford through their continuing studies program. jupyter notebook is only one piece of the workshop. lots of other tools and techniques will be addressed. hope to see you there!details here:  hey all, i\\'m on the fb people analytics team, and we\\'re considering various options for knowledge sharing. functionally speaking, we want/need: access control, toc, tags/topic partitioning, relatively simple set-up/admin, and a nice ui. could you please share what your company and/or team uses? i\\'ve implemented airbnb\\'s knowledge repo () locally, and i personally am a big fan - but i should consider other options as well. thanks a lot!thank you - i recently joined airbnb and the fact that the knowledge repo exists is amazing. it\\'s been so helpful in getting up to speedooh i\\'ll have to check that out too, already dabbled with superset i agree - i may be in touch if we end up using it here. there are some issues i\\'ve come across e.g. needing to reboot the server in order to see any new postsi’m curious about that tool - is it essentially a nice way to organize notebook-like outputs in a centralized location?  can answer this better perhaps, but yes exactly, \"centralizing\" content is one really nice feature of the knowledge repo. some other nice features are: tagging, toc, being able to view the raw markdown (e.g. so you can borrow other people\\'s code snippets)yeah it sounds pretty neat i’ll look more into itdefinitely, feel free to ping me about itgreat! i am going to use superset for my new project deepplay - i have to have a look at knowledge repo too.i don\\'t know much about the technical aspects, but i could put you in touch with people who do if you need it. - that pretty much sums it up. it makes it easy to find others\\' work, reproduce it, and use itif anyone is in the office can you yell at my scheduled people to pick up the phone. do you have a name ?noneok, i\\'ll just shout in general then.i\\'m on itplease embarrass the shit out of anyone who is scheduled.  :grinning:  hey, we\\'re supposed to meet at 5:30pm. i haven\\'t received any hangouts requests.call me. 7144250620or initiate on  \"google can change company data in one part of this database—running an ad, say, or debiting an advertiser’s account—without contradicting changes made on the other side of the planet. what’s more, it can readily and reliably replicate data across multiple data centers in multiple parts of the world—and seamlessly retrieve these copies if any one data center goes down. for a truly global business like google, such transcontinental consistency is enormously powerful.\" good morning - anyone ran into problems importing scrubadub in python3? getting the following error: attributeerror: \\'callable_iterator\\' object has no attribute \\'next\\'hello, has anyone ran into  java.lang.stringindexoutofboundsexception: string index out of range: -1 while trying to write to elasticsearch? (not a corrupt dataset problem) thanks :smiley:not sure if anyone else is working with scala but if you are i found a great book for getting your head around functional programming:also, i\\'ll share the writeup i\\'ve been slowly building about how to use spark dataframes in scala uploaded a file: it is probably not completely uselesshi everyone! i am one of the current ds fellows and i was wondering if anyone has aby experience with the amazon api ? specifically i am interested on how to get the reviews without applying for a merchant license.i think there are data dumps for reviews, which might be more helpful unless you need specific reviews. has used amazon data beforethere are!i know there is an old recommender dataset  that would be fantastic as well! you can download a large sample of previously scraped reviews here they come as json … i can also open up my aws db to you is you want to query reviews by asin thanks for the offer !  i think it could be a good experience understanding how to work with json if i hit too many road bumps then i would definitely take you up on your offer!ok, i’ll send you some code to help extracting the data from the json file  thanks! has also used amazon data beforei used this data source: has anyone exported a data frame to excel with hyperlinks?hi all.  i’m trying to use kubernetes to deploy a cluster on google container engine.  does anyone have any experience with that.  i’ve worked through much of the documentation and hello world examples, and am still having a hard time making all the connections and getting things to actually work.thanks!: have you tried  ?president obama was interviewed by google (2007) and he knew about bubble sort :simple_smile: hi fellows, anyone had experience working with flask, pickle, and heroku?  two questions. 1. i have a training model (~40mb)  need to load to make predictions per user request, except host on aws s3, any other way to host this train model file? 2. loading the model take around 6s, any other way i can improve the efficiency? for a production api, 6s is too much for me. you may want to try something like predictionio for automated model hosting. there\\'s some stuff here about using it w/ heroku i haven\\'t used myself but have heard good thingsthank you  use a hosted service or write your own separate model service.  the challenge with using flask to serve models is that requests are meant to be stateless.  that means that unless you do something really tricky you\\'ll have to load the model into memory for every request.  with a dedicated service you have more control about when and how to manage the model in memory. , any example of hosted service?  i am a little bit clueless. fyi, my current api is running, the working flow now is user send request, i load model, using model make prediction, return the prediction. and you are right, now i had to load model per user request. and  with flask you can load any variable as a global one and reuse it - no need to reload for each request.the view functions will be called per request, but other objects can be shared…i’m not sure why you were hosting it on s3 to begin with, why not host it on the local machine that flask is running from?: i think it\\'s more complicated than that.  see the first answer to this question for a really good explanation if one has more than one instance of the serving process running, mutable shared variables are not a good way to go. but having a static variable (the model doesn’t change, does it?) is fine. db connection objects, for example, are such shared objects for multi-process flask applications., thanks for the info. i will try the global variable way. my model does not change. you may want to look into gevent: the following explains how gevent can be used to import read-only data structures into global scope, which can be shared across all requests:good point  i think that might make the case for using spark or more of a programmatic approach. i think  has some xml files he wants to load into redshift and the easiest thing is to turn it into csv filesah! it only runs on stata. it is probably too much of an investment in time to get everything setup to run the scripts right now. but i completely agree with you   that with a few tweaks the approach is very exciting for gender prediction by name and place of residence. also, race prediction refers to the typical american ethnic backgrounds, while i am looking to a diverse international community. i am now looking into ethnea hi all, i am trying to find a clustering technique (in python) than can work with very large data set (about one million samples, 10-20 features), small 3-5 number of possibly uneven clusters. any help would be appreciated. i tried dbscan and aghromerative clustering, but i get memory problem, or they are too slow. i don\\'t know if that\\'s a technique problem. we\\'ve used those techniques and others in python on larger data sets just last week. what kind of machine are you running this on?windows pc with 32g ram, intel e5-2620 v4 2.1ghz 2 processors cpuwe\\'re doing it on machines with 64gb ram, but i would think 32gb would be enough with data of that size.is the data super sparse? categorical? you could try pca or downsampling before running the clustering and see if the results are sufficient.i am already using pca to reduce the number of features to half. i\\'ll try downsampling.  thank you alex. my problem is exactly as hoa mentioned. now we find that i can also try to transfer xml to json file and then load josn file to redshift. i am wondering if you know any good python package or spark package for this?is anyone using google cloud dataproc?thanks guys, will check those videos out.i\\'ve done the scatter plot picker thing in a jupyter notebook with plotly and it was very fast and easy!hello everyone - time for the seasonal insight discount to odsc! save 65% on an odsc east pass by registering with code insight. this discount applies to base prices, and is in addition to other promotions on their end. code expires february 23rd. hope to see you there! anyone have adhd or know someone with it working in tech? loved one is struggling with it and trying to find her a mentor/buddydoes anyone have any opinions on  ?or more succinctly the idea that soft skills are  important on teams rather than technical prowessi remember the wapo article.  i’d be curious to know the roles the non-stem background people have.i imagine a mix or technical + humanities is good for software engineers, and that tech companies have a strong need for non technical people that play a super important role in the company’s direction (*cough cough facebook cough cough election*), and can see humanities people being self-taught in code (like 75% of engineers are self-taught in the trade i once read), but the report feels a bit hard to judge w/o knowing what type of roles they were in.people use the word \"humanities\" but i\\'m not exactly clear what that means.  awareness of the situational context.  empathy?or in particular, what does humanities bring to the table? awareness of the cultural context?  and can these skills  be learned without formally studying humanitiesso when those articles were discussed on hackernews (take it with a grain of salt) one point brought up was selection effect: they have already selected people who pass google’s tech screen, which is a high bar, so given that you have that, then it sort of makes sense that also having “soft skills” is useful.that makes sensethat said, i’m biased towards the idea that diverse backgrounds makes sense for teams!i guess they don\\'t screen for soft skills as well then?or just a lower bar so more dynamic rangeprobably harder to measurealso true, consulting companies try to at leasttwitter is trying to, at least when i interviewed theresimilar to technical concepts, yes, but also similar to cs and software engineering, cs grads have a head start, in the same way humanities majors have formal training in this.we try to, but we don’t have a good wayyes, i think it has to do with cultural context. i also think formal training isn\\'t required, but it simplifies the process of gaining exposure to diverse viewpoints.i think it\\'s a little bit backwards, in that places tend to select first for technical skills and second for soft skills. i get why from a \"risk\" perspective, but really, of the two, i think technical skills can be picked up much faster.why do you think this? i taed lots of students who i\\'d never want writing code i\\'d have to maintain, hah.ever ta a complete jerk?you can do code reviews and learn along the way (until your code is accepted), but what is a ‘code review’ for people skills?yeah, much harder to change a personality than your code.i think about technical skills on a \"months\" scale and soft skills on a \"years\" scale.i think it also is an issue of what has been historically valued and therefore what has been measured. we\\'ve come up with metrics for what good code looks like because we think of there being a direct link between that and $; but there hasn\\'t been much incentive to create evaluation metrics for soft skills because the connection between them and $ hasn\\'t been clear. (which in turn might be an issue of where people are looking.......)i personally think that being intentionally kind to others is not really something that is taught in a curriculum in school, but definately a young person can be trained in it via  supportive relationships in their lives.*shrug* my thinking is we\\'re still conditioning on people who already go to top 20 schools and are probably pretty smart. if you\\'re asking how long it takes to teach some random nice person to code well enough to work at google i\\'d say that takes a while.in this case the engineering ability equivalent to \"complete jerk\" would be 60 year old mom who can\\'t use email.that’s well put alan!i also agree with mark somewhat; before teaching i thought “i can teach anyone calculus!“. having tried to do so… i conclude that i can not. :disappointed:you definitely want to screen for both tech and soft skills, but i would argue we, in general, probably put too much weight on the tech stuff. there needs to be some, but it’s not the only thing.yeah, there\\'s definitely an expectation around technical skill. and because it\\'s quickly learnable, there\\'s also an expectation that an engineer has put in the time to learn that stuff, and it\\'s easily measurable. the thing that amuses me is the idea that there\\'s probably some point of diminishing return on technical skill, where the effort is more effectively spent on improving soft skills. an unsolved problem!bwahahahathat actually looks pretty deliciousour newest blog post is up, this time covering reinforcement learning to play battleship. does anyone have suggestions/advice on a trustworthy moving company (or any to avoid)?  this is for a move from pennsylvania to florida.  did my relo, pretty satisfied especially since i wasn\\'t there to supervise. i think if you do the full-service and let them pack, the items are insured for any damages during transport.thanks so much ! i had a very good experience with moishe’s (). they were very competitive money-wise and did a great job with packing, travel, and keeping me updated along the way. they have a location in philly.i’ve used abf’s pods before. it’s just what it says on the tin. does anyone know if pickle will save the standardscaler as well? right now i\\'m doing pickle.dump(clf,  pkl_file), and then classf = pickle.load(pkl_file), test = classf.predict_proba(newdatafeature), do i need to redo standardization?is the standardscaler in the clf object to begin with? if not, then it\\'s not going to be there, if so then it will.if not, then you can still pickle it, but make the object a tuple and do something like:      storeme = (clf, scaler)    pickle.dump(storeme, pkl_file)    newclf, newscaler = pickle.load(pkl_file)thanks!hi all! if your team hires undergrads directly into data roles or has undergrad internships, could you message me or share a link to the internship application? i\\'m speaking a an undergrad women in physics conference next weekend and would love to share stories of opportunities at the undergrad level. thanks!and happy new year! :tada:thanks ! uploaded a file: i am trying to upload a powerpoint presentation online that retains all of its animation features, ( tried googledocs, ispring), any other suggestions?  (includes animations with movement paths)workaround: record  a screencast from your animation, upload it to youtube, upload presentation without animation to slideshare, insert youtube video between slides: reach out to  in the 2015b session — he found a way to do it with some animations he had. waiting to hear from you worked out for me. culturecatch up with progress in commercial ai -  shares info on deep learning in industry. anyone have experience with numenta and the hierarchical temporal memory framework? i\\'m impressed by the results i\\'m seeing from my poc but there are few details about the method. that\\'s a great uploada real great upload. i\\'ve talked to a lot of really smart people, and they\\'ve seen great upload, and they tell me that\\'s a real great upload uploaded a file:  and commented: does anyone have any good resources for (hopefully) a really simple problem. i have the shapefiles of the nyc taxi data and want to convert their given coordinates to gps latitude and longitude in decimal format. nyc tlc does not say anything about the initial format and i\\'m having trouble tracking it down. i\\'d like to do this in python, avoiding postgis or anything unnecessarily complicated. please pm me. thanks!has anyone managed to plot roc curves for multiclass situations where the features were text data?anybody have advice for interviewing a candidate who is to be your supervisor?ask if the person likes to give raises?ask if they’ve had any correspondence with russian ambassadors?or more to the point, ask them about something related, and see if the spontaneously deny that they\\'ve been in touch with any russian ambassadors... commented on ’s file : this looks like it might be ny state-plane coordinates, but i\\'m not sure.  not sure if it\\'s helpful, but you can reproject using pyproj. commented on ’s file : thanks so much! i\\'ll look into it. do you know if these things are typically a linear projection, or is that an unsafe assumption?i don\\'t have a good answer, but probably more \"strategy\" level stuff: what do you think is the most important thing for the data team to accomplish in the next 6 months? how should the team measure if they succeed in doing so?definitely ask about their management stylethanks for the replies. i thought of those kinds of questions as well. it\\'s supposed to be a technical interview though. :/reverse a linked list :+1:oh, technical! good luck!interesting. i wonder if it would be useful for you to pose a real-world problem you genuinely don’t know a technical solution to. if they’re supposed to be your advisor, hopefully they can advise you. think of a typical problem you normally encounter at work and see how effortlessly the interviewee solves it.. supervisors need to be able to trouble shoot situations as well as the people he/she oversees. if the supervisor-to-be has problems solving something that comes up normally, it might be a red-flag. :confused:here’s a relevant hbr article about exactly this: that might also help gauge how well (s)he will be able to judge the quality of _your_ work.  i would recommend plotly instead for interactive plots. its python interface is very convenient and can be used offline. can plotly take user input and replot things on the fly? yes. you can find examples of this from my insight project (offline now... but the source code  is still good). in particular, \"limelight/app/templates/parameter_importance.html\" has an example of  a 10-line javascript for a dropdown to get data from flask and update plot. thanks a bunch i\\'ll take a look!forgive me for using , but i believe this is an issue everyone should be aware, and hopefully already is. president trump signed an executive order banning entry for nationals from seven countries, including people with green cards and dual nationals. many of our fellow fellows are affected by this order. they can no longer enter the country for at least 120 days. this means if they leave the country, for any kind of reason, they cannot come back. here is some more information: please reach out to anyone you know who has been affected by this, and consider donating to the aclu or other organizations.political rant over.there is a protest happening at sfo this afternoon &amp; evening. i’m headed there now! write your senators and congressmen, too! have a look at matplotlibd3 as wellyou know it\\'s just going to get worse from here, right? these 7 countries are just the beginning. soon enough it\\'s going to cover all muslim countries, or worsecall those elected officials too! both local and d.c. offices this amounts to religious discrimination.   aclu has already sued.   best guide to political action from former washington staffers:   nuggetbot has no chill with how politically driven the supreme court is at time, i wouldn’t hold my breath unfortunately.hi everyone! i really appreciate if you would please call/write your congressmen or senators to stop this unamerican and unfair eo. the visa status of thousands of people currently on student visas, opt, work visas, and even those who have green cards depends on this! i’m one of them..: it ties up their resources to defend it.     is about how to influence your members of congress.  fortunately california is generally leading the way but feinstein, harris and eshoo need to know we are behind them.   senator feinstein:  senator harris:  representative eshoo:   just to piggy back off what  said: one of the guys who wrote the indivisible guide said congressional offices love it when their constituents call them to thank them.also call your state representatives as well!if voicemail is full and it frequently is you can send messages on their websites:  kamala harris diane feinstein:  anna eshoo:  : good luck to everyone from muslim countriesthank you!my friend and president of sbu’s grad student organization is being detained at jfk wow that\\'s shittyaclu just tweeted an emergency stay was granted i\\'m assuming in the immigration banremember to stay vigilant! keep track of whether the government actually honors the ruling.they still haven\\'t been allowed to enterjudge blocks part of trump’s immigration order  thanks for raising awareness about this issue. this is directly affecting 4 fellows at the current nyc insight data science program.everyone gets embeddings! an insight ai fellow wrote a post about how to embed any sort of data. let us know what you think:well, on the plus side you have to log out of your computer firsthas anyone played with facebook\\'s prophet library much? looking for any thoughts around how much data cleaning / quality checks / model validation visualizations come out-of-the-box vs extra coding to implement them welli only explored it briefly, but decent evaluation of seasonal components out-of-box plus well-reasoned prediction visualization. for data preprocessing, nothing that i saw (and fbprophet was a little fussy with the input dataframe).  has more experience with it than me.i think he means you show the create table on each table and you add comments about what each column means to people.basically yes. for example if the code that generates your table is in github, point people to that file as the data dictionary  ah got it. thanks!ask them how to measure something related to your company that\\'s hard to measure.hello everyone! does anyone have experience performing data integration with cloudera?i tend to like open ended questions that force candidates to stretch their thinking. i kind of don\\'t care how many python libraries they can bend to their will at this moment--that\\'s totally googleable. it\\'s way more important that they are creative and curious. our code challenge questions tend to be things like, \"tell us everything you can about what\\'s going on in this video. you have two days.\" a scaled down version of something like that is  usually what i shoot for in an interview. i really like the \"how to measure something...\" question.sure, going to go down the measure sectionthanksnobody let me do this, but i wanted to have our code challenge be, \"order these images of cows by descending cuteness. explain your cuteness metric and show how it is calculated.\"hahathat is awesome . i’m definitely going to use that it\\'s near the end of our fiscal year and we have leftover budget for tech, training, conferences, etc. who has suggestions on the conference to go to, the new tool to purchase, the course to attend, etc?are you an insight data engineering alumni who will be in the bay area/palo alto next week? insight is holding a reunion gathering for all de alumni at its palo alto office. let me or someone at insight know if you are interested in attending.‘what is the probability of a platypus?’jsm? it isn’t until july but you could pre-pay the registration/hotel/flights. bonus: it’s in vancouver this year!would there be one for ds alumni? i’m in palo alto, but did remote session, so i am not part of sv alumns (yet?).does anyone have an opinion on what\\'s the best linux-like command line install on windows, i\\'m looking at babun, has anyone used it? like? don\\'t like?windows 10 actually has bash. have you tried that?does it already? i thought it was yet to come.. havent tried it yet, im running windows 7you could install cygwin, but i was never a fan of the earlier versions. maybe it\\'s better?in my experience, it’s not.or just learn to live with powershellgasp!babun is built on cygwin but it\\'s a customized version with a lot of the useful tools pre packaged, &amp; with decent online reviews. i\\'ll try it for now..good luckcash looks interestinghope you like javascript!good gravy… i’d rather dual boot.hehor better yet, single boot.. in linux.yea me too, but not my call.. i feel your pain. i think i\\'ll be running windows too when i startmaybe i can run linux, and virtualize windows for an occasional appi would have a virtual machine running ubuntu within windows and that worked pretty well for me...i was using cygwin at work.  i would have preferred to virtualize linux but some places will lock you out from using hardware acceleration to virtualize.  if you find yourself in a restrictive environment, it\\'s probably best to ask the person sitting next to you what they\\'ve triedthanks. my co-workers have varying solutions, one uses git bash, which seems like a more limited alternative to cygwin or babun.. i\\'ll look into vm\\'s as well, it might be an optionbash for non preview versions of windows 10 will come in august. though this may interest many of you: , data challenge incoming?  group 4 is just here staring at each other :slightly_smiling_face:got it thanks!wrong team… sorry for the noise, anyone reading this…curious what you guys are doingfirst data challenge of insight!  i can forward you the email if you’d like?the answer is 4.haha, and they’re giving us 5 hours to figure that out! by all means!: are you going to get into it?found this on product hunt today, a way to “generate realistic data for your app”how progressive uses machine learning:   nuggetbot check on kenny_kwankenny_kwan has a happiness average of 0.5287356321839081that sounds about right :neutral_face:nuggetbot check on romanoromano has a happiness average of -0.36what?nuggetbot what?stupid nuggetbotnuggetbot check on mhoovermhoover has a happiness average of 1.8148148148148149: nice!nuggetbot check on kumbhanikumbhani has a happiness average of 0.8666666666666667so irrational:stuck_out_tongue: nuggetbot check on lenkatilkalenkatilka has no happiness average yethahaha nuggetbot check on pushkarinipushkarini has a happiness average of 1.1578947368421053all because of your support : i guess i am not slacking enough :smile: (pun intended!)thanks for all the previous help, but i have a new issue. my website has an embedded folium map which is rendered from an html. i see that after about five minutes that my app starts to reload old maps instead of rendering new maps, as if somehow the html is being cached. this is absolutely not something that i have implemented (and it is not something that happens on my local machine). the application is running on port 80 using gunicorn with supervisord on aws. i see that if i stop supervisord and kill all gunicorns and then restart it then it fixes this issue, but stopping and starting the application every five minutes is probably not viable. any suggestions?:  not knowing much about your app, my first line of debugging would be to explore the network tab on your browser’s developer tools. you can see the parameters that are passed with each request, as well as what is returned. see if anything looks different between your aws and local versions.no issue with the parameters that are passed. the address bar looks fine. it just loads the wrong map (seemingly at random from whatever was entered in the first 5-10 minutes that supervisor was running)also in the first 5-10 minutes it runs finereturns the right address bar parameters and the correct mapwhat i’d be most interested in are the http status codes of the responses (i.e. if you’re getting 304 wrongly). that’s the sort of thing that could cause weird caching bugs, and which might differ from one server to another.though on second thought, it’s probably getter to look at logs on the server side first. i just don’t remember how gunicorn logging works.i just refreshed ityou can play with itstay positive mamdayi’m taking a look…if nothing else succeeds, you can always set up a cron job to restart gunicorn every 5 minutesyeah, but having the web site constantly restarting and potentially throwing errors all the time could also be bad. better than having it return nonsense, but still not great.ah, for some reason i was thinking your app was refreshing the map via ajax. now that i see it isn’t, the network panel is certainly not going to be too enlightening.yeah. it\\'s foliumwell, now i just get \"this site can’t be reached.” i guess i broke it.hmmah. i think maybe that was the cron job. thought i turned that off...clearly still debugging that option...odd. it seems to cycle through stale results when i refresh.to me it seems like something is caching the map (for which the name does not change. it is always osm.html)i thought about assigning a large random number as part of the name of the map to see if it might fix itstring interpolate the datetime to the second?so i’m not that familiar with folium, but looking at the html source, i notice that each map has a unique identifier (something like map_d15934a15f6445b6b30a50e24bc94e20), but when you get the stale map, you’re also getting a stale identifier.yeah. and when the user clicks, it produces the correct map in the directory where i expect on the remote instance and it has the correct identifierbut the page does not load itand the map it is loading does not exist on the remote instancei feel there has to be some other storage that either supervisor, gunicorn or aws usessoumyadsanyal: i think i do not understand your advice?i still don’t really understand folium, but i would be inclined to vary the name of the map depending on the parameters, i.e. instead of  `map.save(‘osm.html’)` something like `map.save(‘osm-[hash]’)`, where `[hash]` depends on the address, the date, and the cost entered by the user.yeah. i think you are probably right. thanks :slightly_smiling_face:anyone know why a column declared as \\'datetime\\' aws redshift is displayed as datetime structure instead of a string ? for ex, (datetime.datetime(2016, 6, 30, 0, 19, 3), 94, 1541, 12382, 6770, 9) instead of displaying time as \"2016-6-30 0:19:3\"found a sweet little book on becoming a quant in the hackernews comments. the brain teasers are solid. :thumbsup:that is a good point. no one here is going to have preferred stock.side-note: i didn’t know sfers called it “the city”. has that always been the case or is that as a result of lots of east-coast transplants? it was already the case 15 years ago when i first moved there (and it annoyed me then)…my guess is it\\'s been that way for a while.good to know :wink:in an effort to improve data flow across the company, we are trying to integrate our testing procedures during manufacture with our general inventory tracking. does anyone have any suggestions of off the shelf software that would work for this? have you build your own systems? at this point anything is better than the scattered spreadsheets we now use, but i’d like to be systematic about it.hi all!a friend of mine at a security analytics startup () is looking for a senior data scientist. you would be their second data science hire, and would be involved in growing the team and building the heart of their product from the ground up. i consulted for this company for my insight project. they’re tackling some very interesting challenges and have a great team.link to job posting: if you’re interested, please apply and let me know! i’ll help to make sure your application gets noticed. i’m sure there are a lot of enterprise-level (i.e. expensive) solutions to this problem, but i’d think it breaks down more to the data input.a hack i’ve seen used several times is standardizing a google sheet as input and pulling in data from there (there are python libraries for that). but you could also build custom apps for data input that output csv or json or something else that can be more easily shoved into a data warehouse (i.e. redshift) thanks. that’s what we’re leaning towards for the testing. the inventory tracking is a pain, though!totally random question, but is there a chem or biochem person out there that could help me with a quick question? if so, pm me. thanks!many thanks to the awesomely talented  for help. :clap::+1: i know some biochem and if your question is too advanced i can also refer you to some chemiststhanks  -- i\\'m all set at this point, but appreciate the shout.any recovering biochemists want to do some enzyme kinetics assays just for old time\\'s sake? makes some great diagrams in this post on time-series modeling. good read, too. anybody going to database camp (free whole day db event) in the un on july 10? database.campthis weekend in san jose: hi insight alumni,i’m looking for a panelist for the data science panel at the beyond academia conference at uc berkeley.the conference attendees are all graduate students interested in transitioning to non-academic careers. your job would be to speak about your experience transitioning into your career. joel swenson, the insight health data science lead at the san francisco office will help moderate the conversation with five other data scientists.please send me a pm on slack if you’re interested in being a panelist. happy to answer any questions. thanks!more info: when: march 2nd, 2018where: uc berkeley campustime: 1:30 - 3:00pm with short reception to followdear insight alums,i\\'m currently in the jan/feb remote program, and i\\'m interested in connecting with people who have gone the leadership/management/product manager route. if you, or if you know other phds who have ended in up in that type of position, please let me know, i\\'d really love a chance to connect and hear about your experiences going that direction! thanks!!hey ml folks, i have a terminology question. say i have a logistic regression model `f: x -&gt; [0,1]` and a sample of labelled instances `{(x_i in x, y_i in {0,1})}`. so the number of positives in the sample is the sum of the `y_i`, whereas the expected number of positives in the sample according to the model is the sum of the `f(x_i)`. what do you call the ratio `sum(f(x_i))/sum(y_i)`? (does it have a standard name?)sounds like it’s the inverse of ppvor inverse of precision, equivalentlyi would call it the inverse of observed over expected?or are you leaving the outputs of `f(x_i)` as the continuous value while `y_i` are left as binary?yeah, i’m leaving the `f(x_i)` as is — not assigning binary labels ooh, ok scratch what i’d said — inverse ppv would be if `f(x_i)` is then mapped to a binary. i would too, i was just hoping there was a standard shorter term for it :slightly_smiling_face:yet another job alert (for senior candidates): a friend of mine is looking to hire a phd data scientist with at least a couple years of industry experience at the intersection of publishing and healthcare with experience in nlp and sufficient experience, and interest, in leading a team (i.e., developing a vision for the team, providing guidance to more junior team members). based in either new york city, philadelphia, or even amsterdam (nl).i cannot disclose the company (rulz!, but you can guess based on area + location), i think they are doing really interesting work and are, as a company, pivoting towards a \"platform company\" (distribution) driven by machine learning algorithms (e.g., recommendation systems, lots and lots of nlp work). pm me if interested! i would probably call it accuracy ( # correctly classified instances / # relevant instances, e.g.,  1s ). however, i would not use this measure since due to the fitting of the logistic regression, one would have the sum of f(x_i) to be as close as possible to sum(y_i), without regard to how well the classification performed.i don\\'t think that\\'s quite right, if i\\'m reading the parentheses correctly -- it\\'s the ratio of the number of predicted positives to the number of actual positives, without regard to whether they were correctly predicted or not.y = [1,1,1,0,0,0], f(x) = [0,0,0,1,1,1], that metric does poorly on this, no?# correctly classified instances = tp + tn# relevant instances = pf(x_i) is a probability, so this is before the threshold is applied. so one can’t really talk about “predicted positives”, i think.^ right. i’m thinking of `sum(f(x_i))` as the “expected number” of positives (in the sense of a probability distribution) rather than the number of “expected positives”and yeah there are definitely cases where this metric could be misleadinglooking at the example given in the wikipedia page (), one sees that `sum(y_i)=10`, while `sum(f(x_i))=9.9997`as you can see, this metric will give a ratio very close to 1 because the model is fitted to do that.i think in practice, `sum(f(x_i))` and `sum(y_i)` will be very close to one another on the training set. this is because the intercept term will adjust so that the base probability of the classifier will match the base probability in the training data. on the test set, i suppose the two could diverge, but i think there are other metrics that would be much better.echoing , here’s a simple sklearn example:```from sklearn import datasetsfrom sklearn.linear_model import logisticregression# create datax, y = datasets.make_classification(n_samples=10000, n_features=20,                                    n_informative=2, n_redundant=10,                                    random_state=42, weights=[0.25, 0.75])# fit on entire datasetclf = logisticregression(c=1)clf.fit(x, y)# metricsprint(‘sum of probabilities’, clf.predict_proba(x)[:, 1].sum())print(‘sum of observations’, y.sum())```i get very similar results regardless of the weights (the base probability)also perhaps worth noting that as `c`goes down (the regularization strength increases), the two actually do divergedown or up? i’m probably confused`c` is the inverse of the regularization strength. from the docs ```inverse of regularization strength; must be a positive float. like in support vector machines, smaller values specify stronger regularization.```so it’s `loss function + (param vector norm)/c` ?no c multiplies the loss functionoh i see confused by how to interpret \\'strength\\'has anyone run a jupyter notebook on google compute engine?anyone familiar with some rules for choosing #bin /width for visualizing big data? looking at freedman diaconis i\\'ve liked:  for bin sizesthanks!here\\'s an excellent blog post detailing it (and giving a nice example of dynamic programming as well): i\\'m an \"alumna\" now! whoo!🤣hello everyone! my team at athenahealth in boston is hiring for a new senior product analytics position! if you\\'re interested in healthcare, want to work for a fun company, and do a wide range of analytics, please dm me for more info!thank you everyone for the awesome advice! :slightly_smiling_face:hello everyone,my name is hyesoo and i\\'m a recent insight health fellow who received a phd from uc berkeley. at uc berkeley, i also worked as one of the organizers/officers at gdso (graduate data science organization) and they are currently looking for data scientists who would volunteer for mentoring in the data science workshop this summer (previously known as cdips workshop).it is a great chance to meet other data scientists, and also help the future data scientists gaining more experience. you can contact gdso () if you are interested in mentoring the phd / postdocs who are interested in data science.below is the email they sent to the previous mentors:the graduate data science organization (gdso) at uc berkeley has taken over the planning of the annual data science workshop, previously organized by cdips. we\\'re preparing for the sixth annual workshop, and we are seeking mentors. we\\'re reaching out to you because you served as a mentor for last year\\'s workshop and hope that you may be interested in serving as a mentor once again.as a reminder, the workshop is a three-week opportunity for berkeley graduate students and postdocs to work on data science projects under the guidance of a working data scientist. mentors can take on varying levels of commitment. at a minimum, mentors should check in with their team twice per week (either virtually or face-to-face). the only fixed commitment on the part of the mentors is attendance at the opening and closing ceremonies, which will be held on july 21 and august 11 at uc berkeley.if you\\'d like to serve as a mentor, please contact the workshop organizers at . more information about the workshop and the gdso can be found at .best,the gdso data science workshop organizershas anyone had problems using spark-submit w/ the --packages flag on emr? i expected it to \"just work\". it downloads the jars correctly, but they don\\'t seem to be on my classpathone thing where i’ve seen rnns beat cnn/bow is sentiment analysis, because syntactic order is so important to understand the sentiment polarity of a sentence.does anyone else out there work for a b2b data-as-a-service company? i’d love to connect and hear an outside perspective on the tradeoffs between model accuracy and data continuity, and how other companies have dealt with it.that’s awesome, thanks !has anyone systematically gone through and downloaded bureau of labor statistics data by metro area?glad i could helpglad i could help.respiratory issues tend to have low numbers so you either have to sacrifice geographical precision or temporal precision to get any significant values. in my case i looked at the yearly numbers since i was only interested in the long-term effects within a geographical location.a good idea could be to study the opioid crisis. there has been some projects around this subject that you could use as inspiration.“all of this advice can be implemented starting now, so that your company doesn’t miss out on the many benefits that come with a diverse environment where everyone — from underrepresented minorities to introverts to parents — feels truly included.”hear hear! i turned it off ages ago, taking control of my own happiness! :wink:hi all, does anyone here have any experience implementing teacher forcing for lstm in pytorch?hi , i have. what’s up?yeah i saw these thingsits some real passive-aggressive bullshitstay positive mpanciathey did it and then they are apologizing for it. kinda seems stupidstay positive ashrithnuggetbot calmdownevery dollar they spend on their bs ad campaigns is a dollar they could be donating to public housing projects or somethingyep, agreed.for all those who got jipped by coin the first time. here is some good -&gt; thanks are in order?talking about jipped. for all those who never got misfit shine/flash to work -&gt; i would have loved to sit in on the design meetings for those airbnb ads: \"we should put up some ads to make people aware of what we contribute back to the city!\" \"great idea, and they should be passive aggressive, sf hipsters love that!\" \"perfect! you really get how the kids think these days!\" :wink:dear airbnb, welcome to society.love,societyis anyone else going to the mit sloan sports analytics conference in boston this weekend?  i’m going and would love to meet up with any other insight fellows there.how buffer() evolved their pipeline: a blog post about predicting caltrain wait-times that i\\'m working on at svds : so cool, though i am partially biased because my insight project was about trying to predict delays (really ‘bunching’ as everybody in my session has heard 1000 times) in the muni systemthe point on sensor placement near burlingame is interesting; at first i thought it had to do with the bridge replacement in that area...`bunching`bunching as a servicebaaswe need not-bunching as a service:  was that attempt (muni only): yan will be there!does anyone have recommendations for good time series forecasting resources (i.e., books or presentations)?: time_series_analysis__with_applications_in_r-springer 2008 free pdf download.  i\\'m working through it now.other free pdf downloads:  time_series_analysis_and_its_applications_with_r_examplestime_series_analysis_with_rthanks don\\'t mention it.  :simple_smile:free one day training workshop on neo4j and graph databases:  i never knew multithreading and multiprocessing in python could be so easy:  now you knowif anyone has an opinion about fisa see if you can contribute : are you thinking of going to the geoweb summit?  if we get one more, tix are even cheaper :slightly_smiling_face:: sadly, i won’t be able to go this afternoon. but i understand why you’re interested. :wink::  perhaps you’d like to go with  ?: looks great but it’s a bit too short notice for me. enjoy!has anyone had weird pip installation issues on a mac where packages are installed in /library/python/2.7 instead of /usr/local/bin ? how can i fix this? thanks!i\\'m wondering if it\\'s related to homebrew path issues but i\\'m not sure: i haven\\'t come across this problem, but i\\'d recommend the anaconda distribution -- it plays nice w/ homebrew in my experience: you might want to check your ```$pythonpath``` and this might be one of the ways you could install things in local ```pip install —target=/usr/local/bin package_name```: just to add to the chorus, it sounds like you didn’t install python via homebrew (or your homebrew installation is busted, likely with a missing path change)even with another package manager like anaconda, you could run into similar issues, so it’s always good to check that you’re using the correct python/pip that you intend to. ``` ls -lh `which python` ``` and ``` ls -lh `which pip` ``` is a good diagnostic to check your setup is correcthad a bit of fun with tensor flow trying to machine learn what neuroscience sounds like. long story short, here\\'s a dnn neuroscience title generator :smile: thanks everyone! i will look into all these options! i\\'m pretty sure i had no clue what i was doing when i installed python/homebrew: \"amplingle-dependent define sexually behavior.\": \"experience of sexually bindyrin intraoxytics modulation of the action enhances taste inflammation.\"another favorite: \"motility and implicit information enhances orbitofrontal ischemic sex.\"could press refresh on that site all day. \"conditional nucleus in the cat/choice.\" is my favorite so farnicely done!does any1 know max download speed for ec2 downloading stuff from s3 in the same region?    only has upload :xit depends on your instance type and the networking mode enableddefinitely interested, thanks for adding me. work downtown. commute to bki work in union square and live in downtown brooklynawesome! i’m chasing several issues at work today so will be some latency in responding... but what do you think of meeting at insight for monthly meetings? seems like it would split the difference between downtown  / midtown and have reasonable office space to host us.if any other ideas (or office space you know of that we can borrow) please share!so the channel name is ml-meetup-cafeel free to joinwho’s going to be at mlconf tomorrow?just yesterday my boss recommended me a tiny book by paul d. allison ‘missing data’ (with some pessimistic view on the field in general :smiley: )must admit i haven’t started reading it yethi all, i was recently laid off at time inc. if any of your organizations are hiring data scientists in the greater ny-nj (or otherwise) area please ping me. i appreciate it.hey everyone, we’re excited to announce the launch of our new website! check it out!   hi fellows! i would like to do time series modeling using many variables (features),  anyone knows about good tutorial/book or links to do that using python?thanks: you can change the weights of the edges or remove them as neededthanks everyone, i think the insight collective intelligence has lead me to a method for approaching this.anyone have experience using tpot to automate model selection and hyperparameter optimization? i’m just curious about how well it works., you may have your answer already, but 2 people in my session also did route finding with constraints.  or  might be able to point you to useful tools: this is not helpful, however your problem did remind me about microsoft’s “avoid ghetto” patent () that drummed up some news a while back. damn it, i guess i\\'m headed back to the drawing board.stay positive ddelosshi everyone! i am trying to combine 3 databases on mysql to be one database combines all the tables from the 3 databases, anyone knows how to do that?: you can export 2 of them with mysqldump and then use those files to create the tables and data in the third database: can you simply run: `create table newdb.table1 like olddb.table1; insert into newdb.table1 select * from olddb.table1;`? this is a bit faster than `mysqldump`, but slower than `load data local infile`. this also assumes that both databases are on the same server.if they aren’t, i would certainly try to use `load data local infile` for speed.thanks for getting back to me  and . another simple question, how to access  3 databases at the same time? because what i do, is use database ;  then i access the tablewhen i do that, i can’t access the other tables from the other databases: `use database` should not prevent you from reading/writing to other databases on the same server. you just have to specify db-name.tbl-name when accessing tables in a different database.ok.. that make sense :slightly_smiling_face:thanks  !hi all, i’m working with 3d accelerometer data to categorize types of motions from a wearable. i’ve got a lot of avenues i’m considering, but i don’t know which ones are worthwhile. does anyone have experience with this sort of problem? thanks!: that sounds a lot like ’s project from the january 2016 session.cool! i’ll look into it. thanks  !: here is her blog post about the project: oh this is perfect! \"engineering features from accelerometer data\"would anyone have any knowledge on how to go about funding a non-profit project ? would love to buy you coffee and have a chat :slightly_smiling_face:numpy/scipy tips for matlab users cheatsheet: insight health data science - boston 2015 session: just saw a billboard from google:\"now needs data scientistsnext just needs you\"uh oh...incoming jobs where we fix all the mistakes people make by using bad data.e.g. a company that goes and corrects everything on 538wow. i might actually find a use for a windows machine. :simple_smile:hrmshi there! does anyone have a good practical tutorial of using hive and pyspark? i will have an interview next week including working with a data set using hive and pyspark, which i’ve never used before. thank you!i don\\'t know of a good tutorial (tell me if you find one) but w/ spark stuff, i always start with the examples dir they have on their github.if you haven\\'t used pyspark much, this might be helpful. this pretty accurately summarizes me trying to refactor code when i haven’t thought it through….repartition(10000)i highly recommend checking out the community edition from databricks.  its free and they let you work with spark on a baby cluster they launch for you and there are tons of tutorials built in. was useful for me when i was learning pyspark ()someone cracked the code: oh no, i am getting 404 error when i click! :disappointed:that didn\\'t last long... here\\'s a google cache: stakeholders is my favorite! :octagonal_sign:thank you!   user research: we figured out a way to finally get paid for our ph.ds.does anyone have any recommendations for great data science/machine learning conferences?kdd, nips?odsc, pydata, strata data conferenceanyone have experience with using dbscan for outlier detection? i\\'m digging the netflix tech blog post from july 2015 but would love to hear about any real-world application    ^i think   worked with dbscanyes!! thanks   :smiley_cat: are you interested?late to the party, but this link is awesome for a/b testing: is anyone familiar with d3.js? i have a working scatter plot, and am literally trying to add a line using canvas, but it frustratingly doesn\\'t display.for my insight project, i\\'m using hdbscan (a hierarchical dbscan) to do outlier detection on medicare insurance claims -- www.fraudhacker.sitealso happy to answer any questions about either my specific application or the clustering method!does anyone use airflow for “applying” their ml models? i am structuring the process as load data, cleaning/transformation, loading trained model, predicting, outputting in a single .py file. i am wondering if there is a way to within airflow assign each chunk of the code as a task, without having to break each of these processes into it’s own individual .py file. not totally sure i understand, but yes, you should have different files to process different tasksis the discount restricted only to former insight fellows, or is anyone welcome to use the discount code?anyone can use it. please feel free to share with anyone in your network.thanks!does anyone have a recommendation on a good interactive vi tutorial?theano will no longer be maintained after the next yearyeah, a little sad`vimtutor` is probably the best interactive way to start with vim. if you want a short intro this should work fine ().otherwise, practice is the keymxnet!thanks!for all you fellows starting interviews soon:does anyone know of a way to automatically reply to texts with an “away on vacation” type message on iphones? there doesn’t seem to be any option/third party app for this!#insightproject :wink:omg yeah- i guess a data engineering thing. apparently apple doesn’t want to let any third party apps do this for security reasons. wankershey all! i have a marketing funnel i\\'m working with right now to determine consumer purchase intent (classification problem). are there suggestions for models for predicting classes that are in hierarchical structures? thanks!just discovered the most useful function in matplotlib: anyone have experience with unsupervised learning on variable length input?this article is a nice encapsulation of what insight drummed into me when they said projects couldn\\'t be \"data exploration\". the simons institute for the theory of computing is hosting a series of workshops next year on “foundations of data science.” if you are interested, i recommend signing up for the mailing list to stay updated. hi all - has anyone here ever tried using kalman filter for missing data?can someone write a simple slack scraper or bot that has access to  and stores a message history to a database or external webpage or something? i hate when i see useful conversation get deleted over time.usually people ahve to pay for that. but you could probably make one pretty easily with crapyer scrapy: i\\'m trying to mess with the slack api but it\\'s currently \"disabled by admin\" for the insight slack. is that something that\\'s free to enable?cool idea!need to sell movie tickets? maybe you should round your ratings in a really bad way! stay positive alex_gudedoes anyone know of a good implementation or blogpost of “attention” neural networks in keras?have you seen this one? have you seen the \"counting your customers\" paper or pareto /nbd models? there\\'s also a package in python called lifelines that follows that methodology to create a probability that a user is alive given a record of transactions/events over timefb data analysis on the dress hey everyone,i had a great conversation with an excellent nyu physics undergrad, chris ick, who has been doing some interesting stats and ml with prof. david hogg. chris describes himself thusly:```data science and astrophysics researcher at nyu. recently graduated with a bachelors in science in physics, with extensive programming and technical problem solving background. research interests include computational data analysis, data visualization, and bayesian analysis. currently seeking work and education in data-oriented programs and positions, and furthering my understanding of data science and it\\'s methodologies.```i felt like i couldn’t give him the most relevant advice on the current data-science job market and how best to set himself up for a career going forward. i have been off the market myself and we haven’t been hiring data folks here at greenhouse (yet) so i don’t have a sense of the landscape. would any of you perhaps be willing to follow up with chris ()? pm me for his cv if you’d like.hey  sounds like he could be a good fit for the insight data engineering program. we recently had someone with a very similar profile go through the program and is now working at bcg. has he graduated yet? that’d be the only caveat otherwise, i think he might consider applying for the de program.yep, he graduated last may, . would you mind reaching out to him about de?hi all, my team in google is hiring for a data scientist in mtv. dm me if anyone is interested in the role. hi all,  i am 3 weeks into my career as a machine learning engineer (aka data scientist) and have to put together a wish list for conferences i would like to attend in 2018.  can anyone suggest which conferences i should shoot for?  there is a finite amount of funds, so proximity to nyc is a plus.hi all. can anyone recommend a toolset for analyzing large datasets? so far i\\'ve been using pandas + scikit-learn for all of my ds work, but i recently started analyzing a &gt;4gb dataset that is resulting in \"out of memory\" errors. i was considering giving sparkml on a cluster a try, but i\\'m curious to hear your thoughts and experience. could try dask as a drop-in replacement for the pandas componentsdepending on exactly how big your dataset is, you might be able to get away with spinning up an aws ec2 or google compute engine instance with more memory, and working with your usual toolkit therethanks, will take a look at that!that also sounds like a good idea!yep -- would recommend bigger box, same tools as an intermediate stepit might not get you too far, but pandas has features like sparse data frames that are more memory efficient, too. depending on your dataset you might be able to just use a little workaround like thathi everyone -- current fellow doing nlp on a large dataset (200k entries) with a small subset (1k) of multi-class, multi-labeled ‘ground truth’ annotations. has anyone done semi-supervised learning on multilabel data? finding a fair amount of research articles but not a lot of python implementations.some great tips here tl;dr: downcast numeric types to save memory, avoid string-type columnsi haven’t used it for text data, but scikit-learn has some semi-supervised methods. you could pass your text data through `countvectorizer` or tf-idf as input into one of these models?i’m a fellow in this session, and i used tfidf as input for a naive bayes classifier to predict sentence classes (dog, shelter, neither).thanks -- i\\'ve been using countvectorizer to construct the feature matrix but the specific issue i\\'m having is that my classes are multilabel and thus don\\'t fit into a standard column vector for \\'y\\'scikit-multilearn has some implementations for supervised learning on multilabel data but i\\'m not seeing semi-supervised approaches in that packageoh, i misunderstood. for that i’ve never tried semi-supervised methods. i’ve only ever done what has multi-label support or neural networksyeah, i\\'ve investigated some supervised classification algorithms with multilabel support but training on 1k samples and extrapolating to 200k seems like i\\'m leaving some information behind. i may try an iterative classification approachdoes anyone have any advice (or things to avoid) for choosing a data warehouse?i also created a resource dealing with nlp classification in general that could be helpful: advice 1: don\\'t tell the engineers they have to build a data warehousethanks, i\\'ll take a lookhi all. currently for my project i\\'m trying to merge aggregate level data (percentile numbers per occupation/county) with lots of missing with individual level data (actual values per person, with occupation and lat/long) that is very sparse on the county/occupation level.  trying to form a complete pictures with very \\'holey\\' data so to speak. i\\'m doing so with imputation and then training a random forest, does anyone have an idea for an approach (that would be reasonable to implement within the timeframe).hi all! anyone have experience using plotly or folium for bubble maps? do you recommend using one package over the other? or some other package? relatedly, i am using geopy to calculate lat and long for us and international cities and it is very slow. i can run up to about 200 rows, when it is larger it times out before finishing. any insight into why it is so slow? thanks in advance for any input!: i’m working on a multilabel classification problem as well. what i ended up doing is using `multilabelbinarizer()` for transforming labels to binary values and then used excellent  ’s resource.sparkbeyond is hiring data scientists. i think they have remote positions for their singapore office. some thoughts for interviews: : we use tableau at glassdoor, moving all our internal reporting there. i like it sometimes, hate it other times.with regard to connecting to data in many forms i find it works very well.  it\\'s also easy to make some nice visualizations, especially if it is simple and you don\\'t have a specific vision. but some specific things require too much hackery. aspects i don\\'t like which are mostly relevant to large projects:1) no way to document the data2) no solid revision history like you have with scripts in revision control -- i\\'ve made changes that broke sheets that i wasn\\'t looking at, didn\\'t realize they were broken until later, and found it very hard to tell how far back i need to undo even if i know exactly what you did.3) so much clicking and dragging and it is trying to update every time you make a change. a colleague was excited about looker but i haven\\'t used it with any seriousness.thanks, alan! my tableau experience sounds similar to yours. if it is straight-forward query-to-chart it is relatively easy and quick, otherwise it gets murky fast.you can turn off the auto-refresh if you are building up some complicated queries and viewsyeah, i should do that more oftenit is easy to forget to do it until it is too lateis there an easy way to convert unix epoch time stamps into regular date timestamps in redshift postgres?i\\'ve found:dateadd(s,somedatetime, \\'1970-01-01\\') as xand also some other worse ways to do it.to_timestamp() does not seem to work in redshift...the regular, ugly, solution seems to beconvert_timezone(\\'us/pacific\\', (timestamp \\'epoch\\' + somedatetime * interval \\'1 second \\'))::datetimecoconut is clearly the worst, but berry at #8! clearly their methodology is wrong and bad!stay positive alex_gudenice to know that cherry lime made the top 5: coconut #20 - guess everyone agreesweird, since coconut always runs out the fastest at my officecoconut is literally the worst.stay positive till_bereally need to rework nuggetbot’s trigger for thatstay positive johnrwalkyou’re not my supervisori’m surprised ‘not’ doesn’t set off nuggetbotthat and typing in all capsfor my project i\\'m going to to sentiment analysis on patients reports of medication preference. does anyone have any advice about sentiment analysis? #sentimentanalysis    hey all, meetup is looking for someone to do contract data viz in jan, possibly longer. we’re also growing out our ds, ml, and de teams. reach out if interested.a colleague at dia&amp;co intends to spend 3 months (jan to march, exact dates flexible) out at our la office and is looking to sublet her quiet and sunny one-bedroom apartment in brooklyn (bay ridge, subway r at bay ridge ave and n at 59th) during that period. if you are interested or know anyone who is dm me and i\\'ll connect you!i vouch for that apartment!kind of the opposite of my hep days: dry, yes, but packed with as many figures as possible. no time for context! let the convener and the one other person paying attention in your presentation interpret the plots as needed.haha, love itthanks  — we ended up asking questions quite similar to that, and they were revealing!a former coworker of mine is looking for data science help for his startup, avvir ()they are working on automating 3d architecture modeling and detecting real life variations from plans using laser scanning drone generated point clouds. sort of a mix of computer vision and anomaly detection.if anyone is interested in hearing more, send me a message please.sounds really boring and not fun at allmy session may or may not have had a team picture maps where everyone used folium :partyparrot:aaaaaand another former coworker is looking for a data scientist:if anyone is interested in the above, send me a message and i\\'ll intro you.score, thanks! most things i was finding for audio were music-focused, so this is great to find!: hey y\\'all!   another plea from me to sign up for alumni mentoring.  we\\'re in mock interview stage starting today.nyc needs a ton of support as we don\\'t have a large alumni base like sv.  take a look at weeks 5-7: :beer::beer::beer::beer::dart: :smile: imo the pgm course on coursera isn\\'t great. she uses some weird software to visualize/interact with her models.: i like this chapter from bishop for pgm: thanks  !  i completely forgot that bishop has a chapter on them! this solves it.538 put together a really cool app for the elections that lets you play with some of the underlying assumptions of their model and see how it changes the output! this was from a recent meetup in philadelphia and some really cool and impressive work at the command line to reduce work burden. hey, does anyone know any good, publicly-available twitter datasets? i have a friend who wants to poke around with one.: the sentiment140 corpus is available for download here: (1.6 million tweets tagged for sentiment)a friend…. riiiiiiiight+1 for  :slightly_smiling_face:  has a good point though.  i think the issue was that the scope of the conference is broad enough that it doesn’t really know what it wants to be, so the talks are all over the place.  moreover, this means some of the talks have to focus at a vendor/business level rather than being technical, but it seemed people were leery of tagging their talks as “beginner” or “nontechnical” so there were a lot of ostensibly intermediate/technical talks that were super basici didn’t go to this year’s odsc west conference, but the one last year was pretty good. at least it was much better than strata which, when i went two years ago, was a lot like   describes in boston: too general.welcome to all new fellows starting insight today!!!woot woot!any tips on building a collaborative data science tech environment? for the past year, i’ve been the only formal data scientist at the company, so my work has been largely quarantined in my own repo. now that we’re beginning to grow out the team (including remotely), is there any experience/wisdom/tools/best practices you can share to scale out wisely, promote collaboration, and minimize headaches down the road? any advice is appreciated!document your code extensively? *hides*use version control, write automated tests for everything you\\'d check adhoc, use continuous integration. do pair programming and code reviews.enforce style guidelines of some sort, refactor scripts into classes, export code from notebooks into e.g. python packages hosted as github repos, provide readmes, confluence docs, etc.you can run a linter as part of your automated tests to enforce style. but don\\'t try to do all this for all exploratory work necessarily.data dictionaries :slightly_smiling_face:thanks  ... those do look interesting.nice...i think i\\'ll probably be going to the ml conf too!i just started a  channel and re-posted these links there. maybe attendees could post their highlights there eventually too: feel free to start a channel for your session and invite members. private groups are also an option.nuggetbot: weatherweather for new york, ny, usacurrently: mostly cloudy -1.3°c/30°ftoday: mostly cloudy until this evening.coming week: light rain on sunday and monday, with temperatures bottoming out at -1°c/30°f on saturday.nuggetbot: weather san franciscoweather for san francisco, ca, usacurrently: partly cloudy 12.3°c/54°ftoday: partly cloudy until tomorrow morning.coming week: no precipitation throughout the week, with temperatures rising to 17°c/63°f on monday.nuggetbot: nuggetbotnopewhat exactly were you trying to do there joshainsley ?get meta with nuggetbotthe nuggets have nuggets!we can add functionality to nuggetbot from here: nuggetbot: pug bomb 2i think this is fantasticjonathan_whitmore: hope we all find it useful!nuggetbot: animate me welcome everyonethe is hereit\\'s happening.nuggetbot: image me jake klamkahaha nuggetbot\\'s the manjust arrived in nyc and ate a ton of pizza to celebrate - see you friday, andrew set the channel topic: :chart_with_upwards_trend: btw, thx for the alumni panel visit yesterdaysame to you  it was a lot of funnuggetbot: image me data sciencecould it be any other image?nuggetbot: image me insight data sciencehahanicenuggetbot: image me data engineeringhahaha lolthanks , i had a fun time and i hope it was useful to the current fellows. sorry i had to jet at 3, but i just barely made the train back to work.see you fridayok back in the fold after a loooooong time awayhi folks, is there a way to know in which way the feature is important in random forest? such as positively correlated or negatively correlated to the predicted value? xiwang: when i was looking into random forests, i came across this package that seems to interpret feature contributions for sklearn’s decision trees/random forest models: ^ i second using ‘treeinterpreter’.xgboost has a model.booster().get_fscore() function to look at feature importancei\\'ve used that. the f-score is just the number of times that a feature was split on in the tree; i\\'d consider that one way to measure importance, but possibly incomplete. you might have an extremely important feature that does its whole job by splitting once, for exampleone way would be in crude bootstrapping; assess importance by removing features and then ranking importance by assessing the difference in model scores based on a validation set. that\\'d be super time-intensive, though. probably better ways than that ...i don’t use forests a lot, so i’m not an expert, but my understanding is that the fscore is the number of splits in a tree _weighted by the number of observations going through that node_. so *if* our model splits on more senstive features first, it would capture the case of a very important variable that only requires one split. is that “if” satisfied in xgboost? i’m not surei don\\'t think so? wow~ so many replies. thank you, guys! i\\'ll take a deeper look. and get back to you!so there are variable importances (also known as vimp) and things like “minimal depth” for a variable. they don’t give you direction though, just importance. direction would require further tackling, as there could be multiple split points for the variable, with influence direction changing, i.e. if there is a parabolic relationship between the features and target variable. thank you! this is what i thought because each time the split might happen in different ways.if i remember right, randomforest in r has cool ways of visualizing the “effect” of the variable to see if it is linear, nonlinear, etc.here’s a beaut of a solution using ggplot2 :smiley: this sounds cool! will definitely try it! thanks!here is a great event about building your personal brand, from some friends at quora: hi guys, i had an aws instance running for three weeks. i could access my web app all that while. then suddenly two days back my app (webpage) wouldn\\'t load. so i killed all processes and reran supervisor. and it started working again! then today, the same thing happened again (page won\\'t load). i log in to aws and see my instance is running (no errors, alarms). so i\\'m not sure what\\'s wrong and what to even google for to resolve this issue. any help information will be appreciated! thanks .i do nmap -p 80 myelasticip and it says the host is up. i guess there is a problem with supervisor then. i have not used gunicorn and i admit i don\\'t know what purpose it serves :confused:stay positive pushkariniif you are using supervisor, you may be able to change the logging level. that said, if you continue to have problems, you might try a more robust server solution, such as apache, or nginx. additionally, if you really just want to keep it up and running, and don\\'t want a lot of extra work, you can try to log into aws and run your server from within a gnu screen session. hope this helps!thanks for such detailed writeup kevin! having a separate environment for each client is a really good idea.it might even be required -- clients will likely want to know that their data is in no way being used as training data in models that could potentially also help competitorshas anyone used secor (by pinterest) to consume messages from kafka and upload them to s3?hey ladies, i registered for the women in data science conference at stanford next friday (feb 3) but now i can\\'t go.  would anyone like my spot?  it was $350 but it will just go to waste if no one takes it so you can have it for free.  dm me!  thanks, , i\\'ll look into this as well!good morning!  does anyone have experience or a good understanding of what to do when you have highly redundant data samples with different target values?   i\\'m working on a consulting project with a pr firm to predict whether a journalist will click on an email heading containing a media pitch.  the data set consists of some emails that are only sent out once or twice while others are sent out more than a hundred times.  any advice is welcome!if you have a lot of redundant data, you may want to look into sampling to reduce the variance, or use a high bias model that ignores/sums some features. additionally, you might want to look into a bayesian graphical model if you want to hand engineer the relationships between actions and outcomesthanks ! those are helpful tips  have you tried jython? or jpype? that way you can at least run everthing in your python app. jython tries to replicate java in python, while jpype also the python vm to to the java vm. you could run a component analysis (pca/ica) on the set of features. you\\'ll lose the interpretability of the feature sets as the new eigenvectors represent combinations of the original feature set. that’s not entirely true, you can still draw on projections of each feature (depending on how many there are) onto the two eigenvectors being plotted and get an idea if clusters in the pca tend to fall towards one particular direction in the original feature space. however, those projections of the original feature space are certainly not orthogonal thanks for the suggestion. i had looked a little bit into py4j, but had not yet given up on finding something available in python. i will definitely look into jython and jpype as well.i\\'m trying to build a foreign language speech classifier for my project, but i\\'m having difficulty finding freely accessible audio data for this task.  if anyone knows of an appropriate dataset, i\\'d really appreciate the tip! thanks!do you need a specific language? a bunch of languages?this might be a decent start regardless: i worked on this project in grad school, which you should be able to request access to: . not a huge number of languages, but has speech samples that are comparable in each languageif you have a friend in a linguistics department, then the ldc catalog has some good stuff. unfortunately, not free hmm--i saw that, licensing those corpora is pretty expensive.  i\\'ll dig in a little bit, maybe there are some subsets that are openly accessible. allstar is actually perfect for  me. i\\'ll request access, thanks.  yeah, ldc catalog would be ideal, as you said, access it sticky.hi guys, does anyone here know how to render a plot in flask?  maybe even do it \"dynamically\" so that the figure does not have to be saved every time? you can try google chart api. i used flask and javascript to feed data into the chart from python. a simpler solution is plotly, but it has limited amount of access for the free version. thanks .  right now i have a simple python function that processes some data and makes a plot, and i just want to pass it back out of the function to an out.html file. just not sure how to save it and pass it back out. here is a tutorial: thanks is also nice for dynamic graphs+1 for bokeh. you can even use it with matplotlib plots.  not sure if this will help but project gutenberg has free audio books  both human read and computer generateddoes anyone have experience creating an interactive map using leaflet in r? i have some questions about creating dynamic buffer zones based on a user\\'s click...(i\\'m a new fellow in boston)does anyone know how i would go about scraping this website? : it depends on what kind of information you want to get. the search form on the product-search tab of the website is simply querying a php script: `&lt;form name=\"form1\" method=\"get\" action=\"/eng/product.php\" onsubmit=\"return checkwholeform(this)”&gt;`. so, if you have a list of products you wanted to search you could just query that php page and save the results.a much slower way is to emulate a browser (for example using `splinter` module for python) and again save the results.thanks alex! i\\'ll look into it :slightly_smiling_face:: also, an easier way might be to just email them and ask for their database-dump. scraping might be against their user-agreement policy, might crash their website altogether, or be considered a dos attack. you never know how much traffic their php script is designed to handle.stay positive alex.tombergokay, good callthanks :slightly_smiling_face:i’m thinking of starting to organize a list of data science case studies after having a bit of pain trying to find some. does anyone know of such a list that’s already out there that i missed?stay positive joshainsleynookyesi willi guess kaggle doesn\\'t count? :slightly_smiling_face:sure! but it’s not comprehensive enough. plenty of companies are putting their data science work out there publicly, but there’s nothing out there that makes all of them easy to find.sounds like a good pitch for incoming fellows project? :wink:perfect! get someone else to do the work for me. that’s upper level management thinking right there.cc my god; what fantastic epicness!!anyone have any suggestion on strategies/documenting software to keep track of tables and their schema at their work?i\\'ve used alation () before, and it\\'s pretty slick.we\\'re starting to use aws glue, which crawls your database and s3 buckets to create a data catalog.  but i don\\'t think it solve your documentation issueso i have no real tips for documentation, but i highly recommend something like flyway for versioning schemas programmatically.thanks!has anyone used google cloud ml? may i pick your brain?we define metadata in the pipeline code that generates tables in bigquery, and then we have a tool like  mentions that scrapes all our tables and pulls the schema definitions and any comments or metadata into a centralized web resourceif any one wants some level of automation in standing up their aws take a look at this.there is an example at the bottom, hope it helps.pretty cooli also like  which you can run outside google with  eg on spark or flink.not sure if this is old news or not, but nasa has open sourced their a bunch of their resources, including data, tools, apis, and repositories:nasa open data:really fun to deduce why each state has that top emoji:hello insight universe, does anyone have experience in biclustering? what is the main difference if you run biclustering on a matrix vs. do hierachical clustering on rows and columns separately and then combine? thanks in advance! uploaded a file:  and commented: we’ve hit 666 :smiling_imp:: correct, although the bug in question was specifically related to cluster correction (initially suggested in worsley et al. i think). had they been using anything else (e.g. bonferonni) or a different approach like mvpa would\\'ve been fine. does reinforce the notion of thoroughly understanding what method (and its implementation) is being used in an analysishi everyone! my company, blizzard entertainment (makers of world of warcraft, starcraft, diablo, hearthstone, and overwatch) is now looking to hire several more data scientists at lead, senior and junior levels. we\\'re located in irvine, ca. if you\\'re interested in more information, feel free to shoot me a message on slack! do you have an office in nyc?if not, would you consider opening one… please?btw, i have recently been using a notetaking software i wanted to share.  a nice alternative to evernote, and it supports markdown. :+1: sharing is caringanyone have the link to the recent draft paper from ds folks at facebook comparing methods of causal inference?nice job at sfiq today people who presented! :thumbsup:hi! i\\'m one of the current ds fellows and would like to set up a way to go around constantly hitting twitter api limits. i know some previous fellows got around this by switching between vpns- if you have some knowledge on this, could i talk to you? or if you know someone who worked on this, let me know. thanks!that sounds interesting , let us know how you go. i like hacky solutions to such limits.  sarah, you had a pretty good solution for api limits in your project no?  if its tweets you need there is a data dump i can help u get -thanks! i’d like to hear more about this data dump. so more specifically- i’m interested in users that follow certain groups (especially political), and would like to get a large number of tweets (200? but could be less) from each of these users. so if the data dump has user ids, who each user follows, as well as some tweets from them, that would be great! also can help you with a massive twitter data dump  -thanks for the suggestions!fellow windows sufferers: what is the best way to export data from ms access to mysql?i had decent luck with navicat when i had an accdb. was on a mac thoughso jealous is your access linked to mysql via pens?no what is pens?i converted to a database/excel format in access and saved it as then read it into python. are you trying to go directly? if you figure it out let me know!!!autocorrectodbcat least that\\'s how i used to do it back when the dinosaurs roamed... via the right odbc driver, you can use access to read/write to almost any sqldb as far as i knowbeen a nightmare trying to install the right odbc driver. after hours of frustration i realized i prob have the click-to-run version (wtf is that?) so driver does not install properly. stay positive jtsitroni dunno. windows 7 or 10?7this helped me... but i dunno. are you admin on that machine?does you windows machine (user/ip) have permission to connect directly to the db?could be a bunch of thingsi do but i think it\\'s the click to rum thing. i\\'ll take a look at your link. thanks!anyone have suggestions for improving performance of mysql queries beyond indexing joins, checking explain for anything fishy, and optimizing tables?can you be more specific about the kind of query you’re looking to optimize?well...they\\'re all select statements on 1-3 tables, many of them involving aggregation and filtering.anyone in the bay area looking for a good data scientist candidate? a work colleague is looking into moving out that way. pls msg me with your contact info and i can hook you up. thanks!anyone go to dataengconf last year? any good? thinking of going this yeari attended last year in nyc, but focused on the “ds” track. found it a bit more applied than some other conferences, e.g. spark summitawesome! thanks kevinwhat about mlconf in march?  anyone know about that?  :q !ah, vim. the easiest way for me to identify other particle physicists:heart:by far the best editor ever conceived.love vim! :smile: gave a 45 minute talk at work about why it\\'s amazing...not many converts still :confused:hear hearit’s hard to appreciate until you figure out how to not destroy all of yoru files and text documents when you’re not in edit mode &lt;-- a great read on \"thinking vim\"is vim is good. my life changed when i discovered `:!ls`also, spend 15 minutes a day here: i was in astronomy for awhile where emacs was king gives you emacs\\' extensibility and plugins with vim\\'s keybindings.question for you all: i have a canonical user-movie rating matrix. however, all users fall into one of five externally labeled classes (abcd or e, say). normally, we\\'d use matrix factorization to find latent patterns in the data, e.g., users who like horror, comedy, etc. however, the goal isn\\'t to predict new ratings, but rather use a new user\\'s ratings to predict which class he/she belongs to. there isn\\'t a 1:1 mapping between classes and latent genres, but rather class a likes some mixture of (latent) horror and comedy, while class b prefers another.the reason i\\'m not doing straightforward multi-class classification is because the data is already in this matrix form (term-doc, user-rating, etc.). i\\'m wondering if anyone knows an approach that could be useful for this case, e.g., could probabilistic latent semantic analysis be useful here?another analogy would be all documents in a corpus arranged into certain \"chapters.\" the goal is to use some kind of latent analysis not to predict the topic/word distributions, but rather which chapter a new document should be assigned to. do you want to write a quick blog post about this?halfjoking...:  it creeps me out you guys are investing in skin cream :wink:hey fellas. i want to record a training video, like coursera videos. i want to be able to record my screen and then edit / insert clips if any errors are madehow can i do that? what sw (free or on msdn) are available. platform windows 7for my o\\'reilly screencasting course () i used camtasiathey also gave a high quality microphone (headset)i recently saw a new course on data science from google, possibly through a linkedin group, but i\\'m having trouble finding it now. has anyone seen it or have the link available?i found their deep learning course on udacity, but i don\\'t think that\\'s it, i\\'m pretty sure it was from google developers.found it, it\\'s this series of videos: boston health economics () is opening a second office in downtown boston. in their own words, \"we\\'ve developed analytic software for large health care databases which were actively and successfully marketing.we\\'re looking for people with experience in both front and back end in thus field.\"   jordan menzin, the head of the software group, asked to advertise; he understands that he might be contacted by either graduates or program directors. lmk if you are interested - ill send over his email.is there any way to do a map-only job in spark stream ? it looks like every \"output\" operation on a dstream forces a shuffle. even map-side operations lke reducebykey() or combinebykey() eventually coalesce dstreams. i have an application where i don\\'t need any kind of reduce() operation at all. i just want to execute a function in each partition locally. apparently this is possible in hadoop streaming.: i thought just running .map on the dstream would work: does running forearchpartition after foreachrdd not do what you need?no it doesn\\'t. to clarify my problem, what i need is he ability to do an output function after after map and combine phase in the map-combine-reduce pipeline. but spark doesn\\'t let you do that. so eventhough i could do everything i need to right after the combiner phase i am forced to incur the cost of reducer phase which means data gets moved from the combiner partitions to reducer partition unnecessarily.someone should do an insight project to find the best way to redistribute tech people across the us to save on rent + change swing states from red to bluei have had this thought as well. as a native iowan who moved to sf, i feel the need for the latter issue acutely. the electoral college isn\\'t going anywhere.i didn\\'t frame it this way but...  :wink:hi insight fellows, i have a friend who has an opening for a ds-applied scientist on their team(amazon alexa), feel free to pm me if interestedwelcome new fellows! if you feel like this, it\\'s normal! we\\'ve all made huge mistakes. :grinning::upside_down_face:a friend has an opening (security focus) at workday if anyone is interested let me know and i’ll put you in touch directlycome to hey  i work at nielsen on 6th at 21st, happy to chatanyone interested in a director-level position for data engineering? what  said applies to me too.hi all, uber is looking to hire a senior data scientist on our crm team. if you\\'re interested do reach out to me :) thank you.what questions did you have?  though not sure i know much about \\'best practices\\'has anyone built a chrome extension before? some of us are working on them, but have been confused on how to get started so far. you did, right?did you find if there was a best way to preprocess images e.g. normalizing the color channels in a 3 ch (or more) satellite image?i’ve done some bare-bones stuff. i don’t know if i’ll be of use but swing by the ai group and we can talk.would any past fellows here have some expertise in building lstms in keras or tensorflow api?  specifically anyone with experience in encoder / decoder architecture with word embeddingsi’ve done some as well, a few years ago. not sure if things have changed much, but maybe i can help too.hey current fellows. i have a mentoring session from 6-8, but i\\'m still on the red line. i am on my way. sorry for being latemade it could someone let. me in?  you did pan sharpening right? i think the type of pre-processing you’ll do will largely depend on how you intend to process those imagescould you share more details about what you are trying to do with satellite imagery and what are the approaches you are thinking about?i agree with dan. it really depends be what you want to do.  or do you just mean turn you 8/16bit pixel  into into a float 0-1.0? i am not an expert but are about to work on it. let me know if you would like to discuss.this has some useful functions you can use or borrow.  has anyone recently could get caffe installed on mac?yeah, sorry for the vague question. and the answer very well might be “try it, see what works”. but, in essence, just color normalization. i’ve seen there’s different recommendations in doing this: - normalize every channel over the max value `img/img.max()`- normalize on a per channel basis `img[...,i]/img[...,i].max()`- normalize by mean and std `(img - img.mean())/img.std()`, now you have neg values- or, if using pre-trained layers (conv layers from something like resnet), just normalizing by their values.i was wondering if this became more important as you moved to multi-channel (4-16) satellite images which include the non-visible bands?if my memory serves me right, you don’t want to have negative values if using relusif you are using e.g. raw landsat images from aws, i’d suggest normalizing them on a per-channel basisyeah, that’s what i was thinking too. though it seems leaky relus are starting to gain favor, i’m not sure if that makes a differencei know each band has a range of values - people from geotrellis are empirically clamping intensity values to [4000, 15176] ()but i think intuitively, it makes sense to normalize on a per-channel basis, especially if you training from scratchotherwise, when you have 8 channels of 8k by 7k pixels,it seems weird to penalize nir band because there was an oversaturated pixel in the blue bandi did a couple of months ago.  ran into a few problems along the way.  dm me with where you’re running into trouble and i’ll see if i can help.okay, yeah that makes sense. thanks !thanks! we’re in the ds group in seattle right now. we’re stumped about how to get our models in python to talk to a chrome extension. we weren’t sure if we should start using flask and try to figure out if we can build an extension that way or we should be approaching this form a different direction.hmm never tried to get python working directly with a crome extension.. i always spun up a server (django or flask) and then made ajax calls from `background.js` in the chrome extension i have some experience in tf/mxnet/pytorch. if your going the tf way i highly recommend starting from the ptb dataset examplesorry to hear about this. i hope you’re ok. you can try contacting some of the higher end local bike stores and ask if they know of a lawyer who handles a lot of these cases. they can also put you in contact with the organizers of the local group rides. sadly, they will almost certainly know someone who needed a lawyer for similar reasons. good luck and i hope you heal quickly.+1 on spending a considerable amount of energy explaining ds, which is not necessarily a bad thing.as a con i’ll say, make sure that leadership understands the investments and expectations of data science. while i disagree that one should come in and demand certain things, i can say based on my experience that not being clear on what i can expect resource wise (setting up servers, or engineering support) made things challenging.how can we refer a prospective fellow to the new data pm program? any available link? you can use this form to recommend a fellow for any insight program: has anyone been to `ai nextcon`?hi. anyone knows of cities that publish live feeds of their transport system that include gps coordinates? most of the feeds i\\'ve found so far seem to only provide the scheduling per vehicle at its stops...allright, it seems like the boston data will do the trick: i\\'ll try fetch it nowhtml related question: i am trying to generate a simple bar plot each time a user enters an input. i currently have the plots saved as images with unique names but they are still getting placed on top over each other in the output. anyone dealt with this before? got it! added plt.clf() - not certain, but i feel like san francisco does this.  on a much smaller scale, i remember princeton providing access (if not publicly then on request) to live gps on their various shuttles.  uc san diego also does live tracking, not sure about feed availabiliythanks, will take a look: try this link for vancouver transit system (requires registration though): cool, thanks , looking into it: it turns out the vancouver api limits to a maximum of 1,000 requests per day... bummer for my plan, since i want to have real-real-time... don\\'t know if it will be possible with any of the feeds out there though...: vancouver api is designed for mobile apps, so that’s why the limit. can you use proxies to scrape more? i don’t think that’ll get you to real-time resolution, but 1 data point per 3-5 mins, maybe: sounds like a good plan. thanks :simple_smile:: berlin does it. &gt; help.exethanks , it seems though that the dataset does not expose the coordinates of each vehicle... also, i\\'m more interested in buses than trains :simple_smile:thanks  -- do you know of good resources/advice on how to evaluate search engine results?what do you mean by that, alexanderjerneckif you mean metrics for the quality of output of search engines, then there are a variety of information retrieval-theoretic quantities you can calculate  (e.g. ): it looks like there is some gps bus info here:  current remote session fellow with a question for anyone about using python 3 on a machine already running python 2.  am i better off installing a vm and running the python 3 strictly within the vm or can these two (and the relevant respective libraries) happily co-exist?  to give context, py3 is a specific requirement for my project, whereas py2 is what i normally use.  reply here or pm me.  thanks!far as i know, they can coexist, as the libraries have separate install pathways (i.e., pip installs to python2, pip3 installs to python3, at least on ubuntu/debian).  might be better off with a vm just for development best-practices though: anaconda makes it relatively straightforward to switch between python versions on a whim.: use conda to create an environment for py3, switch between environments as needed.virtual environments are also a way to isolate versions and dependencies : +1 for virtual envs over conda. in my experience, anaconda can give you trouble with the rest of your dev stack (my made-up statistic is &gt;60% of set-up problems for my session were due to anaconda)stay positive zkermishyikes.  okaybut definitely don\\'t mix conda enviroments and virtualenvs. both sides agree that\\'s a horrible idea. :wink:: \"don\\'t cross the streams\"\"it would be bad!\"stay positive alex_gudei\\'m trying to use stats.models acf and i keep getting an error that my data is numpy.ndarray and is not callable. anyone get this error before and have a fix?stay positive stephgline can you post the line that’s causing the error?----&gt; 1 monthcor = acf(dfmonth[\\'count\\'])typeerror: \\'numpy.ndarray\\' object is not callablesounds like acf was overwritten to be a numpy array i just got assigned to a search engine team, and i\\'m sure (i certainly hope!) they have metrics - i\\'m more looking for someone with experience n the area that can give advice once i get into it. i seeinformal poll here — heard someone recently say it was pretty standard for company-based health insurance kick in after some waiting period, i.e., 30 days, 60 days, etc. that’s not been my experience before across a variety of sectors (education, government, international development, think tanks, non-profits) so was curious what others know from their past/current experience. feel free to dm me if you’re interested or if there’s overwhelming opinion on this, i’ll set up a new channel. thanks!: that\\'s terrible! i don\\'t have much experience, but i\\'ve never heard of that. what are you supposed to do if you get hurt or sick during the waiting period?stay positive berkeleyalmandhunter: you’re not helpful. :simple_smile:: exactly! was curious about a situation like that. seems like a big risk to ask of a new hire. i guess one could rely on cobra from a last job or something of the such, but that’s not applicable in all cases.: this sounds really strange. i\\'ve never heard of this. maybe if it\\'s for a role/company where they have high turnover and a lot of people drop out in the first month or two? for a tech job, this seems really weird.stay positive jeremykarnowski: i\\'m trying.not an insurance lawyer, but new job counts as a qualifying life eventwhich means you should be able to sign up for insurance immediatelyhowever, often, it\\'s the first of the month after you start: the turnover, i.e., someone doesn’t work out, was one of the reasons someone mentioned for doing it.so, if you start the first, it starts that day, if you start the 2nd, it starts the next month\\'s 1st.but it\\'s worth asking about to be cleartalk about bad timing, huh?stay positive mhoover: i’m the most positive person in the world. give me some love for that.also, maybe make it part of the negotiation process? that it should be available day 1? is it because of time to process paperwork?just to be clear, this isn’t something i’m facing — just something i was a casual observer to. didn’t interject as a bunch of others were also saying they’ve not faced the scenario before. but got me thinking... def a negotiation point though if it comes up!this happened to me at jp morgan and also happened to my wife when she got her job in advertisingi think it is just something that insurance companies will negotiate into their contracts with employersi think your odds of negotiating it would be pretty slimyep. i have seen a three month waiting period.can i start a poll? i\\'m new to data science and my boss asked me what my priorities were for learning skills on my new job. i have the following list, but i\\'m kind of intimidated by deciding this by myself. does anyone think i should add or subtract anything to/from my list?                                                                     machine learningbusinesspythonsql and spark/hadooppredictive analyticsmentoring/working on a teamwhat\\'s the job?i say work on the ones that give best roi.best roi for sure. but the python, sql, simple data analytics, and then (predictive data analysis and hadoop or spark).mainly because coding, sql, data analysis are the first things to focus. are there candidates? took me about 6 months to hire set of ds and de there. well, since you have found 2 people, we must conclude that there are :wink: my company also took about 5 months to hire a ds last year. we are currently looking for a senior de -- 5 weeks and counting.posting for a friend: datacamp is hiring for a data scientist position: no problem! glad it worked out!some researchers collected 100,000 “happy moments” from mech turks. pretty cool: i would be really happy to see a fellow project make use of that!^ i see what you did there.does anyone have a good guide on best practices that facilitate data science initiatives, like a data science project lifecycle, that you can share? thanks!maybe not quite a project lifecycle, but this has helped frame my group\\'s work: cool i’ll check it out. thanks!do you guys who work in data science prefer using dask to spark?i haven’t have the chance to use dask; i like sparkdask is nice if you data science team is doing most of their stuff in pythoncan scikit learn be used effectively in spark? (that is distributed) the wait was too long, and i was ready to give up. but yes it takes time!generally, yes. but there are varying levels of implementation complexity depending on what you want to do. there’s a package called spark-sklearn that lets you easily distribute a grid search across a spark cluster: cool!i’ve also done ensemble learning with sklearn distributed on a spark cluster, but it took quite a bit of work to make it happenhere’s an outdated example: the bootstrap module has been deprecated, but the basic concepts are thereamazing thanks!happy to help!that\\'s really cool! as a general caveat, i was told to be careful about mixing and matching python with spark unless it\\'s configured properly, because by default python expects a single-threaded processor. i\\'m working with fellows (and alumni) who are interested in toronto - if you\\'re still trying to hire people to your group i\\'d be happy to chat anytime!we used dask a bit, but then our de team pushed us to use spark and so now we’re all using (py)sparkspark in a jupyter notebook was interesting omg spark is so figidty!!! i’ve been using it in the last month and i’ve experienced so many quirks is dask able to handle bigger data than spark?i had to switch to scala because pyspark didn’t handle some of the features i needed at the timeduring insight,  suggested that i used dask to handle their data. i was doing a consulting project for them. but i never did use dask or spark back then. (my concern was that if i got stuck, i wanted the fellows and the pd () to be able to help me. so i stuck with sampling down and using pandas…i’ve been using (py)spark to handle 400+million rows of data at a time--that’s about 260gb csv file--with reasonable wait time between each execution command (&lt;10 min for whatever you wanna do, including joins). but i always wonder about daski only used dask a little bit, and only as a bulked-up version of pandas. i now use pyspark in databricks notebooks which attaches to decent-sized clusters and does a lot of pretty quick and efficient heavy lifting etl type stuff for me. it’s still a bit of a black box, but i’m starting to get the hang of pyspark and when you should cache dataframes, etc., to speed things up. and i’m getting ok at keeping everything in spark dfs (as opposed to topandas-ing everything!)and in terms of running ml algos, if using pyspark then do you guys typically use mllib, too?one of my teammates has used mllib, and they say it’s worked fine, but i haven’t heard much about it and never used it myself.i mainly use spark ml and stuff everything into a pipeline.i mainly use spark to clean data, then down sample to import things into pandas and do ml training and evaluation therelittle known fact... a little over 63,000 of those \"happy moments\" occurred in the nyc insight office. #themoreyouknow :rainbow:has anyone used/does anyone have opinions on snowflake? we are thinking of moving our rds postgres warehouse.we use snowflake.  our biggest complaint is that their pricing is opaque.  it\\'s so bad that we actually set up experiments to try to figure out how they\\'re charging us.one of our managers actually demonstrated that they are not charging us properly according to their own documentation, and showed this data to our sales rep about a month ago.  no movement yet.to be fair, though, we are still using snowflake, because their service is reliable and can handle our data.i also use snowflake.  i would echo the issues with the pricing and in flexibility.  if you spin a warehouse for 2 minutes  turn it of and then back on for 56 minutes you pay for 2 hours.however, the underlying tech behaves as advertised and is awesome and very fast.i love how data scientists data-science the pricing.there is one open source schema registry utility by yelp called schematizer thanks. i think roi is pretty difficult to determine when you are first starting out, but clearly focusing on coding and analytics are important.google is using recurrent neural networks to predict your messages: one interesting question about this: do they/how do they avoid leaking training data? one obvious way is to train user-by-user, but that would provide small samples.i can’t find it now, but this reminds me of an article i read which was anti- this idea.   the article was about facebook and likes and birthdays, and how it could be possible to have the computer wish someone happy bday on your behalf, and remove whats left of any human interactionnot sure if it\\'s what you\\'re looking for but here at ibm we adhere to the crisp-dm methodology thanks! i will keep a note on that. who else loves rebecca black??!! #friday#at-channelnochannel-tagging 1500 peoplewew lad:panda-dance:oh no! my desk is dirty!here comes the complaining about channel tagging followed by the complaints about the complaints about the chanell tag.:faye:also... spelling.and jokes about faye :stuck_out_tongue:somehow i needed this. which seat can i taayyyyyyk? :notes: set the channel purpose: this channel is for posting stupid youtube videos and @mentioning everyone.so bad, so, so, bad…odsc discount code message coming up soon :eyes::smirk:better stop using logistic regression in production nowman. these people are taught to use regression models for hypothesis testing and forget that regression can be used for prediction, too... and apparently so do their reviewers? oof. lolthat’s amazing!what\\'s the journal??lol j med internet res.no wondereven open source arxiv is way more reputable when it comes to this topics...careful, i published there! ... yeah it\\'s not very reputable.did anyone work through the nlp tutorial ()? on the first line of code from the github, i get the error unicodeencodeerror: \\'charmap\\' codec can\\'t encode characters in position 64-65: character maps to &lt;undefined&gt;it could be python version. are you using python2.7?3.0 --&gt; nevermind fixed it with: output_file = open(\"socialmedia_relevant_cols_clean.csv\", \"w\", encoding = \\'utf-8\\')this notification woke me up!!! and i ain\\'t mad :ship_it_parrot:ugh, this abomination of autotune again xdwell at least it makes me laugh#deadbtw (the upcoming) spark 2.3 should be much faster for converting spark df to pandas df. demos show up to 10x speedup for some cases. this is due to using apache arrow as a dependency, which can avoid the multiple serialisations.ohh!!!out of curiosity, because i’m not ds, why does one convert spark df to pandas df?haven\\'t tried it myself, but there is a topandas() function in pysparkhi all. what is your preferred visualization library? i\\'m thinking of migrating from matplotlib to  to make animated plots, however its documentation doesn\\'t appear to be very good... i\\'m not very good with javascript so d3 is not my first choice... this is really helpful thanks!you\\'re welcome , nice to hear from you!separate question: why is the pyspark api so unpythonic……??!!!i’d suggest using matplotlib’s animation module if you’re already familiar with matplotlib. otherwise, bokeh and plotly are good web-first alternatives.still using matplotlib myself (&amp; seaborn). what’s your use-case? for webstuff, there is a matplotlib d3 library that works pretty well. i’ve been eyeing other vis libraries.a super overview of vis libraries and when to use:thanks for your reply. my use case is to implement a picker for a scatterplot, but the examples i\\'ve found of matplotlib seem to be quite rudimentary: i would love for example to have a hover mode so i know what i am picking, or to be able to do some polygon or lasso picking of my points...we tend toward seaborn and an internal tool build on bokehthanks guys.  how well documented is dash in your opinion?quite well in my experience i feel a lot more comfortable with pandas than i do with spark for manipulating dfs and getting information out of them. pandas is really powerful and it’s what i’m used to, so for anything that’s even kinda complex that i wanna do with a df, i *like* to convert spark df to pandas df, though when the df is huge it can be slow (or impossible to hold all the data in memory in a pandas df)ah i see, thank you for your response, it’s very helpful  np, and of course that just my humble opinion on the topic (as someone who loves pandas and isn’t super comfortable w/ spark)i’ve worked with some dataframes that are larger than memory - and dask dataframes explicitly copy the pandas api while enabling out-of-memory analytics on single cores. seriously good stuff — and it scales to multi-core and multi-node too!in my workflow, we occasionally convert to pandas when we want some function that we can\\'t do in spark. but it is limiting because afaik the pandas df has to load everything to memory at one time, as  mentioned. we can run larger data sets faster when we stay in spark.you may be able to accomplish something fast and dirty with jupyter notebook widgets wrapping a plot function.for hover-mode and all that, i’ve seen better support in d3-based plotting libraries but haven’t used any myself.also, should mention - d3 based plotting implementations don’t have to be horrible - there are some d3 powered libraries that support a python api, including:* bokeh* plotly* altair* matplotlibd3if you watch that video, there are demos for all that stuff which might provide some hints. its a bit of a time commitment at 30 minutes, but i think its a nice overview of everything.i have a friend who applied early decision to the ds program with ny as first choice. can someone provide me the link so i can write him a recommendation too? (assuming that is still what happens :stuck_out_tongue:)i’ve just sent my recs directly to the program directors…is there an official process other than that?yes! here is our alumni rec form. alumni recs are our best source of good candidates, so we love these. letting pds know directly is fine too.  pinned a message to this channel.thanks  ! uploaded a file: ibm lemon awardthey don’t have a lot of market share, but they make up for it in cost!apparently true! they posted the same revenue as aws last year it looks likebest trollhas anyone had experience with any auto-tagging softwares for unstructured data like meeting notes?pyarrow is pretty nicei think this looks promisingi\\'m glad paperspace is on the low end -- already adjusted to it (thanks to insight) and wouldn\\'t mind using it in the future!anyone have experience deploying logstash 5 through kubernetes? i\\'m having difficulty placing the config files appropriately :(have you used this  ?are you getting any particular error messages in the logstash pod?anyone here played with flink?  hassuper cool, will ping him...thanks  thanks for sharing,  ! really enjoyed the piececuz airbnb doesn’t trust insight?! jsilverman: i can’t get enough of airbnb’s policies regarding hiring and growth. hats off.however, i think the article is completely misunderstanding the purpose of this initiative. it’s not because they don’t have enough qualified applicants (come on now :slightly_smiling_face: ), it’s to diversify the types of people who work on data, which is a very smart and very beneficial approach.there’s actually a number of companies who do this.  i think cisco also does something similar for example.hi! my roommate and i are looking for a roommate in sf!  we share an (enormous) three bedroom, two bath condo right on golden gate park / the panhandle.  the open bedroom is big, and has a big closet and shares a bathroom (with the tidiest person ever).  we have a big kitchen, big nice living room, and roof access.  optional garage parking extra, but plenty street parking available.  and there’s a bus stop in front of the house that heads right to market st.if you’re looking, or know someone that is, let me know!  we’re just looking someone respectful and reasonably tidy, and not, you know, raging every night.  rent is $1850.does anyone have an insight on what the pros and cons are to work in a company that does not have a data science team to start your career?i work for a company with a small ds team. pros: 1) lots of low hanging fruit for projects, 2) small projects can have big impact for the company, 3) since you end up working on lots of different projects, you get to see lots of different types of data.  cons: 1) not very many people to brainstorm ideas with (hence all my slack and linkedin posts), 2) some times you have to build new infrastructure to pull data and push your models to production, 3) you end up having lots of meetings to explain data science jargon (i spend at least 50% of my time in meetings).thank you. but low hanging fruit point: how impressive to put those on your resume for the next job/company, etc..?definitely! my first project was to predict which of our 200000 dialysis patients would be hospitalized soon - both low hanging fruit and big impact.hmmm…this is also a unknown startupi can tell you for a fact that what’s impressive in resume is not complexity of project, but impact of results.  if you can spend a week and save someone hundreds of thousands that goes a much longer way than building a complex model for smaller gainsthere’s some jobs where that’s not true, like if you’re more interested in the research side of things, but for the vast majority it isthanks but how do those effects are measurable when the company is a small startup?that’s quite use-case specific.  you should probably evaluate the project’s value befor etaking it onok. thanks! :slightly_smiling_face:+1000 to the answer for \"but low hanging fruit point: how impressive to put those on your resume for the next job/company, \"impact &gt; many other things. data science is a team sport, and if you do some engineering, some dashboarding, some managing, some count+divide, and little modeling, but that delivers value, that\\'s how you starting building credibility on a small team. and with that credibility you\\'ll create more space for yourself to take on new projectshas anyone gotten into a bicycle car accident or have any friends that went through this and got a lawyer/bicycle injury lawyer? i need to find a good lawyer, but i don\\'t have a lot of friends who bike. please let me know if you have any recommendations. thank you!which state was the accident in? you\\'ll presumably need a lawyer licensed in that state. in any case, i haven\\'t hired that kind of lawyer anywhere, reviews on  can be helpful for vetting lawyers.also, sorry that you\\'re needing to do this! :face_with_head_bandage:the accident was in california. thank you so much for the link above! i\\'ll def check it out see reviews from lawyers i\\'m thinking about.i found the o’reilly book statistics in a nutshell to be a good primer for some stats fundamentals that i hadn’t seen in a while. i have been using gan on time-series. while some of the techniques useful for images can be translated to time-series, others just make the model worse.i’m having issues connecting flink with cassandra. i get an io netty error like abstractepollunsafedan blazevski, dr. flinki wouldn\\'t normalize on a per channel.  then you change the color of the object you are looking at, and you will mess up things like ndvi.   if you want to adjust for exposure you should do that across all channels.dr flink :flink:could anyone review my lstm architecture and let me know if it looks ok to them?  i developed one in keras and one in tensorflow.   i’m basically trying to overfit to one sample, but the prediction is not quite what i was expecting and i’m not sure if this is normal or not.what version of flink + connector are you using?divide by 65535 this is the max value for a 16 bit pixel.the flink is version 1.4 and the connector is 2.1 i thinkdoes the code compile?it works on my local machine but not when i upload it to the clusteras in, you see data in cassandra when you write the job locally?does anyone have best practices for peer review of ml models? i\\'m looking for something like code review best practices, but for the model/algorithm itself, not the code implementation.does anyone have experience with django + postgresql set up?on ubuntuit\\'s insisting that my password is wrong and i\\'ve reset it a million times and googled my hands off.hello, has anyone used facebook\\'s api? i am trying to collect profile pictures, and how many likes they got for project validation. thank you!you dont need to scrape pubmed, it has an apiyou can also download medline if you apply for a licensethere are both python and r packages that will hit the pubmed api and parse the output sensibly for youwould anyone working on search engines be willing to chat?sorry.. i didn\\'t mean scrape. just use the api and parse the results into something useful.new public data galore: : i\\'ve been playing with this, it works for multiple search engines: if you\\'re getting any large amount of data though, you\\'ll want to run through proxy servers, otherwise google will ban you, or worse, all of insight (definitely not a way to become popular) :with open(\\'name.pickle\\',\\'wb\\') as f:    pickle.dump(obj, f)can anyone recommend some good tutorials for node-red? i’m learning to build internet of things. thanks! do you love vim?alex_gude: i have been using vim for years only because i was too lazy to learn anything else :slightly_smiling_face: could you just run matrix factorization then use the user\\'s latent vectors as \"features\" into a multi-class classification model?oh yeah! a good vim plugin for pycharm is the only thing that convinced me to switch. :smiley:non-physicist vim enthusiast reporting in, anybody have a good solution when working in a jupyter notebook?:eyes: looks possible -- i haven\\'t used it though. also, their installation instructions might work, but it feels old to me.and of course there\\'s the  project which, in theory, separates the notebook pieces so that you can run via an editor (and not the browser). i know a few editors have used this plug-in some way, but your mileage may vary (and i don\\'t know of a vim specific one). that seems to be the most feasible approach for now. i recently found the following paper about predicting classes from a multi-class domain using matrix factorization, e.g. if movie reviewers had different ages, occupations, etc. sounds relevant, but i\\'m still working out the details -- not sure if it will end up being a better approach:a great use of deep learning: sv peeps — i have 2 tickets to the banff film festival in redwood city tomorrow (friday) night @ 7pm @ fox theatre. it’s super cool (a series of short films that are well done and usually related to nature, plus gear/product giveaways between films) and i’ve gone the last 3 yrs but i need to unschedule some of my wknd so i want to send the tickets to a good home. $24 each (i’d let you have both for $40). let me know if you’re interested! here’s more info and the film list: hey folks, got a job opportunity in austin, tx if anyone is on the job market and open to move. . the official position title listed on the jobs page is senior manager, data scientist, but it isn\\'t a manager position (schwab\\'s weird job title taxonomy). lots of really interesting data to play with. it\\'s a pretty interdisciplinary team, and financial knowledge is not at all needed. we make lots of internal data products. we\\'re starting to mess around with rnns and deep learning. python, scala, and d3 experience would be plus. dm me if you have questions or want to apply.to add more to this. i know that you can use a chi square test say if you can have association between two categorical variables , there is also goodman krushkal test which can be used  for this purpose. but from understanding there is no way show collinearity using the above two methods. just that they are associated.fellow long-time mac user now on windows.  i\\'d check whether they support windows 10, i\\'m stuck in windows 7 and it lacks a lot of osx-like features i notice in windows 10 (linux subsystem, multiple desktops, etc).  i\\'ll admit to being an apple fanboy, but, man i miss my old macbook :disappointed:i\\'ve used vif in the past:very interesting read- an uptick in down syndrome diagnoses was seen with the introduction of new machinery. quite buffling...hi, i am in the same situation. started a new ds position, into my 10th day. :joy:. long time mac user- needing to convert to window based- where do i start? follow-up: has anyone made a correlation/covariance matrix visualization where the features can be categorical or continuous variables?does anyone have a good tutorial/reference on how to identify important interactions using logistic regression?i’m looking into windows 10, and it actually seems like a number of the useful features have been implemented.  more than anything, i’m really partial to the magic trackpad, so i’m looking into windows implementationsis your question regarding collinearity between two categories of a categorical variable (such as cat, dog, other) or collinearity between two separate categories (such as animals vs weekdays)?hello everyone! two weeks from now mya systems will be holding presentations on our nlp and ml research! there will be free pizza and drinks!hey  so i am talking about two categorical variables and not levels within a categorical variable. so take for example the categorical variables race and gender. in such a situation, chi square is a measure of association but the strength of correlation, which is need for collinearity, cannot be acquired from the chi square test. hence what does one use then ?in that example i think a chi-sq would tell you most of what you\\'d need to know (if the level of one var depends on another var). depending on how you\\'re thinking about modeling the data, you could dummy/contrast code the variables (ie convert to numeric) and then just use correlation to check if there are systematic relationships hey josh thanks for pointing this out. so the thing is, this works for the case when you are working with a regression problem, but when you have all categorical variables, you can\\'t really regress for anything. you can define an r2 to vif. am i wrong on this?  another approach i know is to use anova, if your independent variable is categorical and your target variable is continuous, this worksanybody expert in ascii/unicode? i’m trying to create a dashboard inside a terminal and looking at `termui` and blessed` tools.. or the least create tabs as ui within termansi graphics.. that brings me back to the good old days. yes, i\\'m old. : i\\'d suggest using a global optimizer to avoid getting stuck in local optima. nlopt is pretty good and has a lot of options:  . if you can\\'t compute a closed-form gradient, use a variant of direct, that has worked well for me for gradient-free global optimization in the past (but obviously having a gradient will make it converge faster)working part time as an undergrad in a geophysics lab in about 1996 i spent over a year developing an ansi graphics windowing system in c. those were good times (among many…). it was for a test stand to commission a satellite for measuring solar activity. one interesting thing is that years later i now work with a data scientist who spent time as a graduate student in that exact same small lab. awesome. thanks! have you looked at urwid or newt?  [  ]: thanks for sharing! :slightly_smiling_face: will be trying this out, this weekend@ryan: thanks a bunch. i\\'ll try your idea. but i am curious why the connection pool approach doesn\\'t work. i found that idea in the spark programming guide. it\\'d be really neat if it worked.today\\'s the last day to sign upnuggetbot check on ajmendezajmendez has a happiness average of 0: pretty low there, budnuggetbot check on dstonedstone has a happiness average of 0.12280701754385964i am, in general, a pretty miserable personnuggetbot check on meme has no happiness average yetnuggetbot check on higginschigginsc has a happiness average of 0.5227272727272727apparently i’m a happy dude. did not know you could do this: i doubt you will beat the jubilant nuggetbot check on vmgehmanvmgehman has a happiness average of 1.5161290322580645checks outthere you go: positive feed back. i am a happiness op amp.hey to all you more senior data scientists out there: what is your best approach for continued learning and skill building?: try newer analytical frameworks, use r! and try kaggle datasets on your own. also, look at usgs and nih, the routinely put out challenging datasets to work on.  there are also tons of great data science meetups that give talks and workshops.  its useful to see what and how people are using various technologies and pick up new skills that way.:thanks!   shutdownify, the shutdown notice as a service company, is shutting down: wowi literally can’t even.any advice for setting up my scala/spark development environment?the problem is the split between my laptop and the aws node that\\'s running spark.i can install an ide on my laptop, and use github to sync my source code, but how do i debug?: i use pyspark with ipython notebook so i\\'ve never tried these but i\\'ve heard about people using1) spark notebook (), 2) ispark () and 3) apache zepplin ()i assume these work similar to ipython notebook, for which you would ssh into the master node with w/e port the notebook is using and run stuff on your local browser (much much easier than copy/paste ur codes everytime in a spark shell or having to sbt build everytime when u just use spark-submit)i would give pyspark a try, the --packages flag now means u can directly get packages from thanks, guang.  i\\'ll have to check these out.  talk with you tomorrow night for mentoring.does anyone use weka? is it worth playing around with?does anyone have a pdf of the oreilly data science salary report they could send to me? thanks! thanskhas anyone ever worked on fraud detection? i want to discuss some implementation matters regarding the validation of highly imbalanced dataset.  thanks a lot!!nyc insight peeps i hope everyone is ok especially folks from the current session? that was awfully close to the insight office!!at my company since we\\'re often dealing with either regulated environments and sensitive data we usually do things on prem. that way we in theory should have access to all the data we need and they can control all the security policies. there are some cons with this (1. we often need to be on site since they won\\'t grant remote access or the remote access they do grant is awful. 2. we aren\\'t in control of the environment, including upgrades, installations etc. 3. we often don\\'t have access to all the data we need anyway).anybody in sf / the bay area need a...- mattress? - tv? - bike/bike accessories?  (really need better photos for this)- monitor? - desk/table? (no cl link for this but it’s this, $10 )hi all, i\\'m in the process of learning nlp at insight, and i was wondering if anyone could help me out with this problem? i have a set of tokenized words in a dataframe (each set of words represents a single element). i want to tag each set with a custom label that i have, which are represented in another column in the data frame. anyone have advice about how to approach this? thanks!what are these labels based on, and what exactly are you trying to accomplish by tagging word sets?right now, just something basic. positivity and negativity of sentiments. i planned to build on top of that (or modify) once i can figure out how to run a classification algorithm.my question is more related to the fact that i\\'m unsure what data you have exactly, and what you\\'re trying to accomplish, so it\\'s hard to suggest an approach.  e.g. you have a df of labels, where did they come from?  do you have a set of words that is already labeled that you can train on, and then use some trained model to tag new sets of words?i\\'m not sure exactly what you have :slightly_smiling_face:got itdata: i have a large list of reddit titles i from various mental health reddits (~30k right now, but i plan on expanding this). the titles are represented in columns of a data frame, and i have tokenized them (using word_tokenizer()).i\\'m using reddit text to determine if a person is \"in need\" of additional referral to a mental health resource. i.e., a doctor would input some chunk of text from a medical record or type up something they wrote down, press a button, and the app would determine if the patient is in mental distress and should be referred on to another specialisti\\'m trying to start simple: so i\\'ve taken titles with any mention of being in therapy, and ran a basic (vader) sentiment analysis on them. from there, i categorized those titles as either having positive or negative sentiment. these are my tentative test labels....but i\\'m open to additional ideas regarding how to obtain the appopriate labelsfrom there i want to train a model to then apply the right labels onto a test set / new text. does that make sense?almost, what are these \\'labels\\' exactly, like \\'in need\" vs \"not in need\" and it maps onto a specific value that\\'s spit out by your sentiment analysis?i guess it sounds like your labels come from an unsupervised algorithm already, and then you want to use those labels generated by the unsupervised algorithm with a supervised methodwhich is kind of weird :smile:  so i\\'m probably missing something still.  mainly exactly how the reddit data is labeledthis dropped today and seems like it could be real handy dealing with biased samples: tech talk at airbnbi\\'m having some trouble logging onto skype@par.r.callier thanks. we are using a pre-trained model for now. so will do simple addition (and maybe averaging) for now. but, i am thinking of training a new model with the wikipedia.anyone using luigi to set-up / manage their pipelines?just watched this talk:make that  , i apparently forgot what year it is^ the channel i made for the 2015 jan pa insight fellowsanyone going to pa insight party tomorrow from sf?  would like to carpool pinned a message to this channel.for the current fellows using d3 in their webapps:  might be a useful toolhaven\\'t used it myself, but i do remember trying to learn d3 in a matter of days being kind of a dumpster fire during my project :stuck_out_tongue:rolling the dice, anyone here works at d-wave ? i don’t but have some grad school buddies who do (and interviewed with them many moons ago)hi  ! i remember you came to our session :slightly_smiling_face:  would be nice for a hookup to ask some questions dm me with your questions and i’ll see if i can get you in touchhi everyone - i have a data warehousing/data engineering question. we are re-working our data warehouse of large quantities (a few tb) of time series data. we get about 10 million readings x 4 types of readings per day. postgres is getting unwieldy in the loading process (we can’t take indices off because it takes days to put them back on) and we are going to be pushing up against the limits of an rds instance in a few months. has anyone else encountered this problem? anyone have a good (or halfway decent) solution? we are looking at dumping a lot of data onto s3 and using spectrum, but it is very new.any bitcoin people out there?kevinmercurio: they would prefer to maintain their anonymity.i hear someone just made 30mm shorting it :wink:ride the wave, friend:  seems like a redshift or spectrum issue depending on frequency of queries types now in python!seriously?lifesaver?awesome!this is great!!!30 mm oh my god….finally motivated to read that bitcoin paper!that’s pretty nice. i wonder why they didn’t build static type checking on top of the annotation system?i guess you can have pretty complicated types, and trying to do stuff with annotations might get really messy quickly.oh, nm, it does use annotationssilly me which paper ? i actually started working a bit on blockchain in ibm …. uploaded a file:  uploaded a file:  and commented: one of my colleagues here in tn posted an article of  its a small world and insight makes it smaller :simple_smile:hi, i\\'m another current fellow.  i\\'m looking for someone with experience using lstm rnns with time series data, particularly with multiple features.  others with general deep learning experience could be helpful as well.  i\\'m looking for help figuring out how to set up the data on tensorflow backend. from 2017c used rnns.new blog post by  - heart disease diagnosis with deep learning  and interested others, i\\'m working on a project to identify credit card declines with a rate of about 1%, so with imbalanced samples. does anyone have experience using smote  and/or similar classifiers? thanks!hi everyone, current fellow here. i’m looking at interactive mapping options. i have a set of locations (in the us) that each has a value associated it with it. my vision was to plot points with different sizes to denote the value associated with each point, but wanted to display further information when individual points were selected or even hovered over. any platform suggestions would be appreciated. thanks!`bokeh` can do overlays on a map with interactive annotationsthanks! i’m looking into that in particular can talk to 3rd-party geo mappingfor quick and dirty stuff the bokeh sample_data subpackage has patch coordinates to draw a usa state/county map if that helps as wellthat sounds perfect. thanks for the advice! i\\'ve used smote and some cost sensitive learning techniques for class imbalance, feel free to dm medamn. iphone x is $999!!!!,  you might look into using `folium` which is the python implementation of leaflet.jsi\\'ve used leaflet (in r) and found it practical to use for any mapping / gis work.thanks for the recommendation, i’ll check it outfor :  good to know. thanks for the heads up!i’ve been using it for a while at work, it’s super solid. anyone have experience with tracking web mentions? i have a consulting project this session where i want to see if a lot of small businesses that should be dead have any web activity (i expect some are still active). either on big sites like yelp, craigslist etc., or small blogs, or anywhere else. i see there are a lot of paid/quasi-free apps out there, but my idea was to simply set up a ton of google alerts rss feeds and monitor those, as well as try to tap into some apis for the big sites. is there a good free off-the-shelf app in anyone\\'s experience, and are there any known pitfalls with google alerts? what did you do?equifax, bowing to public pressure, drops credit-freeze fees question about d3 and python in visualizations channel.  thanks for the help.hi everyone! i\\'ve created a whole bunch of new channels by cohort if you want to talk to people who went through the session with you. of course, keep the amazing subject-specific and generally random banter going across all fellows as well! and keep creating aggregate groups if you feel those will be helpful too (e.g. sv-2015a)d3 + pythoni may be able to helptomv: they didn\\'t say shiny :simple_smile:aaaah andrew....irony has done a u-turn and smacked me in the facei\\'ve been flasking like a maniac for the last month and a halfall day everydayi miss shiny so much/giphy shinynuggetbot: image me not so shiny anymore10x\\'ing it! thanks obama!on a scale of 0-10, how many x is obama?here is the original source on medium that says nothing about a shortage in data scientists. the opening of the tc article is fake news.hello insighters! welcome to the new crew, and here’s a question for the alum:*does your company have a data science career ladder?*i’m working on a ladder for the ds team at enigma and i’m not finding a lot of existing examples to work from. if you have any experience or insights to offer i’d be very grateful! please reply in a thread to this message. thanks!  shared this last year, about glassdoor: thank you for posting that  -- i was looking for that a couple of months ago and couldn\\'t find itit lives!  -- we\\'re working on something similar for ds/machine learning roles, tooif you have multiple measured time points and want to predict a single value using all those (like group membership for a binary class) what type of methods do you typically use? or do you just ignore the autocorrelation and use each as a separate predictor?that’s very broad, you can do any number of things (fit some distribution, some time series model, blah blah).  you may get better answers if you’re more explicit.hmm... so say we have multiple measurements at multiple time points for a single variable (something like cholesterol), and we want to predict whether they\\'re likely to have a heart attack, it\\'s not really a forecasting issue and we don\\'t really want 4 rows for that individual because we want them all to contribute you a single outcome. what would a typical approach be using something along the lines of logistic regression? also, no black box models, and the sample size is fairly small.maybe a mixed model?i mean from a statistical view you don\\'t want to ignore the repeated measures aspect, but i\\'m curious if you don\\'t care about the predictive power of any single variable and only care about accuracy of the outcome variable.that\\'s one of the things i was thinking of, but usually you would have 4 rows for each subject and then specify an intercept and/or slope for each subject, how would you specify it such that all 4 observations contribute to a single outcome? mlm comes to mind but it\\'s been a long time since i\\'ve used it.not sure if you\\'ll always have 4 points or if that\\'s variable. i\\'ll assume it\\'s variable. one approach could be to engineer various features about the points, e.g. slope, intercept, min, max, variance, etc and use those as inputs to a classifier that predicts your single outcome.am i missing something more subtle about the problem?yeah, i was thinking about feature engineering. i haven\\'t dealt with time series data very much. i also did the standard insight program but wound up in the health field where sample sizes are much smaller than you\\'d like.i\\'ll always have 4 points. typically measured at roughly the same interval.i basically just wanted to make sure there wasn\\'t something obvious i was missing.does anybody have any experience with the facebook api? i’m wondering whether anybody knows if there is a way to download facebook social network datasets (friend-links) and user activity datasets (post likes, etc.) for the same user id’s… i can’t figure it out so far…for the new fellows that i met tonight during the jupyter notebook tips and tricks -- the updated notebooks can be found here: free mock interviews great article - disappointingly sad, but greatthat is disgusting and scarily eye-openingnuggetbot animate me mind your own businesslol, wow. language, nuggetbotnuggetbot animate me aint nobody got timea/b test your facebook profile pic, lolwhile we were the fellows (i was a remote fellow), were we1) an employee to insight2) a student at insight3) unemployedor 4) self-employed?i need a clarification on some uscis related issue… thanks guys!it depends how you classified your stay at insight and what your visa situation was at the moment.you were not a student if there\\'s no i-20 from insighti would either classify insight as a conference which you were attending it (making you unemployed)or, if on an opt, doing some work (probably volunteer work) for insight. this will depend on getting some letter to the effect from insightdepends what visa you were on, but not 1 or 2 as insight is not involved with your visai have a green card - and applying for a citizenship - i don’t think they would care too much, and i think ‘student’ might be okay???unless you have an i-20 you are not a studentyou should talk to insight about this, they are pretty helpful but on a green card you\\'re not a studentde: i have been hoping to distribute my s3 file requests using spark 2.2+scala 2.11 and squeezing the client through a dataframe map, but i keep getting not serializable issues (from listobjectsrequest, even though it implements serializable natively!). i\\'ve done the normal \"wrap it and extend serializable\" routine, and haven\\'t had much luck. does anyone have experience with this in spark &gt; 2.0?guess you might have already seen these:       somewhat related to the article posted by : anyone know any product managers looking to work with data-heavy advertising products that want to work in nyc?my company (one concern) is hiring multiple data science positions. not only the candidates with domain knowledges (natural disasters such as earthquake, wild fire, floods and etc.) but also general data science experts (deep learning, ai etc.) will be highly appreciated if interested. we are located at palo alto and will be hiring at least 6 data scientists by the end of the year! please dm me if you are interested or know someone who might be. thanks a lot!(i am a 2017a remote data science fellow)hi all,  anyone has suggestions for a good resource to learn spark ? databricks has a free edition of their platform - much like a notebook based spark environment with some how to docs, you might want to make an account and play around with that… (community edition) thanks  : i started it but kind of lost my way around - maybe will give it another go.i didn’t play around there too much but from what i remember they have nice “getting started” notebooks shared for all to copy/usealso other users can share things with all, so try to look for some interesting examples and copy/play around with it in your own space...if you have any specific questions, feel free to ask... ...highly recommend this :nerd_face: :unicorn_face:   ... :  it\\'s a good beginner/intermediate level step-by-step guide to spark, and hadoop, hive, etc. -- just purchased this for myself on your recommendationthere’s also this free book: make sure you know the differences between spark 1.x and 2.x, they’re quite huge:smiley: :unicorn_face:any references for both learning spark and getting it up and running on aws?(i\\'m guessing a docker file does the trick here, but perhaps there are other less container-y ways to get started on aws?)this may be appropriate — anyone know how to launch a jupyter notebook server with all extensions explicitly disabled?hi!  is anybody good at edward, the python bayesian inference library?have you tried modifying the `notebookapp.nbserver_extensions` parameter in your configuration file? looks like you could pass in an empty dict.  maybe?is anyone using  ,  or  in their workflow? i\\'m starting to evaluate each of these. any input you may have would be great to hear as well.my use case is mostly to run sets of sql/api queries and share the results in an interactive way with non-data-centric internal folks.i used tableau for a project. it worked for running sql queries and displaying a variety of charts based on user input. took a bit to get set up and the data munging isn\\'t too great.yeah, we\\'ve got tableau here, find it rather clunky but need to look at it again, and deeper, because we\\'re already paying for it :simple_smile:one of our clients is paying for it. works out well that wayrightanother option if you want to take a look at itused it?looked into it a bit since it was just made free, but i couldn\\'t figure out how to get it talking to a sql database. works fine from excel, but... yuck. excel.agreed: who all is going to strata this week?! join  if you want to sync up and meet other insight people!hi all, i\\'m looking for a room to rent short-term in nyc starting ~april 1, preferably (1) cheap and (2) close to the insight nyc office. i would look to rent for the month of april (preferably with the option to extend). don\\'t mind sharing an apartment with people, etc. please send along any ideas/leads!\"doctorates in physics use python and r, said jake klamka, a former physicist and director of insight data science, a recruitment service for data scientists. moreover, many physicists work with large data sets, such all particles in the universe, he said.\"the economics literature deals with this a lot. instrumental variables, propensity score matching, etc. the book \\'mostly harmless econometrics\\' provides a great overview. great, thanks guys! indeed, i own a copy of \\'mostly harmless econometrics\\', though i never made it to the part where this is discussed:bow:has anyone used the python lifetimes package? i have a few questions about how things are being computed and what they mean.didn’t use it myself, but pointed some coworkers to that code, which i believe is just based on the fader and hardie methods (see the papers on the bottom of the above page)x-post from  johnjoo: i’m sure this is way overkill, but i found this blog post super interesting oh nice! i’ll take a look!importantly, the author attempts to predict not-churn in that they are predicting whether or not an “engagement” event will occur over some future time window given historical infoyeah, i think that’s the line of thinking that i was going through too.fair warning, he wrote an entire thesis on the topic :slightly_smiling_face:oh! sometimes that’s easier to digest.  you wouldn’t by any chance know the guy so i can get a copy?it’s actually free on his website! if you click the hamburger menu on the upper-left of the blog post i linked, it’s in the menu.oh nice! thanks in genetics, similar ideas are also becoming popular. search \"mendelian randomization\".pearl\\'s books are very readable systematic treatment of how to estimate causal effects using causal graph modelsyou could potentially fill the “holes” by imputing with k-nearest neighborsi have used the google maps api to geocode addresses in the past and successfully calculated order 100k of addresses. they have a daily free limit of 100k i think. where can i find a great tutorial on the use of window functions?mode has a great tutorial: oh yes, that is a good onethanks guys!thanks! how did you plot the locations?these are great tips!  i was struggling w/ the same thing the past few daysi also like this handy guide from postgresthe hive language manual page is a pretty good referencei used geopy with folium.  it was pretty simple.  one thing to check with geopy is what geocoding service you\\'re using.  geopy is just a wrapper for talking to a variety of rest endpoints, so latencies as well as rate limits may vary depending on the service.i looked briefly into plotly for heat maps (too briefly to have a real opinion about it for mapping) and ended up using folium, which i\\'ve liked so far! i too used the google maps api to get lat &amp; long for my data (~7k).random question: i don\\'t own an apple watch but know that they put a bunch of sensors in it to acquire the wearer\\'s health data in-situ. do any of you apple people know if the anonymized data is stored by the company? once there is enough data, i bet some very interesting patterns/trends can be found. thanks - i will check that. it looks like with folium you have to be online to render, so i am working with plotly.hey everyone, i\\'m working on a project right now that involves machine learning regression, but i\\'m interested less in having the best average error (mean squared error) and more in having the fewest possible large errors. i\\'m not sure how to set this up, does anyone have experience with this, or can direct me to where i can read about how to approach this? thanks!naively, why not just set up your loss function so that it is more than quadratic in the error? i.e. penalizes larger errors morethis is exactly what i was thinkingsklearn is somewhat lenient, depending on the class, about implementing custom loss functionshi i created a  channel for people who are working on fraud teams or who are interested in the field, please join!bayesian webinarto people interested in seeing some art this weekend: i\\'m showing at open studios at shared,  stop by and say hey if you\\'re nearby! (sat and sun from 11am to 6pm)cool, what’s your stuff like woah, these are neatdo you sell prints?yes i do :slightly_smiling_face:i was there tonight to see a friend’s art! sorry i missed yours!stay positive emily.mclindenshared (my space for this weekend)\\'s address is: 739 bryant street, san francisco, ca 94107import ggplot??i love xkcdhow about mode analytics?yep, looking into that as well! thanks.nugget-bot show me a unicornnuggetbot: show me a unicornnuggetbot: image me a unicornnuggetbot: animate me a unicornfantasticnuggetbot: mustache me a unicornnuggetbot: will you marry anasuya?nuggetbot: pug bomb 5nuggetbot: mustache me simon gelinasnuggetbot: lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me lauren burcawnuggetbot: mustache me there we gonuggetbot: ship itnuggetbot: mustache me nuggetbot: mustache me omg hahahahalulznuggetbot: mustache me hi guys - just wanted to point out we have a great  channel going if you want to join - though i don\\'t know if mustached fellows count as cute animals... #ftw uploaded a file: oops, i should i added that here.anyone at dataengconf today?does anyone have work that overlaps with a definition of what they\\'d consider \"data architecture\"?i choose db schemas, s3 storage formats, and the pipelines that write to them. not sure if you consider that an architect or an engineeralso choosing what data to ingest, how to save it, how to pass it to different processes, data guarantees/policies, etcthanks! i\\'m trying to get a handle on how data scientists and data architects typically overlap. e.g. are there people who spend time thinking of what data the ds will need to build \"fancy model a\\'\" in 3-6 months, and then ensure that that the schema and pipelines exist to collect that data in advancea few of us from insight spoke today at dataengconf today. as a result, i have a few free tickets to the conference for thursday and friday, and likely won’t be able to use them. if anyone is in sf and wants to attend, send me a direct message and i’ll send over an e-ticketroommate wanted: 10mins bike/drive apt from insight. 4150 el camino way, palo alto ($1525). for insight\\'s sake, i\\'ll be gender neutral. :smiley: ah this makes it much clearer, thanks for taking the time to link these. i see how especially for multiclass problems, weighted guessing or shuffling would give a much fairer baseline.  i wasn’t aware of sklearn’s dummyclassifier, it’s a useful wrapper for quick sanity checks!noticed this on an nih blog: request for input on their strategic plan for data science  chernoff bounds yes, negative dependence nope... in which context? just when the la croix wars had died down :stuck_out_tongue:we have always been at war with c******.and here you yelled at me for using the c-wordthank you.you should really add a content warning to this whole discussion. some people find it really traumatic to read about things like this. :snowflake: :coconut: :la_croix:in case you or anyone you know is looking for a data science position in la::pamplemousse:so this exists: account is dead, but some good stuff in therehey data science fellows, anyone looking for freelance data science work? we’ve got an interesting project involving communications metadata predicting attrition. get in touch and we can talk details.;) i spent 80% of time complaining that. i must did ds wrong.hi everyone!insight alum from the ds-idx-2017a remote session here. my company is hiring data scientists and data engineers with positions available in both victoria, bc canada (where i\\'m based) and la! please slack me if you have any questions or know of anyone who might be interested.  this sounds like you!using deep learning to colorize the past using deep learning to measure pollution (!) from photos anyone who needs to install gobblin, eamon and i stumbled upon two bugs during the installation. one had to do with glib-gobblin gio-error **: settings schema \\'org.gnome.system.proxy\\' is not installed. to fix that error, we removed the -djava.net.usesystemproxies=true flag from the javadoc settings in build.gradle (main project) and then when we encountered the second bug, we just rebuilt gobblin but instead of doing a clean build, we did a clean assemble. there is a concern that we might run into runtime errors down the line. anyone have experience with gobblin you want to share?i have come to the conclusion that insight is a throwback to the 90s:  everyday is basically a lan party where we are all chatting on irc (slack) about interesting topics.on hiringthis week on hidden brain, shankar talks to google\\'s laszlo bock for insider tips and insights about what works — and what doesn\\'t work — in recruiting, motivating, and retaining a talented workforce.: have you guys started playing doom () on the lan yet? :wink:stay positive alex_gudewe meet again my nemesis...doom!hmm…i get nothing.nuggetbot has an affinity for me :confused:: throwback suggests this is the doom we are playing ()stay positive ajmendezha  ensures that the alex population is positivethrowback suggests this is the doom we are playing ()stay positive joshainsleythere we godata-driven design at the sf design week tmr evening in sf. anyone going? i’m on the waitlist but also an optimist. for the people of the new batch, you will need this when you start with the interview preps the dancing in specific, not the algorithms. silicon valley has a really obsession with hungarian folk dance... :wink:but of all the algorithms, you really _will_ need bubble sort to succeed here :troll:1 vote for bogosort bubblesort gets a folk dance, bogosort is a rugby scrum i guess?that or shouting fire in the theatreand then telling everyone to go back in and watch the film/playyou know, there is a lot of overlap with the 1st drunken passenger on the airplane problemclearly someone in #ds-sv-2016b needs to make a pod that is bogosort plus that probability problembest consulting project ever: welp, time to pivot...the worst sorting algorithm of all timestay positive mamday: boy you got until week 5 for when you start pods`jobinterviewsort()` is my favorite :wink: thankfully haven\\'t been there in reality, but insight practice interviews don\\'t count as reality.anybody here use aws cfncluster, specifically with the ~/.cfncluster/config `extra_json` option `cfn_scheduler_slots`? i am trying to force the number of jobs per node of the cluster to be 1 for memory reasons; i specify this option in the config file, but the scheduler still fills all threads on each node. i have tried using the mpi and smp parallel environments with qsub, since they use different allocation rules, but to no avail. any input would be hotdoes anyone have experience route finding with constraints? i know people have integrated google maps in their projects but i need some way to divert around certain areas.: would a simple a* work for you?possibly, i\\'m just trying to avoid building everything from the ground up. i know google can route around traffic but i wasn\\'t sure if you can add your own constraints with areas to avoid. it would likely only be directions for chicago, so i\\'m not sure if there are any open source mapping tools that would be useful here.you might talk to  he recently did his insight project using waze datahere’s how uber does it: shout out to  for pointing this out to me…he’s working on something similar now:this isn\\'t a data science related job, but if you know anyone appropriate, please pass it along:the job is running a software development education non-profit. i\\'m a volunteer instructor for the organization, and they\\'re really doing some great stuff!any one knows how to open big csv file , like 90g size, in pandas ? please have me an idea.you can read in batchespandas.read_csv with chunksize?: i think pandas reads all data into memory, if so, you will need a machine with 95gb (at least) ram — not impossible, but very expensive (though if you only need a quick thing, you can get aws machine and do your work quickly and stop it — bills by the hour)thatsomething like this : df = pd.read_csv(\\'check1_900.csv\\', sep=\\'t\\', iterator=true, chunksize=1000)also realize that if you want to do anything complicated with a dataframe that size, it\\'s going to take a long time to process. i recommend using dask… or go with spark :slightly_smiling_face:some docs here: thank you so much guys for all the response. ,  : thank you for the command line. i believe that i have tried the “ chunksize” is not really working. but i will try again. if i use chunksize, how it really works? does it read as a chuck? i would be able to use it as usual usage of pandas?   may be  right. i failed because of my hardware , ram. unfortunatley, i cannot take out my data out of our server.  what is the dask? could you please let me know how to use it?  i do not have enough experience on spark. could you explain a bit more about how to use it?: can you share the code you tried that didn’t work?: the idea with `chunksize` is that it makes `pandas.read_csv` return an iterable in which each “chunk\" is a dataframe of  `chunksize` rows. there’s a good example of how to use it in the docs i linkedspark is a distributed computational framework - you would have a cluster of machines that coordinate work. this is a bit hard to explain its usage in short time/text. if your organization has a data engineer, you might want to ask them and see if they can run your job as a spark job...: i do not have the code i have tried with me right now. :disappointed: but i remember there was “chunksize” setup, just like you gave me. and i remember that after i have tried then i was not able to call the csv file as a table.: i am not sure if our linux machine has spark in it. i probably check with the linux admin.: so once i do  df = pd.read_csv(\\'check1_900.csv\\', sep=\\'t\\', iterator=true, chunksize=1000), then i should be able to use df as usual? or do i need special command ?you can’t do it with just one machine (technically you can, but it will be too weak of a cluster to do anything large) - ask if you have a spark cluster setup that can access the needed data...: yes i will. if we have spark, then do you know any good qiuck start guide site that i can follow?: no, you need to iterate over it. so `for chunk in df:` then within your loop, use `chunk` as a dataframe as usualaha!i see. so once i call in as a chuck, then call it in loop.ok i see, i will try it.: good luck!yes, you can simply run spark-shell (the interactive shell in scala) or pyspark (for python) and load/compute on your data. the amount of work depends on what you are trying to accomplish. if you need just some simple filter/aggregate type tasks, it’s fairly easy to do, if you need something more complex, it will take some time to learn the spark apithanks!   i will try it and let you know if i can manage it.: hmm i hope it would not take so much time to learn. i will check my admin and i think i need to be back to you for more learning about spark..:slightly_smiling_face:: looks like i am able to access to the data by using pandas. thanks !!! :smile:: i think we have spark in out system,  and the simple guide line tells me. now i will study how to use it as i need, so this would be able make me possible to run a massive job there, right ?i’m glad you do! yes, spark is for computationally intense jobs on large data.damn never should have wasted my time learning to code!stay positive markwanghey everyone! fellows &amp; alumni &amp; all. we added a happy hour to the calendar for tomorrow (tuesday) @ 5-6pm at the insight office. we\\'ll be hosting berian james, staff data scientist &amp; head of product analytics at square. come hang out! beverages and snacks provided.….and this is the silicon valley office, in pacific time :slightly_smiling_face:that’s coastism. where’s our party?technically it’s open to anyone on either coast, you just have to travel a bit fartheri guess that’s acceptable :wink::slightly_smiling_face::beers:skype in with drinks?or just drink.due to time difference east coasters will have a head start though, so sv had better pregamestay positive amerbergchief peeps officer. nice.hey guys- i just got a discount link for a geowebsummit happening tomorrow in bklyn (i’m on the last panel).  hello! do you regularly use github for work or play? are you interested in participating in a private beta test of a new github product that helps improve workflows for teams? if so, dm me your name, email, and company and i’ll add you to a list of potential beta testers. we plan on starting within the next week or so.hi all. i will be moving to the bay area next month and i have no idea about it whatsoever. i will be working in san jose (near to campbell). can you guys recommend of any places/websites/neighborhoods/etc all around the bay area. i’ll then factor everything in (rent, commute, neighborhood, w or w/o roommates, closeness to different places/activities, etc) to make a decision. any input is appreciated. thanks (i also posted this in )looking for places in the bay area is typically too high velocity for anyone to give you a list of places.rent: not the bay areathink people shy away from east san jose. cause it\\'s too hood or something like that. i don\\'t think most places in the bay are that bad, but it depends on where you come from and are comfortable with.on the offchance that you want to live in sf but work in south bay, potrero hill is a decent option. (relatively) reasonable prices to rent, and close to 22nd st caltrian station.are you going to drive? you could get an apartment in mountain view and have about a 30-40 minute commute to ebay (as an example of a place in that area). rent (for an old place) could be around $2100, lower with roommates.basically, there are *a lot* of variables. :wink: what’s important? cheap rent? short commute? awesome nightlife around your apartment?looks like campbell has “relatively cheap rent”, so living close should be doable! can probably recommend some places in colma, so long as dreary weather is appealinglol colma is #1thanks so much everyone. yeah i know there are a lot of variables. i just wanna have an idea of where to look for once i physically arrive there.where will you be working ?this just arrived at the sv office  uploaded a file: haha, hot topicif only i had that resource before my interviews!thanks  ! it will make a great addition to the de librarywe should nail down detailsmeet in person? or distributed / remote?evenings / weekends?adding    to the thread based on likes (is this how “adding\" works?)for another project idea, i’d be interested in exploring rbms and working on an implementation from scratch.it’s possible that in-person meetings with  the buddy on a weekly basis and then monthly meetings as a group could be a nice forcing functionthat does sound goodif some people / pairs have tight schedules / long commutes remote meetings could also workmonthly meetings in personwhere does everyone work? (midtown here, commute back to jersey -&gt; soon to ct)i\\'m in midtowni’m in west villagehey everyone — insight is hiring an office manager in palo alto! know anyone who’s a good fit? we’d love some quality referrals from our alumni. tl;dr on the role: \"the office manager will be responsible for maintaining a welcoming, comfortable, fun office for the team and insight fellows, as well as supporting the team with events, travel, and other ad hoc projects. to succeed in this role, this person must be a proven self-starter, an excellent task manager, and be excited about running a fun, productive office! experience with start-ups are a plus.\" i invited everyone who liked and commented to the new channel i created. please join so we can continue the discussion!cool! let’s maybe give it till eod or early tomorrow for people to filter in and we can figure out logistics...does anyone use jupyter and access data with elasticsearch? i\\'m trying to get things up and running and currently i\\'m trying apache toree for the kernel, but i\\'m having a lot of trouble figuring out how to get the elasticsearch libraries installed.hi  i did this about a year ago, i didn’t install a es specific library, i just used the `requests` package to hit the endpoint for the server we had es running onusing toree?ah, in pythoni use the python package `elasticsearch` - works in a python kernelbtw i find toree to be quite unstable, still. if you need scala from jupyter i’ve had more luck with spylonyeah, i saw that as well, a lot of people here seem to be concerned about the performance cost of pyspark vs pure scalaone issue with pyspark is that it lags behind in development. so some packages like xgboost only can work with spark with java or scalame too.  i havent used spark on over a year.  but i went to dean wampler\\'s \"just enough scala for spark\" tut at strata last week and his opinion was that the performance difference vs python wasn\\'t that great anymore.  can anyone recommend a good source of guidance for handling missing values for various machine learning algorithms?hey folks anyone done any work in estimating budgets based on past spend data? particularly i\\'m looking into marketing budgets for different businesses. any ideas/suggestions?thanks !hi all: my company is hiring a junior-level data scientist on my team if you know any folks in boston (or willing to relocate) let me know or share this link with them:  im using accelerometer data to quantify engagements with 360 ads, pm me!oh, just fyi. if you sign up for the free credit monitoring service from equifax that also removes you from any class action lawsuits wow. that is fucked up.oh what! that’s some bs, you’re on the case!haha, luckily i was involved in _a previous_ data breach from some other company and have access to their credit monitoring. :face_with_rolling_eyes:i just meant that if people could be alerted to that clause in the equifax terms of service, things would be much better! and beforeiaccept sounds like the tool to make it happen!note that freezing your credit report is unconnected to their “identity protection” products, and is legally mandated capability it\\'s hard to know how enforceable boilerplate terms of service are, though.it’s true that it might not be enforceable. doesn’t gain them any points though.serious question: if my data is compromised, how can they prove i was the one who signed up? isn\\'t the identifying information they require to enroll you the same information that may have been leaked?harder in this case given for some people they also have more info like driver license, etc. part of it will also depend on the exact info that was takenthat said, less use cases from other people to sign up for the credit monitoring.  at this point seems reasonable to look into a credit freeze. details vary between states but for ca it is $5.this is such an interesting event given i just signed my offer to do fraud and abuse detection in my job…yeah, i do ds for risk for squareoh cool! i’ll start with amazon in about 10 daysso i know close to nothing right now :wink:i guess that is also why they were asking for last 6 digits of social to sign upi am curious if there are good policy solutions that would protect people from identity theft, and other forms of cybercrime, in general... rather than 143m americans all signing up for credit freezes. at the very least it\\'s probably time to give up the charade that ssns are private information and reliable for authentication...btw, nice job in your talk! very well donewhen i moved to the us i was *stunned* that ssn is considered a password-level secret, despite the fact that it can\\'t be changed over my whole life, and i have to give it out to about 25 different entitities a year...and that it\\'s also inferable based on when and where you were born: i have always wondered - what can someone actually *do* with your ssn alone if they get it? (i agree that if the answer to the question is anything other than \"nothing\", it\\'s pretty disturbing, as there is no way to change your ssn)pnas, the journal for those who have a juvenile sense of humor lolan even worse situation is the italian codice fiscale, which i had to give out in many more places than an ssn, and which includes your city of birth and birthdate! (plus there can be collisions!)hi all, current fellow here. has anyone ever used the strava api? i\\'m very new to apis and am quite clueless on how to get started with this. didn’t you work with strava data? didn\\'t you?i didmostly scraped though what’s tripping you up with apis?  general working-with-them problems?happy to chat strava  , i’ve done a few projects with their dataomg fo realz??thanks for the help! my questions right now are very basic as i\\'m still figuring out my insight project. i don\\'t use strava, so i don\\'t know much about it, but i wanted to find some fitness data and it was suggested to me. could you first tell me a bit about the feature space? might be worthwhile to see if my ideas and the data work together before i go about gathering the data. also, what type of projects did you use the data for?hi joao - thank you for the tips!  i looked at hcup but the cost (&gt;$200) was prohibitive.  iowa releases county level data, but aggregated by month...not quite enough resolution there.  california has asthma hospitalizations and ed visits, by zip code, but only annually.so i\\'ve pivoted away from this idea, unfortunately...i was excited about it.  at least i failed fast.pivot, pivot, pivot...go &amp; go that is an excellent article, sina, thanks for sharing.this is incredibledoes any one know if there is a good online course for data modeling (for big data)?  thanks in advancewould love to know the resources you find :slightly_smiling_face:me too :slightly_smiling_face:just in case any current sv fellow would be looking for a more convenient place to live, there are two relatively affordable rooms available for a short term lease very close to the insight office. i stayed there during my time at insight, and i could not have been happier. send me a message if you are interested, and i\\'ll give you more details.,  thanks! i always thought cover letters were a waste of time but wanted to make sure that my sentiment was shared across it, and it wasn’t me being lazy :slightly_smiling_face:, well part of it is still me being lazy.we are currently interviewing for a director of analytics position, anyone has good interview questions for that level of position? thanks!i\\'d be asking: evidence of times you made an impact on a product because of your work, how you work with diverse stakeholders, how you prioritize projects, examples of times you had to learn/decide on something for which you had no existing data sourceshow do you balance rigorous data science/modeling work with real-life deployment requirements? can they come up with a roadmap for research/testing/analysis for the next 6 months, given a prompt about a business question or new product launch?what if you cut that down to 3 months, what would they cut from the original plan, why?:thinking_face: awesome questions!can they come up with a suite of kpis for your business? what dimensions would they be broken down by? what hypotheses could they generate from this list?also time prioritization between short-term ad-hoc needs and longer-term research needs is a tricky areasome use it to gauge applicant interest (have they done any howework about the company?) and alignment with company culture and mission.for director level, you\\'d also want to ask them some pretty pointed questions around how they think data can help their business needs...assuming you\\'d be the head of the data org, sometimes there are assumptions that aren\\'t grounded in reality for what data science / analytics can do for the company.if they\\'re asking the questions  highlighted, you\\'re probably already dealing with more reasonable stakeholdersalso to clarify are you the candidate here, or the interviewer :wink:hm, i am the interviewer :slightly_smiling_face:sorry i reread my statement and it wasn’t clear.ah, that makes more sense thennever mind :wink:quote of the day (on gdpr): dance like nobody is watching, encrypt like everyone is.lolwonder if anyone else here is gdpr-ing and using a strategy that is not encryption as a way to let users be forgotten ????freakin gdpr killing me manencrpytion doesn’t (even) count in gdpr i believewith europe from love :heart:gdpr… :disappointed: i’m glad i’m not in charge of getting our stuff in complianceyeah, i sit next to the people in charge of gdprdelete things?could you expand on \"doesn\\'t count\"?i need to spend some time reviewing the rules, sorry, it’s only that i’ve heard that even encrypting data is not enough to be compliantencryption by itself indeed does not count.  using it as a way to “forget” users is useful if you have a lot of services storing user datado you mean different means of storage accessed by different services?  but what if we still have the means to decrypt?  is that compliant?becomes a case of “easier said than done” if 100s of internal services are independently storing user data storing whether a user wants to be forgotten in one place and forcing all developers to store encrypted versions and when they want decrypted data, force them to call that central place to get that sensitive data and get a “sorry you’re outta luck” if that user asked to be forgottenwould be interested to know how others approach this ^yep same here :grinning:they all turning curmudgeonly? cause i am.deletion is the killer if you use kafka. any topic with a retention longer than 30 days will most likely need to be migrated one of the forst thigs should be monitoring. instument metrics so the emit things like disk space utilization, cpu, memoryyou’ll want o know when hosts are going into compaction which will spike cpu and memoryaws cloudwatch probably has that for you already thoughwe\\'re hiring a buncha people.  no data scientists directly, but data science related roles:    let me know if you\\'re interested or know anyone who is :slightly_smiling_face: agreed but company is not ready for awsthey want to try in house firstthanks for your interest in sharing your experience with the growing ds community! the slot has been filled.in addition to all the awesome suggestions (definitely case studies), i would ask questions like \"how do you best situate a data science team within an organization to set them up for success? and why?\" since there is no best set up follow up with \"what might be some of the downsides of this decision, how would you mitigate these downsides?\" and also, ask them about \"what makes good data science candidates?\" (this will have huge effects on your future team) and finally, a question from a different bucket \"how do you decide whether to go with a vendor or build in house solutions? why?\" finally, i like to ask something about their grander vision: \"where do you think our industry will be in 5 years? how do we best situate ourselves as a company?\" i have seen plenty people come up with disappointing solutions, you do want someone who can develop and advocate for their vision.tempalert is hiring a data analyst - i realize this is too junior for this list, but if you know of anyone who is looking in the boston area please pass this along! has anyone here worked with python notebooks on a windows machine - is it a good idea?? , i\\'ve used python on jupyter notebook on a windows machine, and i haven\\'t found any problems so far.worked fine for me when i did ithi  - i have not really worked with python on windows - do you have a resource to get me starteddoes anaconda etc workasking   as welli installed python 2 first and then used anaconda with python 3, after which running jupyter with python 3 is very straightforward, but i did have some issues switching to python 2 for jupyter.hi there, we\\'re having a bunch of openings at glassdoor that require a few years of experience, such as lead data scientist/analyst. if you would be interested or know of anyone who would be interested, please let me know! thanks:-)but using anaconda to set up whichever python and then running jupyter worked for me and was very straightforward to set up.i have also used anaconda/jupyter on windows without major issues; feel free to dm me if you encounter trouble.i used winpython rather than anaconda, but i know people who used anaconda and it wokredthanks everyone - i am installing anaconda right now and taking it from there - will shoot queries if i get stuck - thanks in advance :slightly_smiling_face:also sqlalchemy etc should work fine as well?i haven’t used that library, but in general, if you can get it with conda, i expect it will work. i was able to use keras/tensorflow and several numerical libraries without any problems at all. :grin:wow - i should not worry it looks like:slightly_smiling_face:anaconda on windows works great. for sqlalchemy, at some point you might have to install some special utils functions like sqlalchemy-utils, for which you can follow the instructions on anaconda: i ran into some issues with the latest anaconda with python 3.6. pyspark wouldn\\'t run and gensim wouldn\\'t compile with the fast version. i had to downgrade to anaconda3 4.1.1. which had python 3.5also, the first thing i install with the conda prompt is pip, after which i use pip install :grinning:...or let me know. [not because i want to steal birgit’s referral bonus. no, certainly not. i just like people to let me know stuff. yeah.]fight! fight! fight!haha, i don\\'t think we would get a referral bonus in this case, alan:-) seriously, glassdoor is worth checking out. fyi: we don’t sell doors. not that there is anything wrong with that.do you sell glass?:wine_glass: perhaps? hey birgit how are you?bits, bytes, and bowling - my company, together with accel and she++, is hosting an evening event at grace hopper conference. it would be fun to see a few insight alums. if you’re at ghc, come by and please, do say “hi!” register here: we just did a webinar (recording available) on how we use machine learning at crowdstrike to identify malware and stop/prevent security breaches: i\\'m using conda + direnv, so i don\\'t even need to remember the name of my environments. plus, i can get environment-specific environment variables, which has been really helpful.simply using miniconda while adding the conda-forge channel should be enough, while able to keep track of dependencies, and speeding up installs by using pre compiled packages.i get chills when i see a conda environment file together with a pip requirements file.i put up a poll about a month back with a bunch of python environment options, pretty much all of them got half a dozen+ responses, seems like everybody has their own favorite at this pointthis situation reminds me of the old windows days where things would start breaking and the best recourse was to nuke the os and start overany insight alums working in insurance/reinsurance in the greater new york city area?we have a cluster of folks at new york lifeany pinterest people out there?:  pinterest ppl -&gt;     so many pinterest people out there loli know there’s more people than just 22 going to odsc east! join the  channel! we’re trying to organize a get-together. you should join the odsc channel!thanks  for tagging me in!!!’s partner isanyone knows if there is perhaps a standard template behind this web page? :star-struck: is it halloween yet?  or april fools day?  my worst nightmare for a company:stay positive dalepiggy back on that: sing it, : wearing a vectra t-shirt is ironical and daemonical, canonical and comical, it’s polyphonical, catatonical, and downright supersonical!i sang it in the voice of jermaine from conchords!for all you market junkies: matar: great :slightly_smiling_face: i was just invited for his meet. i was not sure if i was going till now, but i will definitely be there to hear an insight alumnicool! always fun to meet up with other insighters :) hey everyone, we are hiring for several sr ds roles at grand rounds, a health tech company in the city.  please pm me and i am happy to have an upfront conversation about the ds group current and future. i’ve got a bunch of data engineering questions for anyone who might be able to help out with some or all of them.first, are there any books, videos, courses, etc. that would benefit a junior engineer get up to speed with current best practices?has anyone tried out third party etl tools like fivetran? what do you think? how do they compare to built in cloud solutions like aws kinesis?bigquery seems pretty darn awesome, but is it worth choosing over redshift if all your data is already on aws?i use docker a lot for my data science work. how useful do data engineers find it? are there alternative container options you like?: i’ll answer a few (it might be helpful to number them):(books) there are a few books that could be helpful: for architectural point of view, nathan marz’s “big data” (one of the first books on the subject) is great. for db, i read parts of the “nosql distilled” and i think it is a good book to understanding when/why one should choose distributed db vs transactional.(etl): i used (very briefly) a tool called nifi that made common etl somewhat easy (graphical setup) but i generally like doing things as low level as possible. aws kinesis is apache kafka’s private version and it is not an etl tool but a real-time messaging system (some processes produce messages and others consume). kafka is a very widely used tool and i would suggest reading about it if you have any real-time needs.(containers): most data tools need as much out of the host machine as possible and as little interference as they can get (most need fast network, etc.). containers, like docker, make deployment somewhat easy at a cost of computer resources (i call this a docker tax) and in my opinion are not very good way to setup big data pipelines. instead, look into tools like ansible (there are other provisioning tools as well) to setup an automated server provisioning (run a set of commands against a cluster of machines to install/setup certain services/tools) that can be reused to recreate the same exact cluster anytime you need it.: i totally misunderstood the purpose of kinesis. i missed it was a private version of kafka. thanks! that’s great advice about docker. i didn’t consider the cost of using a tool like that.it’s not a full-blown virtual machine but it is an abstraction layer that still costs, especially considering some of the gymnastic moves they have to do for networking to work properly. hope this helps :slightly_smiling_face:so i have a repeated measures anova i\\'m trying to run, it\\'s a 2(between)x3(within)x3(within) analysis, however, there is a second outcome variable that i want to use as a covariate. it appears ancova isn\\'t appropriate, as it wants a single observation of the covariate per subject, not per subject by condition. it seems like a  mixed model might be the way to go, but i\\'m not sure about exactly how to specify the model. does anyone have experience with anything like this, or have a suggestion for another type of analysis that may be appropriate?: mixed-effcts are probably the way to go as they are more flexible options than an rm-anova. feel free to dm me if you want to discuss further.anyone here use  ?  i\\'m looking for something lighter than jira but more featured than trello.  also, the ability to self-host is a must.there is a lot of overlap, like with the work that hal varian does at google and pat bajari at amazon.in general, i would say that causal inference is a particular strength with economists, and they often have expertise with time series analysis that can come in handy tooalso happy to chat more - feel free to shoot me a pmpivotal tracker () could be a good option.sorry, that was my 1yo son. he likes to contribute.as long as he starts early… :simple_smile:hahalooks like he prefers windowswant to kaggle for a good cause? my friends ralph and lalit are looking to crowdsource solutions for the diabetic retinopathy challenge and win it with the resulting ensemble, donating the cash prize to charity i found a 20% discount code for the sf data science (for anyone who needs this) summit on july 12-13code is: sfdatamining20it is from the sf data mining meetup groupnuggetbot check on mscherrermscherrer has a happiness average of 0.3867924528301887nuggetbot check on zkermishzkermish has a happiness average of 0.7317073170731707that can’t be right, there’s no way i’m as happier than  … he’s too happy for his own goodi’m pretty mean to nuggetbot though.nuggetbot check on soumyadsanyalsoumyadsanyal has a happiness average of 1.1071428571428572dang , you are one happy fellowi ran into a problem with sudoing as a user that i can\\'t figure out. i created a database that can only be accessed by the user mamday. therefore i have to be logged into my remote as mamday and not root to access it. however, to access port 80 i have to run gunicorn with sudo, and i\\'ve found that `sudo gunicorn` is then not logged in as mamday, and cannot access the database. mamday has sudo privileges and even mamday  all=(all:all) all in the /etc/sudoers, but to no availstay positive mamday: most databases allow you to specify a username when connecting: for example, `mysql -u mamday -p` or `psql -u mamday`well, my code is connecting to the database with psycopg2, but that connection will only go through if i am also logged in on the instance as mamdayotherwise it will complain that it cannot validate mamdayeven though psycopg2 has the username and password of the databasebasically i have set up an account and database like this: either this is a bug in psycopg2 (and since no one is complaining about it on the first google page, i don’t think it is) or you are not setting up you database connection properlythe solution would be to make sure that psycopg2 uses the username and password pair given to it to connect; not some environment variablei am not sure why it could only be set up with the user instantiated. trying to implement the user in psql without the linux user caused issueswell can you connect to your database with `sudo su; psql -u mamday`?no. with the setup in the link that leads to \\'psql: fatal:  peer authentication failed for user \"mamday\"\\'stay positive mamdayeven with --passwordsounds like an issue in pg_hba.conf, (you’re using peer authentication rather than password authentication)this works: psql mamday  -h 127.0.0.1 -d meetup_db . not sure what to do with it to connect with psycopg2the stack overflow you linked explains: \"this is then evaluated as a \"network\" connection by postgresql rather than a unix domain socket connection, thus not evaluated as a \"local\" connect as you might see in pg_hba.conf\"changing the pg_hba.conf did not work for me i thinkbut it seems that you are basically correct about my issuesolution is: gunicorn --user mamday run:app -w 4 -b 0.0.0.0:80apparently this is standard with unix daemons?is there a politics channel?want to discuss this more in depth but don\\'t want to spam generalwell anyways i started it if anyone wants to joni will say there\\'s a protest at sfo today and lots of celebrities are matching donations to aclu if people actually want to do somethinghere is a link to  in case people want one-click access to it.any body had similar issue like this with `apache flink` before: hi , look into the versions of your components and make sure they\\'re compatible. i think you might want to start looking into your build file (.sbt or maven) and then make sure you have the corresponding versions available during runtime. also, check the download_tech file in your pegasus repo for info on versions you have installed. i did check for the versions and even tinkered with downgrading.hey all .. just want to know your opinion of automation of data science using software like data robot or skytree .. and also, any experience to share about skytree? .. i have tried it and found it to be interesting!: alumni - let me know if anyone is interested in an opportunity at tesla (contact me direct).  software/data/devops engineerstopic summarization == visually explore a topic model like  ?, or summarize text in some other way?in case that is what you’re looking for,  is a python port of the package  linkedthanks kevin and john.  those are useful, but i was thinking more like a force directed graph with clusters and more info/structure at nodes.  most tools look kinda crappy, but trying to avoid building something custom if i can :disappointed:interestinganyone have any experience with sentiment analysis? specifically decent performance in out-of-domain predictions? training the model on the amazon, yelp, and imdb reviews.hey folks. any one have experience with predicting start time of the process in the time series data? i have data from the bed sensor and i need to predict when person went to sleep. please pm me. aside from renaming of the input columns, i don’t think there is a lot of preprocessing that is required. i use it to make forecasts for hundreds of columns and it does a pretty good job without a lot of data cleaning or parameter tuning. that said, in the future it would be nice if it had an option to feed in multiple columns at once and to return even a coarse type of performance score given the data you feed it. some of my time series columns have very sparse data, which it doesn’t handle well, and since i can’t visually inspect 100’s of columns, returning that type of metadata could be useful to get a better sense of how it is behaving across all my datasets (in the meantime i’ve written functions to perform some of those tasks myself).tomorrow networks is looking for a data scientist (phd not required). tomorrow networks is a new york city based mobile advertising company. feel free to contact me if you or anyone you know is interested. hi all, i’ve been having some issues with jupyter notebook autocompletion it’s randomly crashing the kernel. wondering if anybody else has faced this? currently using  jupyter 4.4 and ipython 5.5 on python 2.7. also what are your opinions on transitioning into python3? thanks!do you mean tab completion? it has become pretty unusable for large dataframes (~1m rows) in my experience. i haven’t had it crash the kernel but it will keep the kernel busy for way too long.facebook prophet is a pretty cool open source library.  quite stable and can handle outliers and missing data sufficiently well.  however, there are many parameters --intuitive parameters-- defined with default values to accomplish the task of curve fitting (it is not a temporal model), which could make a misleading forecast.  for example one useful parameter is the number of change-points that it defaults it to 30-40 (can’t recall exactly), and the reason being, they are dealing with facebook data of daily samples and 30 or 40 is quite small.  however, in case your sample size is small, the model would basically treat every data point as a change point and the forecast becomes very unstable.  the other nice parameter is the mcm_sample which you would only benefit if you understand its application in their model.  luckily, the paper that accompanies the library is very detailed and quite easy to read, so i would recommend reading the paper carefully before applying it to your work.  once you understand the model, applying the api is relatively a simple task.  the only downside i faced was the fact that the model needs to growing capacity for every data point if your function of interested for the growth model is logistic as opposed to linear.  in many of my use cases, finding that carrying capacity would require the entire path of research so somehow forced me to stuck with the linear growth function.: my transition to python 3 was seamless. the main thing you run into is remembering to put () around print statements and some things that used to return lists now return generators. and actually it has better stability than 2.7 (i had a bizarre problem with something resembling a memory leak in 2.7 which i found out should go away in 3). highly recommend switching. also, i think most companies are making the switch as well. regarding transitioning to python3, it’s not really an option anymore — all major scientific python packages are ditching python 2 in less than 2 years:  yes tab completion hm. i feel like i\\'ve been dealing with 1m+ rows for a while now, it\\'s a very specific circumstance that triggers the kernel crash. i guess i just have to get rid of my tabbing habit :(.  cool! the print part is definitely something that i\\'ll need to get used to. i am looking forward to not having to import division from future though.  thanks. i guess i knew python 2 was getting phased out in 2 years but hadn\\'t read into the scientific part which made me wonder. and f’\\' strings will change your life!definitely looking forward to that one as well :stuck_out_tongue:honestly as annoying as this is, its for the best.maintaining things for slightly different versions of the same language can be a bigger pain in the butt than it needs to be. this forces people\\'s hands as well, which they would have preferred not to have to do, but as has been the case in many projects, if they leave the door open for groups to avoid refactoring, many won\\'t do it at all.what would you like to learn? hey michael, want to email me  and i’d be happy to intro you to the sv team if you’d like.it\\'s for a broad team including analysts, data scientists, and data engineers. and we have a pretty large amount of money, so i\\'m open to any suggestions.i am interested - if others from the last de batch are coming. :slightly_smiling_face:hey  did you get the invite? you should’ve got the invite emailed last thursday…lemme know if you didn’t and we can resend itno i have not seen any emails form insight. can you please resend it ?if anyone else is at scipy 2016 and feels like meeting up, feel free to dm me.:   game of thrones if it was happening in silicon valley (and job titles were houses).. okay yeah it\\'s a stretch but it was a lot of fun with photoshop :smile: *hey new york!* i’m organizing a data-centric meetup on august 25, hosted by my company, . check it out!*a million trees, a billion oysters: enumerating urban nature*how do you count and locate all of the trees in new york city? how do you measure progress toward restoring 1 billion oysters to new york harbor? a crucial part of any data story is how the numbers are collected, and the reasons for doing so. at this edition of data io, our speakers will describe their approach to enumerating nature, with an emphasis on the urban environment.*speakers*jacqueline lu, director of analytics for nyc parkspeter malinowski, director of the billion oyster projecttbdlocation: tba shortly (one of two locations in soho or nyu)does anyone have experience with embedding a dynamically generated html file inside another html file, particularly using flask?  i’m able to do this once successfully, but if i make any changes to the python code that calls flask, the changes are not recognized by my browser, even though i can confirm that the dynamically generated html file has changed.  it seems like some kind of browser cache issue, but i’m not sure.  any help is appreciated!hi  we have a dedicated channel for flask: askflaskcould anyone recommend either a person or ‘firm’ that could help us define data science architecture better? i work for a large org in the healthcare space that’s trying to create a data science layer that spans multiple existing product lines or verticals and we could use some help in figuring out some best practices/architecture. we’re hoping to create a flexible and scalable architecture that can ingest data from a broad variety of sources and layer ml predictions on top. for reference – we have all kinds of data – marketing, crm, content, patient, medical. our challenge has been coming up with an architecture that would work both in client deployments and in our own environment, and that can ingest/source data from multiple sources. thanks for your help and p feel free to dm me if you prefer that.i know some great folks at svds () including insight alumni   . they do that kind of workthanks, looking at them now!a couple of years ago, i followed a hashtag for a few months. the twitter api was super easy to use--i had it up and running in a day. sentiment analysis on the tweets wasn’t particularly useful, but the spikes in the hashtag over time were directly correlated to news events, etc. i’d be happy to pass along some scripts, but be forewarned that i did this pre-insight so it’s not so pretty…sounds like you might need some data engineers! let me know  if you or your company might be interested in insight de fellowshard to argue with gelman.  my impression is that if your trying to create something like that you should probably be hiring a team of des to design, build, and then maintain, because that\\'s a pretty massive undertaking.so there is no issue with centering positive only continuous variables at zero? e.g., what does it mean to have negative age or negative income? agreed on the de part and that\\'s part of the overall plan, but we\\'re also trying to understand what technology platforms we should be looking at, so we can hire the right mix of people.negative=below the mean, positive= above the meana quick update: not using the elasticnetparam (the example in the documentation is lr = logisticregression(maxiter=10, regparam=0.3, elasticnetparam=0.8)) helped with the model, the number of jobs triggered is still a mystery (i should dig in to the source code i guess)#drumpfstanford deep learning for natural language processing course is now underway: apologies for the shameless self-promotion, but this may actually be relevant to a few. if you’ve ever wondered how to get started with neural networks in python, i’ve made an intro tutorial (complete with a jupyternb) about getting starter with chainer. fantastic tj! looking forward to working through it carefully. love the idea of getting bonus points :simple_smile:absolutely shameless jejejeje sounds cool. i’ll take a lookdo you do workshops tj? :simple_smile:this is great  - thanks for sharing, i look forward to reading it thoroughly later!thanks everyone…this post was based off an internal workshop i did at stitch fixyeah i read this the other day , ; have you had a chance to play around with tensorflow at all?i haven’t had a chance yet to thoroughly investigate tensorflow yet, but i get the impression it’s quite similar to chainer in some ways: awesome! i was also going to ask how chainer compares to tensorflow and theanochainer doesn’t require any compile time like theanothis has the disadvantage of providing a bit worse performance, but you’re also not sitting around for 45 minutes waiting to be able to run your training script. so for shorter projects and prototyping i think chainer has a leg up.stay positive tjtorresthanks, nuggetbot…i plan to.:thumbsup:great job tj! quick question: why do the light blue lines bounce around to either side of the final magenta state? is the nn overshooting the minimum of the error surface? just wondering, since the learning rate seems rather small for this to be happening. then again, i know nothing about regression with nns :simple_smile:i’m not entirely sure, but i think in that example i was just going for using different optimizers and ended up randomly choosing a momentum sgd which would likely overshoot a bit in order to try and avoid local minima for loss surfaces which are more complex than the mse in that examplestay positive tjtorresah, got it! never heard of momentum sgd, but now i see that it’s proportional to the learning rate plus an additional “velocity,” which leads to larger step sizes than the learning rate alone. thanks :wink:and go lick a doorknob nuggetbotrecommender system blog post (good flow chart): :clap:hi kathy, salesforce also has undergrad/grad-level data science summer internships. i don\\'t know much about the beginning of the recruiting process yet, but here\\'s a general link: anyone know of good open source tools to visualize social networks? with ability to incorporate additional data, e.g. geographic?`networkx` in python. i wrote a r package as well, `ggnet` that’s at . you can use something like gephi as well.neat : there are also several other very nice r packages for doing this type of thing, see e.g.   yeah… i’d go with either of those two options over `ggnet` — those are really solid. thanks for the links npat the end of the day, if you want geographic stuff, you ought to just use cartodbthanks   hey all, we have a data analyst position at backflip studios in boulder, colorado. we’re a mobile gaming company that produces titles like dragonvale, and it’s a really fun environment full of smart people (and fantastic benefits!). if anyone is interested in applying or has any questions about backflip and the team, please email me at . please feel free to pass along my email to anyone you know who might be a good fit.gotta admit, i was definitely addicted to dragonvale for a minute there(edit) meetup canceled. sorry!! i think there is an online course that accompanies the book \"lean analytics\"its on udemyhi, everyone. did anyone have any experience on transferring xml file to csv file?i\\'ve done xml to json, but assuming you have a simple* xml it should be fine. what\\'s up?* the big issue you\\'re going to run into if you have a complicated xml file is xml is nested in general, and csv is columnar.hi all. i am tying to figure out ethnicity and/or gender from a list of names and no training data. a quick google search returns a couple of options at github, but without any metric for performance. does anyone have a suggestion for where to look, or (more generally) an idea about how reliable these algorithms can be? i do expect a multinational sample. thanks! on ethnicity/race i’ve come across:andhave you already looked at these?this looks awesome. thanks !you’re welcome  ! should be possible to adapt that to gender too, might take some work. i had a task at one point (never got done in the end, super low priority) to build a similar analysis for protected classes w.r.t fair lending.anybody know of python libs/code for entity matching?nicholas.k.baro: what type?dedupe ain\\'t bad stay positive mpanciathis is the most beautiful way to visualize machine learning i\\'ve seen: *heads up*: i\\'ve started  for planning group rides. hopefully this will be an improvement over randomly emailing people. join if you\\'re interested in riding! pinned a message to this channel.in case anyone wants to play around with some interesting python edge cases:plotly also has a library to bind to pandas dataframes called cufflinks that i think is great (very similar to jake\\'s pdvega, but more feature-full). folks are generally wary of  because of the online publishing aspect, but you can also use it in full offline modeif only they did a coconut one…we\\'re hiring an infrastructure engineer and a growth analyst at edmodo (). let me know if you or someone you know is interested at hi everyone!  am putting together a primer for my portfolio companies on diy data science.  would love your tips on how you wrangle/clean your data!nice ! but... no images of battleship boards? i want to see the match!when i have a bit more time i want to make a web app that allows you to playand thanks!what do people use for workflow management? we’re finding trello to be ineffective as the number of projects and people have grown. we use jira on bitbucket. it allows for more organizational complexity than trello.we use asana, but it isn\\'t great for our small teams. since we\\'re pretty self directed, updating it is our job, and no one is really on top of it. :confused:i can see how it would be great if you had a manager person doing itwe also use jira. it works fine, but, i\\'m beginning to think that using it too much is a process smell.i’ve become quite partial to pivotal tracker.jira here as well for the larger group, and confluence for some documentation / collaboration. though i still use trello for my own weekly planningjira for us and with github integration has anyone tried github\\'s workflow management that they launched a few weeks ago?not the workflow management .. tried their new pr review process though. yeah i\\'ve used that, it\\'s light, but still nice!we use jira. it\\'s clunky and bloated. the ultimate irony of a tool used for agile development!also not a fan of jiraapart from some clunkiness i find it’s a little too linear for ds developmentyeah, we have jira toojira/confluence/bitbucket here. it\\'s fine. nothing great, nothing awful. all in how you use it.yodiz/confluence/bitbucket for us - easy to get going. been meaning to look into getting jupyter and confluence to talk nicely - anyone tried any of the macros?hadn\\'t even thought of it! would love to see what you figure out.quick reminder from your friendly insight admissions team: applications deadline for insight data science and insight health data science programs is one week from today on october 24th! make sure to tell anyone you think might be a good fit for the program to get their applications in on time and please do send us your recommendations! . a friend needs help debugging an ios app for nov 1 launch. gig pays $5k. any takers? the app is sport technology - enables multiple smart phones to synchronize and take pictures (perhaps movies also) from different angles .. i have some experience building ios apps. i\\'d like to know more about the app and the bug.does anyone have experience implementing a genetic algorithm to solve the \"nurse scheduling problem?\" any suggestions for good references?dan would. he\\'s back friday?excellent, thanks - i\\'ll hit him upi have some experience if its just the basics. but to be honest its been over 10 years since  i\\'ve used oneare you using deap?i have two tickets for explosions in the sky - tomorrow at terminal 5.  anyone interested?  send me a direct messageiclr 2016 talks streaming on periscope: has anyone built a transit shed map using google maps api or perhaps mta api? i\\'m looking for suggestions of tools/libraries, preferably in python. trying to avoid using arcgis which is slow and buggy.  would you suggest using networkx for smthng like this?  any ideas?  thanks in advance! haven’t used this myself, but it looks promising — *\"dopelearning: a computational approach to rap lyrics generation”*nice, i started doing a project like this last year (with lackluster results)i wonder if \\'rack city\\' was in the training set one thing to consider is how fast would this need to be processed. how much lag can you tolerate 1s, 1h, 1d, 1wk? if you can tolerate several hours or days of lag then batch sounds simpler and easier to build. if the lag has to be lower, i’d recommend building a change capture system that pipes to kafka or a msg queue for other dowbstreams to consume and update their local caches or stateif you’d like to chat more about this later feel free to dm me +1if you just need storage of data that you\\'ll eventually query later in daily batches or partitions, the tools may be much different than if you need to take action on the data as it streams inwhat\\'s the source of the data initially?thanks a bunch  and .  i only need to update the tables nightly, but want to store one row per user per day.  the data initially is coming as a packet from a gsm chip, then stored in an s3 bucket either json, text, or csv (the software engineer i\\'m working with left that up to me).  then i want to take those numbers and calculate daily summaries for each customer.  (it\\'s for solar power systems in the field, so stuff like the max voltage reached by the panel on the previous day and the time it takes the battery to discharge.)  i can tolerate several hours of lag in processing the data and adding to the system performance tables.  we\\'ll likely be surfacing the most recent row for each user to an internal interface, but they\\'ll be accessible with a primary key so hopefully fairly fast.  we want to have historical summary metrics there as well, mostly for modeling (which can be on the order of hours, though would be nice to be faster.)  right now the amount of data is small (on the order of 100k users per day) but i\\'d like to build to handle at least a 10-fold increase without dramatic changes to the architecture.  does that seem reasonable, or should we just build for what we have now and expect to have to make changes?  commented on ’s file : if you go the docker route, dm me if you have questions. i use the jupyter stacks for many products at o\\'reilly and although i highly recommend, sometimes there are issues :)previously had a non-technical manager (background in finance and marketing). was a tough relationship because she never understood what i was doing. current technical manager is much better.not me. my manager is technicalhopefullyhey folks!  coming from academia where there was a very blurry line between professional and personal computing, i’m curious how/if people separate those two in the real world  - do you have a personal computer in addition to a work laptop? i’m debating whether to buy a new laptop to replace the university-owned one i have to return when i start my new ds job.personally, i\\'m looking forward to leaving my work computer at work when i start my first ds job in jan.i definitely have a personal computer because i don\\'t want to touch my work even remotely when i\\'m not at work/on holidayi keep a personal laptop and work laptop. while some companies don\\'t care if you put personal stuff on work machines, others have very strict it policies. you should ask once you\\'re employed. in general, it\\'s best to keep a personal computer as you\\'ll most likely have to return the work laptop if you ever leave..i have a work laptop but the info on it is sensitive enough (hipaa and company info) that i try to avoid doing anything not work-related on it as a safeguardalso depending on company, you can\\'t take your computer to certain countries if you ever go on vacation therehaha, nice - hep has had the same effect on me :slightly_smiling_face:if you want to do any remote working, you\\'ll need to be able to remote from your personal to work compthat would be a huge no go at my company...good point about returning it.  additionally, i’ve been a mac user for the last decade and will be using windows for work, so all the more reason to keep a personal (apple) laptop…something for me to keep in mind when i startwhere/how do you draw that line?  what about answering personal emails?  that’s the one i generally do from work on a regular basisamen to thathep\\'s effect on me: dry, text-laden presentation stylehaha, trying to overcompensate with gifs-a-plentyi just got a personal laptop separate from my work laptop. mostly because i want to do some work on my personal that i don\\'t want to get mixed with work. and my work laptop has an automatic back up that stores everything and i don\\'t really want them to save my personal projects.on a more subtle note, many companies have policies that state any software built using company resources (including work laptops) becomes property of the company. if you like to develop your own code and want to keep it, i\\'d use a personal machine.i do that kind of thing all the timei\\'ll watch netflix on my work laptop, etcbut i\\'m not going to do things like watch my bootleg nfl streams on it, hahahi all, especially those in the nyc area looking for a new opportunity! i have a couple contacts at the startup asapp, where they are interested in growing their data science and data engineering teams. if you’re interested in being connected with them, please send me a message! i’d be happy to connect you.some info — asapp is focused on automated customer service (chatbots) - routing customers to experienced human agents, providing agents the appropriate background infotainment about the customer and call, as well as suggestions of what to say,while moving towards full automation to resolve customer inquiries/problems. they will be doing a lot of nlp understanding, prediction and sentiment analysis based on several decades of call center data, with the goal of saving both agents’ and customers’ time and importing the customer experience / time to resolution.haha, nice dividing line, good to know :slightly_smiling_face:oh yeah, wow, i didn’t even think about that!  thanks for sharing your experiences!seems like it !in my experience, this sentence \"machine learning engineers primarily come from data engineering backgrounds. they’re cross-trained enough to become proficient at both data engineering and data science. a less common route is for a data scientist to cross-train on the data engineering side.\" is not really true. at least at my company, it is the opposite.thanks romesh!has anyone used airflow with docker swarm in aws? i could use some adviceour company (glooko) is looking to fill a data/business analyst type role asap... we need someone who has analytical chops and communication skills. ideal candidate is a recent grad, bachelors or masters, with high growth potential into either data science or product management, depending on candidate\\'s interest / skills. would appreciate if you could refer anyone you know who\\'s interested! would anyone know any solid resources to learn xgboost fast? i would look at the video and slides from the author of xgboostthanks!survey data!  i just participated. there will be so much data in there. that might be a ds project for the upcoming batch :wink:just got an email about this if anyone is interested in fielding a team:dear friends, we\\'d like to invite you to participate in round 1 of the big data analytics world championships 2015 (business and enterprise) on saturday september 25, 2015 (4 weeks\\' time). the current world champion is stéphane sbizzera (kpmg france).     thousands of the best data scientists, engineers, statisticians, computer science and data analysts compete in two online qualification rounds (4 hours each). the top performers are flown to austin, texas usa to compete in the live world finals. the focus is on business, mobility and enterprise data skills with real-world case studies, multiple-choice and short-answer questions. feel free to share with work colleagues or friends that might be interested. register at .50% off ebooks and 40% off books from o\\'reilly with the code: authd  not sure how long it will last...in some cases i have gotten cheaper prices than amazon!call me crazy, but this doesn\\'t seem that revolutionary. they have really accurate clocks. what does that buy you? ^ yeah i got that sense too... but then it\\'s wired writing, so not that surprising.also \"faster-than-light coordination\" made me cringebuzzword bingo but strictly speaking correct - if you can finalize an atomic write operation simultaneously worldwide, it’s as if you had sent information from one place of origin faster than light. of course we know that’s not the way information flows.funny actually, atomic clocks is the naive answer to the consistency problem before you delve into all the practical ways of having eventual consistency but someone went out and actually did it. :smile:\"it’s as if you had sent information from one place of origin faster than light\" i\\'d say it\\'s more just assuming you sent it that way :stuck_out_tongue: but i get youyes, in practice it’s just a careful way of distinguishing event time, write time, and agreed-upon ready-to-read time (haven’t read the white paper so it’s probably called something else). pretty much what dataflow was for streaming, this is for databases. pretty neat still.yeah, physicsy grumbling notwithstanding, it works, i work at domino data lab, which makes a knowledge sharing, access control, automatic version control product for data scientists. we have customers from instacart to allstate. let me know if you have any questions or if i can direct your questions to someone elseon the subject of tools that would be nice to have at work, does anyone here know of tools for managing the integration of ux/ui designs with analytic/event instrumentation? especially one that has the ability to do version control for both designers and the back-end analytical schemas?anyone have experience with \"research repositories\" at their companies? along the lines of airbnb\\'s knowledge repo but less technical (aka supplying marketing/execs/etc with critical facts about company research for presentations and sales) fyi deeplearners wayfair has something called the knowledge base, which is mostly just a wikids opportunities in the uk with  - what kind of metrics do you guys usually use when you talk about model performance/accuracy/… to non-technical people? assumption here is that things like roc/auroc etc will be too technical.% improvement over previous effortsif talking to business stakeholders mapping it to a dollar benefit also goes over well, could also build a quick spreadsheet showing changes in funnel/process and cost/revenue numbers before and after.estimates of course i assume this works best when you have previous models to compare, would you use chance as the baseline otherwise?no, i woudl just compare it to not using a model, and sort of fudge it :slightly_smiling_face:  i’m assuming these non tech people are like c-level or upper managementthey want to see stuff they can tie to roi or improvement somehow i second .  a couple times i\\'ve come up with some baseline to compare to, even just \"assume this is a constant value\".  then you can say the model gives % improvement over a basic assumptions model.cool, thanks everyone. guess the question now is what a basic assumption model is :wink:  there\\'s actually a repo: ah, but it\\'s county level, not precinct &gt;_&lt;are there any canadians here who have any experience with getting a tn visa/ssn? could use some help (pm me)! thanks :slightly_smiling_face:you can also contact insight, they have immigration lawyers: ^^^i basically let adroll\\'s lawyers handle it but feel free to pm me any specific questions. ^thanks everyone :)  i think that\\'s what i\\'ll need to do or at least for the company that\\'s thinking to hire me how to think like a data scientisthow to think like a data scientistdoes anyone have a link to mike’s slides from yesterday’s presentation?thanks for the recommendation  looking at it nowhas anyone used this for photo projects before: ?seems like an incredible dataset: thanks ! :clap:can anybody suggest a good site to purchase a plane ticket to fly international (canada)?aware of the big guys (expedia &amp; co.) but looking for alternative choices...  i use  and thanks for the tips , anyone ever use sqlalchemy to automatically read a pandas dataframe into a mysql database using the \"to_sql\" command?: i just use : yesyes, i did for my project:  . search for engine and you should find some examples. no warranty of correctness or nonstupidity!that\\'s for   btwi used postgres because of the mysql/utf/python issue some of us ran/are running into.thanks  for the extra reference! i was able to get it working with  \\'s helphello! does anyone know about a public data set on precision farming, urban/indoor farming, or more generally plant growth in relation to environmental conditions?no, but perhaps you can find someone who works here to chat:thanks! i\\'ll take a look.unemployment has similar issues, and you can get quite different numbers by using different methods. do you have any connections at bowery farming?no i dont, sorryawesome, good luck!who should i email at insight with referrals for partner companies?is it a silicon valley company?no - clearwater fl. probably something for the remote program.like the actively looking for work qualifier? is a solid go-to for all of insight, and  and  (katie@ and lee@ ) are the right folks for the remote program. :slightly_smiling_face:  thanks !maybe try its a digital repository where lot of plant science journal publications ask to deposit data and its open. doing a quick search for the terms you mentioned, it does throw results with data files.... not sure of the quality though, and i suspect it will be a major exercise to clean and compile them togetherin case anyone is looking for housing in south bay(*cough* 18a fellows?), my roommate\\'s moving out in april. check out  for the post (posting here too cuz the channel looks kinda dead &gt;.&gt;)think insight could get the white house to be a mentor \"company\"? :wink:obama could come for the company visit: get on that!you heard about this, right? yesonly because i keep up with the dj scene:sound::sound::sound::sound:nuggetbot: animate me dj kitty does adding single \\' \\' like : person=\\'{0}\\' help? i also assume using read_sql and read_sql_query are the same? about security though, i think you can also use the sqlalchemy orm to avoid some of the security issues you mentioned. we here at grammarly are looking to hire a data scientist in san francisco. if you know anyone who’d be interested please let me know. the job posting is here:  i know some fellows who may be interested ;) i’m all ears katie!i\\'ll message you!hey, anybody at nips currently? we are hosting an event tomorrow and would love to see you there!a lot of my lab is, but none of the insight fellows unfortunately :disappointed:hoodline is hiring a data scientist! that’s strange to seeeveryone needs data scientists :wink: you willing to move to stamford, brad? :wink:heck nothough they do have an office in nyc too...stone cold svmshi fellows, i would like to use a big csv file, like 80g . i would be able to load as rdd in spark but i would like to use this as a matrix by using spark and run with my algorithm code in python.  anyone has a tip for me please?could you elaborate a bit on what you mean by use it at a matrix?also what have you tried doing so far?and by python do you mean pandas/sci-kit learn?but in general pyspark should work for thatexactly: load the csv onto hdfs first (`hdfs dfs -put`), and then read it from spark.the good thing about doing this is that it distributes your data in your cluster first.once you’ve done that, you could do `sc.textfile(”filename\")`hey crowd, anyone know any good data visualisation blogs? my preference is for simple data presentations or ranty blogs, but i’ll take anything :pray::skin-tone-2: and :    thank you for answering. i put the fule in hdfs  and was able to read it as rdd using  sc.textfile(\"filename\") . and i have a separate matrix calculation python script which i have tested with 10x12 csv file. now i want to load 80g csv file in that python code.  up to now, i am able to read 80g file and can do some basic command. rdd.first() , rdd.take(5) something like that. and also can make the first 10000 line as a matrix. but i have 1.5m lines and 1200columns in the big file.  and i cannot read this as a whole matrix which i need to do  in my algorithm python code.in the algorithm, i have to matrix calculation.     uploaded a file: here’s an interesting piece on ethics in data science: hi everyone! the boston fellows were given a document with references to many data sources. one of these references is mimic () - de-identified health data associated with ~40,000 critical care patients. did anyone use this data set before? if so, how long did it take you to get approved for the restricted access? thanks!: i think  used mimic in her project if memory serves me correctly. one of the boston pds may know if she did or not too. i\\'d check with them (and to also get her email just in case.)she wasn\\'t in my cohort but the one before itanybody has suggestions for 3d plotting in jupyter notebooks? i tried using plotly with jupyter before. if registering with an .edu account, you can get more plots access (or unlimited? i don’t remember)you can also use plotly for free in offline mode, or set up your own serverthere’s some limitations with sharing plots that way though, i’m not sure how easy it is to embed that javascript into the notebook itself, i had some issues with thatbut that was a while ago i know seaborn can do some basic 3d plotting. i\\'m not sure if that\\'s what you are looking for. if you peruse this github for islr () you will find some examples.also: i would assume that bokeh and plotly allow you do to 3d plots (more easily) too.those are more interactive, if that\\'s what your goal is. thanks for the tip!:thumbsup:jonathan_whitmore: the gift that keeps on giving!oh, one last thing : i realized the last link might be sparse in examples, but here\\'s a notebook where someone used matplotlib to do some 3d plots (which can be easily adapted into seaborn etc) good luck!i’m also interested in using this dataset for an insight project and i’m in the process of applying for it. not sure how long it’ll take…does anybody have experience with careerjet api or apis of other job searching websites?does anyone have experience in importing a video in python? pinned a message to this channel. pinned a message to this channel.what are you trying to do?opencv has support for working with video if you want some data on how much stock options can be worth it looks like it takes about a day (i was just granted access). i already signed up for a consulting project though. have fun with the data set!hi guys. does anyone have a lead on a user dataset that i can do churn analysis on?hi everyone! does anyone have experience with using rnns for machine translation? my consulting project has a database of ~5000 credit card financial transactions that include an often-messy string corresponding to the business name (e.g., \"united tx\", \"wells fargo ban\") and human translations of these business names (e.g., \\'united airlines\", \"wells fargo\"), and i\\'m wondering if this would be enough data to train an rnn to correct these sorts of mistakes automatically. the client has a strong desire to explore rnns for automating this task, but i don\\'t have enough expertise to know whether this is likely to work or is simply misguided. any input would be extremely appreciated!!i want to extract frames from the video i would start with some basic feature engineering or compute edit distances and then try a service like open calais for entity resolution (i’m assuming you’re interested in mapping company names to business entities).  in other words, try something simple and interpretable first.  btw, this is a problem we deal with a lot where i work so feel free to pm me with questions.thanks! i also changed my min:grinning:d..jeff: agree that opencv is the best python tool i know of for that. ffmpeg is the other workhorse - it\\'s old, but it\\'s been around for a reason and can take a billion parameters if you need to customize the sorts of frames you want to extract.ffmpeg = command line tool, not python, although you could call it with a shell command from python if you want.i use pyspark with python 3 with no problems…wonder what i’m missing :stuck_out_tongue:just have to ensure the driver and each node is pointed to the appropriate python binaryre the larger question, i’d say python 3.  it’s only a handful of packages that i’ve run into the package compatibility issue, and documentation/writeups for data projects is increasingly in py3do you want to build a desktop, buy a desktop w/gpu, or rent a gpu on the cloud?all options require you have a budget in mind. a great single gpu for deep learning will cost around $1k. i’m not sure what the status of windows drivers for deep learning is (it may actually be plug-and-play) but if you really don’t want to do a lot of configuration yourself, know that setting up the drivers on an ubuntu system w/cuda is a pain in the ass.your best bang for your buck will probably be a gpu instance in the cloud, especially if its for a one-off project.if you want to do longer term personal research/play, rolling you own becomes more cost effective. some people need months, not days, to whip out three functioning apps :stuck_out_tongue: it was a week :stuck_out_tongue: :stuck_out_tongue: and bibli2 took us almost 4 months to setup  :stuck_out_tongue: :stuck_out_tongue:yeah that\\'s way worse than i expected of you. shame!i am getting older little pixipentreath, machine learning with spark.  it is a bit dated but has examples in python, java and scala.  karau, learning spark.  has examples in same three languages.  same comment.  edx has a good intro course all in pythonyeah, individually, by coalescing the labeled data up from the leaves to the node in question.we are looking into whether the leaf predictions can be strengthened via the ancestors models, like done here: out of curiosity is flask recommended for production web serving of dynamic content? or are you better off with a dedicated server with a python cgi plug in?i\\'m hearing tensorcon is a *scam*, so don\\'t sign up! () tensorflow team at google have confirmed they are not hosting it and are not affiliated etca tensorcon con?tensorkhaaaaaaaaaaaaaaaaan!who could have predicted that?predicting cons with tensorflow: read all about it here….do they want you to connect through citrix or something?whoa, my slack was not updating, that message was meant for haha! i have no idea actually. they will clarify for me. but in the meantime i\\'ve found my own work around :) have you connected through citrix?yeah, i have used ssms through citrix or also via an install on my own computer with a vpn. i am sure you\\'ll have some reasonable options although citrix can be really annoying to use sometimes!ok thanks! good to know. maybe i\\'ll bug if i have some issues :)hello i was wondering if anyone had experience writing pyspark processing functions and could answer for few questions? please pm me!i have! ill pm you me too, dm menew blog for data engineering! also front page of hn!:+1: nice!has anyone looked into tools for estimating heterogeneous treatment effects, like causal trees/forests?this article just came out in political analysis (no paywall). includes replication code (in r) thanks! i\\'ll have a lookwe used the `causal forests` r package which was a goddamn nightmare. it’s now called `gradient forests` or something like that. i was inspired by j. faghmous\\'s talk @dataengconf. your team? i was talking to him after his talk and he said that susan athey came out with an updated version of her code ~6 months ago that might be better, but the original version they used was a bit buggy and he would consider using it as a beta sort of thing rather than a prod sort of thing:slightly_smiling_face: i ended up posting my code on their gitter, and somebody tried it and couldn\\'t figure out why it didn\\'t work right.but then i tried this, which does exactly what i want (fitting a gaussian distribution\\'s intrinsic width as well as its mean):  yeah that’s us. we just got a rejection for that paper too!hah, go figure - astrophysics to the rescue :slightly_smiling_face:interestingi\\'m having a hack week this week and thinking about working on a python implementation of something similar to return subpopulations with different responses to a testthat’s a good idea kevin. a hacky backup which i’ve never seen implemented but should work is to split the data, go fishing on one set looking for relevant subgroups using what ever algo, then test whether the subgroups remain significant on the hold out set.what sort of other methods did you try for the first part of finding relevant subgroups?for this project we only used the causal forests packagewhat about propensity score matching?  model the likelihood to receive the treatment then estimate treatment effect across similar groups with similar propensities.  i worked with this package long ago for this and it seemed decent: about multi-level models with varying slopes, one for each group? that way you can get estimates of effect for each subpopulationhey all, wondering if you could give me a sense of where data science sits in your company. what’s the team consist of and what’s the reporting structure like (outside of the team, not internal)?under the product org, which we’re not super happy about. manager of ds reports to director of data (who also has das under him), who reports to the vp of product who reports to the ceo.: you might want to look into aws glue.  i haven’t used it yet myself, but it looks like a promising method to use spark without having to worry about emr clusters.  i believe you can set timers for jobs as well within the glue interface or via the aws cli.all good ideas --  i think in this case we don\\'t know what the groups should be a priori, which is why we were hoping for a technique that could generate the groups. i suppose hierarchical models should be able to do this as wellah, interesting - i\\'ll definitely look into that!hi all.. any tips on how to present data science to senior management so they can come back  with  business use cases? im thinking of having an overview of ds in general in our area (regtech) and showcase some algorithms with some concrete use cases..  the goal is to make management rethink about their problems, pain points, areas of opportunity and frame these problems as data science/analytics problems so we can build a solution for them.  thoughts, suggestions, experiences?thanks jeffrey! we’re currently structured under a chief strategy officer (which i like), but trying to think of a new solution as the company evolves. looking for opinions on reporting to engineering or product or ?when doing this at zipcar we found that the best way, even if it was to solicit top-down project ideas was to show examples of completed projects, and additionally suggest projects we thought would be high roi and could be solved in a data driven way. this made starting the conversation easier, and made it an overall collaborative effort.thanks emmanuel.. yeah.. i was debating a bottom-up vs top down approach in soliciting ideas. but then settled on the top-down since they their buy in is e$$ential not just in getting started but also they are influential on the making the users use the intelligent solution or the data driven product.from my experience, reporting to engineering could lead you to be responsible for building your solutions into apis and data products, while reporting to product will lead you to do more ad hoc work and experimentation to guide the product roadmap via analysis and decision support. of course a company needs all types of work done, so both roles need to exist in one form or another. we have those two roles mostly on different “teams”, so for example a data scientist on a product team might analyze data and hypothesize that a new checkout flow would increase conversion, build a prototype, run a test, then the more eng-focused team may build out a production version of that prototype. we’re a small team (2-4 currently depending on how you count), but situated inside of tech / engineering. the product folks operate in their own space between us and business stakeholders. like kevin suggests, we’re very much responsible for building and maintaining production systems. we work alongside engineering + software development and all report to a cto. cto handles the back and forth between tech and the rest of the business, including product.a note about the top 10 example that niharika, mudit and i discussed this morning. if you look at the program am listing around page 62 an 63, you will notice that the mapper class declarer a map, that maps a \"reputation\" (which is derived from the input record) to the entire record. as it keeps reading more input, the map keeps growing. whenever the size gets bigger than \\'k\\', the \"first_key\" is dropper. after all records are procssed by the mapper, the map contains \\'k\\' elements. now the \"cleanup() is the critical part of the process. it\\'s this \\'cleanup() that takes the map and emits a (key, record) so the reducer can combine all these inputs and pick out the top \\'k\\' elements. if you look at the program, the mapper class has two mthods, map() and cleanup(). presumably map() gets called every time for each input, but cleanup(0 is called after call input has been processed. the cleanup is doing what a local combner does. but it is not clear where cleanup() is getting called. it\\'s clearly not called explicitly from the mapper anywhere. it must be happening behind the scenes my feeling is cleanup() is being registered as the \\'combiner\\' method. but this needs clarification.fyi to whoever is considering a project using airbnb data this session (i heard there was someone but didn\\'t get your name), my project used airbnb data and i\\'m happy to discuss what i did and share my code, i believe someone from the first boston session scraped data from strava. is that what you mean by “running records” or do you mean e.g. world records? trying to remember who it was.:  has done something like thisdoes anyone have a suggestion for a database ide like  ?  preferably something that is free and supports at least postgres, sybase, and ms sql server?: i wrote some scripts to scrape data from strava for runners. let me know if you’re indeed interested and i can point you to them. it wasn’t trivial!does anybody know of a data-set for mooc websites that would allow for dropout prediction? i know there is a kaggle data-set but i do not want to use that :slightly_smiling_face: thank you!thank you   , i will talk to eddie. i think  also scraped strava.i have an invite for the oneplus2, if anyone is interested expires in 24hrs…goneif anyone needs i’ve got an invite.goneif anyone is looking for sf housing, my college roommate has an opening for the master bedroom of a 3 bedroom apartment in potrero hill. dm me if you\\'re interested and i can put you in touch.: i’ll be going, if you want to meet up at some point.thanks , that\\'s what i suspected. make the top level \"god-emperor of data\"and they can have  as their slack profilethanks guys for the tips!and , i will definitely pass along your god-emperor-of-date title :slightly_smiling_face: don\\'t encourage him, he\\'s been lobbying for months to use this title for himself.:slightly_smiling_face:hahaha, sorry, i will stopdoes anyone have experience plotting timelines of overlaid events such as check deposit dates and phone call dates? i\\'ve used simple bar charts but was wondering if there were cool timeline features i was unaware of.and it\\'s totally not my github profile pic &gt;_&gt; i would add that we need gender neutral titles and representations, such as data dragon  fair enough :slightly_smiling_face: hey! i keep hearing good things about plotly, but i haven’t used it myself: any postgres gui recommendations for mac? currently using sql workbench j, which is a colossal resource hog. interested in something that might be more kind to my laptop batterypostico?  i\\'ve used pgadmin3, but it\\'s kind of a hot mess too...  postico looks cool, i\\'ll check that out there are no great solutions. you have to pick the least bad one, the one that irritates you the least.hey all, domino is putting on a data science popup in sf: use `datapopupvip` code for 30% discountpostico does look like it could be a bit more battery friendly… thanks everyone!datagrip is good, if you/your company has $200 to spare here\\'s a list of clients: also, i\\'ve used valentina on linux, it\\'s halfway decent.does anyone have a favorite gui for sqlite on linux (besides sqlitebrowser and firefox)?i do like datagrip i know all the insighters at dia and co will be there.  recent ai fellows   and  are attending and have accepted papers.  insight staff    are hosting a ai careers panel with the above alumni and  .  the plan is to maybe find a time for everyone to have dinner?  jeremy is the point of contact.  sadly i will not be going :confused:anyone else??since the panel is on wed, how about dinner that evening? lmk!ah, nice. i\\'m here with 22 of my colleagues so it almost feels like regular work days! i\\'d love to join for dinner if you figure out a time.i’ll be there and would love to meet some other fellowsmy coworker and i will be there.  would love to join.ha! i floated a very similar idea for my project, but ended up doing a consulting project instead.how about wed before the panel?when is the panel? i don’t see it on the schedule on the nips websitethe plan was to have an insight sponsored drinks and apps after the panel if people are downpanel is not on schedule. it\\'s 7-8 pm on wedsanother decisive victory for alphago! 3-0de people its a must read, i keep revisiting this article for perspective on data engineering. its simply awesome, talks so much about basics of de.the robot just has no idea who he is messing with. :wink:jimmy v. nuggetbot is a match i would watch. jujitsu that talking robot into submission!that robot has no idea how a good i am at dismantling things… and how terrible at putting them back together uploaded a file:  and commented: an astronomer colleague of mine sent me these job postings to share around. if you have an astronomy background and weren’t 100% sold on leaving academia in the dust, maybe these roles are a nice middle-ground.: dust or _interstellar  dust_? (sorry, bad joke; and possible one that doesn’t make any sense!)hahahey everyone! does anyone here have experience deploying a genomics web application? i have some questions about cloud service providers and data security.hey new york folks, my former boss is hiring a data scientist for the growth/marketing team at better mortgage.  happy to make introductions and pass word along.  job posting here: heard that was a neat company, also ^just mentioned in the  channel -  (techstars alumni) are looking for person 3 to join their company and lead their ds and ai efforts.  if you know someone who might be interested, please let me know (slightly more info in the channel) does anyone have experience applying gans to problems other than image creation? i\\'m curious to hear about other applications. here we go!uh oh :smile::fire:quick, clean your desk!this is my favorite part of this slack team!!*so it begins* i guess u can use them for clusteringget your announcements in everyone!at least it was here, not channel … ?don’t forget to sign up for alumni mentoring and in a couple weeks mock interviews!^stay tuned for more lightning talks and awesome alumni events in the near future!, if you\\'re not already in on it - there are a variety of (strong) opinions about using the at here or at channel tags, opinions which are generally re-aired whenever someone uses one of themdon’t forget to cheer for the vikings you have disciplesyeah, i slipped up and used our slack approach from work :papologies to the hundreds of you who just got spammedit\\'s never bothered me, fwiw better add at here to your apology to make sure everyone gets itseems like at here gets less response than at channelseems like a testable hypothesisbut i\\'d have to try it multiple times to get a big enough sample*psa*: go to `notifications preferences` in `channel settings` and tick `ignore notifications for at everyone and at here`here is for active users. channel is for all usersi believe a slack admin can limit usage of both at here / at channel through this page: should at this message :slightly_smiling_face:be my guest :laughing:has anyone worked with amazon sagemaker for deploying custom algorithms? any thoughts on it generally?has anyone worked with incorporating distance based correlations into a model. (for example, i\\'d expect data from people living close together to be more correlated.) or even just encoding zipcode/county/etc data into a continuous variable which maintains their distance. if this is for a project, try to keep it simple at first and use an average of the neighbors as a feature. if it ends up being an important feature, you can go back and change it to an weighted average (i\\'d recommend an exponential weighting) where the decay rate could be a hyper-parameter.i\\'m doing salary prediction for medical specialities per countyi have a lot of data from bls for most combinations, but i\\'m trying to fill in the gaps with prediction basicallyso i was thinking of doing kriging with the exponential model, but i wanted to also incorporate other covariates(from census data etc)which is why i am thinking of how to actually model it by incorporating correlation structuresif i\\'m reading this correctly, you want to predict salaries for where you don\\'t have enough data.if that\\'s the case, devise a distance metric (for actual distance, population distribution, urban/rural breakdown, dates, etc.) and get the most similar cases to the county in case (these are your neighbors). use these to predict. you can later go back and apply an exponential interpolation using kriging or another method you desire.you mean just take a mean of the surrounding counties?the problem is only having mean data per countyso if i localize my models the sample sizes are going to be minisculeyou can also take a weighted average per state, or even by region. try to run a preliminary model with all of these averages and see which one might have a higher predictive power.also, remember that the geographical distance is only one of the distances you can incorporate.what do you mean?what other distances?i have been using a gan to create fake structured data. but will have a blog post out about it sooni mean how apart counties are to each other in other metrics. e.g., by population wealth.certain counties could be very similar, despite being far apart.oh yeah those are the covariates i would be including, which is why i didn\\'t want to use exponential smoothing and simple metrics like that.hello! i\\'m using convolutional neural networks for real-time emotion recognition from videos. i want to display the video in a web app while my python algorithm labels the emotions in real time. currently, i\\'m thinking to use the tensorflow object detection api with either flask or django. has anyone done anything similar? does anyone have any recommendations for platforms/tools to use? thanks!hi, i am trying to download a db share from onedrive, and it is 67 gb. i setup an aws account for it. i tried using curl but it concatanating the file instead of downloading which had worked before. does anyone have any suggestion on how to download a directory from onedrive to a aws volume? data engineering fellow in ny, pamela wu did something similari think i resolved it. thank you! :slightly_smiling_face:hopefully this is relevant: cool thanks!no, i did not. is there anyone to chipin?anyone have experience with  or something similar?yes, i tried that .. it was not great to be frank. i will suggest jupyterhub instead of this. but at the same time, it might fit your requirements better than mine.: we use it internally extensively, it fits the team need very well - we can easily share results with business partners, and keep a record of all the important past work.  haven’t used other alternatives, so can’t speak to a comparison there,a plus to knowledge-repo is that, one of our alumni actual is a main contributor to the code base!congrates on expanding to seattle, insight! some advice on choosing where to work: thanks josiahmwalton , stitch fix is killin it lately with the blog postsdoes anyone have a recommendation for a high-level crash-course or a good pedagogical textbook on quantum computing and algorithms? my phd was in hep experiment, so i\\'m starting from about halfway through peskin &amp; schroeder.still hard to beat nielsen and chuangnot an expert but at a high level data modeling looks like object oriented design without methods.  try stanford lagunita\\'s database class with jennifer widom.  it has a section on data modeling.  for big data specifically, kleppman designing data intensive applications has a couple of good chapters.  what do you guys think of the topic of this session   do you think it’s useful for implementing compliance to gdpr?using blockchain? :wtf:i see. so if you roll your own, then you’ll most likely want to automate the deployment process. be able to spin this up from scratch. pop in and out a hosts with 0 downtime. if you do require downtime be sure that you have multi colo replication so that you can do upgrades or what not to each cassandra cluster in each region without affecting data staleness or availability. for automation you can look into chef, puppet, ansible (config management) for provision of hosts you may need to write your own plugin for terraform. monitoring you can farm out if it’s easier, but look into what metrics cassandra provides already and which vendors they are compatible with (nagios, prometheus, datadog). if you’re plan is to move to aws alwasy keep in the back of your mind, is it worth it to build something specific to your datacenter or generalize it, so that it’s minimal work getting it onto aws.haha i kind of thought so too..our data science team at cz biohub is looking for someone to help us use phylogenomics and machine learning to detect, prevent, and understand infectious disease.dynamic, collaborative, and supportive team.job posting: does anyone have experience with creating a churn predictor?  i’d like to bounce some ideas off of you!: i do. what do you want to know?same. feel free to ping me.same!does anyone know of a good tutorial, book, course or videos that  teach time series analysis. i understand some of the basics but definitely want to grow in that area.: ask :  is a good introduction to a lot of the standard methodsbut i wouldn’t recommend reading up on arima until you’ve also read this blog post: i have a really basic regular expressions question. i have a column with mixed entries some of which are people\\'s names and some are strings of mixed numbers and letters. i just want to filter out the names and tried this names = lambda x: re.findall(\"[a-z]*s[a-z]*w\", str(x))but it returned letters in the mixed cells as well: i recommend one of the online regex tools (like this one which also explains the expression you enter: ) to test your regex with some sample datathanks! i may have just solved it but will need this in the future!!another question, has anyone dealt with columns with empty lists in a dataframe? i want to replace them with nans but am having some difficulty. thanks!oh solved it!: also, if you\\'re not interested in time-series forecasting but more on the inferential side of things (same base principles, different applications and practice), you may want to check this out:  this one also has a full chapter on time-series () digested here too (): we use “quantitative forecasting methods” by farnum &amp; stanton, pretty decent and focuses and data mining aspects.does anybody know someone who has been through hackbright? or hear anything positive/negative about the program? wifey is kinda keen to explore it..: insight is hosting a happy hour in la tomorrow evening. if you’re in the area, dm me! i’d be happy to get you the information a recent blog post by someone at my company may be of interest:   i am looking at it.  thanks! as far as i know everybody is fine, we did hear the bang in the office last nighthi guys -- does anyone have experience building bokeh apps in python, and would have a few minutes to chat?we have three positions open at travelers for those experienced in software dev in the ai and text analytics space. they are looking to fill these positions asap. be sure to mention that i referred you.   go to  and look for job id 9881br and 9560br hello! we\\'re looking for a data engineer who\\'s worked with big data systems at scale (aws, hadoop, spark), preferably with some experience deploying machine-learning models. if you know anyone, please dm me!monty python for me :wink:fyi, :cvs: is hiring a data scientist (~3+ years experience, so senior-level i guess) to lead some initiatives on the pharmacy operations side. the position is located at our corporate headquarters in woonsocket, ri. i\\'ve been doing data science work here, in support of the retail side of the organization, for six months now, and so if you have any questions for me please don\\'t hesitate to comment or dm me. here\\'s the job posting:  if i remember correctly, 1 week of insight = 1 year of experience, so everyone from insight should be good!new principle of relativity when applied to insight?cheers!austin and i just posted a blog on cassandra that’s featured on hackernews.  it features fellow projects and code on how to run queries and setup cassandra with spark:link to the entry: &gt; we also got a great introduction to nosql and cassandra internals from insight alumnus mike grinolds and patricio echague, director of engineering at relateiq and a committer to hector - the java client for cassandra. he also discussed data modeling for cassandra, which requires a different approach than relational databases like mysql or postgresql.that paragraph needs a rewrite.: thanks, it’s a bit of a run-on sentencefwiw: i guess the point would be not to make it faster for them but experience how it is to live like regular citizens i\\'m a data scientist working with economists.  does that count?i also know a guy who works as an economic fellow at the fda pm me if you want me to make a connectionthanks! i’ll dmknowledge graph @ apple is hosting a dbpedia community event: drop by and say hello! happy to answer your questions about what its like working at apple :slightly_smiling_face:for those in nyc and interested in causal inference : never mind. i got it to work. has anyone worked with facebook\\'s graph api to track daily social media engagement? so far i\\'ve pulled daily video views and timestamped comments, having some problems pulling daily sharedposts or other proxies of daily social media engagement (most fields can just return aggregated values for the lifetime of a post)so they’re including bonus and equity for employee compensation, but only include base salary for ceo compensation? seems highly misleading.even if that’s total compensation, where i come from that’s still mucho dinero.hi! we are hiring out our data science team at thirdlove. we are profitable/rapidly growing, just hired kim larsen - formerly stitch fix/uber - as our vp of data, and we are moving into a brand new building in sf next week.  if anyone is interested let me know! ( )anyone found a good forecasting library in python? i\\'ve seen prophet, wondering if there\\'s anything else. particularly interested in adding external regression variables to an arima. feel free to dm :slightly_smiling_face:hrm, i now see statsmodel has an arimax with `exog`. anyone have experience using exogenous variables in this way? completely agree. just pointing out that, unfortunately, in many organizations the structure doesn\\'t guarantee a project leader with the skills to advise on technical roadblocks. this is different from an academic setting, where almost by definition, your advisor has more experience/expertise and can steer the project in the right direction.strategic incompetence colin. :wink:anyone have any suggestions for picking up scala/spark? tutorials, courses, books, etc.there is an interactive tutorial with dean wampler called “just enough scala for spark”. if you don’t have a safaribooksonline account, you can use a free 10-day subscription to check it out: this is a great book, but not very introductory: because i know everyone asked this question every day of insight: are any insight fellows at the spark+ai summit right now?so what do people say about the whole github issue. for some reason most of the people around me already started migrating to gitlab ….  are going from my team! there are other insight alumni from intuit going as wellerrr…  is going, nick is on vacation. :wink:\"microsoft will need to really mess things up for me to want to switch\" said our team leadi see people saying “oh there going to pull off the same thing they did to skype and turn it to a crappy product” and others saying “this is a good thing, nadella has definitely been doing a good job bringing life back to the company”  i don’t use much of their products really (except linkedin + now github!), but their reputation + adoption seems to have improved recently.we had engineers planning the migration to gitlab out of paranoia, that conversation was shut down pretty quickly by our ceo ^we have two data analyst positions at rovi  in boston. if anyone wants to apply please send me your resume: .  i can confirm that it\\'s a great place to work. we build an analytics product for television networks that predicts their viewership and optimizes promo and ad placement. we get to work with big data, do machine learning, and all sorts of fun data sciency things.also, feel free to pass this along to other good candidates you may know. the position is entry level so for someone who has a b.s. or a m.s.hi all, does anyone have experience working with mobile phone datasets?  i’ve seen some interesting things come out of academia, but seems hard to obtain.ive heard there are european datasets that are obtainable, but never fleshed that outhi guys.. any data engineers/architects/scientists  in the health care field?  would love to chat about prod issues that you might have encountered in the day-to-day job.&lt;--question to data engineers about “staging data” in etl process, due to technical complexities we can’t have a day behind refresh for our staging databases. i am thinking that having refreshed data is not necessary, and there must be methods were we can test by randomly sampling data? i guess in general, i am interested in knowing how de’s go about testing their etl in staging. thanks!yeah, random sampling sounds like a good idea.for instance user 2017 conference live-stream link and schedule has anyone ever used coursera’s tool called dataducthi  thanks for the reply, when you are testing do have a suite of things to check off?or are you mostly just throwing the whole data set? in the context of datawarehousing.i also think the snapshot operations are not available to us, as i think our ops team would restrict us from doing that.what are the pieces in ur etl? mysql-&gt;redshift?combination of mysql, and oracleto s3, then s3 to redshiftuseful guide for effective t-sne viz: man, that thing has spread like wildfire across the ds worldfood for thoughti wonder which category insight would fall into… i also think that the people who do moocs might be very well practiced in answering technical questions in a similar format to . assuming they did the homework, this would mean 10-20 “practice” questions.i think everyone would like likely benefit from  practicing on  :slightly_smiling_face:the blog makes me think of claims like “people who buy baby books are good parents because they care enough to buy baby books, not because of the content of the books\"* nah, i\\'d rather coast on my sweet pedigree of the university of iowa. who needs to study and practice?yeah, i don\\'t quite believe the causal implications of that lost*posti don’t want to minimize the importance of wanting to succeed and taking action toward that purpose, it just seemed to completely ignore an equally, if not more, important (and very well studied) factor of practice makes you better at things.for the physicists: \"don’t underestimate the power of giving people a common platform and operating language.”  uploaded a file:  and commented: if anyone is looking for something different, general assembly is looking for a full time instructor for their new data science course in san francisco ().  they are also looking for people in the field to give lunch talks and work as tas after work hours.  if you’re interested in any of these things you can contact teresa todoroff at .feel free to spread this around.#makedonalddrumpfagain :smile:anyone going to this tomorrow night?gosh i love 2fai think i\\'ll refrain from resetting everyone\\'s password herebut i encourage 2fa:tada:hi! we have an opening in our group at jpmorgan for  senior data engineer specifically with hadoop and spark expertise. feel free to pm if interested or know somebody.. :grinning:anyone have a good javascript tutorial for crawling websites? i\\'m working with data that\\'s stored on a vendor portal, and it would nice to figure out how to crawl the page and download each file without having to manually do it myself. :stuck_out_tongue:dk about javascript but i’ve done autodownload scripts in python with `requests`i did something using `requests` combined with `selenium webdriver` to go past javascript queries and reach specific pages i was interested in scraping. if that\\'s what you\\'re interested in, let me know.never crawled with javascript before, are you talking about scraping a page w/ javascript.  if you want to run full fledge crawler, scrapy and nutch are both greatwas talking to a consultant, and he says many companies just turn on a ec2 node &amp; have schema registry be hosted there, and producer, lambda, and consumer can all talk to it if it’s in the same vpc.after speaking with 50+ top applied ai teams, we wrote up some tips for academics who want to transition to applied ai roles. if you like the content, could you \\'applaud\\' the post on medium and share it with people who might find it useful? sort of.for some context: the data i\\'m working with is parking meter data (how long a person parked at a meter, how much they money they spent at the meter).right now, the vendors for the parking meters have the data, and make us go through a webportal to download the data.the portal asks for a the meter id, and a date range. then it will pull up all transactions for that date range.there is a download button that creates another popup with some radio button options for how to dump the data. one of the options is just to simply dump it as a raw .csvi might have to pick your brain at some point.pretty amazing deal even if you\\'ll only use a few of the books.i assume it\\'s just the e-versions?yeah, just the ebooks (pdf, mobi and epub). though you also get to donate to a charity in the process, you can even choose to have all the money go to charity.perhaps doing the scraping with python\\'s scrapy would be easieris there a specific reason for using javascript?  would be happy to answer any dataproc question that you might have. my sister-in-law just started a new job that is a bit more ds/coding heavy than her prior post-doc and is looking for a good bash tutorial - any recommendations? this tutorial covers everything you would want to know as a beginner/intermediate. it is paid, but you can get a free 10-day subscription.  can’t compare it to anything else, but this book (you can download as pdf for free) was helpful for me: thanks  and !nice podcast of interview with  has anybody worked  with datasets, where number of features is growing as you do ml?i understand that it is kinda like missing data handling. but in this case missing data has a very clear pattern to it, so many techniques won’t work.i couldn’t find any discussion about this topic.anyone have book recommendations for a ds to learn user research methods?has anyone worked with pachyderm?like ux research?alums: how is data documentation handled at *your* company? can anyone share best practices? where do you store documentation (dropbox? github? google docs?) do you use version control?  what is this \"documentation\" you speak of? :stuck_out_tongue:actually, yeah. what do you mean by documentation? metadata? provenance? uses? change history?all if the abovelet\\'s with data dictionaries (metadata) and we would like to track updates to this documentation. *let\\'s start withi\\'ll just sum up our practices with: :disappointed:we actually don\\'t have a great way of tracking \"raw\" data, but i think that\\'s partially our nature as an org that does something completely different every 4-6 months. for \"raw\" -&gt; \"results\" tracking we use sacred: we have a similar problem to  in that we\\'re a consulting company constrained to whatever our clients have in place. i have yet to see an org do it well thoughstay positive higginscstuff it nuggetboti will say i have seen several startups that promise to help you do it; i haven\\'t found them to be great either. :confused: often it is a lot of overhead, and forces a major change in your work flow.we\\'ve had great success with mkdocs for both general and technical documentation.  super easy to write new pages (it\\'s all markdown) and if you put your site on git then you\\'ve got both version control and easy access for your whole teamanyone use readthedocs? just linked there from sacred, looks coolhost your static mkdocs site on an aws box (or your favorite server) and bam! documentation that anyone can access over the internet :) yeah, we looked at readthedocs but decided mkdocs was nicernice. thanks!  i’ve used readthedocs for academic code i wrote, was easy to use. also creates documentation from your python docstrings which is nice. i imagine mkdocs can also do that aswell.for both mkdocs and readthedocs, can these sites be private? can we set up so that only team members part of a bitbucket (private) repo have access?no idea, sorrydefinitely. all mkdocs does is create static web pages, so the hosting is up to you(it also has a built-in server, but that\\'s more for interactive documentation writing) gotchathanks all!hi all, now that slack has \"thread\" features, can we start using that more proactively in ? i find it hard to follow multiple simultaneous conversations some times and threads are a nice way to keep each conversation modular. please give it a try!agreed. the drawback is the lack of visibility for things that might be of interest to the whole channel. i also encourage the usage the checkbox below to post back to the channel when appropriate.hi insight! we are hosting am r-ladies meetup at flatiron health next tuesday. stop by! current insight fellow in the news!awesome article hey sf folks (although there are also satellite locations in every other place insight has programs), i co-organize a free event in town called astronomy on tap bay area and our next event is on feb 21 at dna lounge at 7:30 pm. it consists of 3 presentations from astronomers, an astronomy in the news segment (hosted by yours truly), trivia (with prizes), and free giveaways. no science background is required and all ages are welcome. because space is always better with beer! for any of you who moved cross country, can you recommend a good mover that loads/unloads. google uses ace relocation for their new employees. i didn\\'t look at the prices, but we had good service with them. data science conference at stanford via pat.r.callier: *”become awesome at technical interviews”**\"practice interviewing with engineers from top companies, anonymously. get better at algorithmic problems, find a job, or just see if you\\'ve still got it. it\\'s like chatroulette[1] for technical interviews.”*[1] but without the dickspretty great footnote: depends on which meaning of the word.i think it works in the senses i can think ofit was soooo awesome!!!! one of my fav conferences everis this a rogue, rabid ai trying to eat us?just make sure no one gives you demonetized 500/1000 rupee notes. when in doubt as for the new series notes.  old series:  new series: : sorry i missed this thread earlier. feel free to message me directly if there’s anything i can help with!when you’re debugging why commands don’t work, it can always be helpful to type “which &lt;cmd&gt;”such as “which wget”the result shows you the binary which is executed for the commandif you see nothing as an output, it generally means that the directory containing the binary is not in your pathi.e. $pathif you just installed something in a shell, but you didn’t reload the shell environment, you can always try opening a shell (which triggers sourcing files like “.bash_profile” and “.bashrc” which modify your $path variable to include searching for stuff you just installednot sure who to ping about this - but when posting a link to insight websites on pages which produce automatic url image thumbnails, sometimes it seems to just pick this one fellow’s image as the thumbnail. not anything wrong with that, i guess, but it might be worth investigating how to set up the page so that a representative image is chosen instead? just fyi. is this your department?just copy/paste a link to each program into a linkedin chat - you can see the behavior.yes, i’m not sure why that photo was randomly pulled from all the ones on our front-page (though i’m happy to feature alvin). i’ve spent some time trying to get this photo uncached, and replacing our social cards and opengraph profile, but never invested enough time to fix this. will look into it more - thanks for the reminder.thank you! i am originally an ubuntu user, and \".bashrc\" was coming built in. looks like, there are quiet a lot of differences between 2 systems. i fixed the bash &amp; path problem with installing brew (which was not straight forward due to the new mac os and new updates); and i was able to install the problem i was originally going for after. so far so good, up until the next problem! :slightly_smiling_face: thank you for all the replies. i appreciate it.how do those headphones even stay on?velcrodj kitty has commitment to his arrartnuggetbot: weather palo altoweather for palo alto, ca, usacurrently: partly cloudy 21.1°c/70°ftoday: mostly cloudy until tomorrow afternoon.coming week: no precipitation throughout the week, with temperatures bottoming out at 14°c/57°f on saturday.nuggetbot: weather nycweather for new york, ny, usacurrently: overcast -3.5°c/26°ftoday: light snow tomorrow afternoon.coming week: mixed precipitation throughout the week, with temperatures bottoming out at -4°c/25°f on saturday.nuggetbot: weather bostonweather for boston, ma, usacurrently: overcast -6.3°c/21°ftoday: mostly cloudy throughout the day.coming week: mixed precipitation throughout the week, with temperatures rising to 6°c/43°f on wednesday.nuggetbot: image me data scientistanyone know a nice, simple, pythonic way to grab street maps by lat lon which dont look gross when plotted over?: you can use geoplotlib to overlay maps on top of street maps; using the boundingbox method lets you specify the (lat,lon) of the corners of the map, so you implicitly specify zoom as well. you should be able to just call the map, without any overlaying pieces as wellalso, more for  thanks for the rec! someone lost a phonestay positive thang and  : thanks for the help guys. for now, it seems like i have a good enough fix. will check sunlight if i need  better quality data.a good small tutorial on how to use flickr\\'s api using a python wrapper! a good tutorial on reading tweets and doing some natural language processing on them via python: flickrapi is great; it\\'s what i used for whereto.photowhereto.photo is great.  i wonder what happens if i try to find pictures of \"yoga\" in sf?or maybe i want to go to a \"fair\" in soma.inappropriate:disappointed:now i\\'m going to get firedi was going to let the inquisitive minds find it for themselves.  no need to make it easy for them.news-reading habits dataset: lol  your hot dog post disappeared!cia take it down?:wink:: for those interested in scraping data, this is a very easy to use tool.. just create an account and click buttons. no code required. the example they use looks very apropos for  cool good read: looking in to it now - also conversing with  channel creator,  what could go wrong with 500+ insight fellows having access to the api?  :wink: anyone interested in going to a deep learning/tensor flow meetup in sv?just signed up. thanks  !insight will be at strata this week! looking forward to seeing you all there! hey fellows! if any engineers are currently looking for new opportunities, my company rally health is aggressively hiring software and data engineers for our core team! here is my referral link: positions for data engineers aren’t currently posted, but if interested please let me know and i can refer you in person. thanks! for 2016 you\\'ll have to scrape it from the secretary of state on every state, or ap\\'s election service api  not sure if still freeanyone using zeppelin with spark? or just using zeppelin in general i suppose?i\\'ve got the team here at placeiq going on it and i\\'m finding it pretty decent if you need a notebook that works well with yarn(we had too many problems trying to get jupyter working)we’re doing some work on azure, and you can configure jupyter notebooks that connect to a cluster thereso we connect locally and it uses yarn in the backgroundthough i do want to get into zeppelin at some pointi’m also supposedly learning graphx — have you all worked with graphs as well?i don\\'t know exactly the issue we had with jupyter on yarn but i think it was around user queuessince politics are part of that channel...here is an interesting article worth sharing about false info. super fun! neural nets recognize your drawings...   other onesreally cooldamn, quickdraw would kick my ass in a game of pictionary :stuck_out_tongue:stay positive ramalldfany headless-browser pros out there know how you’d go about building a crawler that can tell you whether a given site has a mobile-optimized version?well for crawlers i like splinter hey all: does anyone have any good resources for marketing data science (eg attribution analysis, customer segmentation)? looking for more methodologies than cheerleading (finding a lot of the latter)liverence:  papadimitriou.c: awesome! thanksi can share a bunch of academic papers with you via dropbox if you\\'re interestedmorning, are there alum working on online grocery delivery business? i’m working on a similar insight project and am eager to learn about this business. your two cents will be my precious gem! thanks! :slightly_smiling_face:hey jeff, i used opencv in python to get frames from a video for my insight project. you can see it here. warning though - really poorly written code: i am doing an insight project to identify trolls on twitter. i have some data and i’m pulling from the api but if any fellows have twitter dumps they would be willing to share, i would be grateful! i am particularly interested in @replies. i hear you might  … i believe had a twitter dump? has a twitter dump too: nicole wong () from my cohort did a similar project, might want to ping herthanks!i didn\\'t cache any tweets so unfortunately i don\\'t have any twitter dumps. happy to chat abt any other q\\'s you might have about the project tho! i have grabbed an additional pile of tweets that i’m using now so i may be close to having what i need. i am curious if you did any sentiment analysis. i’m going to be working up a model to classify users as troll or not.does anyone have experience extracting information from pdf tables (of a range of different formats)?kevinmercurio: yes please! email is i\\'m not claiming it\\'s comprehensive or well-organized, but there is some content :wink:willfrost: it\\'s hard. i used image processing techniques to clean up the pdfs first and then ocr (tessaract) to read in the datai used  for a small personal project and it worked well except that i had to manually tweak its automatic bounding boxes to get the last rows in my particular tables.  thank you! i will have a look into these options.hey kevin, mind sharing those with me too? email is i used pretrained vader sentiment analysis, it was pretty good imo. it was developed specifically for twitter and social media analysis. a friend of mine made this recently and it was a hot topic on hacker news. i haven\\'t used it though and don\\'t know if it will suit your needs. if you come up short i may be able to dredge up a tweet dump from awhile back but i am unsure if i archived it properlyplease feel free to contribute more :slightly_smiling_face:if you\\'re familiar with bash, i\\'ve used pdftotext with a combination with sed to get information from very old pdfs with tablesjust use `pdftotext -layout -eol unix &lt;file&gt;` to get the pdf translated into a text format that you can later filter the information you wantyou might want to check facebook as well as you can get the post sentiment and reaction built-in via likes and the sorts. especially for facebook posts from news sites and other public-oriented entities.  might be able to help.always interested in learning more. share away at latest that came to my table was :+1: for opencv. i\\'ve used it in both python and c++ for image analysis and features recognition, and works great!although if you are only looking to get particular frames from a video, there might be simpler solutions to it. e.g., vlc is able to save any number of frames every x frames. it allows you to do it graphically or via terminal (as ffmpeg)have you tried fuzzywuzzy?maryamtaba: insight pds should have some example datasets and i are planning to go climbing at brooklyn boulders somerville, ma tomorrow at 11 am. anyone interested in joining us? check out stephaniemari: will do. thanks! i don\\'t have a twitter dump (yet), but just wanted to mention that i\\'m also going to be doing a twitter project that will (in part) involve sentiment analysis. so let me know if you want to swap thoughts/insights (pun intended) over the next few weeks. that would be great! maybe we can find a time where we both are awake and free next week and do a skype call? actually, i’ll message you and share my week1 files…cyucheng: no can do for this weekend, but i\\'d love to get back to it. my climbing gear has been collecting dust :cry: i have a blood feud with bkb.  come to metrorock instead :stuck_out_tongue: mind sending the link to ?thanks everyone! i was able to get what i wanted with moviepywould love to have a copy too. , thanks kevin! last time i listen to a ground hog...the snow/slush is starting to accumulate. at least in parts of jersey.^ that pic in the article is from a south indian movie “enthiran” (robot in other languages). i was hoping for some reference to it and found this instead: uploaded a file: omg what could this mean??(former) theoretical physicists out there: there’s some string theorists down the street (in sf) collecting 3 million dollars a piece:hey all, i’d love to have some honest first-impression feedback about my resume from fellows in the industry. i’m not asking for a detailed analysis, just your quick opinion. pm me if interested, it’d help a lot. thanks!!is anyone familiar with negative dependence/chernoff bounds?does anyone have experience running pyspark jobs on an emr cluster using spark-submit?  i have a job that hangs indefinitely in \"running\" mode even though it should finish in about a minute (which i know because it worked ok inside of a jupyter notebook) and would love some advice on how to debug...maybe  \\'s territory? is it possible that another job is being submitted to the same cluster? i imagine that amazon emr will split the resources if there are multipile though. also see if you can get any sort of logs our of emr that may point to the problem. possibly permission issues accessing data.another austin! :wink:#replicaskubernetes pods of austins?sure!thanks everybody for your help!glad to hear it! could you expand on what you mean by random shuffling and what it helps with ? very curious about that.anyone at strata?  if you’re here come by the  booth :slightly_smiling_face:sure. you simply do a random shuffle of your test labels (`random.shuffle(y)`) and then compare against the unshuffled labels. this will give a worse overall accuracy, but it gives you a starting point for f1 scores for the minority classes. it is essentially choosing randomly given the relative number of each label in the dataset.hi there! any de\\'s mindd giving a noobie some advice? i think most of the de channels are kinda dead, but these seem active so i thought i might try my luck. specific questions get specific answers! try it out :slightly_smiling_face:i didn\\'t want to spam the channel w/ a convo, so i was hoping to just pm a couple people with specific questionsnobody knows if they\\'re the expert you need unless you describe the expertise you need!is anyone else a data engineer at a startup w/ less big data needs creating data pipelines from the ground up?i had some architecture ideas that use big data frameworks, but since there aren\\'t foreseeable scaling problems, i think in some regards they\\'re overkill, but as far as integration w/ other software they might be a good fit despite maintenance overhead / cluster management.mostly because glue has transforms that can translate into spark calls, andd those spark calls can be edited later as requirements change.that’s very cool, i’ve never encountered that before! do you happen to have any references to the method i could look up to dive deeper?whoa... will be checking out right awayiqvia is looking for a tech-lead for our data science platform. if you are a ml engineer or data engineer looking for a larger role, we would love to have you on the team. dm if you have any questions about the role or the company.role is in philadelphia or possibly anywhere in the northeast  (remote)i don’t have any great references specifically on shuffling :confused:.  i originally saw the idea of shuffling in the context of bootstrapping p-values for hypothesis tests in mine cetinkaya-rundel’s statistics course (e.g. section 1.8.2 in [1]). for a comparison of uniform random guessing, weighted guessing (which is very close to shuffling, but would be based on your training data distribution), and guess the most frequent; see [2]. also sklearn’s `dummyclassifier` can easily do all of these methods [3]. i suppose if you wanted a weighted random baseline, only predicting based on the training data would technically be best practice (and now that i know about `dummyclassifier` maybe that’s what i’ll do in the future). shuffling is just so quick```y_shuffled = y.copy()random.shuffle(y_shuffled)print(sklearn.metrics.classification_report(y, y_shuffled))```[1] [2] [3]  posted a nice tutorial:  fwiw, i\\'ve had good results forecasting thousands of time series using the `spark-ts` package. i think it\\'s available in pyspark, so low barrier to entry if you don\\'t work in scala. :wink:we (salesforce einstein) are looking for senior data engineers with scala/spark experience. see posting here: feel free to contact me if you have any questions!anyone here use jira to report bugs and track tasks? we currently use asana and very much don’t like it… we use jira for task tracking for data and engineering folks at my shop, asana for client facing folks.jira is also horrible but i don’t know which is worse since i haven’t used asanai\\'ve seen a shop that used a combination of github, trello and slack integrations for engineeringthanks for the feedback!if anyone is looking for an apartment in an excellent location (greenwich village), we are looking for someone to take over our lease sometime in the near future (trying to buy a place). no broker fee is pretty cool! or if anyone knows anyone... tell that someone! was looking for somewhere i think?anybody here going to data engineering conference in bk today and tomorrow? will be there! as wellyay! hope to see you there :slightly_smiling_face: i’m there now! hhh also is here! as well as jinchao and practically the entire capital one team! nothing special, ease of use mostly.  just want to play around with streaming and processing. wasn\\'t sure how easy/hard it was to get going with different brandsthat apartment is super cute, and on the plus side makes me feel less bad about my rent in silicon valley! :smile: best of luck renting it out! i think it shouldn’t be too bad. when you plug in the webcam it typically registers it with the computer as a camera option. you just request a frame from that device. maybe borrow one and try it out?stay positive jeremykarnowskii\\'ve had problems connecting to the internal webcam on macs with opencv in the past, but this was using the c++ interface and an older version of opencv (you using 2.4.13 or 3.1?)it seemed more stable in linux (which was a surprise).. sometimes it depends on the webcam manufacturer.. some high performance usb3 webcams are still problematic due to driver issues on macs.any experience on using arangodb. or are there any other graph or multi model dbs you can suggest?  any war stories on neo4j and orientdb?neo4j is really great in my experience, i can’t compare it to other graph dbsbut cypher is real nice for sure uploaded a file:  and commented: selfie courtesy of alihello all! does anyone have experience with domon dashboards here?#domopossibly i’m supposed to learn the capabilities as well :wink:this must be nuggetbot’s finest work. these days your isp is almost 100% determined by where you live. are you going to be in sf? nyc? at least here in nyc your options are either cable from time warner cable or fios from verizon (but only if it is available for your building). i would probably lean toward fios if you can get it but it is really just picking from the least of two nearly identical evils.maybe, (hopefully?), some other insighter has some other secret knowledge to jump in with?ah... in australia everything is federated, so one has a lot of choice.i\\'ll be jumping into the sf area, probably around san mateo... who are serviced by comcast. is adsl comparable in performance? more federated?(thanks btw)there are a few recent start up isps that have been trying to compete with the likes of comcast. i don’t think any have made it to nyc but hopefully the story is different in sf. i haven’t seen any talk of adsl in years so i think it is mostly dead in favor of cable or fiberah... it\\'s interesting that that is almost the opposite of the situation in australia.cable ultimately seems to have fizzled in favour of adsl / nbn (fibre)anyway... thanks for the insight.:wink:: might help: my rough (and highly opinionated) summary of the situation in the bay area is as follows:- comcast is the fastest and most evil- at&amp;t is slower, but less evil- wave is not so evil, and pretty fast- sonic is not evil, but often just resells at&amp;t so the speeds are comparablesonic is the only one in the area without datacaps(i use sonic dsl in mountain view and get 22mbps consistently, which is what they advertise at least, but also as fast as they offer down here)thanks alex! looks like sonic is the best choice :)new insight data science blog post! check it out when you get a chance. eli talks about the need to look at your data as your very first step.  has anyone used scikit-multilearn with text data? hi, has anyone here had the experience of installing a virtual box on windows machine ( windows is the host)  and then using cuda-gpu on the linux guest platform?! i\\'ve had difficulties installing cuda on ubuntu! you likely need pci passthrough working, which tends to be temperamental. are you on windows 10? can you do the things you want in bash for windows?its not possible i think. i use virtualbox on windows 10 (host) and run ubuntu as the guest os. the gpu is not fully exposed to the guest os. for example, the maximum gpu memory you can assign to the guest os is 128mb.your best bet is dual booting or using cuda sdk along with conda on windows. that’s true.. i shall perhaps think about ways to make comparisons with this. thanks for your advice :slightly_smiling_face:has anyone worked with trying to detect anomalies in time series data, or, for example, trying to detect statistically significant differences in a metric across ~100 ab tests where you have daily measurements? #multiplecomparisonfunhahaha if only there were such a thing! :parrot_mustache: :beers:thanks. i am on windows 7. i\\'d like to use pytorch, but i should have linux installation for that.thanks nishan, i think your suggestion of dual-booting is the most feasible solution for my problem.sheareraj: wut! that corgi is amazing... this is from 2013 :slightly_smiling_face: :hourglass_flowing_sand: what income on the side???  show me the :moneybag::grinning:greetings, i would like to contact former fellow yuhong wang to ask about a job at radius intelligence, but i can\\'t seem to find her on slack. anyone know the best way to get in touch with her?hahaha it actually belongs to i think she was a fellow in the silicon valley ds sessions. her name is remy :slightly_smiling_face:kevinmercurio: i did a lot of time series anomaly detection in grad schoolaxl002: hey alvin. yuhong left radius a few months ago. let’s touch bases in a biti see! perhaps we can chat this week?hi smart people!! i do financial forecasting at wayfair. one of the challenges i face is the financial info is constantly being updated, so i end up doing a lot of gymnastics to recreate the past. we\\'re in the process of scoping out a solution so that we have snapshots of the table across time. i\\'m wondering if anyone has a smart way of tracking changes to order-related financial data across time.  the data is currently all stored in sql, but is quite large (i.e., the base table is 400 gb) and getting larger. thanks!!take snapshot once every week and then store change logs the other times?sounds like the abc of functional connectivity analysis in neuroimaging :smile: what\\'s significant when you have pair-wise correlation of 15k time-series.my company, driver, is hosting a tech talk next week: \"curing cancer as a machine learning problem!\".  the event will be at our headquarters in san francisco on thursday, june 22nd from 6-8 pm, and presenters include our ceo and members of the engineering team.more details and rsvp at   hope to see some of you there!\"this is the last century that our children will ever have been taught that one times one is one,\" he says. \"they won\\'t have to grow up in ignorance. twenty years from now, they\\'ll know that one times one equals two.” - terrence howardread more: lolwut^ i wonder if it’s possible to choose different axioms and construct an algebra wherein 1*1 = 2. what would the axioms have to be? what would the consequences be?axiom 1: do a bunch of shots.axiom 2: accept terryology.the future is already here: brad.deutsch: in a ring (cf.  ) this can only happen if 1 = 0, just look ```1 = 1*1 = 2 = 1 + 1 =&gt; 0 = 1 ```provided your definition of 2 is 1 + 1the existence of a thing is questionable, but there is a lot of virtue in behaving as if it does exist (at least if you want the thing to be a field -- there might be exotic rings that have strange zero divisor)bad news terrance: we already know why bubbles form spheres and not triangles... :disappointed:stay positive alex_gudehas anyone calculated nuggetbot’s false positive rate? : thanks. another mathematician friend of mine agrees. he suggests that this type of structure would be a semigroup since we can’t find an identity element, and would be fairly uninteresting. ah well, i wanted to give mr. howard the benefit of the doubt!not strictly ds/de related, but this paper shows mathematically that under very modest assumptions about error and bias that we should expect *most* medical science studies to generate incorrect conclusions. (no paywall)stay positive brad.deutschstep in front of a bus thanksgiving data from 538!is anyone familiar with n-back tasks? for example you\\'re presented a color on each trial and have to remember if the color n trials ago was the same. i\\'ve been tasked with classifying the difficulty of different patterns. for example, for n=4, a pattern could be blue red yellow green blue. i have quite a bit of data, but aside from some arbitrary cut off of difficulty ranges based on average percentage of correct trials i\\'m having trouble figuring out the best way to classify their difficulty. i was curious if anyone had any suggestions for methods that could be used classify the difficulty of these different patterns for a wide range of n-back levels (1-12). another issue is that as the n-back level increases, so do the number of patterns, so coverage is light beyond n levels of about 4. one standard thing to deal with low sampling in the levels above 4 would be a permutation test on bootstrap values. as for the performance estimation goes… if this is a change detection task (you mentioned observers need to say if the color was same or different on n back trials) you could probably figure out a way to use d\\' as a measurement of sensitivity [d\\' = z(hit rate)-z(false alarm rate)]. another idea: to measure the performance on this task (or the classification difficulty) one way would be to reduce this problem to 1d space and play with the distance between accurately classified colors in n back conditions in the cielab color space. or perhaps look into using pse (point of subjective equality) which is basically the moment at which observers respond randomly to “same/different” question on your experiment. anyway, feel free to pm me, probably no need to get into details here :slightly_smiling_face:d3 question. has anyone ever passed d3 strings to flask? it doesn\\'t seem to like quotes. for example i have a list of dictionaries like {\\'label\\': \\'ruby\\', \\'value\\': 345}meant strings from flask!oh figured it out! {{var|safe}}why not use stan?oh nice, i see edward is at least 35x faster than stan and 6x faster than pymc3wow that\\'s quite a speed-upi\\'d be very interested in hearing more about edward... looks very promising...it does seem really powerful!  which is why i\\'m trying to learn how to use it, although nobody else at my office has done it.  the gitter forum for edward is good for help, though.does anybody have experience making customer-facing reports with embedded python scripts? suggestions for what to use?oh boy do i! what is your output format? `jinja2` is a good start for many use casesright now i want to just output to pdf. would you recommend it for that?caveat that i haven\\'t used it, but i\\'d probably start with the `fpdf` library.depending on what you are outputting, matplotlib has a pdf backend the library reportlab is more comprehensivei’ve actually used `simpleidml` to generate adobe indesign pieces and then gone to pdf from there, simply because indesign is much better at…designfor the purposes of making pretty reports, i meanthanks all!i’ve played with it a bit but found it very confusing. can’t tell if it’s actually confusing or due to my poor bayesian abilitiesknight foundation, omidyar network and linkedin founder reid hoffman create $27 million fund to research artificial intelligence for the public interest   hey insight people: looking or have a friend looking for a job? - i recently joined a data analytics startup called kofa in san francisco, and we’re looking for more engineers to join our growing team! we’re a self-service analytics platform that prevents companies from reinventing the wheel when it comes to data pipelining, analysis, and modeling. we’re backed by top-tier investment firms such as accel and sv angel, and the founders previously sold their first company to palantir. all of us are ex-stanford ee/cs excited about working with new tech and building a company from 0-&gt;hero. over the past few months, we’ve doubled our team size and nailed down contracts and pilots with companies across the country. we recently closed a six-figure deal and will be doubling the number of companies using us by the end of q1 this year with the intention of raising series a by decemeber.we’re actively looking for talented and driven backend/data engineers/fullstack, and we’d love to chat about the product in more detail if you or any of your friends are interested. you can see the full job description at , and send me a message at  if you’d like to learn more. hello insight friends, our company is hiring machine learning engineer in san francisco, and here is the job description  feel free to apply using the link above or contact me directly if you are interested. thanks!friendly request that we not blast 800+ people in the general channel with `@here` or `@channel` messagesokay :wink:hello insight fellows:  ibm is looking for volunteers for ux/ui testing for a cloud platform.  there is an honorarium if you participate!  here\\'s the info:we\\'re currently trying to interview some data engineers about this idea to find out what your biggest challenges are.   would you be interested in talking with us for 45 minutes about your experiences trying to ensure data quality, low latency, and scalability?  qualified volunteers will receive a monetary honorarium as a thank you for your feedback.  contact me at  or .  thanks!there\\'s a new movie about snowden if you\\'re interested in the silk road case, deep web is a pretty interesting documentary that covers the investigation and ulbricht\\'s trial.domino is having a conference for data science leaders. nate silver, cathy o’neil, and wes mckinney will be speakers. there will be a practitioners track and a leadership track. use `dominodl` for $100 discount off the current early bird price.we’re also still looking for good speakers, so let me know if you’re interested or know someone who might be!what are the conference hours each day? 9:15-5:15cool, thanks!:+1:hi i am looking forward for this conf. is the dominodl a promotional code to get the $100 discount?, yes that’s should be the code. let me know if you have issuesok thanks for thishere is an upcoming opening for product intelligence manager in my current company (liveramp). let me know if you are interested:manager, product intelligence san francisco, cado you love working with incredibly smart people who inspire you to be better and work harder every day? do you thrive in highly cross-functional roles that work across engineering, product, and sales? are you excited to contribute to building a product that will transform marketing? consider joining us.product intelligence is a newly-formed group charged with deepening liveramp’s understanding of how customers experience our products and also with helping pms make strategic, data-driven product decisions. you will:lead the product intelligence team, shaping its charter and priorities to maximize impactprovide direction to stakeholders on how to measure the impact of product changesscope and manage your own analyses and those of your teamcreate frameworks that enable your team to execute analysis quicklyserve as coach and mentor to analysts on your teamcontribute towards building a great product team and great companyyour team will:deliver actionable insights to pms, engineering teams, and other stakeholders according to the priorities you set in conjunction with stakeholdersbuild and maintain reports, dashboards, and metrics to monitor the performance of our products, understand the customer experience, and track relevant kpisabout you:you have 3+ years of analytical experience in business intelligence, data science, sales operations, business operations, management consulting, or investment bankingyou have built models and dashboards for large audiences using tools like sql, ruby/rails, python, r, looker, tableau, etc. you are amazingly detail-oriented, but can also bottom-line analysis for an executiveyou can understand, manage, prioritize dozens of projects guided by a strong sense of priority and focusyou can communicate clearly and quickly. you synthesize effortlessly, and always answer emails and calls within 12 hoursyou have a bachelor’s degree in computer science, math, physics, engineering, or other quantitative field.you have a type s(tartup) personality: smart, ethical, friendly, hardworking and proactive you have a desire to work in san francisco. we will relocate you if necessarybonus points:experience leading an analytics teambenefits:people. work with talented, collaborative, and friendly people who love what they do.food. enjoy catered meals, boundless snacks, and the occasional food truckfun. we host events such as game nights, happy hours, camping trips, and sports leagues stock. every employee is a stakeholder in our future. health and saving. receive the benefits of comprehensive health, dental, vision and disability insurance along with a 401k matching planlocation. work in the heart of san francisco and take advantage of our commuter benefitsmore about us:liveramp is the leader in data connectivity, helping the world’s largest brands use their data to improve customer interactions on any channel and device. we thrive on mind-bending technical challenges and value entrepreneurship, humility, and constant personal growth.there is so much more that we want to build and that we could continue to improve. we value strong engineers who are agile enough to hit the ground running and tackle challenges.to all recruitment agencies: liveramp does not accept agency resumes. please do not forward resumes to our jobs alias, liveramp employees or any other company location. liveramp is not responsible for any fees related to unsolicited resumes.liveramp is an affirmative action and equal opportunity employer (aa/eoe/w/m/vet/disabled) and does not discriminate in recruiting, hiring, training, promotion or other employment of associates or the awarding of subcontracts because of a person\\'s race, color, sex, age, religion, national origin, protected veteran, disability, sexual orientation, gender identity, genetics or other protected status. qualified applicants with arrest and conviction records will be considered for the position in accordance with the san francisco fair chance ordinance. is there an extended version with more details? :stuck_out_tongue:i\\'m guessing you don\\'t mean it but to tell you the truth, this is all we\\'ve gotit hasn\\'t been posted anywhere yet so pardon me for the crudenessjust read about youtube incident. hope everyone there is safeapplication for speakers: this worked fine, thanks for the promotion.posting this for a friend. please let me know if you’re interested and i’ll forward on your resume.it is a machine learning engineer role.hi all - does anyone have any deep experience with gsuite domain management? we’re running into a problem where we’d like to change our primary domain and email records with no service downtime (i.e. the company’s name and domain is changing) but it’s apparently not that simple. private message me or email me at  if you’d be willing to get on a call (for $$).sorry i\\'m late to the party gang. i applied for a tn in fall 2017, and tried to use the physicist category. the person interviewing me said that was a no-go because there isn\\'t any category that is just \"physics\", they need to tick a box for \"astrophysics\"/\"atomic physics\"/\"geophysics\" etc... none of these will adequately match the job description for data science or engineering.similar to the other fellows above, i had my category changed to mathematician, which matched up with a minor in math on my bachelors degree.if you have more questions about the tn application feel free to reach outmaybe not so applicable to insight fellows but still interesting ^+1 for that medium articleinteresting premise, horrible analysis:  from the comments:&gt; i like it when statistics are properly applied to support a theory. unfortunately, the author makes massive statistical errors.&gt; the new york city (nyc) graph shows a strong seasonal effect in a time series. the ordinary least squares regression line (i\\'m going to assume those are the solid lines) are strongly influenced by where the series starts and ends, especially when looking at series of just a few seasons.&gt; for example, a series of temperature in new york city starting in january of 2011 and ending in july of 2015 will show a positive slope, one starting in july of 2011 and ending in january of 2015 will show a negative slope.&gt; indeed, the real story of the nyc graph is that there are strong seasonal trends in taxi cab complaints (probably driven by nyc weather). without accounting for those trends, further analysis is meaningless.more importantly, the author never compared against the null hypothesis, which in this case wold be to evaluate a single linear regression (higher order seasonal trends aside) for the entire pre- and post- uber periodyes, clearly had a point to prove, data be damned!haha yesunfortunately this seems to be a trend at the atlantic: (long read) : just started  channel to cover all things rdbms/sql, inspired by my presentation/tutorial with the ds-ny-2015b fellows yesterday. feel free to make the first post!awesome: is your presentation/tutorial available online?the tutorial part is just ah it\\'s modei can post my notes in the  channelthankswe had derek heregave a little intro that perhaps was usefulproject-basedah, okay, so you went straight to the co-founder!:simple_smile:haha :simple_smile:i\\'m looking for a python guru to contract for a short series of on-site lessons/tutorials at my company in nyc, with an emphasis on data science workflows - any recommendations/referrals out there?finally, i’m a real data engineer :simple_smile:  congratulations david.  this is super awesome!what is the best way to make sense of json files using pandas dataframe? read_json() results in error ( valueerror: mixing dicts with non-series may lead to ambiguous ordering.) #json  #pandas: that is awesome!!! thanks for mentioning insight!:+1:&gt;this is another secret in the hacker community. written instructions and online videos are only useful up to a certain point; to get really good, or to make something really fast, you have to be in the same room with people and you have to talk to them face to face.fun read: i\\'ve never loaded json into a dataframe, but i have loaded it into a sparse matrix. if you load your file into a list (or iterator), you can json.loads() each element to get a list of python dictionaries. you can then use the sklearn dictvectorizer to convert that list of dictionaries to a sparse matrix. so this is good if you want to do machine learning or something, but not if you want to explore the data.a word of warning: json with integers (rather than strings) as keys is not, technically, valid json and json.loads() will throw an error. there is a yaml parser that can load such json, but it is pretty slow. just fyi since i happened to have this problem recently.stay positive miothanks  it was a technical warning, not a personal warning, but if you don\\'t shut yer bot hole...stay positive miololnuggetbot animate me high fivehahahah bonus plug for partially derivative because i once got a beer suggestion shout-out. :)new stitch fix blog post that i wrote just went live. deep style: inferring the unknown to predict the future of fashion. nice! pinned a message to this channel.:   here’s a free event in palo alto on quantum computing by prof. maria spiropulu:i\\'m not sure if you guys have seen this, but i thought it was really neat. they can simulate how objects would move when interacted with based on short videos. : i’d be interested, too! for a good, in depth overview of agile workflows, i enjoyed learning agile  (in the insight library, btw, if you’re back in town).: try the mythical man-month (). good for developer and managers.good morning fellows, my name is yiyun, a current insight fellow at boston. my project is related to user retention and engagement and i am planning to quantify webpage traffic by drawing a web heatmap with a slider control. the heatmap will change as a function of time with the slider so it will show which part of the web is most hit at specific time by users. i have plotted heatmap before but not sure how to put these together. any suggestion is welcome. thank you in advance!: i’d use a jqueryui slider () with a change callback that retrieves data for the selected time and refreshes the heatmap accordingly. (i’m happy to explain in more detail if that’s not clear. let me know.): this looks awesome! thanks for the suggestion!:  this might be a useful way to visualize something similar : wow, that is fancy. thanks jonathan!hello. i\\'m one of the current boston fellows. anyone have experience with a modeling package written in python called numberjack? it\\'s supposed to include a c/c++ solver called mistral. the installation says everything installed correctly and numberjack import works but when i try to import mistral i get a \"module not found\" error.: i don’t have experience with numberjack, but for solving combinatorial problems/linear programming i use pyomo.  its super quick and the best i have found outside of commercial languages. not sure if this is what you are quite after or not but might be worth checking out.  do you know what class of constraints you have?  if they have a convex relaxation you could check a corresponding feasibility problem with something like cvxpy: thank you   and  ! those look like good alternatives. pyomo even has an example that\\'s exactly what i want to do.hello. has anyone used flask socket io? i\\'m wondering how to call an outside function, after i receive a response. i want to create a custom response depending on what\\'s been received. i tried some things but keep getting errors when running the app.thanks  and !hi all, does anyone have a recommendation for a digital marketing course? ideally, i’d like something that covers a good amount of theory and practice in digital marketing and analytics, perhaps using the google analytics or salesforce ecosystems, and not spending much time on rudimentary data analysis. thanks!happy thanksgiving, fellows and alumni :slightly_smiling_face::turkey: :+1:gobble,  gobble! happy tofurkey day!:stuck_out_tongue: virtualenv is your friend for multiple version of python.  the curse of minimal viable products... scary stuff. stay positive kumbhaniyou seem to like... nmf. have you tried item-item cosine similarity?anyone want to grab mexican food on/walkable from castro st. mountain view in ~ 1 hour?yes.hello! has anyone used the mimic data set? how long did it take to get access and how long to take the course? thanks!? i think you should request it right now just in case, as it takes a few days to access it. also used it in her project to my recollection tooi requested access last night, so i\\'m hoping it doesn\\'t take too long. the course takes ~2-3 hours to complete, depending on how fast you read.it took me a few days but i had to choose project before that happened so i chose something else. i haven\\'t investigated this beyond a quick google search, but i know that casinos have used facial recognition software for many years.there also seem to be a handful of face recognition api hooks offered by big companies. great for building a project quickly, but if you go that route, i\\'d think about coupling it with something you build yourself.and as always, think about your target audience and what business problem this addresses.i guess the thing i\\'d worry about *more* is how you plan to validate this.thanks guys!now get back to work everyone!!!anyone interested in using the mimic database should be aware that they have demo data available which doesn\\'t require a waiting time for access: there isn\\'t much data in there, but it does give you an idea of how the data is formatted.superwoman has woman only screening, now it has clown only screening.  victory for clowns!  and the best part is that this time mras can attend! #itburnhave anyone previously done an accelerometry-based activity classification project?try  for some relevant experience. from my session did something like classification of boxing punches based on data from accelerometers in boxing glovesthis is true! from my session did something like this, iirci want to think someone from one of the bos health sessions did something with fitbit datahi former fellows, does anyone know of a good repository for terms of service and/or privacy policies for popular apps/services? trying to stand up an nlp project for 2017 remote session. much thanks! used iphone data was that for hykso?yep! could you please send me a link to your app or demo?who uses rnns or cnns for nlp classification tasks? i have read a lot of articles on nlp classification and one thing i have noticed is that it is really hard to beat bag of words and the models that do are by a very small amount.  i\\'m just curious what other people do in practice. i work at  we provide moving help for packing and moving stuff. if it was a short distance move we could have tried to get someone to pick truck up for you. if you are still looking for help, shoot me a message or call us at  and someone at concierge can help. also, if you did decide to use us, i can shoot some coupons your way :wink:i still feel like that in week 8. :sweat_smile: welcome seattle!i would start by considering a discrete version of the kolmogorov–smirnov test, comparing the percentiles. if you\\'re not aware of the number of data points from each distributions, you probably have to consider the number of percentiles values as n.more info here: interesting, , thanks for the link! i\\'m not sure ks test is appropriate. i\\'m asking for specific percentiles. like given two distributions, is there a significant difference in let\\'s say the 90th percentile, etc. currently i have the harrell-davis estimator on the table because it gives l-estimators with standard deviationsbut it\\'s computationally expensivehey all, any recommendations for a large text summarization dataset (paragraph -&gt; sentence)?*any with fields for citations, click rates or commentsif you\\'re comparing just a specific percentile from 2 distributions, the harrell-davis estimator is definitely the way to go, as it gives a more accurate estimate of the percentile and an error, which you can use a welch\\'s t-test to see if they are significantly different.but given a sufficient number of percentiles from both distributions, you want to compare all of them at once, i.e., you have to take into account that the percentiles comes form the same underlying distribution.ks test might work given enough percentiles, but there\\'s probably a better way when one doesn\\'t have enough percentiles (e.g., only every 10%)there\\'s at least one reddit bot that summarizes news articles. you could scrape the text of those articles and the subsequent summaries.here\\'s a python package that implements several algorithms, i think: some are slow, i think. gensim also has some functionality like this.thanks!awesome! thanks for the links anyone getting through registration for grace hopper?does anyone have experience using pymc3 for time series? generally i\\'d like to include some kind of time lag feature but not sure what\\'s the best way and can\\'t seem to find good examples using autoregression in the pymc framework check in with austin rochford on twitter - he might have a good blog post on this! uploaded a file:  and commented: another question in the que! has anyone ever worked with character cnn? do you have a implementation or resource you could share for the following architecture?for fellows and alumni in boston: the city of boston is having a data challenge using data collected from speed feedback signs. the kickoff is happening at cic boston on july 26th from 6-7pm, on the 16th floor. there are prizes for winners! :slightly_smiling_face:link here: also  has a lot of great nlp features including summarization.  i’ve always wondered if those worked…i didn\\'t realize they saved the data!if anybody does this i\\'m super interested to know how it turns out! vision zero is near and dear to my :bicyclist: heartof course you didn’t looks aroundwho, me? why would i know anything about government surveillance of cars?:stuck_out_tongue:demo: i use spacy and stanfordnlp for tokenization!also, it looks like apple has launched a ml journal today: ‘journal’apparently they do! if only there was some one in city government that would (not) do anything nefarious with that data....hey guys, has anybody worked with synthetic lift studies (where one tries to establish a causal link for the effect of an ad directly from the observational data, without performing an experiment)? all i have found are a bunch of papers by some yahoo folks. if you have any lead, please pm me or add a thread. thanks!i was like \"wait, i know someone who would be interested in that as well\" :joy:might be worth checking out propensity modeling to build a synthetic control groupwill you be there ?how does one tell if a query is cpu bound or more so memory/disk intensive?hey bay area folks- new insight sessions are around the corner in sf and palo alto and i\\'ve already had some questions about housing. if any of you are looking to sublease/rent a room to a fellow starting in august or september, send me a dm!i think i\\'m required to be there! :blush:quick question... are the lightning talks available to the general public? or just insight fellows/alums? that video is a big part of my motivation to make the survey in the first place. the other was this general unease/uncertainty about how to assess what a fair salary in the area. feel free to bring a friend / some friends!sweet article: it was nice seeing you this weekend….  for 5 secondspretty nice:: that is both beautiful and nauseating: 5 seconds in heaven is better than 4 seconds in heavenhah.  heaven’s overrated theni’m looking forward to my 5 seconds in hellstay positive alex_rhee: tony is my colleague at h2o :smiley:nicewonder if anyone has seen this internal error in spark stream:  error jobscheduler: error running job streaming job 1467073992000 ms.0org.apache.spark.sparkexception: an exception was raised by python.  it attributes it to an attempt to run foreachdd on a dstream: \"user_accident.foreachrdd(lambda rdd: rdd.foreachpartition(processaccident))\". and it\\'s fairly easy to reproduce. \"user_accident\" is a dstream from a kafka stream( a slow one) and it comes about after a simple map operation to extract some  fields and put out  a (k, v) pair.stay positive sathyabas i suspected python is generating an error when the dstream is empty i believe. i found this discussion in   in . the poster was asking if there was some way to avoid calling the foreachrdd(). is there way to check if a dstream is empty ?stay positive sathyabthe bootcamp setup has windows drivers by apple for the magic trackpad. you\\'re right, i missed that part.isn\\'t collinearity a concept only applicable to continuous variables? it implies that some change in one feature might result in a proportionate change in another. however, for categorical variables there is no connection between 2 different categories, right? that\\'s why mapping them to numeric and calculating the correlation is not valid.i think in this case, you are best served with the simple chi test with cramer\\'s v test, or the anova test when you have both categorical and numeric variables.otherwise, depending on the number of different categories within each variable, one can one-hot encode the variables and apply the generic numeric correlation procedure.you mean, how to identify important interactions that you can then add to your logistic regression model? one can train a shallow xgboost (2-3 depth) model first, then feed the most important paths as nonlinear features.just finished the signal and the noise, really loved it! started on black swan this morning. :+1:yes, i\\'m interested in identifying important interactions.that\\'s an interesting approach. thanks for suggesting it. i imagined i could interpret \"important\" interactions as those with high coefficients after fitting a logistic regression model with interaction terms. however, cursory google searches aren\\'t pulling up much on finding interactions using this approach. now i\\'m suspecting there\\'s something fundamentally wrong with it.thanks again for your input carlos!hi everyone! the insight sf health data science team is hosting a meetup this thursday at 6:30pm with invitae! we only have a few spots left, so sign up if you\\'re interested. food and drinks included! details about the speakers in the link below.if you\\'re referring to \"important\" interactions between a feature and the target variable, then the magnitude of the coefficients, as you mentioned, is absolutely it.yes, that\\'s what i mean :wink: did this during his project. from what i remember, he just created a metric boatload of interaction terms and used a regularized model to find the most important.hey insight data scientists and data engineers — do you love video games? i have an opportunity for a data scientist/engineer at activision blizzard in sunny santa monica ca, to work on building an analytic platform for some of the world’s most popular video games (e.g. call of duty, destiny, skylanders, etc). this specific role calls on the ability to foresee and support business and enterprise-level questions for franchises and titles as part of building a web-based data platform. this team is looking to hire someone very soon. send me an email at  if interested. you get to play video games at work! nuggetbot u so swoony. what a hot mess. this is really interestingamazon machine learning advanced coursewhile that blog brings up valid points, it wouldn\\'t address the not serializable issue. often times, that issue arises due to scope issues, such as creating a database connection in the master and then trying to pass that connection to a worker. maybe post a code snippet?thanks! yeah, those are why i thought it might be possible.i think you may be right---i was definitely creating the client on the driver and passing to a worker. this was what the blogs were doing, so i thought it wouldn\\'t be an issue. i\\'ll try instantiating it within the map unit and report back. thanks!that fixed it! lesson learned. thank you *so* much!great to hear ! now consider creating it within mappartition and then passing it into map. this way, you are creating it in a worker and then sharing it among the threads rather than instantiating it for every worker thread. the whole insight team is on holiday until jan 2, but  is correct, we\\'d be happy to help you sort this out! can you drop us a note in the new year? you can reach me directly here: thanks~ david from your office contacted me and explained things this morning….*awesome!* that is a cool optimization. implementing it got me a lot more familiar with how to effectively leverage mappartition.does anyone know of any companies doing internships in data science for undergrads?thanks for sharing . this is terrible but also a good reminder that these things happen everywhere and that we need to be vigilant. perhaps legendary entertainment in boston, i heard they have some internships at a panel discussion, but not sure of the level at wayfair we definitely have internships and co-op programs i know square brings in lots of interns for data sciencestate farm has some available  a call to all bay area folks and especially those who have experience working both on the east *and* the west coast: i am going to be on a panel to talk about the tech, data science, and machine learning community in nyc and i am curious to hear how others think it differs from the community in the bay area. what do you think?i have nothing useful to contribute, but i\\'d like to say i imagine you guys (in nyc) wear more ties. :wink:… haha, and fewer hoodies! :white_check_mark:this was making the rounds amongst physicist-turned-data-scientist friends:physicist turned data scientist is about 60% of my friends... i thought the article could basically be summed up as: \"why does deep learning work so well? because taylor series work so well at describing the universe! but we don\\'t know why that is...\"&lt;-- biases particle physicists who only knows how to solve things through series expansions i agree and i did not very much understand that jump in logic(to why this explains that neural nets only have to work in a very small corner of parameter space) yeah, my local theorist also passed that around. : ) i agree about the jump; _intuitively_ it sort of makes sense \"oh, the universe is in a small part of the function parameter space, so obviously neural nets only need a small part of the parameter space!\", but it sure isn\\'t a rigorous demonstration. i worry it makes us come off like this:  :wink:righti think theoretical particle physicists in particular, in the spirit of ken wilson, think the rg flow applies to…almost everythingin our defense, it was hugely impactful in condensed matter and particle physicsyeah… max tegmark is basically that xkcd in a nutshellformer biologist here.  i frequently see that cartoon passed around (half) jokingly.  =pyeah, max has gone outside of his realm of expertise a lot recently…i think the only reason that arxiv paper got a call out in the mit tech review is max is a prof there now…so grain of saltbut i was very amused to see the cmb and “cat pictures” on the same figurehe really has… like a less famous michio kakui don’t remember him being _that_ wacky when i took relativity &amp; cosmology from himhaha…indeed, but with probably more impactful work before he went off and started being a philosopher  \"a less famous michio kaku\" what an un-flattering comparison! (note: do not take that as me disagreeing with it. ;-))for solving tab completion/kernel hanging issues try this at top of your notebook: `%config completer.use_jedi = false`  &lt;-- solved the issue for me!hey , we\\'re looking for something like:  or have anybody used crunchbase api before? how long it usually takes for them to give you a user key since you submit the request?hey, i registered a couple of weeks ago and still haven\\'t heard anything...what kind of strategies do you all use for isolating your \\'production\\' database from your \\'research\\' database?(e.g. clone + custom tables?,  read-only slave + ?,  data-warehouse layer like redshift?)advantages? disadvantages? they can work depending on your use-cases.  it becomes hard to optimize for very specific things or build custom versions of an algorithm that may be better-suited for a specific data set,  with these general-purpose model platforms.  on the other hand they can save time if you know you need something general, and i think skytree specifically i remember being fairly performant also.i have very little experience with it thoughanother way to think about it is that it is not aimed at someone who needs ‘superuser’ level of control of models, but you may not need that for many or most things you do. thanks till (and others) for posting this. we are all concerned and uncertain of our future in the us. my sister went on a vacation to visit our parents back in iran a few days ago and now she is not so sure what\\'s gonna happen to her. i am a de fellow this session and quite frankly it is very challenging to focus on my project with all the uncertainty revolving our lives in the us. seeing all the protests and support today was very heart warming. thanks to all for showing us your support. i couldn’t find an appropriate emoji to react to your message, but i just want to say that i’m glad you’re here and hope your sister can return safely. please know that many of us are fighting for you and all immigrants.then i don\\'t think i\\'ll be able to say good bye in person! best of luck back up north, and let us know if you come back to visit!hey guys, would anyone with some expertise relating to nlp be willing to chat about project validation? my project deals with identifying topics of interest from unstructured customer reviews and assigning a sentiment value to them. any help would be much appreciated! i don\\'t know if this has been shared already or not, but open data science is matching aclu or united nations refugee agency donations up to $5k per person.hi everyone! i\\'m trying to scrape some data from a javascript query that seems to send a submission form via post. however, i\\'m not sure how to get the responding page or what the keys should be. here\\'s the page: i\\'m trying to look at what possible keys they are to send through a  function, but i\\'ve been unsuccessful so far.does anyone know how to get that, or another tool that might be more helpful, considering that the resulting webpage is also a javascript page?joao: have you tried watching the \"network\" panel in chrome\\'s developer tools (view&gt; developer&gt; developer tools)? that will show you the different requests that are made, and you can click on them to see request and response data. why post when you can get!  for instance, just a little playing around chrome\\'s developer\\'s tools and looking at some source code...   geog = ctyevent = ased | asho  (for emergancy department vists vs hospitalizations)year = 2000 - 2014race = totl|afam|aspi|hisp|whit|othr (for all races, african-american, asian-american, hispanic, white, otherage = totl|0004|0517|1834|3564|65ov|0017|18ovsex = totl|feml|malemodel = conv|bbym (for conventional vs spatially modelled)type = r10k|cr10l (for age-adjusted vs crude adjusted)are the form names:just iterate through them all and profit!ex: what he said! :stuck_out_tongue:thanks. i\\'m looking at it now trying to decipher it. from what i had looked before, it seems like the query page sends a post request via , but i\\'m not sure about the keys being sentawesome!thanks a lot just watch out since certain combinations of options may return an error (according to the website). good luck!by the way, where did you find the list of the form names?any advice on how to render a page with some javascript generated table, before being able to scrape it?i\\'m trying using pyqt5 but not having luck so farselenium?not sure if it\\'ll dump the pagehey sf bay area folks, anyone need a great bike? i bought one brand new from mike’s bikes in palo alto about 4 months ago, used it to just get around palo alto, and it’s in near perfect shape and i have all the original paperwork for it. it’s a raleigh 2016 detour 2 (), red, size 17. i also bought rechargeable front and rear led bike lights and a pretty heavy duty u-lock. all that stuff combined plus tax came about to just under $500 but i’d be willing to sell it all for $350 (or best offer!). anyone interested should feel free to dm me (and i have some pics of it as well).i was finally able to get it via selenium and chromedriverhad to get the chromedriver, but now i can simply render the page on chrome on demand :slightly_smiling_face:cool beans! don\\'t forget the little people when you become rich and famous off your demo!lol. i think at the top of my priorities right now is just to get some sleepfor future, you may want to consider splinter.   swears by it.  selenium is a very round about way.i do enjoy splinterworked well for interactive scrapping @ ny  help!  i need a place to stay from october 5th.  please let me know if you hear something.  thanks^ \"what it\\'s like to be on the data science job market\"anyone interested in checking out the mayweather-pacquiao fight tonight in mountain view? i\\'m thinking of heading to st. stephen\\'s on castro st.what time does it start?i think it starts ~8pm,so i was thinking of getting there ~7pmfor folks in bay area, i have a clipper card loaded with a caltrain monthly pass for september, zone 4 to zone 2.  i paid $190 and willing to sell at half price. pm me if interested.   anyone have experience with testing database schema? my team basically wants tests in place such that when someone changes one of the schema, our scripts don’t all break. i think that counts as an integration test? any suggestions or references would be super helpful :slightly_smiling_face:what type of database is this?exploring the spacenet dataset using digits: (shout out to my lab\\'s sister lab: cosmiq works!)further promoting: they just started blogging about some of their works on _spacenet_ here: sheareraj: what are “our scripts” in this case? etl?here\\'s a quick informal survey: what is your company\\'s policy on the use of and contribution to open source software packages? if you need to go through a formal approval process for certain licenses not pre-approved, what is the turn-around time? thanks for any input!biased answer: my company open sources all of my teams work; we regularly contribute to open source and the process is \"hit the pull request button on github\". i expect that is on the crazy open end of the spectrum. a little history: we do this because we work with a bunch of collaborators and \"we just open source everything\" was an easy way around fighting about ip.stay positive alex_gude yeah, etl-type stuff, analyses that get repeated periodically, really a little bit of everythingnuggetbot check on alex_gudealex_gude has a happiness average of 0.5905172413793104i know, i\\'m as surprised as you that i\\'m not negative yet :wink:i mean...nuggetbot check on scstarkscstark has a happiness average of 0.24100156494522693 ^turns out i\\'m just a lot more positive! :wink:also that\\'s a lot of sigfigsnuggetbot must be precisenuggetbot do you know the `rats()` matlab functionnuggetbot check on kumbhanikumbhani has a happiness average of 1.02  here come cheap ingredients for robotshi guys, i had a question about project validation i was hoping someone could help me with. my insight project deals with extracting interesting (most talked about) topics in online reviews, and assigning a sentiment to them based on the context of the sentence they appear in. the reviews do have a 5-star rating attached, but i was told they would like to get more \"granular\" and parse line-by-line rather than just using this overall rating.does anyone have any good ideas on how i could validate this procedure? i don\\'t have a \"sentiment-labeled\" review set or anything unfortunately.if in doubt, start with textblob: cool paper published yesterday on nature: application of deep learning to classify different types of skin cancer, with similar or slightly better results than dermatologists. ring a bell, ?how to not data science :open_mouth:wait. someone thought it was a good idea to name a project skynet?gotta admire the gallows humor in the defense sector...on another note, does anyone know anyone with experience in data governance in hadoop environments — including the creation and enforcement of security policies within hdfs via ranger or other methods?. also curious about encryption-at rest and in transit- tools and other meta-data tagging capabilities. we use edgenodes with our internal hdfs cluster with active directory integration (i think via kerberos)i’m not sure of the exact implementation details, but files/folders on hdfs have groups associated with them for which we get access to with a separate request.any former fellows out there have a good starter tutorial regarding working with flask? this might help . step by step instructions here hi guys! i’m having some problems deploying flask with aws (see screenshot below). if you guys have some time, i would really appreciate any help!!! thanks so much in advance!! :heart::sunglasses: uploaded a file:  and commented: what does this mean……?! thanks so much for any insights! what\\'s in your requirements.txt file? looks like it tried to do a pip install of a dependency and failed. did you check the log file?does anyone have any experience comparing data from two samples both from a negative binomial distribution? i\\'m considering a mann-whitney u test, but i\\'m not sure if that\\'s the best approach. i\\'m also considering negative binomial regression.i\\'m also thinking about simulation but i\\'m not sure of how to best approach that given that the negative binomial distribution has two parameters.  permutation test? then you\\'re not losing info by converting to ranks like with m-w.a pretty cool tutorial - (warning it’s long)i\\'m not that familiar with permutation tests, given that the negative binomial distribution is not symmetric i believe that randomization tests aren\\'t appropriate, what specific type of permutation tests should i look at? you could probably use the kolmogorov-smirnov test after adapting it to your discrete data. however, i\\'d probably use the kullback–leibler divergence.on the other hand, if you already know the data comes from the same distribution but with possible different parameters, a m-w u-test or even a t-test will do the trick, as there\\'s no need to compare the whole distribution.here\\'s more on our upcoming 4-year reunion gathering in palo alto open to all  insight de fellows next week: my company (finance firm) is restructuring our office floor plans! do people have insights into what types of office set ups (eg, open floor plan, meeting rooms, other types of work/gathering rooms) tend to work well in terms of productivity and enjoyability?insights for insight office set up :stuck_out_tongue:but seriously, i found your nyc office in flat iron so enjoyable!that was open plan, with a few meeting rooms and a fridge full of beers  &gt; private offices with doors that close were absolutely required and not open to negotiation.oh maybe i should clarify - i’m not working for insight. i’m a boston alum working for a finance company (ie not traditional tech office floor plans)read “quiet”, decide where your team falls on the introvert/extrovert/sensitive spectrum, then decide. finishes that reflect sound make open plans horrible because you can hear everything. white noise is often necessary (i’ve seen people install white noise machines in open offices, literally)    text to speech in python 3.  haven’t tried it yet.a series of pieces by nyt called “reimagining the office”.pt 1 is a really good piece about what google learned from its quest to build the perfect team.  also part 4 deals with diversity.  highly recommend.it really is a good series of articles. i enjoyed reading them.current idx fellow,  will be on all things considered today around 4:44pm et. she’ll be speaking to her academic position and how the brain responds to teleportation.pretty cool ()if i `pip` install theano on macbook air, how many cores will it use out of the box? is there a way to set this parameter in (say) a jupyter notebook? i know a comp sci graduate student at uic that might be interested, but its harder for him during the semesteryeah. its urgent. only 2 weeks timealso, someone with slack admin rights might want to delete old files since we are over the limit ()i need to post dog photosit’s urgenti\\'m not sure if everyone is already aware of these, but i found them quite a bit better than the default: ooo prettystill waiting on promised dog photos over here uploaded a file: the dogs of adrolli present mochi.her favorite things in no particular order are1.  food uploaded a file: this is lady.  she is from thailand.her favorite things are, in no particular order, are:1. food2. her owner alligood dogshe is also very fashionablethxi got you i’ve been stalking the office dogs for youalsothe white one bit meloli was poking it thoughand by poke, i mean petting it to get its attentionwho knows a lot about probabilistic graphical models (pgm)? someone i know (at mount sinai in nyc) is looking to chat with someone, who knows about those pgms. i believe it’s location agnostic … and … for a good cause!hi all. i am involved in a project where a non-profit is looking into possibilities to take advantage of their huge database for some (undetermined) advanced analytics in the near future. the current discussion with the database engineer is about various data warehousing/lake solutions, as the first step in the process. they have all types of data from sql server, to csvs, to unstructured reports.  i need to make sure that any solution proposed would allow analytics in the future. (they currently only display various information in their website and standardized reports.) . does anyone have thoughts about things to avoid or make sure are implemented for data storage? thanks!ps. they are currently considering salesforce. delayed response here, but i use files dumped to a semi-organized structure in s3 and spectrum to join that data in to other sources in our existing redshift cluster and it\\'s pretty greatthe advantage of the files just living in s3 is you can also do more specialized analyses via spark in addition to having some standardized queries run regularly with redshift/spectrum thanks! i was reluctant to do the switchover based only on the press release (which makes it sound great). i hadn’t even thought of the advantage of using spark with it too.yeah, i was exclusively doing things via spark until spectrum came out and spectrum has made my life a bit easier (no worrying about tuning the cluster parameters for memory issues, etc) i had held a few bitcoin for a number of years but i actually sold them earlier this year to help pay for housing during insight~~zkermish: how do you decide between redshift or spectrum? do you use spectrum for data transformations that you then throw into redshift or are these just big one-off or seldom queries for stuff you’d rather not put in redshift?btw, we do similar dumps to s3 and loads to redshift, with occasional spark jobs for data transformations.the data we now link through spectrum was large enough that we had issues loading it into redshift…also, it would have brought our costs upwe have enough one-off queries to do that it’s worthwhile to link to redshift via spectrum and just run those as neededand eventually the hope is to have some regular queries done that right back to redshift for longer term use (overall descriptive metrics to condense the information in the raw sources) i guess for us it’s a good way to explore a larger source of data before having a clear process around it and a way to load in some distilled info from that data into redshiftall at a lower costawesome, thanks. we’re in the process of offloading some in-redshift transformations to spectrum (instead of spark) to keep redshift resources dedicated to our bi tools.and the migration from redshift -&gt; spectrum is much cleaner/faster than  redshift -&gt; sparkcoolhi  what is spectrum? ?“redshift spectrum”no  , it’s an aws thing: ah, i thought it\\'s called athena!i’m sure it’s all spark/hive/presto/whatever on the backend that they manageathena is very similarbut that doesn’t link to redshiftah!subtle distinction…they’re actually super linked. if you make a schema accessible in athena it’s there in spectrum too. the advantage is you can use your existing redshift cluster to run the queries / join to other data that’s therecost is the same too (they both just charge per tb scanned in the query)what is a good tool that you guys have used for schema management?but spectrum queries aren\\'t as fast as redshift, right?some things to consider, maybe being able to separate out personally identifiable and non personally identifiable data and store in 2 different db?spectrum queries use athena and redshift under the cover it’s all parallelized under the cover so it’s quite fast. the only thing missing is a dist_key and the associated optimization but my sense is spectrum approaches that by using compute cycles on the storage nodes…not entirely sure..but it’s all fasthey past fellows! does picking a la croix flavor get any easier? :confused: ending the 4th week and i’m still having troubleben_stclair: ask  , he\\'s a lacroix connoisseureventually i just went with one of the beer flavored ones.haha just try them allexcept lemon. ew.when in doubt, don\\'t feel blue, go green!citrus are the clearly superior options. berry used to be... but unfortunately i think it\\'s gone. :disappointed:there is only one la croix its all about the mango (or anything that comes in a tall can)it does get easier. lime is the clear winner.just stay clear or coconut and lemoncoconut and mango and berry are the only acceptable flavors coconut? what the hell man, i thought you were coolhahaha, i know, i’m very much in the minority on that one….thanks, everybody! this is really helping my decisions be more data driven :r: the nakamoto paper  … i’m very much a noob :slightly_smiling_face:would be very interested in any references you could recommend though!definitely coconut. #teamcoconutprevious fellows: at week 4, did you start feeling excited for demos but convinced this real-world job thing would never happen? it seems to be a common fear in some circles.100% coconuti’m with cocoi go with the two flavor ones usuallypassion fruit for life!! :  maybe the current fellows want to start a channel to discuss their project ideas with the alumni?good ideaor even if they\\'re stuck with a particular issuemaybe someone can have some helpful input: it would be good for more alumni to sign up for mentoring slots too (especially us nyc folks!). jenainsley would really appreciate it!how many alumni are actually on here? i think it might just be me and british matt from my session.there\\'s maybe 10 from my session (nyc 2014) i thinki was really just pestering them :simple_smile:i just updated my profile to include insight info.  that could be one way we know who\\'s from where and which classslackers = a exp(-k(now - session date)).  find a and k!good idea talldave , i\\'ve done the samek = i: thanks for the mention!if anyone has questions about signing up for alumni mentoring spots (project feedback or mock interviews) - email me! :sparkles:nuggetbot: animate me welcome everyone!:  is a good package. i used it in my project: www.nerddit.club/graphalso easy to use with neo4j :simple_smile:happy  international women’s day! check out our blog post! lol…i love that the two slack icons of women used to “like” my post above are called “ok_woman” and “information_desk_person”. how…ironic?: i used the ‘information_desk_person’ because previously a friend used it to indicate (in their words) ‘it’s like a hair flip heyo\\'so it was supposed to be chill:okman:oh, okay..if anyone wants 20% of the upcoming dataengconf in sf, we have a discount code: ids20xthey also run a regular (free) meet up for data engineers: and in ny as well: this match is fun to watch! deep learning + reinforcement learning against the world\\'s best! alphago beat lee sedol!i did not expect that.how is x distributed?fun data on bacteria in nyc subways he that was my supervisor :simple_smile: did you help gather data by licking poles in the subway tom? tell us how you guys wrangled the pole data!!? no, nooooo not *those poles* :speak_no_evil:gosh.also the github which has all of the code used with mimiccurrent remote fellow here: shot in the dark - anybody have experience with audio/speech processing in python?  i’m building a web app that processes audio of the user giving a presentation and does nlp, sentiment analysis, etc. does! or  have done some as well sounds good :slightly_smiling_face: might be useful to take a look at  in addition to whichever other tool you use (i recommend just going straight to `requests`)is any one going to the google cloud summit in seattle on wednesday?  trying to get in some networking as i look for my first ds job! pinned a message to this channel.current fellow here working on my project...have any previous fellows used the  api?  i\\'m having trouble getting the vehicle api key and was wondering if any past fellows might be able to help. thanks.  ? i did some for my insight projectgot it. that\\'s two strong reasons to go for it then (i\\'m using python). thanks!hello a new job posting in my company not exactly related to data science, but in case anyone is interested hello all!  i’m starting a ds position at a place that gives dss an option to work on a windows or ubuntu machine.  i am traditionally a mac user, and like the idea that ubuntu has much more overlap with mac — but am not sure i’m up for all the fiddling that i’ve heard can be required with ubuntu. what are your opinions on the two options?if it\\'s windows 10, you\\'d have access to the bash subsystem, so even less of a difference between them.i made the switch from mac to ubuntu last year. not much fiddling at all. 16.04 lts feels very much like osx did. and if you ever used homebrew on mac, the linux/ubuntu package manager system is easy to use. no problems at all installing &amp; using anaconda, tensorflow, etc.i have a windows 10 machine &amp; run a virtualbox w/ ubuntu. best of both worlds for scripting + ease of use with microsoft products (ppt, skype, etc.) that can be challenging on ubuntucongrats on the ds position! i was a long-time user of mac and linux from my time in academia, and i am now working at a place that operates exclusively under windows, but i was very pleasantly surprised by how minimal the hassle was for me to use windows to do my job. that having been said, i do wish i had the option to use linux. either way, i think the fears and concerns with one versus the other are generally overblown.very good post on ai...congrats on the new job! i would carefully examine how the it infrastructure supports ubuntu vs windows. daimler (my company) is historically a windows company, and has recently been transitioning to support mac. there are a lot of network issues that are purpose built to be easy to handle for windows and are ultra frustrating to deal with on a mac or linux box (i.e. proxy settings, email server, shared directories, etc). if your company deals primarily with web-based stuff, then i’d recommend sticking with ubuntu, but if you need lots of productivity software (ms office, etc) it might be easier to just use windows.just go windows. besides the bleeding edge, you\\'ll be able to use most data science tools just fine and you\\'ll have less hassle producing results you can share more broadly.if your company maintains its own fleet distro, then it\\'s a strong indicator in favor of linux - patch tuesday still sucks decades on-going.a contrary indicator would be if everyone\\'s account is office365 and activedirectory instead of say ldap.hey people, hope you are all having a great day. i just had a general question, for a classification problem, is there a way to check for collinearity between categorical variables. been looking around for a method, thanks in advancewhile this channel is still reacting to faye\\'s last @ channel blow-up, i have an actual question for everyone. my team is in the process of interviewing for a new data analyst role, which is an entry level data position. i\\'m building a rubric to help my team judge resumes. i was wondering what you all look for in junior positions like this, at least on paper. any ideas would be great! :slightly_smiling_face:what are you hoping the person will work on? experimentation, dashboarding, ad-hoc work, modeling? how much will they need to get their own data and write etl? design experiments? do they need to work with engineers in python/scala? how do you share data internally, via tableau, jupyter, something else?frankly, they will be doing most of the above (our team is small, so most roles we hire for are generalist roles).there will be a hefty amount of guideance in the new role, but i expect the new analyst to help with dashboarding, aid in modeling (even if they aren\\'t versed in it) and do etl as necessary in conjunction with our data engineers. we are pretty platform agnostic, so any knowledge of r/python is a plus.usually data is shared in jupyter, rnotebooks, and tableau.the biggest thing for this role is their willingness to learn and interest in government, since they won\\'t be on one single project with one focused task.anyone have any experience with speaker separation / diarization? i\\'m having moderate success with the typical approach of agglomerative clustering, gmms, and structured perceptron. would love to talk details with someone who has some experience though.samba tv is a long-standing insight partner, but we just had a de (insight alum) decide to move on to salesforce, so we have an opening: does anyone have a suggestion for a resource (anything from a blog post to a coursera course) to understand the basic things to consider when deciding on a data pipeline?  i want to set up a pipeline to handle a few tables with about 10m rows added per day, and i want to figure out whether a typical s3-&gt;spark-&gt;redshift pipeline can handle it.  i\\'m not a data engineer, but i definitely want to learn more!anybody interested in latent dirichlet processes? or if you have worked with them, please let me know:grinning:fantastic!does anyone have experience with the facebook api?  i\\'m pretty sure what i\\'m trying to do is really simple, but i\\'m getting lost in all the set-up documentation.what are you trying to do?i just want to get the number of likes for a list of pages.  i have the addresses already.hmmm...unless you have access tokens for all those pages, might be impossiblewhat endpoint are you trying to hit?does anyone have a link to the mentoring sheet for tonight?pretty cool:  uploaded a file: yeah! we were surprised by how well that worked...i really got to start trolling for meetups... :confused:hey everyone, if anyone is looking to pick up some o\\'reilly books, here\\'s a 40% off discount code, i just ordered a few and it worked fine: wcyazif anybody wants a chance to learn tableau without paying the $1000 for a license, you can get a free (and legal) key by registering for this competition:  . (the site doesn’t say how long the key is good for, but mine says it expires in mid-august, which is considerably better than the usual 14 day trial)for those of you still studying, there is a decent documentary on netflix called algorithms that explains some of the sorting algorithms, pagerank, and decision trees using fancy made for tv graphics.\"any bias contained in word embeddings like those from word2vec is automatically passed on in any application that exploits it\"tableau licensing is notoriously expensive. part of their pricing model assumes that all potential users of the product should be factored into licensing, and the scope of a potential user is very broad (e.g. an employee that may use excel -- in other words, everybody -- might use tableau, thus they should be factored into the license cost). it wouldn\\'t be uncommon to see tableau offer a site subscription to a small/medium organization at a rate of ~$1m+ for 3 years.consider your use cases and existing tooling... power bi is improving quite a bit and many organizations already have licensing arrangements with microsoft. also take a look at qlik, which offers their desktop product for free (windows only). the main limitation with the free product is sharing/publishing/collaboration.after talking with a lot of teams,  and i wrote down what we learned about transitioning from software engineering to applied ai. let us know what you think :slightly_smiling_face: does anyone every use any gui tools for debugging extremely long, hard to understand sql queries?awesome post.  but, what is ai? can you give examples that are ai, and not machine learning?  is reinforcement learning - ai or ml?we interviewed someone recently and reading my colleagues comments i was reminded of a gap i have (e.g. i could not have answered the questions they asked this candidate) --- particularly, the trade-offs between all the different ways we can store data (think broad here, not just sql vs. nosql, but that\\'s a piece) and how to think more deeply about schema choices.  when i asked my colleagues they uniformly said something like, \"i learned it on the job\" or \"key is to try the different options and see why it doesn\\'t work\".  i\\'m learning some of it on the job, but it seems like we are in a place where we already made a lot of choices and i want to understand why better... of course, i don\\'t have infinite time --- maybe a chart to start with, or say one section of an online course i can listen to on a bus... perhaps this can help: provided you\\'re using postgresany opinions on whether this is worth attending?went last year for free two of the days. some decent talks.i have a question about data science platforms .. my team requirements are simple: have a collaborative workspace supporting multiple languages and easy to deploy. i know of tools that solve the pieces of puzzle but i am looking for something more comprehensive .. i looked into dominodata labs and it helps, but i think its very expensive for something that is completely open source tools. any experiences or leads into other projects or tools? what do you mean by \"collaborative\"? does \"we share code on github\" count? or do they need to have like jupyter notebooks that anyone can edit at any time?predicting wheat rust in ethiopia with the bill &amp; melinda gates foundation: : lets say a big more than we share code on github .. commenting on files and general discussions .. i think many of the tools like bitbucket, gitlabs do that. but do suffer from a lot of complexity in ci/cd.i think bitbucket plus docker is what i am looking for ..seems likely. we used domino and went back to docker + github on our local servers, but we\\'re weirdhas anyone ever used the python package lime? my team is working on ways to easily explain individual predictions from a model uploaded a file:  we\\'ve been testing lime. we found it worked under... exactly the circumstances that they presented in their talk. :confused:it falls flat on anything complicated (or with dimensions that are not explanatory, like word2vec dimensions)thanks  any thoughts on how well it worked with random forest of (ballpark) 10-100k data, 50 features? other methods you have liked?i don\\'t believe we used it on a random forest (but that seems like a model that it would excel at). we\\'re still exploring options; our boss is a big believer that model explainers are the future. (which means it is one of the things we\\'re tasked with looking at.)more frost this morning...  uploaded a file: here\\'s a little gem:  sublime text key bindings for jupyter notebook!(add the following to `.jupyter/custom/custom.js`)hey, i\\'m looking at your chsi data hanlerand i have a couple questions whenever you\\'re around.what\\'s up?i figured it out in the mean time sorry i didn\\'t get a notification when you messaged me!it was very useful, i pulled out tons of interesting predictors.oh i did want to ask in your ridge regression, why did you weight by population size? is it because you were looking at state wide effects?if you\\'re running the regression to develop hypotheses of causal relationships at an individual level (which is a sketchy thing to do -- what with causation not being correlation and all that -- but it was assigned to me for a data challenge for a company i interviewed at...) then you probably care more about prediction error in counties with more people because they represent more individualsyeah, makes sense. i was just curious. usually we only weight by population size when we\\'re trying to weight survey sampling by strata.i wanted to make sure i wasn\\'t missing that i should do it with my county level inference. :smile:did anyone ever encountered virtual box problem such as \"the virtual machine \\'ubuntu\\' has terminated unexpectedly during startup with exit code 1 (0x1). more details may be available in \\'c:usersselinvirtualbox vmsubuntulogsvboxhardening.log\\'.\". my windows restarted itself and now my vb, therefore ubuntu is not starting.hyper-v has been much more stable for me than virtual box. i was getting blue screened every time i tried to launch a ubuntu virtual box instance recently.yes. pretty much any other virtual environment is more stable. after 4 hours of troubleshooting, i fixed it. i think it is time for me to spend money on some nice macbook, and never risk something like this again. btw: the problem is caused by windows 10 update, windows 10 firewall was attacking the saved ubuntu status files. for many people, removing the firewall was helping, but it did not help me. i fixed it by going back in time. :slightly_smiling_face: i\\'ve been doing all my development in wsl and it\\'s working a dream.might be an option for you.i will check it out! thanks!anyone want to do some nlp on emoji? witness the sputtering of the hive mind.that would be fun!anyone else interested in future insight alumni hackathons? once we (the current session) get some free time we\\'re thinking of doing this. join at  , did you ever find that voting data?anyone looking for room mates?if so,come join the new  channel!!!does anyone know a good *zeppelin* tutorial that walks through working with a dataset? i\\'m tasked with comparing jupyter and zeppelin, and i\\'m a complete zeppeling n00b. :wink: most i\\'ve found so far focus on installing it, but i\\'ve got that solved with docker.horton works has a gallery of notebooks that seems like a good start: : check out toree, which literally runs in jupyterit’s still in the incubation phase though...anyone know what happened to nuggetbot?don’t wake the beasti\\'m not sure how to stay positive without his friendly remindersph\\'nglui mglw\\'nafh nuggetbot r\\'lyeh wgah\\'nagl fhtagn.agreed stay positive, jollyhrothgar! all will be well.:stuck_out_tongue: dyfrig - could you please send the name of the textbook for time series analysis?: : you could also check out google scholar - they show h-indexthe arxiv has an api too, im not sure if it has citation information: the arxiv does not have citation or precomputed h-index infoinspires is much more rich, but limited to astro and hep: many thanks. google scholar does not provide any api, and it blocks if requesting too often -- based on the info i search online but didn\\'t try out myself.i got something to start with from inspirehep. thanks a lot, guys i looked into this at some point. all of the citation data are owned by either scopus or web of science. google has a deal to make it available basically as a community service, but they\\'re not allowed to have an api. for medicine/biology you could scrape a well formed pubmed search to get publications. :  if you need to scrap pubmed talk to   he did some nice things with pubmed data. here is an example scraping pubmed with ri have a perl script for it as well... did anyone (or know of anyone) go into machine learning/deep learning academic research in industry? i was curious what the career trajectory and market compensation is like. please dm me. thanks!can anyone help with getting a postgresql database set up with aws? i have an e2 instance running ubuntu, and i\\'m able to connect and load my app. i also have an rdb instance running postgresql, and i\\'m able to connect and query the database on my local machine with sqlalchemy and psycopg2. however, when i try to run my app on the e2 instance, i get the following error after running `gunicorn flaskexample:app`,```operationalerror: could not connect to server: no such file or directory    is the server running locally and accepting    connections on unix domain socket \"/var/run/postgresql/.s.pgsql.5432\"? ```if i comment out the line where i try to connect to the database, it will run. i\\'ve changed the permissions on amazon rdb, so it should be able to connect. i\\'m stuck, can anyone help?hi all, i fixed it! it turns out i wasn\\'t accessing the rds database as i originally thought. i was still accessing the local database, which is why it didn\\'t work on my e2 instance.hi everyone,  does anyone knows a good tutorial to deploy flask apps in redhat?  (beyond the ones you find googleling)does anyone have a suggestion for an hourly rate for data science consulting work? a large range -- perhaps $100-250 depending on if your client is a startup or fortune 500 :wink: many many factors can influence the rate thoughthanks! :expressionless::expressionless::expressionless:thoughts?it’s strange how we see computer scientists, politicians, and economists arguing that gender differences are a result of biology, but never population geneticists.note that google’s response in internal memo is on the motherboard article now (as is the full text of the original) .in terms of social issues, i find that it\\'s (un)surprising that we hear critique from (presumed) white guys who feel like they aren\\'t treated equally. regardless of background. we\\'ve always used \"biology\" as a way to argue for exclusion historically, unfortunately. you have to wonder that if there\\'s a general consensus  about what this guy asserts, where are different minority groups in supporting this statement? /sjwseems like a typical corporate response. i wish they took a harder stance. :thinking_face:i found his screed to be an exhausting rehash of garbage ideas and talking points. for nearly all of his arguments: [citation needed]. and i\\'d bet the farm this guy subscribes to red pill and men\\'s rights on reddit.yes, i thought the response sounded like bsjust read it. i thought those [#] markers in the google email were citations.turns out they were footnotes with more ranting.lolcontroversial male opinion: just read the whole thing. i think the pseudo evolutionary science explanations for differences in representations are bullshit, or at the very least difficult to prove. however, i agree with his point that there\\'s a culture of trying to shame people who think non party line thoughts on diversity and that it\\'s unhelpfuli find that pretty suspect and am curious what your examples would be. for this specific case, the medium article is pretty clear in describing how toxic this kind of ideology is.  it is important for leadership to take a stance against it so the organization can continue to function instead of tearing itself apart with suspicion and lack of collaboration. there is rightful anger against this person for perpetuating  the sexism and racism that has troubled our society for so long, and if they’re shamed by that, that’s their own fault.i\\'m going to sorta sound like a bitch here, but i\\'m going to say it anyways: there\\'s a distinction between compassionate educating and \"justified\" calling out. but i\\'m generally, if you are perpetuating sexism, racism, islamaphobia, homophobia, transphobia etc, an someone calls you out, i\\'m not gonna massage your hurt feelings. sometimes we say things or believe things that are hurtful (and/or wrong) to others. and sometimes we need to learn from those mistakes. i place myself in this statement too because i have also have done and said things that have been hurtful or off base.i usually don\\'t bring my own identity into the discussion, but i will here. i applaud aggressive inclusion practices and believe we need more of it. as a *trans* person, i have no effing patience for people who perpetuate oppression or discrimination. i\\'ve spent my past and current life being on the fringes of society, and puke when people who are not trans or have not consulted w/trans people try to speak for me.anyways, hopping down my pedestal now. 🤣i guess my example of would be the reaction to this postin the medium post itself the guy says \"i need to be very clear here: not only was nearly everything you said in that document wrong, the fact that you did that has caused significant harm to people across this company, and to the company’s entire ability to function. \"the idea that this one person\\'s opinion can cause significant harm to people seems like a pretty strong assertionand if we\\'re working on that assumption then the follwing \"if you feel isolated by this, that your views are basically unwelcome in tech and can’t be spoken about… well, that’s a fair point.\" seems justifiedbut i guess i question the underlying assumption, and think if we really do think he\\'s wrong, we should engage with him, not threaten to immediately crucify him and anyone who might be within 50 feet of himie, don\\'t be like yale i don\\'t think the crux of this debate is whether callouts are justified, it\\'s whether this employee (and perhaps others who think like him) should be straight-up fired (and perhaps blacklisted by the industry) via the usual social media dynamics. how far do we need to shift the overton window in order to have an environment that\\'s considered sufficiently inclusive to whatever groups?so it sounds like your argument is that you think his statement is not that harmful and therefore he should be spared all of the criticism he is receiving.  statements that support the kinds of discrimination and oppression that are the norm in technical careers (and elsewhere) are absolutely harmful. that system is absolutely harmful and we are trying so so hard to improve it. every time someone makes hateful and uneducated statements like this, we take a step backwards. given the public forum his statements were made in, criticisms absolutely must be in public forum as well. if someone said this stuff to me privately, i would argue them privately but not post it on the internet. but he is the one who made this the the forum. i have zero pity for him. also, the fact that you claim people  “crucify him and anyone who might be within 50 feet of him” is absurd hyperbole. he is being countered, as he should be. he might be removed from his job, once again, as he should be if he has prevented himself from being a functional part of the organization by attacking his colleagues. warning i am not going to  respond any further to this thread since i don’t actually know you and don’t want to go down an internet argument black hole.also, not to continue to belabor the point (against my better judgement), but what reaction would you expect if someone aired their misogyny (or other awful opinions) to a wide audience? would you expect everyone to be all ok with that?i think it\\'s worth understanding why many women got up in arms about this manifesto and made their grievances known, rather than being reactionary about it the grievances themselves. this is generally also a good model to follow for any social justice issue.i would say that there is room for both anger and critique on the discourse we have about gender disparities (and other disparities in general). it\\'s not an either/or. i find it slightly disengeniuous to compare gender disparities in tech (especially with regular headlines through this year) to an incident at yale. we know there is icky stuff going on in the world place at many tech companies, and is actually a good reason for people to be less than pleased with every new piece of news that comes out.like  said, i have no sympathy for this guy. he aired his opinions company wide. he should\\'ve been prepared for critique, angry or not.i think i misspoke in my original comment so i\\'ll clarify: yes, of course people should be able to get angry. but in the medium post that was also posted, yonatan says, \"you would have heard part (3) in a much smaller meeting, including you, me, your manager, your hrbp, and someone from legal. and it would have ended with you being escorted from the building by security and told that your personal items will be mailed to you.\"which basically says he would have been immediately fired for his opinions. judging from the near universal praise of that piece, i think many people here think that\\'s an appropriate response, and i don\\'t agree. the original piece might be wrong, but i think instantly terminating him doesn\\'t remove the underlying biases that lead to the piece. it just drives them underground so the next person who feels this way will just talk about it in redpill on reddit. if we\\'re serious about trying to convince people that diversity is good, how can we ever hope to convince anyone if they\\'re afraid to share their opposing views because they\\'re afraid they\\'ll get fired.so uh, mea culpa for making it sound like i think it\\'s wrong to get angry or critique him. that\\'s a totally appropriate response and no one has a right to tell anyone not to get angry. i just don\\'t think he should be fired or sent to the basement for rat clearing duty.+1 to that answer!hello all! can anyone recommend any tools for labeling data sets that have worked well for you? (and possibly ones to avoid) thanks!what type of data are you labeling? i used a java tool to do image segmentation… i can try to find ithi everyone (if i get enough thumbsups, i will hit the dreaded at channel on this) i am aggregating data on parental leave policies in tech startups.. hard to find data on small companies, and since scraping glassdoor is not the first thing i would like to do :wink: i am trying my luck with crowd sourcing! it is extremely important for startups to have parental policies in place if we have any chance at truly diversifying tech! women (and men) should not feel the need to work at select big companies if they want to make babies.. especially since having a family and kicking ass in tech are not mutually exclusive! so please help me aggregate some data since we all know data is power :slightly_smiling_face:  i started a sheet with the big companies, please add to it any small company whose policy you know of and any more data on big companies the better!wow super useful. thanks!so if we work at a non-startup should we enter our info or not?yes! please also enter! but would love to get some data on startups.. so please also include the number of employeeswhat’s the :+1: threshold for going at-channel?can we get more music fanatics in ?if you want a pure python solution for image segmentation, bleeding edge scikit-image has that capability we\\'re experimenting with  for text labellingthe developers are super helpful with deployment/customizationhehe.. good question soumya.. 30?anyone aware of openly available data on hiv anti-retroviral treatment (art) adherence with associated social/demographic/medical factors? i have an old project i’m trying to revive and get a publication out of, but out outcome (art adherence) was poorly measured and subject to a lot of bias, and i’d like to try and train a model on decent data of art adherence that i could then use for these empirical data we’ve collected to predict art adherence. i’ve been reaching out to researchers based on publications that might have data willing to share, but wanted to tap into this collective bundle of genius as well. i’m open to anything, but if it happens to be data from sub-saharan africa, all the better. if you’ve got any thoughts or leads, pm me. thanks!for those who do a lot of exploratory analysis or ml algorithm prototyping, what kind of tools (internally developed or externally available) do you use? the reason i ask is we are thinking about building some infrastructure for this internally (leveraging what\\'s available as much as possible). what are the considerations you would suggest before embarking on such a project? i\\'m thinking about, for example, infrastructure for database connection, scaling compute resources vs. quickly seeing if an analysis is promising vs. how easy it is to move from exploration to production. those might not be all or even the most important things to consider, though. basically, before we waste time building infrastructure that turns out to be useless, i want to see if there are any best practices for an exploration/prototyping platform?i have a potential co-founder opportunity if anyone\\'s interested. i\\'ve worked with christian for over 2 years now and can attest to his judgment and ability. he\\'s recommending his friend (sam\\'s) soon-to-be-startup:```sam is launching a startup and looking for folks with deep data science/statistics backgrounds, to be a cofounder or an early employee. he\\'s building machine-learning tools that help companies make better people management decisions, starting with hiring. the venture is very new, but they\\'ve signed up 6 relatively large companies for pilots and have assembled rich workplace data on 50,000+ employees. they\\'ve got a few hundred $k in funding and are opening an sf-office this summer. if you\\'re interested, sam\\'s email is it\\'s simple: if you want to start a company and build a data product in this space, you should do it with sam. he\\'s everything you\\'d want in a co-founder, and also everything that you didn\\'t know you wanted but would if you knew the future. he\\'s really smart, super organised and hard working, and most importantly posseses an outstanding character--he will always have your back. the only reason i\\'m not joining him is that i already have something else going on. do yourself a favor and reach out to him if you or someone you know is interested in building a data product that could improve how companies hire.```i think that python 3.4+ has been supported since spark 1.4thanks everyone, this has been super helpful!thanks !postscript: i tried applying the updated `prophet` r package (v. 0.1.1) to my forecasting task and it no longer crashes. hahathat would be funny uploaded a file:  commented on ’s file : kevin take some more for me? thanks!videos? uploaded a file: here\\'s a short clip: what\\'s the conclusion/summary from this salary slide?my takeaway was that salary differences by gender for outgoing insight fellows is much better than the industry norm  uploaded a file: not to be a buzz kill but is that true? i was under the impression men/women mostly got paid the same for the same positions but just men got promoted sooneranyone in this group interested to form a team for this event? or if you know someone who might be interested? might be a long shot, but i think any phd who currently maintains student status (even defended) is still eligiblepm me if interested :slightly_smiling_face:::  \"note: the nba basketball analytics hackathon is open only to undergraduate and graduate students studying at accredited universities in the united states.”you can do it anthonyif you believedon’t dream it, be it~/giphy rhpssquare is hosting a data science meetup for anyone who interestedhey guys, there are various ml meetups in the bay area; i was thinking to start a ml meetup with insight fellows. we can meet sometime on the weekend at the insight office. let me know if there is interest!on ^, who wants one for nyc?anyone have experience with large-scale product merchandising? we have a client who manually curates groupings of similar or related products that would like to start automating the process i have some experience, not sure if applicable, but feel free to dm me with details so i can see if it’s relevantwould be interested!same!i’d be interested. the quality of meetups in nyc is very…mixed, so i think this would be an improvement.super! let’s collect names and set something up.agree with varun. there are so many meetups of very little utility. how can we ensure ours is unique + useful? … because _we’re_ uniquely awesome?personally, i’d be more likely to attend something that involved actually writing code or building a project over time to learn a new skillsi upvote that(heading into meeting, will respond shortly)like working in a group of 2-3 people to build some new prototype for a month, then sharing it with the group, then switching projects and switching partners:+1: :+1:how about open source issues?perhaps as one option, though in general that might be too narrow for me depending on the librarymore options the betteri have some issues i’ve been wanting to finish for statsmodels, maybe also a small contribution to mlxtend or sklearnwhat other sorts of projects would you want to work on?user embedding?potentially, though i’m not quite certain just yet as things will change soon i love the idea of building a project on a small team. count me in!looks like we’re gathering a crowd!tell your friends!interestedi would! but live in oakland.  if we switched between the  two offices that would be preferablesign me upi\\'m interested, but i am a simple data engineer. what would be focus of meetup? so there is a lot of cool stuff happening in the ml world right now, and basically all of it is freely accessible. i work on some stuff on my own, but i think it would be more productive and more fun to do it with peers :slightly_smiling_face:for example something that we could do is do the stanford deep learning nlp course that is available on youtube. but tonight i\\'ll make a new channel and we can discuss what people\\'s interests are and what they hope to get out of something like this. we\\'ll come up with something :slightly_smiling_face:we can also form a kaggle team :slightly_smiling_face:all of our conference rooms have glass walls to encourage transparency and openness--if those are important aspects of your corporate culture, i highly recommend them! also i love the gentle hum of the industrial hvac  :blush:does anyone out there know of a good e-mail dataset with labels other than spam/non-spam?if you work near campbell, i would recommend living in campbell or even better at los gatos. campbell should be more affordable, though.have you looked at the enron email dataset? not sure about labels thoughhey  i’m going to be working at velo3d (4 year old startup on 3d printing) as a ds starting in july :slightly_smiling_face:yes, so far that has been my best option. i would be interested to hear about the tech they use at velo3d. i did research involving metals 3d printing in grad school :slightly_smiling_face:they are in campbell ? then i\\'d def live in the south bay or peninsula, or even santa cruz:thumbsup: uploaded a file:  and commented: machine translation is hard.(how does dishonored 2 become rayman 2?????)hey all, i\\'ve got a question on classification (and possibly anomaly detection): i have five known classes with labeled objects. in the future i want to classify objects as they show up, but there is the possibility that something will be anomalous (in a way i can not predict now). such an anomalous thing should not be put in one of the five classes, but should instead get a \"weirdo!\" label. what are my options? any good readings on this?two different models might be a good option. one for classification, one for anomaly detection.that certainly seems reasonable!i’ve had a similar problem in the past, and that was my solutionit worked out fine as this wasn’t something that needed to be done in real timedo you have any weirdo’s in your current dataset?no :confused:stay positive alex_gudecan you simulate some?how complex is the data?i can actually tell you the whole problem. :wink: we\\'re looking (well another team here is) at network traffic between devices. classes are things like \"web server, dns server, phone, laptop, apple tv...\". basically, everything on the network is already know. if another laptop is added we\\'d like to be able to say \\'that network traffic looks a lot like a dev laptop!\\', and if something weird shows up we\\'d like to say \"weird new thing on the network!\"so simulating anomalous network traffic is outside my knowledge basei am making this up on the fly now but what would be the problem if you were to model 5 models. 1 for each class and if all five models are pointing towards not its own class maybe there is your weirdo?just make it really weird then. add 1 billion to everything:wink:\"anything that connects to port 1,000,000,022 is weird!\" :wink:hahawhen i did this previously, i used a one-class svm for the anomaly detection where all the example data was used for training. then i used a multiclass model for the actual classification. can’t remember that one...yeahthat seems like the right way to attack itit\\'s sort of inline with tolgao\\'s idea, except instead of training n classifiers, you train 1 that says \"anamolous\" or notit wasn’t something running on streaming data, so the added time from that two step process wasn’t a big dealyeahyeah, in theory our group hopes to run these classifiers on the sdn hardware... but they\\'re looking for ideas, so anything helps :wink:hello smart insight peeps.  i need to get up to speed on tableau quickly.  any suggestions?run away :slightly_smiling_face:seriously though, i never came across any great learning materials out there, what was most useful to me was to reproduce tables and viz that i had produced using other tools like sql+pandastableau has some nice tutorials nowthere is a feature where you can set up the data source and then look at the generated sql, i found that usefuldamienrj: got any links?looking for the one i usedi liked this one, got me up and running pretty quicklythanks. jonneff: what are you being asked to produce via tableau?not sure yet.  :grinning::grin: got called into boss\\'s office at 4 pm to ask if i could repurpose an existing tableau app and write some python to automate alerts.  saluted smartly and said yes sir!it\\'s going to be a poor man\\'s rule based expert system.  anomaly detection without the statistics. alerts without statistics sounds a little dangerous to me. we’ve had issues with some “dumb” alerts in datadog that were wrong often enough that people just ignore them now even though they may indicate an actual problem some fraction of the timestay positive andrewzirmit\\'s rules like \"chip card in chip card country being used as mag stripe = fraud\"fair enough, as long as you aren’t building the alert-that-cried-wolfi will say, jonneff, having lived in “chip card country”, sometimes you just gotta failover to the mag stripe. :slightly_smiling_face:i think that the problem will likely be that it is a real wolf but he doesn\\'t approach the pasture the same way next time.  :eyes: dat wolfdoes anybody have experience with nginx? i\\'m using it as an http proxy to make calls to a rate-limited api. the api service gave me two more keys, so i can now make three times the number of calls from three different servers. if anyone knows how to configure nginx to rate-limit _across_ concurrent http proxies (not individually), please dm me. thanks!fyi, this _may_ be what i was looking for. still playing with it, appreciate comments if anyone has done something similar. sure. i’ll give you an idea once i’m there. i did my phd on this (3d printing + ds) so i’m excited about it yes they are in campbell. thanks for the advicefun post from yhat:  dear all.  happy hour tomorrow thurs, starting 530p @ b-restaurant and bar in sf.less data more drinking!2016a-sv is hostinghello! has anyone had experience opening outlook email .msg files in python? i\\'m having trouble figuring the best way to do it. thanks!!!(   )thanks!!has anyone used databricks? opinions?it\\'s sweet: i have and use it quite extensively at work.  its a great and easy way to launch and maintain spark clusters for various data pipline tasks and analysis.  the notebooks are very similar to zepellin and work well with different languages.  they have a pretty complete and free community version that you can use to test it out.i took a spark class last month that used it. it’s pretty nice for handling a lot of the background configuration required by spark auto-magically in the background. i get the sense though that if you need to do something with a special or unusual configuration, it might not be so nice. that is wild conjecture though—it could work just fine.also it’s written in scala, so things tend to work in that language a little better and a little earlier than they do in python. i didn’t have any problems using the python bindings though.awesome. thanks for the input! i\\'m playing with the community version now and like it so far. i\\'d like to play with sparkling h2o also.&gt; i get the sense though that if you need to do something with a special or unusual configuration, it might not be so nice.can affirm ’s hunch. we have a some pretty specific requirements that we struggle with on databricks… but i will say, their support is really pretty responsive. they have shown us some backdoors that are useful. i would also second the use of scala over python (we use python almost exclusively) — there are things we have done in scala because it’s possible and not  possible in python. but, all in all, most days, we are thankful we have db managing most of the backend magic for us.  : reddit popped up in my seo demo. would\\'ve been funny if that happened a day earlier during the actual demo there. can you say \\'awkward\\'? :flushed:hey all, have to move out of my current place on july 16.  if you know of any available shorterm furnished rental opportunities please let me know!  thanks these are just my thouhgts so far.. you suggested s3 -&gt; spark -&gt; redshift.  given your goals this makes sense to me but i\\'d want to understand more what do you plan to do with spark?  if you are planning to work mostly with tabular/structured data (csv one option your engineer mentioned) then it seems to me that all you need is s3 -&gt; processing daily in python/pandas on a relatively high ram machine -&gt; redshift.  spark will be needed if you want to build ml parallel models using mllib, and/or if your data is coming less structured and you want to do some parallel processing of it... otherwise it may be adding more infrastructure w/o need for it... on the other side redshift can be considerably more expensive than spark and if you are planning to grow to 100 mm rows per day you\\'ll need some relatively large redshift cluster... if i were you i\\'d try some combination of this, starting with the simplest s3 -&gt; pandas -&gt; redshift and based on that decide... for that i have used this with no problem...  good point about spark possibly being overkill.  i can definitely write the code first with pandas, then switch to pyspark later if need be.  thanks!does anyone out her have a favorite weather information api? i just need basic stuff like temperature range, humidity, air pressure, precipitation (basic local news weather report stuff). it would be really great if their coverage also included more rural areas, and not just urban centers.i’m also thinking of heading in this direction for a project, so i’m interested to hear what you have found so far!noaa has an api.  if you’re still working where i think you’re working, then we have a similar use casesee for example. i use the `requests` library to go hunting and parsing for information; i’ll leave the rest to you(ps you’ll need an api key from them)if you are going from audio files, bilstms have been state of the art from my understanding (a paper example here ). you can basically cluster on their hidden states once they are trainedi recently joined a top-vc backed ai &amp; cloud services early stage start-up in palo alto. it is in stealth mode so i cannot share so much, but it is an amazing opportunity to work with people who have had several successful exits in the last decade. basically they know what they are doing. and needless to say, the stock option is going to be pretty good. we are looking for senior backend data platform engineers in java / scala to build data stores, pipelines, micro-services, data orchestration, communication, distributed systems management, availability and scalability.  if you are interested, dm me.and machine learning engineers and data scientists, as well.nice. do you know if the noaa api  has wether information for just the us or all over the world? in either case, i\\'ll look into it! thanks, !great question. it is a national service…haven’t tried internationallyi\\'ll let you know...looks like it does... which is nice. did some of this for his insight project.first to chime in on the jira discussion --- (1) it can integrate with github and atlassian and for that it is nice that i can write a spec doc and link my tickets. (2) it is not really designed for data science, but i find that it can be helpful to find info on things my colleagues are doing that i need by reading their descriptions and comments --- when they fill that in --- so please do that. (3) i am in the middle of my first \"self-assessment\" as part of our annual assessment and i found it really helpful to look at all my tickets since i started (next year since my last review).  it took some help from an se with more experience to know how to get that, but now that i know, it is pretty cool.   i don\\'t have experience with other systems outside of using trello in insight for personal stuff.second: i have googled and googled and googled and i need to sleep, so i need to ask for help and i thought i would start here.  i have found lots of pages that note that using string formatting for sql queries with sqlalchemy (and more generally) poses significant security risks, but what i can\\'t find is how to properly identify the parameter in the string and in the sqlalchemy call.  what is most important here is that i\\'m using mysql.everything i\\'ve tried either throws and error, or returns an empty dataframe and i know that the query itself is fundamentally sound, it is simply formatting the parameters correctly that is the problem. uploaded a file:  and commented: here is a super simple example.  assume i have already successfully set up the engine.  the question is how to correctly indicate the parameter in the query string and now to correctly enter it in `read_sql`.  i\\'ve read the docs, so know that tuple, list or dict are options, and i\\'ve had the same results with `{person}` and`@person` and it has to be a basic formatting issue --- everything suggests this.  but none of what i\\'ve found is mysql specific.try `%(perons)s`i think the interface is fairly standard and i know that works for the mssql connector (used it literally minutes ago).hi fellow fellows!i\\'m looking to hire 1 (maybe 2!) data scientists for my team at coach, inc in nyc.i\\'m a director in the customer intelligence and data labs group. we\\'re focused on customer-facing analytics and data science to grow our multi-brand (coach, stuart weitzman, and maybe more soon), global fashion retail business.there is a great team in place right now with 4 data engineers and 6 people focused on analytics and some basic data science. it\\'s still a pretty green group as far as machine learning goes, but everyone is very excited to dive in and learn. they have really invested in strong, clean internal data and coach itself is a very numbers driven organization. i report to an amazing vp (best boss i\\'ve ever had) who reports to the ceo. we\\'re well positioned to get some really cool stuff done, it\\'s just too much work to get done myself. if anyone is interested, i\\'d be happy to share an official job description, answer questions, meet for coffee, talk about our employee discount, etc.stahpkeep it classy bartwatching a spark tutorial and wondering why the instructor keeps referring to some package called \"skippy.\" later realize she\\'s talking about scipy... :open_mouth:hi,does anyone have any experience with armax models?i do  anything i can help you with?hi all, i’m wondering if i can pick the brain of a data engineer who has experience with in-memory databases (redis, memcached, etc.). i have a few questions regarding why you would choose one service over the other. this looks like a good read for comparing redis and memcache . i\\'ve only worked with redis in the past and it\\'s super easy to get started. we have had issues at insight with security in that the default port (6379) on redis has been vulnerable to getting hacked  i\\'d be interested in learning about your experience with armax models! i created a #time-series-model channel if you don\\'t mind having a conversation therethanks austin, i’m hoping to take things in the redis direction, but i have a few co-workers who want the decision made with a bit of evidence, this is perfect, thanks!what if we\\'re not trying to convince people that diversity is good but trying to send the message that we don\\'t tolerate supremacist ideology? because i think you might be mistaking the point of these reactions.ok i read everything before weighing in.  1. biology is not destiny.2. if we devalue empathy, why should i give a rat\\'s a$$ that a conservative feels stifled?3. google makes tons of money.  affirmative action isn\\'t hurting their business.  4. another clueless male with a high paying job complains about being mistreated by programs that help people who have been held back by centuries of prejudice.  ps i predict appearances on fox news, a book deal and a job offer from trump\\'s equal opportunity office. i don\\'t feel sorry for him.  great resource for redshift tuning my team has implemented some hierarchical classifiers in the healthcare setting (classifying onto a medical ontology that is hierarchical).in a tree-structure, we have implemented hierarchical classifiers, where each internal node has a classifier for its children, and the classification is done by starting at the root, and traversing down via the node-wise classifiers, until a leaf-node has been established. we found that this is usually a bit better than a flat classifier on the leaf nodes.we are also looking into some alternatives. let me know if you find some sweet ways of doing this.hi leifur, thanks for your msg! do you mind my asking how did you train each classifier at each internal node?individually or you can somehow optimize the entire tree of classifiers all-together?any bostonians in this channel interested in presenting their insight project to a broader audience at pydata boston meetup? i\\'m in contact with one of the organizers, who says there\\'s a slot for a bunch of alumni/a/ae/arghcannotevergetitright to present at the end of august. are you after international companies?no just the usa.. the rest of the world has paid leave.:sad_parrot: :burn:thank you for doing this! also this is depressing :sadpanda:hi all! does anyone have insight on advantages/disadvantages between chartio and mode analytics as bi tools? any specific reasons you would choose one over the other?would also love to know if anyone has used aws quicksight and has thoughts on it. thanks!hi all, has anyone done basic calculations on geospatial data, let\\'s say i have a table with lat, lon and a float (i.e., phone signal), and then i have a shapefile with boundaries, and i\\'d want to know the average(float) for each polygon in the shape file... the table with lat, lon could be billions of records (so could be quiet large)... i considered postgis as an option but if my table is too big it\\'s not the ideal dbcan’t edit notifications away  :stuck_out_tongue:loli later remember the non //here policylolhow fast do you anticipate your table to be growing?not that fast... the table currently has 1 bn+ recordsat most 100 mm / moor something like thatif it’s for exploring/prototyping, i’d suggest giving postgis a try firstfrom what i’ve heard, it’s usually not the number of records but the total size of your table that slows postgis downsince you only have point data, it should be able to handle lots of objectsif postgis, doesn’t work, you can try geomesa for which you can choose various backends (e.g. cassandra, accumulo, hbase, etc)but of course, it’ll be a lot more overhead to maintain itmmm nicelet me give this tools a tryof course, if you have lots of unused gpus lying around, you can give mapd a try :slightly_smiling_face:(they have since made mapd open source)my team uses chartio, but now we’re looking to host our own bi tool\"current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong ai. to achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. to demonstrate the essential role of such models, i will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.\" - judea pearl ()thanks for your response ! any specific reasons you weren\\'t satisfied with chartio?because we want to reduce an extra step in the data pipeline where chartio needs to copy over data into it’s own data store, and also for cost savings.  we found open source tools to be pretty feature complete as it is.i work for the city of ny and we get zero paid leave. in fact, after cuomo\\'s latest paid leave law went into effect in jan, unionized city employees are the only workers in the state, private or public, without paid leave. oh and i just had a baby, used all my vacation and sick days and am already back at work 6 weeks later. yea i it writes to cassandra locally. could it have to do with maven?hello everyone - i\\'m currently executing a pyspark batch job on a cluster. my data transformations are a series of map, flatmap, and reducebykey operations. it seems like my driver is the executor for every single task, and the reducebykey operation is taking a long time. i know that reducebykey is a wide task and will require shuffling, but i\\'m confused as to why the driver seems to be doing everything. any thoughts/gotchas i should look into? make sure you have executors alive on the spark master ui and when you spark submit be sure to set the spark master flag to the master ip:7077. it’s possible your job is executing in local modethanks! didn\\'t set the --master flag, so i was running this on local modenice. glad it worked! hope your week 3 goes great!i’ve checked out lifelines but it still seems to require the definition of a “dead” user before the modeling phase. but i have not seen the pareto/nbd paper. will check it out!this is such a perfect depiction of how i think we all feel in seattle right now!!oh actually, i have checked out the pareto/nbd models too. can’t remember why i thought they wouldn’t work now though… i’ll have to revisiti have not, but will take a look. thanks ayseni, for one, welcome our new algorithmic overlordsi’m surprised nuggetbot didn’t have anything to say about that, carloswhy would nuggetbot have anything to say about algorithmic overlo- oh my godhi all, some good friends of mine at ucsd are beginning a funding campaign for their non-profit startup that is focused on bringing the online education revolution to offline communities. they\\'ve been at it for a few years now and made huge strides (near the beginning: ). this is their first public fundraiser. they would love to get some donations before monday (when they will make a more public push) so those that come to the site will see funds already being donated. if you are at all interested in bringing the education revolution to everyone and not just to those with internet, please consider donating. also, if your company does matching contributions, that would be awesome for them. thanks! : well, it was nice having a job while it lasted. :wink:\"stay positive alex_gude\"(where is  when you need it?)nuggetbot helpnuggetbot adapter - reply with the adapternuggetbot animate me &lt;query&gt; - the same thing as `image me`, except adds a few parameters to try to return an animated gif instead.nuggetbot echo &lt;text&gt; - reply back with &lt;text&gt;nuggetbot help - displays all of the help commands that nuggetbot knows about.nuggetbot help &lt;query&gt; - displays all help commands that match &lt;query&gt;.nuggetbot image me &lt;query&gt; - the original. queries google images for &lt;query&gt; and returns a random top result.nuggetbot map me &lt;query&gt; - returns a map view of the area returned by `query`.nuggetbot mustache me &lt;query&gt; - searches google images for the specified query and mustaches it.nuggetbot mustache me &lt;url&gt; - adds a mustache to the specified url.nuggetbot ping - reply with pongnuggetbot pug bomb n - get n pugsnuggetbot pug me - receive a pugnuggetbot the rules - make sure nuggetbot still knows the rules.nuggetbot the rules - make sure nuggetbot still knows the rules.nuggetbot time - reply with current timenuggetbot translate me &lt;phrase&gt; - searches for a translation for the &lt;phrase&gt; and then prints that bad boy out.nuggetbot translate me from &lt;source&gt; into &lt;target&gt; &lt;phrase&gt; - translates &lt;phrase&gt; from &lt;source&gt; into &lt;target&gt;. both &lt;source&gt; and &lt;target&gt; are optionalnuggetbot weather - get the weather for nuggetbot_dark_sky_default_locationnuggetbot weather &lt;location&gt; - get the weather for &lt;location&gt;nuggetbot youtube me &lt;query&gt; - searches youtube for the query and returns the video embed link.victory - display an victorious imagenuggetbot animate me ninjas fighting(i’ll accept it)nuggetbot tell me rules of the internetfyi we have a channel dedicated to nuggetbot  *raises hand*but ask  i think he\\'s taught a class on itare you trying to track a fixed value or will your set point be moving around a lot?yep i have! not an expert but i\\'ve mentored students to run pid controllers on roombas that get discrete location data my strategy was to hold off mentioning this until the end, otherwise you risk getting cut by an hr person who doesn’t really know how easy it is. getting a tn visa is easy easy easy and your company will hire a lawyer deal with it. i gave the guy the letter from the lawyer, talked to him for like 5 minutes and i was in. lol, i keep telling people to use it but no one listens to me.everyone wants to do some optimal control bs.    i’ve also heard some stories of major delays with tn at the border, so just be sure to do your due diligence on your specific situation :slightly_smiling_face: the set point will be moving around a lot.  i’m just looking for something simple, better than the baseline of nothingmoving around a lot compared to how long it takes you to compute the control signal?if the computation time is orders of magnitude faster than your dynamics you should still be finemodulo some nonlinear thingscomputation time is much fasterdo you know of any frameworks that would fit this problem?what problem?  pid is a pretty general control algorithmthe problem for most engineers is gain-tuningi.e. picking the parameters of your controllerthere are some automatic algorithms, but most people can get away with heuristicsthe algorithms won\\'t do much better anywayone of the big advantages of pid over more sophisticated methods is that you can tune the parameters online and by handi can pm youre: tn, definitely give yourself lots of time for the application. i got pulled into secondary screening at yyz and had to run to the gate to avoid missing my flight. never had any issues once i actually had the tn stamp though. on lawyers’ advice i’ve brought my original diplomas with me every time i cross the border but i’ve never actually been asked to show them. was also given the tip that they usually have more experienced immigration officers on duty during regular business hours (9-5 m-f) and you’re less likely to run into issues with them, so book your flight to the us then if you canstay positive bradit’s hard sometimes, re: tn, don\\'t snooze on it, there may not be a nafta agreement in the near future that\\'s a happy thought :disappointed:  .. though if that were the case, it would still buy 6 months (jul 20) at the very least, if it were revoked on day 1 of the trumpian era... so more than just 6 months...demandbase is hiring a data scientist and data engineer! our triumphs in machine learning end up directly affecting the quality of our product, which has been super exciting (demandbase is one of the pioneers of account-based marketing). let me know if you\\'re interested, or if you know anyone else who would be! graphs, python, machine learning, spark, aws, etc.has anyone used open street map data before for routing and encountered issues where the shortest_path function gives really poor results?does anyone know if the data is bad enough that roads in manhattan could end up being disconnected?stay positive mattkrems:  had a similar problem with his project.: my issue was that searching for the nearest node to an address would often return something like a node on a foot path, which didn\\'t seem to be actually connected to the street grid... i \"solved\" this by instead querying for the nearest intersection node (node with more than one associated way) and using that as the origin/dest of the path search. not sure if you\\'re facing the same problem, but hope this helps!if anyone is using natural-language processing for their projects (or alums that still work with it), just made a  channel for sharing ideasanybody have experience with/currently working on  clustering (hierarchical or otherwise) that involves non-numeric data (ie string-delimited categories)???? can you give an example of the kind of data you’re interested in?what is the usual procedure to standardize images of different sizes and shapes to the same size and shape, if i want to feed the pixels into svm etc. as features?  resize the images and pad using white pixels?hi , feels to me will be best to go into the fourier domain and then revert back with fourier interpolation onto the plane region you aim to cover. personally do not like the artificial white background you\\'ll be adding. note i have no idea how an svm will respond to your patching idea.: i’m not sure what the usual procedure would be, but photoshop has some pretty good options for batch resizing. i’m sure you can specify things like “keep aspect ratio” and fill background with white (whatever tasks can be done by clicking in ps can also be automated with a script)  i\\'m trying to cluster data that involves fec filings/ super pac expenditures - some of the data is numeric, but i also want to factor in expenditure type, candidate, ect... i agree that the padded white background is not ideal... i think i get the rough idea of the fourier transform idea; do you have a good reference or good package to perform that on images?   i have the images as numpy arrays so padding should be relatively straightforward, i hope: one good way to deal with categorical feature is the \"one hot\" encoding scheme which transforms one discrete feature into several binary features. sklearn has a function that does that: : pandas has a feature pd.get_dummies() which is more straight forward than sklearn.preprocessing.  if it’s really sparse, you might want to consider using sparse matrices: of the top of my head all ones does is ft, zero padding in the frequency domain to satisfy your size requirements and then back to real space. you could use numpy to achieve all this but  the  suggestion may save you some time.  as to references      best bet is to google for fourier interpolation of @d signals and similar2d signals i meantdear all, does anyone has some publication data with bibliography information which could be used to calculated h-index? many thanksbtw, tangentially related, careful when doing interpolation in the frequency domain (if you ever need to do so).  fourier amplitudes decay fast with frequency thus posing challenges to most familiar interpolation techniques. here a link with a nice comparison (only sending the conclusions page):   thanks!: what field are you looking at?  has loads of particle physics and astrophysics data, and an api to boot(and counts citations and computes h-indices of authors)many thx, dstone.  any field would work. i\\'ll check it out.is there a channel for posting job listings?my parent company is looking to start a new data team and want to hire an ‘experienced’ data scientist to lead itis that a job with the tender team? i would love to work for tender.classic example of \"too much hackery\" in tableau: my colleague wanted to show zero rather than nothing in a table of numbers when there is no data. instead of a friendly formatting option \"show null as zero\" or similar the standard prescription () is a new calculated field with zn(lookup(sum([sales]),0)).for tableau fanatics this is second nature \"sure, that\\'s easy, just create the following three new auxiliary calculated fields...\" but for most this cuts into the easy-for-anyone aspect of the product. my colleague who is a very smart, hardworking guy just said \"uh... no\" and now hates the product a little bit more.oh, also i forgot he also went and looked them up and asked, \"so, how is it that tableau is $6.5b company?\" my response: we just paid for an enterprise license.thanks ddrum001 and johnjoo ! (and andrewmo for posting)got my last job from someone i met there. :grinning:also i remember a session on hidden markov models that was pretty good.  of the general data science conferences i’ve been to, odsc was the best. that was odsc west two years ago. sounds like either data architecting (higher level) or data modeling (more detailed).  for architecting try covert, \"how to make sense of any mess.\"  for schemas try data modeling section of stanford\\'s free database course on lagunita. my friend justus, is coming to palo alto on aug 23. he and i will rent a place together starting sep 1st. between aug 23 and sep 1 he will need a place to stay. anyone has a room to rent out on such short notice between aug 23 and sep 1? or a couch to surf? thanks!anybody in the bay area with experience and interest in building recommendation engines in production, hit me up. there\\'s a role opening up on vevo\\'s (music videos) personalization and recommendation team for a senior ml engineer. hi all, i\\'m looking to make a transition from my current position, so if anyone knows of any opportunities for an experienced data scientist/machine learning engineer at your company in the nyc area, feel free to pm me. thanks.a thing i’m working is is identifying events in remote places as they happen. anyone here do super hyper targeting of individuals and events? i’m wondering what sources of info you might use outside of twitter/fb/youtube/google trends? - i’m looking to hire a spark/scala guru for part-time consulting, needed now through the next week or so at 10-20 hours/week, with opportunity for ongoing work at fewer hours. we’re in nyc but remote is fine. pm for more info such as rate and scope. thanks! have you looked at gdelt events data?this looks very useful, thanks !: acled is another event dataset you might like thanks ! , 80gb doesn’t sound that big. can you just put it in  big aws machine? they have instances that have 2 tb now  thank you, yeah, i agree, i do not think 80g should be handled by spark.  the matter would be, the manipulation in spark. do you know any good source to study this?why are you trying to manipulate with spark?if you’re trying to prototype, you can either run spark locally or use something like pysparkling, if you’re using spark with pythonnot in the spark, maybe, but in my python code, there is matrix manipulation. the problem is , i think , how do i load my matrix  from my data.i’m saying that you could just load up an 80gb dataset in a big ec2 instance and use python instead of having to deal with sparkdomino has all this set up for you. pm me if you want to try it out as john suggested in the past i’ve just used a notebook environment on an ec2 instance for stuff like that, i forget what such an instance would cost to know if it’s in your budget, but it’s probably the most straight-forward way to work with that data in python. i have my data in our system , cannot take it out side of the system. but i can work interactively. i may want to try what you said.  by the way, what is the different between running python script and running interactively?if you mean script vs notebook, the only differences are what you want to do - i didn’t read your initial goals, just the memory aspect.  i was just saying in the past when i wanted to do exploratory stuff in a notebook with a similar data set size i was able to do it on a notebook environment set up on an ec2 instance with lots of memory.  an alternative is to see if you can come up with an algorithm that allows yo uto somehow do your operation iteratively, not sure if this is possible, but if it is you can then maybe stream the data bits at a time.  thanks harry, “if it is you can then maybe stream the data bits at a time.”  i am now thinking if i can make this happen, do you think parallel  calculation would work?it depends what you want to do - for basic linear algebra computations like matrix-matrix multiplication you certainly don’t need the full data in memory all the time so if you can figure out how to partially load the parts you need, do the relevant parts of the computation, dump them out of memory, and load other parts, then you can get that to work.  i would bet someone has already written stuff like this evenif you’re using python, you can use daskat some point it becomes worth it to use spark.: thank you, what is the \"dask\"? dask:   thanks!!  a team at ebay needs to hire data science engineers in nyc.  it\\'s not my team but i work with them.hey all, i\\'ve got my first post up on the lab41 website, would love if you\\'d take a look! it\\'s about the datasets we\\'re using in our recommender challenge.  disclosure: i have an editor writing the headlines, and he is a bit enamored of buzzfeed :wink:hey all, if you are interested, here are the sentiment results from the super bowl last night for every 5 minutes of game timei am still working on mapping the plays to utc time and found a better avenue using the coaches film for each playi\\'ll confess mickey: based on the words being used by the panthers fans in my living room, i think your sentiment might be \"overly positive\". :wink:haha you need to tell them to get on twitter more!the most likely outcome of that would have likely been phones thrown to the floor, so perhaps it is best that they were not.: where is the positive spike for puppymonkeybaby?one of the best salary negotiation posts i\\'ve ever read, fwiw: twitter news:  +1 for the floodinghi jeremy - if you are still looking for some indian rupee change, give me a call or e-mail.  i am not as active in slack, so bit slow to respondhas anyone worked on spatio temporal models for irregularly space data (i.e., wind turbines in a farm field located in random positions, some meters away, some kilometers away to the closest one).  .. on a more general topic, anyone working on spatiotemporal models?oooops keeps forgetting not to put the at here sorry everyoneandrew ng’ s new book---machine learning yearning - chapters 1-14 —  a colleague combined krieging and facebook’s prophet modelhi all, i\\'m a bit confused with the newer spark api (dataframes and datasets)... does it still make sense to use mapreduce or is it better to let spark handle the operations under the hood? thanks! good hint... makes sense, sounds like a good approach, would you have some more details on the models used/approach, etc.sure, she wrote a blog post. (incidentally she is also an insight alum) nice article!  love the classic \"stay tuned for part two, coming... never!\"lol, indeed. the work is done, but she just needs to write it up :slightly_smiling_face:wow nice!thanks for sharingyou bet!hey program directors - i’d like to see if its feasible to drum up a consulting project relationship between insight and the smart-electric business unit at mercedes. they have a pretty rich data set which i’ve been working on as a side project and it seems like an fruitful place for fellows to get some experience with a consulting project. might also lead to job opportunities for a associate to mid-range data scientist position. ping me if there’s some interest and we can set up a meeting.so cool!nice! this should be interesting.thanks for mentioning it,  could probably help arrange thiswhich language are you using? if  python, you probably want to stick with the dataframe api as much as possible. that’s because you will get near-native (scala) performance when using the dataframe api with built-in functions.also, for machine learning you’ll want to be using dataframes, since  will only support the dataframe api going forwardhi everyone! i\\'ll be a speaker at odsc east in boston in a couple of weeks, if you are attending, come and say hi! excited to meet other current and former fellows! is anyone familiar with apache hive? i’m trying to run a cross join with a temporary table, but having some errors.my company was just acquired and our new corporate overlords are not doing a great job of assuaging the doubt and uncertainty that comes with. it doesn’t help that there is a cultural divide (it turns out the job market and job culture are very different between a corporation in minnetonka and a start-up in boston) and they are taking away many of our benefits. has anyone dealt with this? any advice for dealing with the stress/knowing when to jump ship/anything else?i was at a publicis-owned ad agency when it merged with another publicis-owned agency. it created a fairly long-feeling/anxiety-producing period of about 6 months where everything felt stalled.all goal setting at the company level had to be re-evaluated and took ages, people expecting promotions were basically told, yes it\\'s still going through but wait, keep waiting, then eventually they came through, which was great, but it took a while. several higher-ups left as they didn\\'t have a clear place in the new org.recruiters usually pounce on people when they hear the news as wellafter that 6 months, things were pretty much back to normal in a cultural sense though. my girlfriend also went through something similar with her company having had a majority stake go to a pe firm this year.we also both jumped ship at least in part due to the inertia / uncertainty that it causedthanks . this is really useful.somewhat -- at least to the extent that spark supports hive queries.i think the general wisdom is to not run cross joins without any join conditions, but i\\'m aware that may not be helpful :slightly_smiling_face: i feel ya. i just went through/living through that. my company was a child company and hq decided last year to fully integrate. imo it was/is a poorly planned process. many left, i have visa issues  that made the jumping ship harder. i am on month 8-10 and things haven’t been back to normal, but on the positive i have started to do ds which was something i was unable to do consistently when i was part of my original company. to be honest the uncertainty kills, and oftentimes i’ve just wandered figuring it out how to advance my career within the chaos. also feel that this was not helpful :stuck_out_tongue:probably best to look around in the job market and leverage it to keep your benefits? most of the times in these takeovers, the most valuable thing is the employees, not the product.not sure how to deal with the uncertainty though.i\\'ll send you a dm.yeah, cross joins are to be avoided unless absolutely necessary.one advice is to have all selects and where clauses done before and then use the cross join on the resulting tables. if you have those conditions after the cross join, the cross join is done first and then the result is filtered.that shouldn’t matter. predicate pushdown should actually do the right thing regardless.i’ve actually seen worse performance trying to optimize predicates in ctes or subqueriesthough it’s been a while since i’ve used hivethank you so much, will definitely experiment with ithello insight community! looking for a great way to give back to stem education? check out letters to a pre-scientist, a pen-pal program connecting professional scientists with middle school students in high needs areas! this program has officially obtained fiscal sponsorship by a 501(c)(3) organization and is pursuing their first ever fundraiser. check it out and spread the word: this is useful - i’m in collecting data phase right now (because i have to be a scientist about this) but hearing different experiences just normalizes everything i’m feeling and seeing. check-ins with me from 10:30-12 tomorrow. reply to this thread if you want to chat - i can plan to see 9 peoplelol here we gooh no i know what\\'s gonna happen now:popcorn:ahahahahah no, i\\'m good.woooooo!  real career advice time!clean your desks everybody :)i\\'m down to chat!:oncoming_police_car: @-channel alert :oncoming_police_car::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice::nooice:ruuuutttttthhhhhhhh:bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::bunching::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::the-fu-man-cometh::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::animated-alex::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji::did-someone-ask-for-an-emoji:2016a has still got the only emoji game in towni’ve been honing my skills for 18 monthspoor faye 🤣twitch had a  guy-fieri-themed @-channel incident a few months back. it was carnage for 16 hours straight.the frosted tips jokes alone must have lasted for weeks...my mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the  mom bought me this new laptop and it gets really hot when the chat is being spamed. now my leg is starting to hurt because it is getting so hot. please, if you don’t want me to get burned, then dont spam the chat. uploaded a file: ohmygod, i am so sorry. this happens every single time i want to @ channel our session from my phone. am mortified. [goes to corner and cries]nuggetbot pug bomb 10did not know pug bomb was a thing; i support the pug bombmixpanel, amplitude, and heap do a half-decent job of ingesting events data and letting you do basic analysis on top.  however, you always end up running into their limits and wishing you had more flexibility and direct access to your data.segment is good to collect events from your site and it can get them into redshift for you.  or you can use their webhook to ingest it yourself using kafka or kinesis.if you want data from multiple sources (events, apis, other databases) there are companies like treasure data, fivetran, astronaut, or alooma.  but they are just piping this data in and tossing it into your redshift cluster, you get to deal with creating a system to update derivative tables.(this is where the pitch starts :slightly_smiling_face: ).  i work for a start-up that’ll get all of your data from these disparate data sources, put it into s3 and redshift (soon spark, maybe hive), and make it easy to create derivative tables that are automatically updated, hook up jupyterhub or a sql dashboarding tool (metabase), or any tool that can hook up to redshift.  and! pipe it back out again to another database or apis.hopefully some of this is useful.  i got involved with this start-up because i was tired of doing all these things instead of actually working with the data.  i wanted to build this tool so that i would never have to deal with it again.nuggetbot animate me  oops. i had trouble searching \\'@ruthtoner \\'. try later.as luke was told to go to dagobah, so you shall be told to go to nuggetbot animate me this is finegood bot invitae bought good start genetics (boston-based) in august and we\\'re learning a lot about how to do things better next time.  - reduce uncertainty by making sure everyone has full-time offers (except the people that you need to cut right away)- benefits etc... should be clearly communicated possibly honored the old style through the year and then transitioned to the new company- people are needed to maintain existing systems and the company should be willing to pay to keep people around.  if you lose too many key people then the point of the acquisition could be meaningless.- going from a small company to a big company is hard.  people won\\'t have full insight into all the products and that\\'s just the reality because with a bigger company things are more complicated.  that\\'s hard for people to get used to.- speaking as someone from the other side, if you can see they are making an effort (and you believe in the mission of the company), i would try it out for 6 months.  it\\'s not too hard to find a new job. - you can also use this as a time to demonstrate value to the new company by stepping up to help with the integration and make yourself more valuable, if you believe in their mission.   hope that helpsdoes it need to be in person? can we do it by phone or skype? :laughing: :faye:thanks guys! cross join was running fine in athena, but it\\'s giving me error on hive. also i am trying to cross join a temporary table. still don\\'t know the exact reason., i\\'m flattered that you want to check in with me to see how i\\'m doing, but i usually reserve that for partners that i have more investment in (maybe after the third or fourth date?)i wish you the best though!aren\\'t there close to a thousand fellows on this channel? i think you might have your work cut out for you if you\\'re only doing 9 at a time.hey all, you using data science for social good? come share it at bloomberg\\'s data science for social good exchange.  abstracts due aug 1st! looks like \"the data science handbook\" is finally out. preface by , featuring alumni chris moody and diane wu:wow very coolit’s all about topo chico.hi all, just curious, what are the policies regarding database access for you? my company decided to hand us 90 days database credentials that are ugly and random just wondering if this is the norm for data teams. :slightly_smiling_face: thanks!/giphy topogigiowait… you only get access to your data for 90 days?if so, that’s kinda… broken.: does that just mean that each 90 days you get a new set of credentials?yeah :disappointed:every 90 days i have to request a new set of credsit’s self service which is not that bad except i have to update all my creds for python and other sql stuffstay positive kenny_kwanwant to know the standard for people like usthat sounds annoying, but not unworkable.:stuck_out_tongue:: i just started a free trial of the slack “standard plan” that runs to october 1st. so, enjoy the unlimited message history and other perks of standard while it lasts!: create a file called `.credentials` in your home directory and store all of your credentials in there as bash export commands, yaml/json-encoded python dict or whatever. add a line to your ipython startup script and to your .bash_profile that reads this file and loads its contents into memory as either a python dict (in ipython) or environment variables (in bash). now your scripts will always work as long as you have a valid `.credentials` file, on any computer you work with. thanks! i am already doing so i don’t have to copy them around, but was interested in knowing more about what the community faces. another point that i have thought is that probably in startups these things are more lax, whereas in my case where all our policies are set by the parent company, then things have policies and all that fun stuff...: that\\'s great! while we have it, could you also perhaps setup archiving at ? that way we can continue to have full history even after the trial runs outon second thoughts... that archive is public :confused:: yeah, i was just looking at that. do you know if they have a private version?: 90 days is better than 60 days :slightly_smiling_face:there’s this: good documentary, i watched it this weekend. made algorithms fun and accessible. upcoming data eng meetup at netflix on wednesday april 27 at 6pm: : is the meetup recorded?i do not believe it is: confirmed, it will not be recordedsad.. okay thanks a lotwow this has been there since 2009.  interesting algorithmwow what a break through with deep learning… i read about itinterestingi’ve built a couple slack reporting apps using r and python. they report some various metrics on a daily or weekly basis. the upfront time investment is minimal and it’s a very easy way to keep interested people at my company informed.`corgi_gif_generator %&gt;% slack_spammer `stay positive mpanciathe american statistician association weighs in on p-values and hypothesis testing misuse:they make 6 statements. summary:1. p-values can indicate how incompatible the data are with a specified statistical model.2. p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.3. scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.4. proper inference requires full reporting and transparency (e.g. p-values and related analyses should not be reported selectively).5. a p-value, or statistical significance, does not measure the size of an effect or the importance of a result.6. by itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.*other approaches*in view of the prevalent misuses of and misconceptions concerning p-values, some statisticians prefer to supplement or even replace p-values with other approaches. these include methods that emphasize estimation over testing, such as confidence, credibility, or prediction intervals; bayesian methods; alternative measures of evidence, such as likelihood ratios or bayes factors; and other approaches such as decision-theoretic modeling and false discovery rates. all these measures and approaches rely on further assumptions, but they may more directly address the size of an effect (and its associated uncertainty) or whether the hypothesis is correct.\"no single index should substitute for scientific reasoning.\"thank you for the summary,  !there was a reporting article on nature:we need to have a go board at the office, : i have an extra one! i\\'ll bring it to the office on friday and leave it there!alphago and lee sedol are playing game 2!hey, if any of you are attending the nips conference in montreal and want to hang out, let me know!current ai fellow here. anyone interviewed and / or interviews at netflix? i have a onsite next week (and it\\'s the last opportunity in my pipeline). looking to get as much help as i can get!  :pray:role: analytics engineer for original content promotion, topics: product metrics, a/b testing (stats). thanks in advance! (please dm me if you or you know someone else that can help). i\\'m very very new to them, so just getting started was throwing me. i found some helpful beginner videos though and am feeling much more confident about them now. just finished week 1 of this fellowship, so i\\'m sure i\\'ll be back here freaking out again soonnot now nuggetbotformer fellow (16a boston) tamas madl launched a website tonight for the debate that will be checking and plotting hillary clinton\\'s and donald trump\\'s heart rate in real time.  tamas’ insight project involved being able to detect an individuals heart rate using only a webcam, based on the subtle fluctuation of skin tone on an individual’s forehead. the url is awesome! i look forward to watching it with that in the background!^in week 4 i was excited for demos and terrified of not getting an interview while simultaneously being terrified of getting an interview (because i was incredibly intimidated by the interview process). btw, i got a great freakin job as a ds at samba tv in sf wholly because of insight.also, how is there not a coconut emoji?! i think the first demo (and the leadup to it) was the hardest just because it was such a shift from the project phase coming out of week 4 (and because i was still tinkering with my presentation right up to the wire, so i didn’t get to practice it much :stuck_out_tongue: )though it’s definitely a breath of relief when the first callback comes…joyceschan: schema registry is great. we use a lot of kafka+spark streaming internally and sr was the obvious choice. it\\'s a common belief at week 4, but stubborn reality always proves the belief to be wrong. you\\'ll see... by week 8-9, you\\'ll start to wonder why were you ever worried in the first place. also... past fellows seem to be determined too to prove the belief wrong! :wink: [that said, ny fellows: looking forward to your demos in a few weeks!! ]i think at this point, i was still tinkering with some visualizations on the webapp, but was ready and excited to show it off already. the most stressful part was waiting for interviews (not preparing, just waiting), especially since it\\'s such a contrast to the fast-paced project building stage. once the first interview comes, it\\'s a sigh of relief :relieved:besides, imagine where you were 4 weeks ago. probably can\\'t even fathom how far ago that was. imagine now 4 weeks from now. that\\'s what insight is all about: quick and sudden changes :laughing:you will get call backs after week 7. :blush:although, getting call backs and getting a job is a different issue. for some folks, a job comes within a few weeks to a few months. which is awesome!or you could be in my shoes and have to go essentially three months before landing something. not to be a negative nancy, but the road has a possibility to be long!to piggyback..it is a matter of fit in many cases and some will find that sooner than others but don’t let that discourage you.  i took a bit longer than most to land something but in the end i found a role that was well worth the wait! :slightly_smiling_face:for those where it took a bit longer, how did you spend your time? more work to refine the project? other learning on the side? or perhaps get some much-needed rest? any suggestions (just in case!)?also...focus on the things you can control and don\\'t worry about the ones you can\\'t. there\\'s lots of variables at play how companies choose their employees and a lot of it has nothing to do with you (like...office politics).  that being said, your interview performance and attitude (positivity) will play a huge role obviously- so make sure you are ready for interviews and nail them! ^ def keep up the practice :slightly_smiling_face: we did a round-robin setup of giving walkthroughs/lectures/whateveryouwanttocallit on topics we had particular knowledge of during interview prep, e.g. one fellow goes through more straight-cs algorithms for coding interviews, one does stats, etcthere’s always time for that :stuck_out_tongue:i think in week 4, this feeling kind of creeps in  because after the crazy pace of working on the project, ranking and demoing at companies reminds fellows the real reason all of this is happening. i agree that it is best to work on what you can control and approach things positively. i was having a lot of visiting and demoing companies because it is a genuinely exciting experience getting a behind-the-scenes look at all these companies that play a role in our daily lives. advice re: beyond the 7 weeks, i enjoyed preparing for subjects i was weak in because it was a real opportunity to acquisition new skills and do it together with others around me. it helps to do it with others. also callbacks and interviews are pretty unpredictable, in my case, most fellows in my session already had jobs by the time i got callbacks, but i ended up with multiple offers coming in, in one week and ended up at the top company in my wish list!study and practice practice practice..but def take time off, or even a weekend off to replenish yourself.  its a draining process!#teamcoconut :team-coconut:  with regards to callbacks. given the variance in the process, trying to figure out how it all works will just result in a headache (at least it did for me). i know it\\'s difficult since we\\'re trained to look for patterns (more so than other job seekers), but without knowing all the relevant parameters, you tend to over-fit based on your small sample size. :stuck_out_tongue:  remember, as long as this process takes, it\\'s still far shorter than what you would have to do in academia.: i was doing alot of studying for my weak areas, specifically cs and whiteboard programming. usually working on leetcode questions everyday and/or reading about data structures and algorithmsi also structured part of every other day or so to comb through job ads, and bringing them to the insight pdsmichelle_tat: great ideas, thank you! only because you mentioned it: do not work on refining your project. it’s really important to focus on simply presenting your results (however crappy they might be…mine were) as best as you can and move on to practicing interview-relevant skills. everyone thinks their project can be a little bit better with another 1-2 days of work (or that their project is horrible and needs another month) but hiring managers rarely care about / notice the marginal improvements you’ll be able to make now.hi everyone, we’re looking to collect some feedback from a handful of practicing data scientists.  this would involve you taking a 20 minute survey, and the results will be used to directly inform the roadmap for our product.  we’re also offering a $25 amazon gift card to everyone who participates.  if you’re interested please dm me :slightly_smiling_face:  thanks!^lack of coconut emoji fixed!thanks everyone for your interest, our survey slots are now full :slightly_smiling_face:  appreciate it!hey everyone! one of the ai fellows just posted a blog post about his work doing audio super-resolution (recovering high frequency audio signal from just the low frequency audio signal). could you please heart on medium? interesting, there was a similar attempt at google brain but in video:the coconut one smells like sunscreen :laughing:what my grad school education consisted of:  applied math, optimization and estimation theory, orbital mechanics and control systems.  what it should have been:  how to make pretty charts in powerpoint.  hey!  hope all is well! i was wondering if anyone works at nordstrom or has any experience doing a data scientist screen with them. i\\'d love to hear about your experience...thanks much! :slightly_smiling_face:\"the result is called a tensor processing unit (tpu), a custom asic we built specifically for machine learning — and tailored for tensorflow. we’ve been running tpus inside our data centers for more than a year, and have found them to deliver *an order of magnitude better-optimized performance per watt for machine learning*. this is roughly equivalent to fast-forwarding technology about seven years into the future (three generations of moore’s law).\"i wonder what physical considerations go into optimizing for that sort of thinghow much of a step is it over designing gpus to be optimized for linear algebra (low-rank tensors)they don’t make it clear in the blog post, but i believe these do 8-bit integer processing and probably only get the 10x performance / watt for inference tasks. for learning tasks gpus probably are still the way to go. there is some discussion of this on hacker news.caveat: i know nothing about gpu designi know that graphics cards were also pretty powerful at bitcoin mining and were most commonly used by most miners. however, asics have been around now for years that easily break 1000x the performance of the fastest gpus ($1300 asic vs. a tesla  gpu). as long as the task is specialized enough i could easily see how an asic could outperform gpus and be cheaper in the long run. it may require optimizing the code to run on it, but i don\\'t see why people would see it as being hard to believe.here\\'s a breakdown of the speeds of asic/fpga miners, you can see graphic card speeds at the top by clicking on \"non-specialized hardware comparison\". almost by definition, an asic implementation can always be faster than fpga or gpu or an any pre-structured architecture. the problem is that the fixed costs of asic are huge, and redesigns are equally costly.stay positive alanasic -&gt; application specific integrated circuitasic -&gt; fpga -&gt; general purpose processor  gpu / cpuin laymans terms, a dedicated chip for your specific use casethe costs of building your chip include many phases of application definition, chip design, lithography/tape-out/fabwhereas with an fpga or gpu/cpu, you are using something that already exists and writing an application (e.g. code) to work within its architectureyes (though you can’t assume fpga is always better than gpu or cpu. they are more flexible of course, but for what they are designed for a gpu or cpu is always going to be faster.)with an asic, you are allocating, by design, all of the registers, gates, etc to implement the logic for your applicationwith an fpga and your hdl + libraries, you are allocating a finite number of reprogrammingable registers+gates to implement the logic for your applicationwith a gpu/cpu, all of those elements are already allocated into a the processor design and you are writing instructions to be executedyou have execution units in a gpu/cpuin real world practice, the ability to develop good software/applications is affected by things such as the developer/development team’s ability to work with the tooling/language, speed to execute and iterate, etcwhile an asic or fpga offer theoretically better performance, the ability to materialize that benefit in practice tends to not happen as oftendoes anyone have experience on  how to build a sci-kit classification model that mix numerical/categorical features and text related features.(that generated with tf-idf)?brad: uh oh! does this mean you\\'re leaving us! :disappointed:yeah, i’m moving back to the 6ix and working remotely from there permutation tests make no assumptions about the underlying distributions/data types, which is their beauty. (also, the m-w is actually a randomization test, but it converts data to ranks first).it looks like if there isn\\'t random assignment it requires symmetric distributions. i\\'m still trying to determine if their randomization technique is sufficient. i\\'ll have to look into it more.well best of luck! when do you run off?maybe more relevant in a few weeks for the new fellows :wink:these are actually good / fun questions, i wish that’s what my experience was when interviewing at most places haha, instead it was mostly dice / coin problems and reversing python stringsyeah, a lot of silly code questions.  still worth knowing though, i reckondoes anyone here use crisp-dm or similar methodology structure?  interested in getting some thoughts about that from people using it.thanks! i fly out on tuesdayhey guys, had a question. i need a python program to call a function \"outdata\" from a different python program called \"process_data\" (\"outdata is within \"process_data\").if i say \"from process_data import outdata\" everything works fine. but if i *change* the contents of \"outdata\", python does not recognize those changes even if i import again. any ideas?you are sure the path you are importing `process_data` from is the same as the path where you are editing its contents?also what changes are you making? on the fly?if it’s in a jupyter notebook, you have to explicitly re-load the package, re-running the cell with the `import` statement is not enough^ use `reload()`yeah reload did the trick!hey all, i am organizing a hackathon to download government data before it disappears :scream_cat:   eventbrite link to be added to the event, but feel free to share in the meantime! for nyc folks, there’s an event on feb 3, but it’s all full i thinkanyone looking for a mattress + bed frame? practically brand new, $100 each or best offer, i\\'m just looking to get rid of it!  jackieb: it’s coordinated with the other efforts out there?yep!all run by a lab out of upennafaik no other bay area events yetok, so i need to writeup a quick example of the various ways to use the ipython magic `autoreload` but this gives a good starting point:  regarding \\'s point -- in python 3 (if you\\'re using it) `reload()` is no longer a core function, it must be imported explicitly before use: `from importlib import reload`anyone in east bay, my dog got let loose by maintenance. 17 lbs blond freshly groomed female lhasa apso yorkie mix with purple harness. named circe. piedmont area kingston ave. please call number on tag.if anyone sees her pinned a message to this channel.found her!glad to hear. that was fast. hey guys, does anyone have experience using bokeh for apps/dashboards in python? they have some really nice example templates, but if i try to alter them at all (i.e., try to plot a histogram in the app rather than a simple line) the code breaks. is there a good tutorial on customizing these apps other than just the bokeh site itself?took 3 hours, but glad it ended wellone of my friends team at uber is hiring for a backend software engineer role. please pm me if interested and i can put you in touch with the right person. here goes the description \"if the world of maps interest you, there\\'s a position open in uber maps team for a backend engineer role. the position is similar to this one, and focuses on maps data. #softwareengineer - #maps #infrastructure, #data, and #services... \": it might be that the page (or part of it, say the map piece) is being cached by either the browser or the server. you should look into disabling caching for both. there are some html tags that do this (it’s just a suggestion) for the browser and there are settings that are server specific. i’m not familiar with gunicorn settings, but try to find the caching and disable it.making the map name different every time worked, so i am positive it is being cached somewhere (but i am positive it is not the browser). i will look into aws caching...i have found that some files are turning up in /var/tmp so that subtly suggests to me that it is probably supervisor/gunicorn (but i am not certain)linkedin open-sources a url detector and parser using finite-state machines. blog post by tj from the anti-abuse team: stay positive carlosfahamanti-abuse nuggetbotstay positive carlosfahamloldude someone needs to rework the sentiment analysis nuggetbot usesseriously!nuggetbot check on mscherrermscherrer has a happiness average of 0.3925233644859813i’m so super happy, life is so wonderful like rainbows and unicorns. amazing!!!!nuggetbot check on mscherrermscherrer has a happiness average of 0.5370370370370371: more decimal places pleasedoes anyone know of a clever way to read files out of an s3 path with wildcards like you can do on a local filesystem with glob? i poked around on stack overflow and came up with nothing. i’m doing a bunch of string manipulation in python and kind of getting there, but i can’t shake the sensation that i’m doing this the hard way.the  —include —exclude flags?yeah, that’s kind of the best way to do it. i keep foolishly thinking of s3 like it’s a file system...i’ve gotten spoiled by how easy this is with glob. sorry to bother everybody...anyone use the databrick’s spark-xml parser and elasticsearch run into problems with accessing an s3 bucket? we have a fellow who successfully uses the spark-xml parser but the minute he tries to also import elasticsearch libraries, he gets runtime errors complaining about s3. i know there are sometimes problems with accessing s3 with spark and hadoop, and we\\'ve been able to fix that (setting the classpath to add the aws jars) but the minute he tries to import elasticsearch, he gets the error. any suggestions?stay positive reporterhoacan anyone recommend some good java tutorials?umm, i normally use this:i hope it\\'s helpfulany nyu alumni that want to go to the nyu alumni day on oct 25th?nyu alumni day? what\\'s that?: just some fun activitieshey all - i\\'m a current health data sciences fellow in sf; looking to get less-limited access to the locu api for some restaurant menus. if anyone has an \"in\" with a contact at locu, any chance you\\'d be willing to put me in touch? thanks! -andy hey alumni rockstars, did anyone use tensorflow in docker and do you know how to grab the output so that it can be used/displayed in a web app?you can exchange data between flask and a web front-end using jquery/ajaxis that what you\\'re looking for? you can mount a data volume and access data from outside the containter, there are a bunch of examples explained here:thank you guys for the links!hi everyone! we are looking for a spark expert with hands-on experience to consult on a project. we expect around 10-15 hours. paid. please dm me if interested or know anyone who might be interested. thanks!dear sage data scientists, how do i extract a representative data subset (~200k rows)  from a large sql table (20m to 90m rows)?  what would be the best way to proceed if i need a representative data subset from  four or five related sql tables, each in excess of 20m rows?`select * from table order by rand() limit 200000` ?rand() ! brilliant. thankstake care if you have some rare but important things in there :slightly_smiling_face:mdhardy: some db use random()i think this solution has somewhat poor time complexity (o(n log n)), because of the sorting. there’s some discussion in this thread i believe `select * from table where rand() &lt;= .3` (replace 0.3 with your actual percentage) should be better if performance is a concern.(although this solution has the disadvantage of not returning exactly the desired number of rows.)friend of insight karl heibron on 23andme\\'s parkinson\\'s team is launching a microbiomics meetup in sf:it will likely be rather data heavy! definitely check it out on february 27th if you can!i will definitely be theremy girlfriend is looking for a job, ideally in boston (where i am!). she\\'s an nlp engineer with three years of experience in search, a master\\'s in computer science, and a phd in linguistics. she also speaks french and german fluently. much appreciated if you have any leads, e.g. friends working on search teams.not too get too political again, but all fellows currently on opt should contact their employers to explore alternative visa options. there is talk that opt will be either scrapped or modified, so extensions might not be possible by the time you need one. see: , paragraph: \"limiting legal immigration\"varunrajan:  thanks! rand() &lt;= threshold-percentage worked nicely.hey guys, had a question about jupyter notebooks. i looks like some of my figures aren\\'t rendering in \"dashboard\" mode after a certain point. anyone else run into this before? i have included the %matplotlib inline. could you give more details?in my notebook i\\'ve written a bunch of code snippets that generate figures in different places. if in the notebook i click on \"view\", then \"dashboard preview\", they all show up fine up to a certain point. then for some reason subsequent figures don\\'t render past that point.okay, now it does show up somehow. i have no idea what happened: wanted to add one more thing. i had in fact tried the idea of \"rdd.foreachrdd(lambda rdd: rdd.foreachpartition(processrdd))\". it appears as though that should take the rdds from a partition and apply processrdd() to each of them without causing any shuffle. but it does cause approximately the same amount of shuffle read and shuffle writes (you can see this in the \"executor in the :4040 page of the spark master) as an alternate approach that was doing the processing after the reduce phase. what it means is that the foreachrdd() happens *after* the reduce phase. we want it to executed *before* the reduce or rather in lieu of it. when i try rdd.mappartitions() instead, it thinks there\\'s no action  on the stream stream at all and ignores the input completely with the result that the incoming rate of this stream drop to zero !does anyone have any tips about getting a unicode data frame in pandas (collected through webscraping) into mysql? i have a function that uses sqlalchemy and pymysql which works, but not for this dataframe in particular. i\\'ve tried a number of data type conversions, but can\\'t seem to get anything to work. looking for any kind of work around, i only need to get one df with 10 or so columns into mysql. thanks!:  this is what my pandas to mysql code looks like (using mysqldb):def pandas_to_sql(df, database, table):    import mysqldb    db = mysqldb.connect(host = \\'localhost\\',                         user = \\'root\\',                         passwd = \"\")         db.query(\\'create database if not exists \\' + database + \\';\\')    db.query(\\'use \\' + database + \\';\\')                                                                                                                                                                                        df.to_sql(name = table,                    con = db,                    flavor = \\'mysql\\',                    if_exists = \\'replace\\')                                                                                                                                                         db.close()        return \\'sent \\' + table + \\' to mysql.\\': did you look at my post  ? i show how to use utf8 encoding to get unicode into mysql. in some cases you need to set the mysql character set to utf8mb4, not just utf8 (see e.g. ) mentioned a file: .thanks so much   this looks great! i\\'ve been struggling with unicode to mysql for way too long :simple_smile:what\\'s reddit\\'s button all about??: the button!  :smile:our ceo (who’s non-technical but really smart) asked for resources to learn more about data science so she could be more informed when in discussions with the ds team.  does anyone have suggestions for books, blogposts, or talks?this has been making the rounds recently: some of the relationship managers at insight gave positive reviews of data science for dummieshi team, i am building a classification model for imbalanced data (1% positive prevalence). we have a lot of data so i am subsampling the negative cases to match the number of positive cases. i plotted the calibration probability plots following this link () and the subsampled train &amp; validation curves look good. however, the test data (not subsampled) is essentially just a flat line near zero (slightly upward sloping). does it make sense to plot the calibration curve for imbalanced test data? has anyone used calibratedclassifiercv with imbalanced data? the use case for this model is to provide a ranked list with highest probability at the top. unfortunately due to the imbalance, the number of false positives greatly exceeds the number of true positives at the top of this list. any recommendations would be helpful. i can provide more details if needed.if you have high false positives, i would recommend using an ensemble model approach. train several different models on different undersampled subsets of the data, and then take a majority vote of all models. just make sure your test set is not in the training set for any of the models.you might still be able to do a calibration curve if you have a mean probability cutoff instead of a voting cutoff for the models. one huuuge caveat - if you\\'re using a boosted classifier this will likely push all your probabilities to 0 or 1, and i would not recommend using a calibration curve, or using a probability cutoff instead of a voting cutoff for your ensemble model.thanks  and !thanks. i forgot to mention my \\'client\\' wants the top 5 contributing features to provide insight into interventions. i\\'m currently using xgboost and grabbing the top 5 features when pred_contribs =true. i\\'m unsure of what pred_contribs = true does - but you might want to be a bit wary if you have correlated features. in feature importances for a scikit tree model, that will affect their rankings. we are looking for ds and de interns at tempalert (also in boston).thanks for getting back to me everyone! very helpful.hey guys, a technical question: is there a way/algorithm to partition a continuos variable into a series of intervals, while maximizing the predictive power of this new variable? let me make an example: you have a time variable that you want to discretize into, say, 10 intervals. how do you choose these intervals so that most of the information of that variable is retained?haven\\'t tried this before, but off the cuff, can you just do a series of binary splits using a similar information metric as in rf?would it be relative to one dependent measure?you might be looking for something like bayesian blocks: sounds very much like what we did in physics with continuous physical variables to try to maximize signal / background ratios across several bins of a given variablewe did it similar to what   recommends — automated a loop over several possible binning configurations and chose the best one. perhaps there’s a much smarter way, thoughthanks guys, this is helpful. ideally, i would like to have an algorithm choose the best splits, as  suggests, rather than testing a bunch of binning configurations. will look into the bayesian block stuff, this is new to me. thanks again!you can probably use sklearn’s decisiontreeregressor (or classifier equivalent) to find the first split, then search the two resulting splits for the next best split, etc. but beyond the first split, i’m not sure if you can somehow pull the information metric out of it so you can compare the next steps.  but it’s probably not too tough to figure out how to get that.got it. will give this a try and maybe post an update in a few days...i’m very intrigued by the bayesian blocks thing, very cool way to do thatthe bayesian blocks is pretty awesome, i think this is the current up-to-date implementation: originally here ya that looks righti was trying to find an implementation that wasn\\'t buried in `astropy` but couldn\\'t find oneyeah the astroml package is linked in the article you posted ``deep learning ftw uploaded a file: that\\'s hilariously terrifyingthis is the stuff of my dreams. and nightmares.wowjobsalaryreminds be of meowpurrdylast day for abstracts!has anyone dealt with issues with decoded_latlngs = google.maps.geometry.encoding.decodepath(latlngs)?what sort of issues are you encountering,  ?are job postings on-topic here?hi, ii\\'m current fellow, and i need twitter graph data (who follow who ) outside of snap. any help is greatly appreciated. thanksdoes anybody try using scikit-learn model across different  http requests on flask? i want to train a model based on the first request and predict something based on the second requst. new donald trump programming language seeks to make programming great againicymi, a recent de.ny fellow just posted a nice post on cassandra for “newbies”: dear respected elders, do you have any suggestions for a new fellow like me on how to best pitch an api key request to a company?  if you were at an api-key-granting company, what sort of request would you get excited to grant vs. what would be red flags in a request?  i humbly submit this question to the oraclenot sure, but likely the words \"academic purpose\" could be helpful yeah? right on.  this company states “we share our api with a limited set of corporate, education, and research partners”they’re more into academic projects than business ones? i guess i just don’t understand why they’d turn somebody downlike, they’re worried i’ll take their data and use it better than them and make all the money?katie suggested name-dropping insight, but that kinda seems contrary to your suggestionour company would view the insight connection as positivewe consider our data to be both sensitive (it has pii) and monetizable. so we would be looking for partners whose use wouldn\\'t compete with us, and who wouldn\\'t disclose our data (especially to the general public) irresonsibly.nice, thanks! i’ll try to angle it with that in mind!hi guys, i am trying to get tripadvisor api key but there is a limitation: \"tripadvisor grants a limited number of api keys to official tourism organizations and select other websites. it does not grant access for purposes of data analysis, research, testing, or similar uses.\"  any advice?is there a specific reason why you are trying to access tripadvisor data? is there another data source that\\'s similar you could use?yeah, i guess i can use wikitravel and expedia data instead but tripadvisor also has these reviews that could be usefulwould scraping the data be a viable alternative?hey cats - anyone at google? just put in a request for google flu data - hoping somehow that could be approved or expedited. also hoping to know the geographic/time \"resolution\" of it (zipcode/weekly/etc.) would greatly appreciate any help.hi fellows, has anybody seen a dataset that has diagnosis or medical records that have valid date and geographic information? i\\'m thinking specifically for respiratory issues and zipcode or hospital level spatial precision, so i can correlate with metereological information. (the mimic-iii database \\'dedates\\' the time information and adds a random offset to the date so it falls between 2100 and 2300, making it unsuitable for this purpose.)  this data set is likely not for public access because of the risk of identifying individuals.  thank you!what kind of reviews are you looking for? yelp has a handy data api. also, google might as well.welcome new fellows! anyone who has even the slightest chance of using the mimic-iii database should request access right now as it takes a couple days to get access to the data.you could probably try hospitalization data from hcup or the children\\'s database at kiid and associate the hospital zip code to the patients.however, depending on the condition you choose, it might be very hard to get a breakdown by zip code. much more likely to get it by county, but is very dependent on each state\\'s department of public health.try to focus on a single state and, if given enough time, expand to the us and eventually the world!i actually did something similar studying pediatric asthma.hello fellows, i\\'m in the first week of insight. still brainstorming the project idea. i would like to propose a project doing the face classification, i.e., like different disney character, or different famous people, etc. any advice is welcome. thank you.this blog is a great resource for image classification that doesn\\'t require nn using histogram of gradient features,  hello! hope you\\'re hanging in there during your first week. i guess my question to you is, what is the use case for doing face classification? i imagine this has been done by several fellows before. thinking about your use case and what kind of practical and/or business value of your idea should be your mindset when thinking about your question.there is a huge celebrity face data set, celeba is this for the ai program or for the data science program? if it\\'s ds, think about how to productizeeither way, actually disney!!! :slightly_smiling_face:hehe, i don’t think i’ve seen a disney-related project ever before…thank all for your comments. very helpful. p.s. this is for ds project.the local bike community or cyclist union should be the best bet for that information. i\\'m only aware of the ones in boston though.new blog post by  featuring some alumnae giving advice to their younger selves. check it out: hey friends - if anyone\\'s interested i\\'m giving a talk on using data science to disseminate off-grid solar in sub-saharan africa in sf on april 4: people who work/live in the south bay, if you\\'re interested in and enjoy art and work in technology, the palo alto arts center foundation (paacf) is starting an initiative of art and tech ambassadors. if you (or someone you know or work with) is interested, sign up here: i\\'m on the paacf board of directors so feel free to ping me directly if you have questions. :point_up:encountered this today, missy elliot came to the rescue. nuggetbot animate me missy elliot flip it and reverse itberkeley grade average by department: also, cal data: hey, domino is doing another data popup in nyc. sv folks know angela, who comes by to advise fellows often. she posted a blog about the popup domino did in sv earlier this year: .the nyc data popup is on june 14, . if you want to attend, here’s a 20%  off discount code: datapopupnycinsight alum, , will also be presenting!new seattle fellows?? :grin:woot! welcome seattle :slightly_smiling_face:dear seattle fellows,don\\'t worry if you start feeling like this very soon. it\\'s normal. :laughing: seattle fellows, plus the first cohort of ai fellows in new york!:wave:coolthis is awesome. what\\'s the location of insight seattle by the way?seattledamn, edited too fast!right by pike place market, i\\'ve been toldyep, right by pike place - 1st and lenora. come visit? :slightly_smiling_face:are there snacks?there are! :la_croix: :cookie: :coffee:i live close by (3rd and battery) will do yay! i want to come visit too!hey all, random expertise request.is there anyone here familiar with tv programming/advertising in argentina?please dm me. thank you! pinned a message to this channel.has anyone here done significance testing with comparing percentiles between two different distributions ?does anyone have any recommendations for an egpu setup? if you\\'re mac inclinedapple is also has one has anyone generated google static maps requests via python’s ‘requests’ library?i think its mutating the parameters passed in via the ‘params’ keywordi.e. r = requests.get(&lt;api_url&gt;,params=some_python_dict)tried unicode encoding, no dice, =(oh, nevermind, stupid error on my partplease ignore the above, :sadkeanu:yeah, it is pretty handyi really enjoyed nate silver\\'s book. he does a good job of showing that some fields have become excellent at updating models over the past 20 years (e.g. weather prediction) while others have not improved but recognize that (e.g. earthquake prediction) while other fields still have lots of players engaged in puffery (e.g. politics/elections)another book i recommend is \"the visual display of quantitative information\"has anyone here filed for a tn visa? i had a few questions? what category do i file under ? i was planning to file under physicist.fyi: my phd is in physicsawesome, thanks ! i\\'m in chapter three right now and really enjoying it!  and thanks for the tip on the tufte, i knew it was famous but not sure if it was actually worth a read, i\\'ll put it on the list!depends on your job.. many data scientists are under mathematiciani came in under mathematicianah thanks guys ! if you have a phd, you should file under the activity you are performing. uscis is very particular about this. make sure that your employer\\'s letter with a job description matches as closely as possible nafta\\'s definition for that category. start by looking at -&gt; to \\'s point, mathematician (or statistician, which falls under that umbrella) is a commonly aligned description.  (also, you may want them to file that in your behalf to ensure that it gets approved easily, as of this year, uscis has started to get really picky about tns)hi everyone,another boston 18a fellow here like pawan. what if my phd in physics is still in progress? i have an undergraduate degree in engineering.thanks and regards,abdullahthanks marco !+1 to applying ahead of any attempted border crossing. they still asked a lot of questions (which they continue to do every time i re-enter the us), but it was a big help.out of curiosity, has anyone ever heard of “equal first-author swap” on a cv/resume list of publications? i just saw this for the first time and after some googling it appears that it’s a thing people sometimes do when the first 2 authors of a paper “did equal work” but the second author is “allowed” to list themselves first on their cv/resume, even though they are listed second in the actual reference.most people online seem to think this is super sketchy and i tend to agree… thoughts?why do you think it’s sketchy?  is this not a situation where the publication supports co-first author?many publications have status for that, where it does in fact mean they did equal work, without quotes :smile:it does seem odd to me to do something other than publication order… but then i come from a field that puts little emphasis on author order for papersi had this come up once in a very specific instance... during grad school, another graduate student and i ended up doing nearly identical experiments, completely independently, and unintentionally. when it came time to publish, we talked about the idea of publishing our results in a joint paper and explored this as an approach to having equal credit as first author.in the end, we decided it was simpler (and more impactful) to publish separately, so we ended up publishing in the same journal at the same time.i could see it being a reasonable approach sometimes, in fields where author order is important.(i\\'m coming from ecology and evolutionary biology)i come from a field that puts emphasis on the alphabetical order of authors last names. first author swaps are not allowed. at least in astro, if you list a paper in your cv/resume and you’re in the top ~3 listed authors, then it’s assumed you did a lot of work on it. if you’re listed first, you’re assumed to have done the plurality of the work. if you’re in the alphabetical section, then you probably didn’t do much. the sketchy view comes from the idea that what is stopping me from saying i “split” first author credit every time i’m second author or a trio of authors when i”m third?i think it’s totally fine in cases wehre the publication supports and validates co-first authorship.  at that point i might as well be questioning their finding or p-values.  but if they do it on their own it’s sort of weirdif you want to explain how much work you did when you’re “only” second author, then add a bullet under it? i would just read it and assume, ‘cool, second author, they did a decently large amount of work’yesterday a paper of mine got accepted in a journal where i share first authorship with another postdoc.  i fully agree that we put in equal amount of work and were equally invested from the beginning.i’ve never seen a publication in astro support this in any waythink of it as pair programming.there’s a bunch of quality ones for biology stuffdo commits of pair programmed code have both author’s names on it?i’ve done second author work that can range from a week to like 1/3rd of the work^so have i!if i am co first on something (i’m not) it woudl be because its’ very close to 1/2 and due to much fewer papers in that field it matters(monkey work stuff)no, they would only have one.i know in deep learning/ml papers they often have little bullets saying “authors did equal work” for the first `n&lt;=3` authors. i suspect that is what some people are saying when they say “the paper supports it”but coming from a field that doesn’t have author ordering… y’all weird! :wink:i bet its a function of “time / resources per paper” with the more required, the more likely for co-first :smile:`academic politics are so vicious precisely because the stakes are so small.`does anyone know of any good vendors/services/workflows that keep the docker image together with the dockerfile that created it? have you looked into kubernetes?have you looked at jfrog artifactory? others that come to mind include nexus sonatype and aws\\'s container registry (ecr). i think you can use ecr without actually using ecs.i’m not sure if i follow the question exactly, but if the main concern is linking a docker image to the code that originally generated it (perhaps long ago), then i’d: store the docker images in any container registry (docker, jfrog, ecr, etc.), use git to track changes in the dockerfile, and then tag each created image with the hash of the latest commit in the git repo containing the dockerfile. that way, if you have a random image and want to find the code that generated, you just check out the repo at that point in time. yeah we use kubernetes internally but that’s more container orchestration. i’m thinking about a more dumb service than that , yeah we currently use a combination of circle ci, github, and  for this. i saw that  would let you build images on their service from dockerfiles but was disappointed that you couldn’t link the image id with the dockerfile so that if you changed the tag, everything would follow along. i would have thought there’d be something out there that did that elegantly.  i’ve used ecr but not ecs with ecr. my impression of most aws services is that they do a low level thing really well but don’t really take features beyond that. i’ll check it out though  they have a link to an 8h-long youtube video of the whole thing(which is actually a playlist of many 30min talks)anyone else hanging out at the scipy 2015 in austin?does any one have experience with parallelization for assessing model performance on ~100k different data sets? spark seems like a good option, but it seems like it would be limited in terms of library selection. (i\\'m using python)help! there seem to be so many possible options and approaches while it\\'s not memory efficient, you could run them via multiprocessing if you have a decent machine that can handle it. how big are the datasets? can you hold nvirtualcores x dataset_filesize in memory? if so, the model is static and you\\'re only applying it to different data sets, you could pass model and the filenames of the dataset and let it go. if the data live in a db, it\\'s even less io intenstive. do you need to run it on a cluster?does anyone have experience with creating technical training materials for software use at a startup? my company would be interested in doing a lunch &amp; learn :)i’ve done some, what technical level are you thinking?more specifically, i’ve given the “lunch and learn” talk about some tech i’m using (but that’s kinda one-directional), as well as created some training materials for our analysts you could try`luigi` as well, i’ve used it for task graph specification.  sounds like you basically just need to queue up the same model run for a bunch of different self-contained datasets?  like said most approaches are going to be memory-limited, but at least luigi makes it pretty friendly to run arbitrary python code for the task nodeswhat about dask?i faced a similar problem during my phd research. i concur with lonnie, dask is an excellent way to approach this problem. it has a really easy syntax for constructing arbitrary directed acyclic graphs using dask.delayed, interfaces well with scikit-learn, has distributed dataframes like spark,  and scales well up to about 1000 cores on a cluster.can some insight fellow make a chrome extension that reads your jupyter notebook and automatically suggests pages on stackoverflow that are relevant?brilliant!!! but... how do you validate??? :stuck_out_tongue_closed_eyes:copy-paste til it works:wink: for building it, i\\'d say you have a feedback loop sort of like how search engines determine what links are effectivewould be hard to build the model / solve the cold start problem maybe*consulting solicitation - image classification guru*i’m looking for an image classification guru to consult on a short-term project, 2 - 4 weeks, starting asap (like, next week). it’s a very interesting project for a fortune 500 company, hourly compensation at competitive market rate. pm me for details!surely google can help solve the cold start problem?assuming they’re logging everything you type into chrome...?anyone around who can help me think through a problem with time-series data? :point_up:does anyone here have experience calculating regression prediction confidence intervals with sklearn models? although this doesn’t answer your question, i believe `statsmodels` has prediction intervals, so if you’re doing something simple, that might be a good choice. happy to help  as well if you need ithi all! does anyone have experience with javascript? i have none. what i\\'m trying to do is:1) have a drop down menu populated with states.2) once a state is chosen, have a second menu populated with counties.3) once the county is chosen, query a database i have on the (county, state) and display the other values in that particular row.i would like to do all of this without refreshing the page since there is a relatively expensive map that takes a little time to render. feel free to dm me if you think you can help (code snippets, good tutorial, examples, etc...)!look into jquery, that should be able to help youhi guys! any podcast suggestions for big-data? i have about an hour commute per day and would like to use it listening to something more meaningful :simple_smile:partially derivative is fun and good about giving you and update on the recent news in data sciencewhat’s the point from fivethirtyeight is also good about current topicsi’ve also been listening to linear digressions and talking machines which seem to focus more on teaching specific techniques/algorithms/toolsdang, beat me to it on those.everything else i tried was just… not very good. so that is a list of 4 (ranked in order of how i’ve enjoyed them) culled from a list of like 10there’s also select episodes of: oop, you got that one too! nice job, paul.brennerhaha! i’ll dance on your grave!or… maybe that was a bit too intense.haha, yesnuggetbot: animate me that escalated quicklynot the one i had in mindsorry, i’ll back that up… but i’m laughing at least looks good but i haven’t listened yet^ oreilly data show/giphy anchorman that escalated quicklyi just couldn’t get into data skeptic. the subject matter is great thoughthanks guys! i’ll give those a try...*hi all* - i’d like to connect with ds/de alum who are working in *data-driven finance* or *commercial insurance*, and who’d be willing to chat a bit about sourcing data. pm me if you can spare a few minutes to talk at some point. thanks!for the ml people in sv : :  try cca since you have an output variable.   this is better than pca in this case. while pca will give you components which explain variance in x, what you want is the components of x which explain variance in y.    pca is not appropriate for this and may bring in features that have unnecessary noise, or don\\'t correlate with the output.hello! geopandas question! i\\'m having a weird issues where i now can not import geopandas in ipython. i\\'m getting errors like failed `cdll(/usr/local/lib/libgeos_c.dylib)` and warning:shapely.geos:failed `cdll(/usr/local/lib/libgeos_c.dylib)`failed `cdll(/library/frameworks/geos.framework/versions/current/geos)` and warning:shapely.geos:failed `cdll(/library/frameworks/geos.framework/versions/current/geos)`failed `cdll(/opt/local/lib/libgeos_c.dylib)`  i\\'m really bad with directories and knowing if i am missing folders or if my libraries are in the wrong place. any ideas?stay positive stephgline: we already had a channel for scala! it is called so fun: this algorithm unpacks the rappers whose rhymes influenced the musical hamilton .  plus some details on how they built it here: : nice! we\\'ll migrate hi people. does anyone have any good book recommendations about project planning and management? :partyparrot: good talk from last week’s spark summit regarding data culture you should also send this info directly over to    or anyone else on the data engineering team at insight! it\\'s the best way to quickly find which current fellows might be interested in joining your team! :raised_hands:san diego is trying to poach tech talent from the bay area.event on feb 1: (the event’s in mountain view) thanks！oh dang, i\\'ve been going around saying thistimeseries finding trends example: computing rolling averages for timeseries: another good tutorial, the resample function looks incredibly powerful: a chance for free ebooks:has anyone heard of predictive analytics innovation summit? is it worth attending or being a speaker for? nicolestrang: storing the changes of the table across time should do the trick (similar to a version control like git). however, it would decrease read time, as you\\'d have to get the base table and then the cumulative changes. nonetheless, depending on how little it changes with time, it would save plenty of space. take a look at `log triggers` for more information.dear data science sages: i\\'m interviewing w/ a few companies and have seen a couple of startups using a ruby on rails stack. what would be a good way to integrate python into such a set-up? - micro-services.  most companies are polygots (multiple languages / stacks etc) and let teams choose the one that they are most productive in.  so the way i have worked in such environment is make the data science model as api and let ruby on rails app just communicate via the api and get a json response just like any other prediction api services out there.anyone have experience passing google maps autocompleted addresses from javascript to python?sampathweb: - thanks ramesh! i was thinking micoservice applications but wasn\\'t sure; it\\'s great garnering perspective from a professional - thanks! i really appreciate it.sure. when/how?classic papers determined by googleif you’re ever alone on a friday night wondering what to doanyone have any tips or favorite packages for creating web dashboards? i\\'m currently looking at bokeh, data will be coming in from mongodb and postgresql. videos, tutorials, etc. are appreciated.shiny is pretty quick for adhoc stuffrshiny depending on what you want to do with itsuperset is pretty awesome. the built in admin and roles is hard to pass up especially if you\\'re deploying for your own company. user interface was a bit strange but gets super easy once you\\'re used to ittinkering with superset now, debating that vs metabasenb it doesn’t *yet* work with nosql databases. i’ve had issues with mongo. there is active contribution for elastic search (which i’ve never used), thoughinsight ai fellow  shares her project on separating overlapping chromosomes with deep learning: who\\'s on  !!???so i\\'ve found that, when preparing for contests where you are hopelessly out matched, it is best to talk a lot of trash. so: come get some you junior university trees! go bears! :wink: :bear: :football: well that worked out poorly. are there mercy rules in college football? :wink:so cold in nyc... must be nice to be in the valley...probably warmer than nyc, but colder than i\\'m used to!if it’s any consolation, boston is colder than nyc now. but i’ll take a bit of cold over measles outbreaks, dysfunctional transit, and absurd housing policy. (signed, a former bay area resident of 6+ years)&lt;— smug former boston resident in sv here….i saw it was in the high 40s this morning... a real weenie shrinker (la story reference)this is a great series of in browser demos of deep learning with javascript: pretty fun to play withwhoops, sorry the above link was to a specific demo. here is the full list: the paper has a bunch more, and is open: : i thought that plot was really important tooevery difference that they list before that last one they report it like: (62.7%) (χ2 (df = 2, n = 2, 473, 190) = 492, p &lt; .001).or 74.6% [74.56%, 74.67%] 95% confidence intervalbut they don\\'t list the values or the confidence interval for the gendered outsider difference (it just shows up in the plot). so is this not a statistically significant difference?haven’t read the paper, but reading the bbc pr\"\"women have a higher acceptance rate of pull requests overall, but when they\\'re outsiders and their gender is identifiable, they have a lower acceptance rate than men.\"maybe their paper makes this clear… the figure doesn\\'trather… when people don’t know somone’s gender and  know the person (insider) men and women’s rates are equal within their errorstarting the y-axis at 60% also distorts the overall size of any differenceswithout scrutinizing the stats, the outsider numbers seem “significant” but the results are either within the error bars or very close to being within the error barsstay positive jsapangood try nuggetbotgoogle embeds more words in more vectorspaces making *swivel* (obviously they tried pretty hard for that acronmy): knock if off, dudesleave words as strings¡backronyms ftw if you are interested in attending odsc west in san francisco (11/2-11/4), please use the following link to get 60% off your tickets : . (the discount code is insightdatascience)woooo! please see some earlier comments regarding using the at channel here. you effectively just pinged 1.3k people from a lot of different time zones.#thoughtleadersso much is wrong... starts with the hair@thushy works for insight. i think she meant to ping the entire channel. just sayin\\'.i’m sure she’s super sorry about providing 60% off tickets to one of the largest professional data science gatherings in the usi don\\'t find the notifications that annoying? but i\\'m biased ;)clean your dishes it took me way too long to figure out what odsc stands for. define your acronyms people!!!this.any data engineers or data scientists working with streaming pipelines that are attending strata in ny this week? interested in attending the apache beam dinner/talk on wednesday night? let me know and i can send you an invite :slightly_smiling_face:: sorry for the 2 notifications from me today. trying to share the discounts to as many people as possible.lolwill the @ channel vs never-@ channel debate become the great tech rift of this decade, much like vi vs emacs or tabs vs spaces?or the great la croix wars of 2017? it’s telling that there’s only an emoji for team coconut edit: nm there’s a pamplemousse we\\'re trying to keep this channel a civil and safe place, so please don\\'t use the c-wordi only have a few truly deeply held convictions, but most of them are about flavors of foodi think there is a civil war happening at fellows desks right now.i suggest that all current fellows clean up all their crap. :stuck_out_tongue_winking_eye:actually all of them are about flavors of food except for the one about the direction toilet paper rolls should be fed on the roller thingsi think the tp thing is only important in households with cats or toddlers#provemewrongdoes clark count as a toddler?we both know the answer to that questionspaces. always. spaces!!!!!team coconut is tearing this insight family apart...uhhh husbands who do it the wrong way are definitely a problem #justsayingmy wife thinks i do it the wrong way, but i\\'m not convinced there is a right way.oh do the different colored la croix cans taste differently? i never noticed. team tap water :potable_water: why is there a debate about the direction of toilet paper rolls?because people do it wrongbut... there can be only one way!exactly, that’s the right way* unless you have a toddler or cat that\\'s why you get one of these: not the best of reviews thought...this could be gamechanging...if you’ve forgotten about all the mvps we keep obsessing over at insight, we wrote a blog post to remind you to always start with a baseline :slightly_smiling_face: would love any thoughts you all have on that role^ tell yo friends? how excited are you about the ipo?yes. great post!for a really simple baseline, i also like a random shuffle of the labels. i find it gives me a useful starting point in multiclass problems.i liked how you mentioned the baseline you choose depends on the data and the task. i feel like you could write more great posts (e.g. your post on approaching nlp problems) with that idea. for example, i didn’t know i should reach for a u-net if i have an image segmentation task.canadian fellows: what can i do in vancouver in the next 36 hrs?jira is the bane of my fucking existencei use jira for backlogging things and loosely organizing sprints and it’s fine. that said, i’m glad i’m not managing it and setting it up. seems a pain. convenient when it’s integrated with github.haha i don\\'t mind it in itself so much. ppl closing my painstakingly created tickets without actually finishing the work would drive me to tears of barbaric nausea however we used to use jira... painful. switched to zenhub (which integrates with github), amazing! a happier life is possible :slightly_smiling_face:i definitely prefer jira to asana, but there’s probably options that suck less than either of themexcellent info! what pain points did zenhub alleviate? (hate jira too)^yes! it seems like no one’s a real fan of jira or asana, but maybe zenhub is good?does anyone know of tools to track per-dataset bigquery usage across a gcp account? i.e. identify which individuals in an organization consume which datasets?it doesn’t seem like anyone is given any concrete reasons why they hate jira but i would give my impressions. my experience: we use jira at glassdoor but when i started we were using rally. i’ve tried trello on my own time and heard some talk about asana (my wife has used it a little in non-engineering contexts).• it is built to be extremely flexible and do all the things but for many uses this is way overkill (boards, filtered boards, projects, epics, versions, labels, issues, sub-task - how do i map each to our org and workflow? why are issues not called tasks, why are sub-tasks not called sub-issues?)• its defaults and structure are very engineering oriented, but as an engineering org it is still going to require much configuration• advance search is the only easy way to find stuff, but of course there is learning curve before this is actually easy.• when i got started i found i could fairly easily create issues and then not be able to find them, this is because it is super easy to create issues that won’t be visible in the current context. in my experience - shared by the team as well - jira is extremely inflexible precisely for the reasons than  points out: it is built with software dev/engineering in mind, and much catered to the agile religion... it requires task hierarchies, not built to be very intuitive, things get lost, blocks certain functions if not all task classifications are filled out... a lot of complications and unnecessary time investment if you need a simple tool to track your backlog, report issues, and track progress in a very flexible manner.while zenhub _could_ also cater to the agile way, it it not built to trip you if you don\\'t. so very, very flexible. ui devs in our team use it as jira-like, the ds team uses it more as a backlog tracker, the general team uses it as a mixture of both. since it\\'s natively integrated with github, it\\'s seamless to link your backlog with commits/pulls. we also use it to document issues and the solutions that have been developed. this is extremely useful when you have offshore teams in different time zones, contractors coming in and out of projects, or kt when a project transitions to different teams within the orgyes, it sound like pie in the sky... but for whatever it\\'s worth, the whole of ds and it here transferred to zenhub, and we\\'re not planning to go back...awesome...thanks  ...makes a lot of sensewe’re sending out dev setup recommendations to incoming fellows and currently suggest python 2, vote below if think we should stick with that recommendation or should start suggesting python 3. feel free to comment if you have strong arguments either way!bmregner: while i think 3 is unarguably better, i\\'m still seeing 2.7 in much wider usage.i’d say 2 since more packages are compatible with it.  i try to use 3 when i can so i’m not part of the transition problem though :slightly_smiling_face:i’ve heard the package compatibility argument, but i haven’t run into that problem at all, any example packages worth noting?the big ones are mostly compatible, but there’s various small ones that aren’t - though i don’t have a list handy.  should be able to do most non-niche things with python 3 packages.we have some internal stuff that isn\\'t py3 compatible yet.also spark i believe only works with python 2.7yep, unfortunately pyspark is only compatible with 2.7though maybe that’s less relevant/giphy such visiondocker and ci question: does anyone use (or know of) a continuous integration service they like that is \"smarter\" about building docker containers? none that i know of cache the various layers, and so a change anywhere triggers a complete rebuild (at the cost of about 15 minutes).great question — ?i don’t want to speak for austin, but worth noting he is out of office through next weekis anyone here going to be at nips in december? that gif is from `heat vision and jack`, btw, which everyone ought to watch immediately: did u try go-cdgo has job, stage and task layerswill take a look!quick question, does anyone have any recommendations for a web cam that is good to use with something like opencv? you should be able to connect to any cam connected to your computer with opencv (afaik). anything particular you need from one?yay!on another note: internet, you\\'re grounded!y2.016k bug.hi all. i\\'m going to be arriving back in the us soon , moving from australia. can anyone recommend an isp? and which modality (adsl vs cable) i should best consider?ddeloss: you could probably use an arima model where each measurement will influence the final result dependent on some parameter and how long ago was the measurement taken (akin to an exponential decay with a certain half-life). that way you can sum up all the time-related information of one variable into one quantity and use a simple logistic regression afterwards.created  in case anyone wants to share tips and materials on back-end architectures for ad-serving, chat about targeting, audience-building, yield optimization, etcthis is interesting:and here is a related  nytimes article, “what a million syllabuses can teach us”: i also stumbled across plotly’s dash () and dashboards () a week ago and they didn’t seem bad ( limits the number of private plots though unless you’re a paying customer…depending on the balance of engineering time and desire for interactivity, it might be cost effective). but haven’t used them myself eithertoo bad the data from the open syllabus project is so limited. :disappointed: cute for playing around with their dashboard, much less useful in general though.i am working on a project where i need to determine the utc time of nfl plays. i have the short text description of each play and the time of the play on the game clock. i am trying to match each play to one of the 5 minute intervals from game start to game end. for each 5 minute interval, i have the text from several hundred tweets within the interval.so far, i can predict distinct events like halftime by using a tf-idf/sgdclassifer pipeline in scikit-learn. however, i am having trouble matching specific plays to the intervals, since the description of the plays are very similar to each other. can anyone suggest alternative methods for converting game clock time to utc time besides brute force?:  pandas has some functionality to help with this.  thanks for the info! i am able to bracket into 5 minute intervals with pandas, but i need help matching the game time to utc based on the tweet text and play text.:  can you convert the utc time to the appropriate time.i think an example of a row of data would be helpful. :wink:: hey alex.hey!so i have a list of plays in an nfl game with the time on the game clock, which does not correspond to real timeah, so like \"4th quarter, 5 minutes left\"?exactlyand i have a pandas dataframe of tweet texts with corresponding utc time bracketed into 5 minute intervals: can you show us several rows of the nfl data time stamp.: you can just copy and paste into here. uploaded a file: : yeah i can see this is very hard. do you have any reference start times for the clock and times for commercials.so your big issues is going to be that the play clock doesn\\'t match the real worldunfortunately, there is no timestamp associated with each play - only the time on the game clocki tried treating it as a classification problem and i do a good job of predicting distinct things, like halftime: yeah without a reference time to index against - and also without knowing times for commercial breaks. it seems like a hard problemyeah, i’ve been struggling with it - i might just estimateadd 2 minutes for every scoring play for commercials or something like thati guess i should have chosen the other football for my project :stuck_out_tongue:i don’t see a way to do it without knowing a lot more. injuries, time outs, the occasional power loss, etc.stay positive joshainsleyi have injuries and time outs as wellis pulling from another data source an option? or are you trying to infer this strictly from tweets?i checked other sources, but none include the actual timethe closest i got was the nfldb, which has utc time of database insertion - varies wildlysince this sounds pretty difficult to do accurately, is there something you can do to approximate? just add on times from the expected game start time?: i think going with your heuristic is the way to go. ..or getting another data source.or even figure out if it’s really necessary and if there is another way to get at your goal.i’m leaning towards just adding up the time and adding time for breaksi was just wondering if there was a more elegant way to do it: without reference times or another data source, i don’t imagine that there is….you can determine the average number of breaks for games and the average length of breaks and use that.. with some cis..use all the data to create a distribution of how long each type of play takes and then probabilistically sample from it? is that elegant? :stuck_out_tongue::  i like that idea…: good luck!!i like that too, i only have to get each scoring play within a 5 minute windowthanks for the advice!i hope it works out! good luckhi everyone ! does anyone have any experience deploying a shinyapp that contains rcharts? not in years, but have you seen this? keld_lundgaard: pricing for pro accounts is prohibitively expensive for a team of this size. something like (at least) 2-3k / month. if losing message history diminishes the value of this slack enough, we’d probably have to charge users and i think that may be a non-starter as well. the other option, that a group like this should be able to handle, is to build a searchable message archive via the slack api.sounds like an awesome de project :wink:my office uses rocket.chat, an open source slack knockoff. the downside is it needs to be self hosted, which involves some expense and maintenance. does anyone know the problem with```-bash: brew: command not found```i have a new mac and can\\'t seem to work with standard bash commandsneed to install `brew`i was doing that but this line is giving a problem, even though the directory exists: ```$ ls -l binbin/wget -&gt; ../cellar/wget/1.16.1/bin/wget```you’re just typing `ls -l /usr/local/bin` right? you’re not adding the other stuff?i was adding the other stuff as wellbut i think the error was popping because the homebrew is at 1.19.4_1 version now, and it comes prelinkedi followed the other steps and then tried installing the package i was trying by brew install ...  looks like it worked.cool!very random, but from what i understand, women entering workplace/gdp numbers are very flawed because unpaid housework isn\\'t counted as gdp but paid housework is, so if a former house wife enters the workplace, takes a person\\'s job, and the person becomes a paid houseworker, then gdp has now gone up even though no additional output has been addedbut that\\'s only something that could be known if you knew the details of how gdp in this instance are calculatedanyone know of other statistics that are only seen as misleading if you dig into the methodologyi used dbscan mainly for cluster detection with the purpose of not assigning clusters to a lot of the \"noise\" in my data. it\\'s really helpful if you don\\'t know / don\\'t want to know how many clusters you\\'ll have beforehand, and also if you don\\'t want all your data points assigned to a cluster.hey all, does anyone have any good tutorial/reads on how to model rare events with historical data using statistical inference and creating simulations?you can try zero inflated models, like zero inflated poisson, or negative binomial. you can make inference and simulate from such a process using bayesian models. i can\\'t think of any specific tutorial nowgood start for me to look into, thanks tadaishi.hey! i\\'m leaving tomorrow and would like to get rid of a cheap working bike. i\\'m at the insight office (in sv)  now. anybody who might be interested please message me directly.fyi the rally positions are in sf and dc. perhaps all hiring messages should start with a square bracketed line with a 2 letter country code an optional region, a city name, and a general capacity (role). eg:&gt; [us az mountain vu - cashiers (wfh)]&gt; excitingly creativ naeme is rapidly expanding and looking for people to help process the bulk transport of large cash assets. familiarity with usps and envelopes a plus.woah i\\'m in:  interruptible map function for multiprocessinghi all, anyone here with a computer vision background? (physicist asking) looking to define feature vectors to characterize images details in binary images (black/white) while keeping the vector spec as robust as possible to geometric attacks such as rotations, scaling, translations, etc? hints?...  might helpthanks for that . will take a pick. still on the mvp phase thus building very simple features that my common sense can recognize. thinking about histograms of sizes for connected (contiguous) areas of black/white patches.  the winning entry for that is a conv net  . it probably wont be very useful carlos unless he comes up with a decoder version - carlos doesn\\'t have labels.  this is doable but tough.  however, would recommend reading the forums.  there might be good tips there  apologies should have clarified, take a look at the forums, specifically “so what were your approaches” some of them didn’t use convenets, hope it helpsdoes anyone have experience with svm handling network derived metrics (pagerank or community (louvain/walktrap derived)?i come from a more “statistical” background and am more comfortable with permutation based/ bayesian approaches to modeling networks.thanks in advance: here is a cheat map that i find helpful for choosing  ml algorithm according to the given data:anyone a master of putting nested json in to pandas? — dm, i do this for work pretty frequently: used recently with twitter feed data. you can email me @ @general: happy to connect with you on linkedin. feel free to send an invite. welcome new fellows!: thanks - it\\'s really helpful. i was looking for the map a few weeks ago, but didn\\'t know where to find it. really basic strptime question, how would i parse out year month day from 2014-06-05t15:48:00:000? the time stamp part is making it tough. thanks!!the boneheaded way is just do yourstring = 2014-06-05t15:48:00:000; year=yourstring[:4];  month = yourstring[6:7]; etc.ahhhhh! thanks!in general, i have used the parse function from the dateutil.parser package which is pretty amazing in parsing dates.`pandas.timestamp` has a very robust parser toothanks so much!```import pandas as pdt = pd.timestamp(yourstring[:-4])t.year, t.month, t.day```no problem :simple_smile:i am trying to import a string into a text datatype in a mysql database in python, i get the following error: \"unicodeencodeerror: \\'latin-1\\' codec can\\'t encode characters in position 0-5: ordinal not in range(256)\". it is due to trying to enter: \"богота\". anyone know how to fix this?hi guys, here are some cheat sheets for python, matplotlib, and pandas that i\\'ve found extremely helpful. hope this helps: has anyone played around with scraping reddit data in python using praw? 2 points: a) i think mysql is not very good with unicode/can handle it with the right config (feel free to correct if anyone knows more).b) try pythons decode () with error=\\'ignore\\' or the like to get rid of it problems in a simple, hackish way.  : you might be interested in looking at textons. a good start would be to look at the visual geometry group at oxford (many other nice little piece of code to get you from zero to hero) :wink:hey all, in our fall session of insight, we made some custom insight-themed emoji for our slack channel.  i\\'m adding them here as well.  examples::insight_left: insight :insight_right::insight_left: john :insight_right: :john::insight_left: wafa :insight_right: :wafa::insight_left: zach :insight_right: :zach:if you had other pds you\\'d like to add, the process is super simple and takes about a minute, see iderjerneck i was trying to avoid the hackish way but might just go with that for now. thanks!whoops, above was meant for does anyone have experience using the meetup api? i\\'ve posted on unicode in mysql and python before in the sql channel. if the infos not there anymore dm mea bit like kaggle competitions, but for forecasting world events and trends: does anyone know a good, automated way to get approximate latitude,longitude boundaries for a city?like an approximate square area (for example) if you can get the boundaries of cities, try the python shapely library anyone know how to integrate variables into urls you are reading into python? i\\'ve been  using requests.get(my url here) but i am using a url in which you can query so i would like to add variables within the url but am having trouble concatenating them. thanks!can you give a specific example?( ^^):  try requests.get(\\'\\' % yourvariable) # yourvariable\\'s value is substituted into where \\'%s\\' isalso : maybe you can move your questions to ds-sv-2016a?what, and deprive the rest of us of the opportunity to help? :stuck_out_tongue:but yeah,   would be more appropriatealso, check out `str.format()`. you can use named tags like:```s = \"hi, my name is {name} and i’m a {occupation}.”s.format(name=‘brad’, occupation=‘data scientist’)```: i got (very) approximate city bounding boxes from google geocoding: does this help:   (at least for the us)does anyone know if there if there is a not completely roundabout way of getting postgressql running on an aws linux instance?`sudo apt-get install postgresql`?fair enough, they just had a quite involved white paper about the whole thing, that i probably need to just pick through a bitbut thank you , that’ll probably take me 90% of the wayand apologies for the sillinessjust spin up an ubuntu ec2 instance and 3-4 commands later you\\'re good to goayedoes anyone know where to find granular (e.g. precinct) voting data for the 2016 election?judging by the sources listed here you have to get it from individual countiesor beg the authors of that page i had a great experience with two men and a truck.  they are on the pricier side, though.is anyone interested in a job as a data scientist at a consulting company in nyc? my former employer is looking for someone new. the work would be focused mainly on the energy and pharma industries. send me a pm if you\\'d like to know more.an old but relevant blog post by  hahaha, i was just thinking about my insight project as we were rewatching the force awakens on friday. “this movie got me a job!” :nooice:thank you for the kind shoutout!spoiler alert:  random forest works pretty well for nlp. :grinning:does anyone in the bay area have indian rupees they\\'d like to exchange with me? i\\'m heading to india soon for a trip.this came out, has some interesting things perhaps even for those that are experiencedyep! especially on the attitudinal side like surveys, interviews, diary studies, ethnographic studies, focus groups, participatory design*hot off the press*: we just starting looking for more data scientists at samba tv in sf! samba’s an awesome place to work and long-time friend of insight, so if you might be interested in working with us on gobs of tv data, dm me! hi alumni,my name is joel swenson and i’m a program director for the health data science program at insight. i would like to invite you to our san francisco office for a data science meetup featuring color genomics.food and drinks will be provided.please rsvp here () by this friday (11/10) as space is limited.hope to see some of you there!joeldomino is doing a data science management survey. chance to win $250: has anyone had experience with saving and embedding an image from pyldavis in flask?nevermind- found a solution!:  :wink:i\\'ve always thought that the last post in the thread on this comic should say, \"nevermind, got it to work!\" -- would be more true to life.agree!for anyone curious about setting up neural networks with gpus in windows i recently published a post about it.profeshcould someone help me interpret something… i’m trying to install xgboost on osx following these directions:  and have run into trouble at the last step. i think it’s because i use osx el capitan. it says: \"note: if you use osx el capitan, brew installs gcc the latest version gcc-6. so you may need to modify makefile#l46 and change gcc-5 to gcc-6. after that change gcc-5/g++-5 to gcc-6/g++-6 in make/config.mk then build using the following commands.” can someone help me interpret what that means?i think it is just asking you to edit line 46 of the makefile in the xgboost directorythat might be out of date, it looks like that change has already been made in a previous commit but i’m on yosemite and had no trouble running makestay positive andrewzirmalso, what brad said :slightly_smiling_face:ok so i guess that wasn’t the reason why i’ve been having an error, but i’m going to try something else… thanks for the help!thanks mike. i saw the supervisor log file and noticed an error right before the site stopped loaded. apparently there was a problem with flask (i found this on googling the error message). updating to new flask has solved the problem right now or it seems like. i will look into your tips if i have more problems.stay positive pushkarinip.s everyone else in current session, you may want to update your flask (you may notice an update in your werkzeug installation).glad to see that solved the issue!nuggetbot animate me \\'log file\\'haha :smile: :smile:hi! i am interested!! i am at an airbnb in east palo alto and there was a shooting and cops outside my door last night…i gotta get outta here! thanks!: article on recruiting data scientists (and how we do it at sailthru):thumbsup::thumbsup::smiley_cat:thanks dvats, very interesting articleyes, now get everyone in the world to change their interview process please :simple_smile:\"working on it\" -- data scientist at  :simple_smile:re-post from  :nice! sounds like a couple of interesting discussionspreetika [11:18 am]tamr cofounder michael stonebraker won the turing award..neatsean\\'s project!hahgood read. insight gets mentioned 2x!! also, interesting point about code and data sets being considered in academic advancement. at o’reilly jupytercon in nyc last week, there was much discussion about how jupyter notebooks will help with curating and publishing such code / data. (if you have a subscription to safaribooksonline, you can access the conference talks in about 2 weeks.)anyone work with ads and dealing w gdpr? i know you\\'re excited to talk with me about it\\'s impact on the ability to personalize advertising for a better user experience if anyone’s interested in entering product management, one of my friends is giving a talk about being a pm for linkedin not sure, but  _might_ be using / have used gcp dataproc feel free to dm me. work as a data scientist in a prod capacity at health tech startup.thanks  and  ! will dm.. thanks  for the cartodb suggestion! :simple_smile:yes, cartodb is wonderfulhappy new year! hey all, i have a good friend in the bay area (and who’d like to stay here) who’s been doing webdev/ui/ux/design stuff for a few different companies for about a decade now and she’s really not pleased with her current company. she’s also recently gotten interested in and a little experience with doing some analytics (usage stats, etc.) related to website designs and stuff like that. if anyone knows of any possible positions that she might be good for, please dm me. thanks!airbnb seems to have a great data science on boarding education. lovely! they also just open-sourced their data exploration + viz tool: ai and the competition for talent in silicon valley uploaded a file:  and commented: really great piece of research, just the best, reallydoes anyone know how to export hive query results to parquet format?i have used pyarrow for sql to parquet. might be worth a try. thanks dr neff.how about this:  create table a stored as parquet as [sql].  then pull the file off hadoop fs.  you can use spark to do this :slightly_smiling_face:that\\'s amazing seiji, thanks for sharing! uploaded a file:  and commented: my company gac r&amp;d has an opening for a research position oriented on ai, experience in robotics or control desired but not required, in the automobile industry (domain knowledge not required), see the post here commented on ’s file : good ideas! thanks!any one has a good way of building data dictionaries for their databases?hire an intern?  i actually have yet to see anyone have a good data dictionary.  i always have to build these out for whatever data i have to use, and it\\'s a painful process of exporting schema and then poking at engineers and jira tickets to fill it inwhat do you document it on?just a google doc?a spreadsheet with an intro sheet describing each table and then additional tabs, one for each table.  i color-code them based on data source if the warehouse is integrating multiple sourcesahi was hoping to avoid spreadsheets but…it is the inevitable thing to do.what else did you have in mind?i feel like describing a table is easy to do in a... table :smile:ha! good point. i was thinking i guess of something more in the lines of having it being part of a general database wiki document.you can always link to it but i feel like document format for something like that would be pretty impractical to navigate around for a complex warehousei would recommend having the data dictionary live as close to code as you can, so you don\\'t have to update field descriptions in multiple places when things changequestion: i need to explain random forest to a bunch of analysts in my company. i want to convey the main idea and not get bogged down into the nitty-gritty. does anybody have a good reference for this?i always hear it explained in layman terms as \"wisdom of the crowd\".  analysts may deserve better though, they\\'re probably somewhat technical :slightly_smiling_face:i\\'d like to go further than that (but not too much)... ideally i would like to show some simple visualsi like the idea of drawing an example of a 2-d feature set + classification label, then thinking about how you can draw lines in those dimensions that would help you best separate the classesthanks , will check this outand make the connection that the lines are the splits in your decision treei.e. just start with one treethen explain random forest after that have you seen the r2d3 demo? don\\'t think it necessarily talks random forests but it\\'s good for visualizing decision tree learninggood ideas guys, i\\'ll check them out. last time we did a mini-lesson on decision trees, so that should also helpa long while ago i made this for a business client who wanted to know wtf i was doing uploaded a file: it\\'s very rudimentary but it worked pretty well, they \\'got it\\' they feltcan you elaborate on the “as close to code as you can” part?hey, i got roped into doing an technical interview for an analyst position. anyone have a analytical question they like? commented on ’s file : not sure what you mean by linear projection.  as i understand it, state-plane is based on \"flattening\" the state\\'s map so you can do point-to-point distances in 2-dimensions.  i don\\'t think it would translate easily into wgs (what google uses) commented on ’s file : thanks! i expected it was not so simple a problem. i\\'m looking more into pyproj today and will let you know how that goes. the help is much appreciated.supervisor does not an advisor make this was just open sourced. looks like it might be a cool alternative to bokehoof. outstanding name!hey fellows, i got a friend from the uk who\\'s thinking of joining the program. he was doing a phd in vancouver, but he\\'s grown rather disenchanted with it. he\\'s looking for options for visas, and i\\'m wondering if anyone here has been in a similar situation, and how they made it work. let\\'s start a thread, or reply to me privately!actually, i think that is a critical role of a supervisor. they have to have useful thoughts on the path forward.update for those curious: i basically just did an interview i would give an insight fellow but at a faster pace, covering more topics but trying to stay at a relatively high level. no whiteboarding, but she did bust out a pen and paper to start writing some equations and sketching graphs. i did delve a bit into management and recruitment style. i also did a longer real world scenario of an experiment i\\'m planning to pitch on a real product we\\'re launching. apparently she was so impressed that she talked about it in her interviews with other people. that\\'s all cool--but now i\\'ve been roped in to interview two other candidates tomorrow and monday :cry:: i saw this and thought of you. we\\'re hoping it sends people to our blog! :wink: (where, of course, our lizard people mind control rays will get then!)anyone thinking of giving it a crack?i do not have time at the moment, but if anyone decides, you can ask me for tips. i did my phd related research in helioseismology. there is a overlap with planetary seismology. is reddit famous!hey team insight, long shot: anyone working as an economist, or have economists in your team? i’m interested in what they’re doing. - recent fellow  was working as an economist at the ftc before joining insighti’ve used it, but i don’t have much brain to pick. i retrained the tensorflow object detection api for new classes. training was expensive, i whistled through $250 without even realising. since google wrote the training scripts it was easy. i can really appreciate the workflow they have set up.glassdoor has a chief economist (andrew chamberlain) and one data scientist assigned to his team. much of their work appears at i worked as an economist pre-insight :raising_hand:\\u200d♂️thanks ! that’s super useful :slightly_smiling_face: hey jesse, do you have any thoughts on econ v ds? like, is there much/any use for econ roles in industry?meh. they would cause such a hassle with security it\\'s probably worth not having them take it. i would be curious to hear the thoughts of someone with a neuroscience background on this article. boffins and luvvies eh? was this article written at hogwarts?: !: i also felt like this article needed to be translated from english into american. i don’t know what a boffin or a luvvie is either.: smells a lot like the look elsewhere effect (and related p-hacking/p-value misunderstandings) being abused: in my personal opinion i would say that article is about 80% accurate. there are some brain regions (mostly sensory/motor) that activate pretty reliably but a lot of the behavioral economics and social neuroscience research is on shakier ground. i don\\'t trust the results of a neuroscience study unless they\\'ve been replicated at least twice, and that happens very rarely. probably partially bc results aren\\'t real and partially because there\\'s little grant money and interest in replication. and of course no one wants to publish null results or replications. ugh. and  that is pretty much what i thought. thanks!stay positive pamela.clevengeri totally missed the article that started the thread  (and  and )  i just brought us full circle.  sorry about that.stay positive pamela.clevenger: have you seen similar issues with erp? i was in linguistics/cog sci and n400 and n600 are used super frequentlym/hey, i remember brianna talking about not trusting neuroscience results during the session!: i don\\'t have enough experience with erp to know, but i\\'m sure there are other people who could comment!this isn\\'t just about people not correcting for multiple comparisons, the bigger issue, which they only link to and barely mention in the article, is because of a software bug that existed in very popular fmri software. there was a  bug in the software that was used to estimate the probability of false positives, so it means even some people who were trying to account for multiple comparisons could have spurious results because the software they used had a serious bug in it....for 15 years.  i am hearing terrible news from new york. i hope all of you guys are safe and sound.  everybody here is safe!: thank you. it is getting uglier everyday.    glad to hear everyone is ok. stay safe!!anyone know about the command line split (to split a large json files into multiple files)?i\\'ve used split for json before. what\\'s your question? so when you do (for example): \"split -b 40k myfile segment\" does this mean each segment will have 40k at maximum? so if my file is 165kb will it make four 40kb files and one 5kb file? is there a way to split it into four even (having the same amount of data) segments?do each of your lines have the same amount of data or is it variable? you will need to determine if your ‘json files’ contain a single record per line of the fileif that condition is true, you can use ‘split’ to divide that file into chunks, by line (see the options for ‘split’ on how to divide a file by lines)divide/partition  yes the files did contain a single record per line so that was the perfect solution. thanks!ny! thanks for your hospitality to us boston fellows today. great to meet you and see some familiar faces too. if you\\'re ever in boston, hit us up.wardrobe question for the dudes: my husband is interviewing for a job at a startup, the role is on the business (as opposed to technical) side. would khakis and a button-up be too casual? was thinking that a suit wouldn’t fit with the culture. definitely wouldn\\'t do suit, i\\'d say+ sportcoat! (not the top part of a suit, but an actual sportcoat!)+1 for sport coat/blazerand very shiny brown shoes that match the brown of beltthat plus button down, nice high quality jeans and the right shoes have a magical ability to look professional and casual at the same timeidk if i\\'d go shiny shoes thoughthinking somewhere between tier 3 or 4chukkas are a go todepends on where the startup is locatedeast coast more formal, west coast less formaldo any of you data scientists develop your ml workflows in an object oriented programming paradigm? my code gets messy very quickly so i\\'m thinking of adopting an oop design for my projects, and i\\'d be very curious to hear about some existing paradigms (what classes and methods to use).i use scikit-learn’s pipelines[1]. they mirror the level of abstraction i would use to describe my model to someone on a whiteboard. i’d also suggest writing your code such that the data structure passed around to different transformations is _effectively_ immutable (i.e. your transformers should take in an object and output a new object instead of mutating the old one). this makes parallelizing the code easier, and, at least for me, makes the code easier to understand. [1] +1 to pipelines, or at least using them as inspiration for rolling your own classes+1 on the immutabilityhi fellows, we just launched our latest insight fellows program:devops engineeringmany insight data engineering alums are moving into the field, and we\\'re seeing a lot demand for these roles, so we\\'re going to run a dedicated fellowship starting in july in silicon valley. if you know of any engineers or systems folks who are excited about getting into devops, sre or infrastructure roles please send them our way. if you\\'re working in this field currently, please definitely reach out as well. very excited to be expanding our engineering fellowships!wow ! you guys are on a roll with launching new stuff!a :trophy: to the person who can most succinctly define devopsthe team holding up the walls and ceiling around product / ds:austin_and_his_puppy:^^ shortest definition of devopssuperherosanyone know java? can you point to good starting materials?“site/database reliability”head first java is pretty goodthanks guysyeah, that one was good\"devops is intersection of lover of cloud and hater of wake up at 3 in morning.\" allbirds, good jeans, button updevops: the people you call to redeploy your code after it breaks from changes made to the environment by devops people. :troll:me, i like apache beam\\'s abstractions; but they\\'re not necessarily oo.hi all, preetika just wrote a great blog post on how she chose here project and technologies.  check it out here::thumbsup:hi fellow fellows!! my team here at memorial sloan kettering is looking for  a comprehensive public/government database of death records. the social security death index used to be the best source but since 2011 they have stopped integrating state level records so are woe-fully incomplete. someone else suggested the ndi maintained by the cdc but the timeline for accessing that data is doesnt work for us. the data cannot be an api and has to have identifying information. any thoughts or suggestions would be great!!: ^^i would also be interested in this, , let me know what you come up withfacebook algorithmic factory in 3 parts - a neat read: hi folks.  i know a marketing guy with lots of experience in tech startups who\\'s looking for a new position.  if you know of any openings feel free to pm me.  thanks.  at fast forward labs, we are hiring one machine learning engineer and one data engineer for a short, 6 months engagement to develop a novel prototype using quite fun nlp techniques, clustering, outlier detection etc. it’s a fun opportunity to develop an entirely new product from scratch and an ideal role for folks who want to dive into new techniques to e.g. level up for the next role. we’ve got strong partners: prehype (an incubator) and “an international news organization” (i can’t say the company name but, let’s say, you’ve heard of them). here is more: send me a pm if you’re interested!we’re hiring another data engineer for the data science and machine learning team at crowdstrike:my roommate is likely to move out by november. if anyone is looking to share a 2b/2b condo in central redwood city ping me!  ^not sure if this has been talked about here or not: anyone have experience building interviews for data leadership positions (ie the person running a team)? looking to create a list of questions/discussion topics for an interview and would love some advicedoes anyone have experience with `sklearn.cross_decomposition.cca` module, especially in the ways that it relates to the `canoncorr` implementation of cca in matlab? i have some matlab code that i am trying to reproduce in python, but answers never seem to be close. thanks in advance for any pointers! i asked this question a couple weeks ago. i can give you advice about how it went for me. what is your relationship to the candidate?hey, oh awesome!that would be great! basically we have a ds team and a ba team and we are looking to hire a high- level person to both run ba as well as deal with upper management on behalf of both ba and dsdamn, looks like my question slid off the back end of yeah, insight too cheap to buy real slack :wink:yeah, sounds pretty similar to my situation. we are/were hiring a director of analytics and insights (de/ds/ba). i was interviewing for technical depth in terms of data science and analytics. what worked well was to run through a series of high level scenarios that mimic the kind of work your team handles (or that you anticipate you will handle). ask them how they would approach the problem, and basically ask them to explain the approach as if you were a business stakeholder, then a junior analyst/ds who would have to do the work, then dig a bit into their technical depth on the problem, then bring it back up to the stakeholder level--asking how to interpret some fictitious results or ask them to communicate some bad news to you (my favorite) as if you were the stakeholder.that way you get to see how they would communicate with management to receive requirements, how well they can explain things at a basic level, how deep their expertise goes, and how well they can synthesize and be diplomaticyeah that totally makes sensethis is what i was imagining doingwhat about more high-level stuff? questions that assess management style, culture fit, that type of thing - nontechnicali don\\'t have any advice in particular about that, sorryi think these are the women who will be speaking at/organizing this event, not all the women who work at the company :wink: . also, two of the eight are directors, two are data analysts, one is a data engineer, one does analytics, one does data visualization, and the others represent different roles that are integral to building data products. it doesn\\'t seem like the lack of a specific title is diagnostic here.all good manthanks for the advice on case studies, i think that’s an awesome direction to take  it seems to have decreased how big figures and fonts appear for the same size values!hooray backwards compatibility! :smile:\"the available shortcuts are currently limited to command mode, but we invite contributors to add support for edit mode!\" i would kill for a good vim implementation....+1 on the case study questions. i\\'ve had to interview someone for a position like that on my previous team (and we also designed case study questions that were almost identical to ones we were trying to solve on our team) and i\\'ve also been interviewed myself for a position like that (they too asked me case study questions)it helps both sides to understand the problems and whether the person is the right fit for it or notin terms of management style/meeting deadlines, they looped in questions like \"if you had a problem like this, what would your thought process be like? how would you set goals/deadlines for your team?\" etcyou can also just be direct and ask them about their management stylesome people will give vague answers like \"i like to follow the agile process\", but be sure to ask them to elaborateha, just read \\'s response. yup, basically sums it up, well put! :slightly_smiling_face:i think culture fit pretty much becomes apparent through their mannerisms thanks so much! :smile:all good points, definitely understand and will keep all of this in mind (we’re hiring a head of data science)hi , cape analytics is hiring data engineers!  we use machine learning and deep learning on geospatial imagery to help property insurers \"see\" the built world at scale.  if you or anyone you know is interested, feel free to ping me here or at  and i\\'ll  tell you more :slightly_smiling_face:there _is_ a vim mode (though it hurts for me to say it since emacs is da best). the edit mode uses codemirror.js, so you can just turn out the vim mode there: that feeling of disappointment when you get a slack notification and it was an @ channel :sadkeanu:  that or wondering why the hell someone from work is slacking you at this hour.  like getting one from your boss and you’re just like “i’m at a bar, i just did a shot of hot sauce, _we will deal with this in the morning_\"shot of hot sauce?like tabasco or is that a type of shot?no like literal hot sauce…i am not a clever manshot of johnnie walker would have been betterthat’s a little on the nose, don’t you think?just pulling your leg:stuck_out_tongue:but i’m glad we see eye to eyei’ll be that insight staff member and caaaasually point out that anyone on this channel can always hire a data engineer / scientist _through_ us instead of posting a job announcement directly to the network and connections we’re providing for you :slightly_smiling_face: * refreshes inbox for eager alumni emails who want more team members * * but seriously you know where to find us and obviously this is what we do *anyone know of a good online/remote masters program for data science?  i have a work colleague (who\\'s a good data engineer) who wants to do a masters focused on ds. u. illinois cs dept has an online ms in data science for $19k.  i can\\'t vouch for the program but the cs dept has a good rep.  also georgia tech has on an online ms cs and you can choose a ml or ai focus.  it is much cheaper and still a good school.  hi  arm is hiring in us for two roles: an analyst role and a data scientist. if you know of anyone that is interested, please ask them to mail me at . here are the links : , sorry to do everyone here ..i think uc berkeley has a great ds online programhey  -- please scroll up to see my message a couple posts up. this looks like a role we could help you fill via our current fellows (it appears to be entry level / new grad?) you can message anyone from our team directly when you have open roles, rather than posting widely to this network that we connected you with (including new fellows we\\'re currently working on placing).hey  .. sorry did not see your message. was in a hurry, will get in contact with team for these positions. do you want to learn about quantum computing as a general thing ? or something specific ? if general then your really can’t beat  nielsen and chuangok i am never testing anything on vanilla mnist again   nielsen and chung is good for basics like knowing what quantum gates are and such but depending on what you are interested there are huge varieties of good intros.. are you interested in annealer based algorithm or universal related algorithms? are you looking at particulars like shors grovers hhl and such.. or even more basic like techniques?   i don\\'t know what any of those things are. so probably basic.  i guess then it depends on what your interest and how much time you want to invest in it. neilson and chung is great but it is a whole textbook. if your interest is more casual there are some great posts by rigetti computing staff. i personally like ti read review papers and wikipedia when i need helpanyone here working in ds roles in canada (vancouver, toronto, montreal)?i\\'m not but i\\'m interested to hear of anything/anyone particularly in mtl or toronto (any other :maple_leaf: folk feeling like moving home lately?)friends of mine are looking to hire a data scientist in toronto. let me know if you\\'d like an introduction.canadian feeling vancouver hard here.  hi! is anyone good with debugging website issues, or know of anyone who is good with this? the web app i built for my project was working perfectly a week ago, but starting monday i started having really weird bugs with embedding instagram posts to my site, and i can’t seem to figure out what’s going on. any help would be greatly appreciated! -- you may also want to have a chat with , she\\'s got a pretty good grasp on the canada ds scene! but he rarely checks slack any pointers in the logs in the webapp?i’m currently looking through the errors with dev tools in my browser, as i’m not generating log files for the webapp i am in toronto, but i don\\'t really know any companies that are hiring atmany errors in the chrome console?hi , i’m getting errors like the following: uncaught error: minified exception occurred; use the non-minified dev environment for the full error message and additional helpful warnings.    at k ()    at ka ()    at  uncaught error: minified exception occurred; use the non-minified dev environment for the full error message and additional helpful warnings.    at e.exports ()    at v ()previously, embedding posts worked fine with the embedded html instagram’s api returned to me, but then all of a sudden this week it stopped working. you can check out the (broken) web app here: the formatting of each post is what’s not working. they’re all different sizes, and overlapping. previously they were coming in perfectly formatted yep if there\\'s a military parade on all the tv channels i\\'m going to definitely do a few searches on job sites...i can see in the dom that it\\'s assigning a fixed row-height of 26 to those \"overlapping\" bits; unsure if it\\'s inheriting that from elsewhere.looks from your github that you tried to manually loading the embed.js, but when you do that you need to get the html you have to use the omitscript parameter in the request url some of your embeds get the js from  and some get it from . see what’s different between those posts toothanks for taking a look! and yeah, i did try loading in the embed.js manually. when i did that i also used the omitscript parameter for the request url, and it didn’t seem to helpdid you get any idea about why some of the embeds link to a different js file?no, and i suspect this is the cause of my issue. i have a window open from last week, when the app was displaying properly, and it only doesn’t have any errors and lists embeds.js from within a “scripts” folder. now, my app with all the errors points to an embeds.js from an “extra scripts” folderperhaps the instagram urls look different for these two embeds?  too!has setting `set node_env=development` helped?, no, as far as i can tell, the urls look the same. i’ve been spot checking a bunch of them, and nothing stands out. , i haven’t. i’m not familiar with that, but i’m willing to give it a shot. where exactly do i set it?this would be in the shell that you started the webappso you’d have to take it down, then run `$ set node_env=development`, then bring the webapp back upit should provide you with more debug info hopefullyawesome, i’ll give that a shot!after a whole lot of frustration, i was able to get the site back up and running! , , , i just wanted to thank you so much for taking the time to offer some help! i really appreciate it!anytime!: one of the fellows from my session ended up doing something along those lines -- the startup he was working with ended up giving him anonymized text messages that were like `aaaaaaaaa. aaaaaaaaaaaaaaaa :)`, he ended up using the emojis a lotp(excitement | number of a’s)?very highimodoes anyone have experience with mobile app analytics platforms like segment, mixpanel, amplitude, mparticle, app annie, and appfigures? i\\'d like to hear your impressions of any of them.shit, i’m supposed to be using amplitude right now but haven’t played with it enoughstay positive lafonjstay positive lafonjstay positive lafonjsorry, had a little issue with the robot...stay positive i try but a robot telling me how to act...stay positive *data scientists in healthcare:* i host a weekly youtube live show on ml in healthcare and am looking for someone to interview this week. the show is on thursday from 11:30-12:30 pst (including sound set up). if you’re interesting in discussing:- your work- ml in healthcare generally- the past or future of ml in healthcare…please let me know! we’d love to have you on the show! ?has anyone attended the open data science conference? if so, would you recommend it? i’d like to attend at least one conference on the west coast this year, and was wondering if anyone had any suggestions on conferences to attend or ones to avoid. thanks!btamadio: i just went to one at boston. i don\\'t plan to go again. the one in boston was mostly targeted at business folks, with a mix of vendors and some practitioners as speakers. audience was mostly business folk new to data science and students. it was ok, some specialized talks, but most were very general, i would suggest looking at the schedule of the boston conference. dm me if you can\\'t find it. of the workshops at the conference, only the platinum pass would have been worthwhile (spark training) for me. i signed up for the gold pass before knowing what they were training on, it was on scikit learn. good refresher but not really worth the time and money.btamadio: if you can tell me more about what you\\'re looking for, i can tell you if it\\'s a good fit\\u200b or notfor what it’s worth, several in the boston insight crowd have decided to submit talks for next year at odsc. it was a little rough this year, so we want to make it better :slightly_smiling_face:hi all, due to visa issues, i am going to be leaving the us this  august. does anyone know of any companies that are hiring for remote work? also, not necessarily opposed to other countries outside of the us as long as immigration work can be done. thanks!this announcement may seem familiar to some of you! it\\'s a re-post with different start date.a colleague at dia&amp;co is looking to sublet her apartment for a few months, here are some details:a quiet and sunny furnished one-bedroom apartment is available for sublet in brooklyn (bay ridge) for 2-3 months, starting early/mid february. actual dates are flexible.dm me if interested and i\\'ll put you in touch!i’ve never specifically worked from onedrive, but when i had to transfer large files (100+ gb) i always used `rsync`hey, if there are any alumni looking for senior data engineering roles in boston, please let me (or someone at insight) know. thanks!is there a way to download all the county data sets at once on census fact finder when you\\'re looking at the demographic (age vs race) tables.you can download specific tables and variables here: and if you\\'re an r user, the tidycensus package is quick and easy yeah i\\'m using the factfinder download center uploaded a file: so for example, i get to this page: so it is making me go to each county through that dropdown to download it, i was wondering if there\\'s a way to expand the table so that it has each county as a columnhi guys, i am making mood-based playlists for indian classical music for my insight project. i am able to pull out data from spotify api and analyze them using clustering techniques, however, since my data is unlabeled, i am considering getting \"human in the loop\" as a mechanism to establish ground truth. simply put, i am asking you guys to do me a small favor and fill this survey form for me: . i would send out one form every day; each form needs you to listen to 10 songs and provide labels to each of the song wherein you rate a song on the basis of energy and positivity. i am also thinking of extending my work to give user-based recommendations for which i will ask you whether you like a song or not. send this to whoever you think would be interested in participating in this project. a big thank you to all of you in advance! :slightly_smiling_face:sounds like a really cool project!  so, are you aware you can get energy/positivity measures from the spotify api?positivity -&gt; \"valence\"yup :slightly_smiling_face: i used them for clustering my data points :slightly_smiling_face:cool, so then you\\'re trying to validate spotify\\'s measurements of those quantities, is that the idea?yes, in the context of indian classical music. i may have to come up with different set of features eventually since indian classical is quite different from genre-based western music.i see, very interesting!thank you! :slightly_smiling_face:playing around with that r library, looks promising. thanks.planning to listen later, but had a suggestion on the form - having dropdown menus with the expected response choices to choose from would standardize the inputs and could make your data much cleaner...  even though we\\'re all phds in my experience people aren\\'t super consistent with following complex instructions...that\\'s a great suggestion! i was considering that but then i will have 50 items to choose from for each song. that\\'s not very pretty :disappointed: any thoughts on how to optimize selections?i was picturing just three drop-down menus per song, one with [\"calm\", \"mildly calm\", \"neutral\", \"mildly energetic\", \"energetic\"], one with {\"dark\", \"mildly dark\", \"neutral\", \"mildly positive\", \"positive\"}, and one with {\\'like\\',\\'unlike\\'}or the \"linear scale\" response option might work tooeach song could be a \"section\" in the google form, with 3 questions in itjust from experience, if people have to type things in there will be misspellings, capitalization differences, etcchanging the format now! thanks a ton, jamie!! :slightly_smiling_face:great! glad to be of help :slightly_smiling_face:done! :slightly_smiling_face:would something like this help? just fyi the order of the questions doesn\\'t match the spotify playlistnow it will. i had asked it to shuffle before which i removedthat website was where i took the screenshot from. :slightly_smiling_face:hi guys, i\\'m working on extracting features from blood pressure waveform signals and performing classification (e.g. diseased vs. healthy) based on that and i wonder if anyone has ideas about what features might work best for the classification task. i\\'m thinking about morphological features and frequency domain-related features. please let me know if you have any insights. thanks!what’s the pm track?hi all, i was wondering if any past fellows have worked with satellite images? i have a couple of “best practices” questions as well as questions about available data that i was hoping to askinsight started a project manager track for “ds ai ml” stuffjust when you though they couldn’t fit anyone else into palo alto :wink:interesting. i guess i assumed that woulda been announced somewhere… has all the details... it\\'s kind of a pilot.yeah, i only learned through the grape vine; it think it’s very much a betahappy to chat with anyone who is interested - just message me.^i’d be curious in the executive summary, personally!hello current and past fellows. would anyone who\\'s worked in the ad-tech space be able to have a quick chat? i\\'m looking to verify that my project idea is a practical use-case. thank you!this is so importantplease let us hire them ;)  we can talk tomorrow too i have, briefly.  did a project using satellite imagery.thanks. i spent two years at the oracle data cloud ad-tech firm, so let me know if you want to chati\\'m technically in an ad tech space and curious to hear about your project!not sure if this talk would be beneficial or not (in terms of available data): thank you all for your quick replies.  and  i\\'m currently part of the nyc session. i\\'m open to chatting over any platform  (i.e., slack chat or skype call) that works best and can accommodate your schedule. thank you again.awesome, thank you all!:  we just contracted domo for a few use cases for internal clients. their pitch is flexible aggregation, storage and great visualization. i am talking to then about options for hosting code on their platform which they don\\'t support currently. unless you have a small dataset to download from the api and compute on locally and push back results, it becomes impractical very quickly. they are open to supporting a few use cases, and are currently doing so for julia. python is still not supported.anyone know a good reference for statistical testing?i found these notes to be useful: i’d like to share interactive matplotlib figures with business people. does any one have suggestions on mpld3 v. bokeh v. something else?never used mpld3, but big fan of bokehi just learned tableau via . you can very easily create interactive graphics via data you load into it. it\\'s all click and drag, no coding. udemy thanks guys!i’m getting errors in converting my complicated figures to bokeh (“unboundlocalerror: local variable ‘axis_l\\' referenced before assignment”), so i’ll see if mpld3 just workstableau’s good for simple plots but i find that if you want to customize it much then you have to do weird hacky thingscant stress enough how valuable this post is, especially if you are in the startup world.  shared a file:  commented on ’s file : this better not be a rick roll: well there is only one way to check :trollface:aww. only snow on sunday...anyone has got access to windows event logs - dataset and is in a position to easily share it? could you let me know? thank you.naive regression question, has anyone ever used lowess regression to predict future values? if so any tips on implementing that in python?:  (though statsmodels is notoriously buggy, so if the module doesn’t work out of the box, i’d google other python solutions…chances are someone else has implemented it)how important is it that we show error bars or p-values in our figures since we are working with less than ideal data especially if large (yet significant) error bars take away from the aesthetic presentation of the data?: as all things in life, it depends: this seems like a philosophical question of fundamental importance.  i’m with keats on this one, for what little my opinion is worth.  \"beauty is truth, truth beauty\"hello, is there anyone who has successfully gotten a bokeh server working on an aws ec2 instance? i can get it working locally to produce interactive plots on my site but am having trouble once deploying it to aws.anyone at o’reilly strata london this week?anyone have any favorite resources for learning scala, most directly related to data pipelining jobs?scala cookbook!!has anyone had to use mongodb for analytics and dashboards? from what i understand it\\'s kind of a pain, but i\\'m curious if there are any decent resources out there for learning it. the best i\\'ve found so far is this: folks, thanks again for participating in the comp survey. results have been broken out into sheets according to seniority. i\\'d like to make this survey a somewhat regular thing, to serve as a resource for all fellows during performance review and job offer evaluation. if any of you have feedback on the kinds of questions you\\'d like to see on the survey, clarifications on the results, or other feedback - please pm me (or better yet, start a thread so people can see the discussion).thanks again!hi everyone, i was just wondering if anyone had resources for good measurement ave attribution techniques beyond ab testing? *andwhen i google measurement analytics without ab testing i only get results about ab testing :dgraphical models? amassari: looks promising, i\\'ll take a look. thanks andrea!! when did you stop being maxos :di’ll never stop being maxos! i’m maxos on the sv16c slack descriptive stats global\\'s total comp min and max are wrong because the row order of the base comp has min and max swapped, but the cell references used to compute the total comp point to the opposite cell. also the total comp min and max (didn\\'t look at the other summary stats) are not correct as computed. the totals should be calculated per individual and then the max taken on that--not taking the sum of all respondents\\' max for each individual categorythanks for pointing that outerrors fixedi;m getting a giant black box for some of the statsmeh, if natural language generation ever gets up to a human level, software developers will be out of a job, since the expressiveness of human language is a proper superset of the expressiveness of programming languages.gac r&amp;d (the company i work at) is looking for a senior data scientist: with experience in the data engineering part at production level (with knowledge of bigdata technologies) .here is the post:please contact me if you are interestedhas anyone had problems adding maven packages to their spark runtime w/ zeppelin? i tried adding comma separated repos in the maven format to thespark.jars.packages option under the zeppelin spark interpreter, but it doesn\\'t seem to be pulling in the library at runtime even after a restart.does anyone happen to have any resource materials on how to parallelize python lambda functions consuming from kinesis firehose ?get data scientist as a role. anyone who cares will read your resume, anyone too dumb (riding the hype) or lazy(or overwhelmed) to look into it will glance over it. also this:i\\'m a data engineer though, so, that\\'s just my grain of salt.a good list of machine learning resources on hn today: went for the 15$ package :slightly_smiling_face:lots of talk around this, similarly lots of talk on the pending cookie-pocalypse coming soonholy moly! this is amazing! thanks for sharing! $15 for medoes anyone have any connection to ds/ml in fashion industry in nyc? would love to grab coffee/chat. pm me if available. thanks!macys, dia&amp;co? talk to  , toodoes anyone know if avro schemas can handle unknown fields?  i know that json can do that.  was tying to find some info on this.does anyone know if this is an accurate representation of dl research?can\\'t comment as i\\'m a developer but i know the swordfish hacking scene is a pretty accurate portrayal of my life i haven\\'t used it . i came to know about it while i was presenting my insight project at yelpdoes anyone have a clear sense of what sort of data will be \"off limits\" for targeting? it seems to be a bit opaque from what i\\'ve heard. for example i don\\'t want to target anyone based on their pii, but if i have data about a user\\'s past engagement with ads (e.g. you click on a lot of native ads in finance) it would seem strange to consider that off limits for targetingthe youtube video shows him working on ipython, noice!addictiveyou\\'re welcomewhat have you done here you goi grew up in la crosse, wi! the brewery there makes crap beer, but at least we did something right.stay positive bmregnerthat sounded pretty positive to me nuggetbot. shut your mouth.poor little nuggetbotnuggetbot is a beer egalitarian and didn\\'t appreciate my beer snobberynuggetbot, o\\'douls rocks!nothing.nah nuggetbot is a bigger beer snob than the rest of ushi guys and gals. i\\'m looking for insights into this error metric called the \\'gini coefficient\\'. its routinely used in kaggle competitions (particularly related to insurance). how does one train a machine learning algorithm differently in order to get a high gini score (as against say mse or roc)? any insights will be appreciated! more about the metric here: the tree based methods in scikit-learn minimise the gini coefficient  by default, maybe that a place to startthanks chris. the \\'gini index\\' (also called gini impurity) is used in tree based classification methods to determine what a good split is. i\\'m not sure it minimizes the overall \\'gini coefficient\\' which is a different quantity. also, i\\'m working with a regression problem.okay, i see.: the _gini_ _coefficient_ is a measure of variance of a random variable. basically, it is 0 for a constant r.v. (uniform distribution) and 1 for a delta function r.v., all other positive random variables should fall in-between. i always thought of it as an approximation to the entropy. i am not really certain how you can use it to measure the _error_ of a regression.: i agree! but since that\\'s the metric used in the kaggle competition, i wanted an insight into what kinds of error metrics would give a good gini coefficient. essentially, what i want is predicted outcomes yhat which have a \\'distribution\\' similar to the actual outcomes y. the exact values f(x) don\\'t matter, only the distribution of all the yhats does (and this distribution should be close to distribution of the y\\'s). thanks.i don’t think it’s so much the distribution of yhats that’s important as their _order_so, obviously, if every yhat was exactly the corresponding y, you’d get a perfect score. but you’d get the exact same score if every yhat were double the corresponding y. and in fact, even if every yhat were the square of the actual y.however, the extent to which you’re penalized for deviations from the perfect ordering _does_ depend on the _actual_ y values.anyway, i don’t have any real answers here, but i’ll think about it a bit. uploaded a file: : this book chapter might be helpful to your questionhi all - i’m speaking at strata+hadoop in san jose on march 15 about my insight project. i have a friend of a speaker discount to the conference - dm me and i’ll share it with you!i am speaking there on the 4th. if you guys are deciding on a place, then may be bukowski, or the pour house boylston/newbury?would love to meet the insight boston crew can bootcamp be deployed on pc hardware? i was under the impression bootcamp was just for running windows on your mac.i found this to be a pretty good speed comparison of different methods to save/load dataframes: anyone got some insight on how `feather` compares?i did some quick benchmarks a few months ago. for a 50 gb dataframe i tested, mixed categorical, string, and float data, pyarrow to parquet was the fastest (available in dask and recent pandas versions). the second fastest was feather.i think pyarrow was faster because it compresses data by default, and feather does not. i could be wrong, but i think they use the same in-memory representation.anyone have experience with facebook\\'s prophet forecasting tool? i\\'m trying to determine if the scalability aspect is as good as claimed.thanks, `parquet` is indeed a good option if given a large amount of data.good to know that `feather` compares really well to all these other methods, although it\\'s still not recommended as a permanent storage method. lasso is another approach that could help me identify interactions. to be clear, i\\'m trying to find interaction effects between features for a classification problem. the point is to both tell a story and to identify the most predictive interaction terms to add to a different model.very excited to launch insight in toronto this morning! expanding to my home town has been something i\\'d always hoped we\\'d do, and today\\'s the day. to read more about insight  in canada check out the blog post we just published, and if you know any great scientists or engineers who want to work in data in canada, definitely send them our way  anyone familiar with the hidden markov model and know of a good resource to learn quickly? my boss suggested it today and i had never heard of it before.hi  dm mehi  --i have some experience with prophet.  happy to discuss.  i found the first few versions to be buggy and crash-prone, but it\\'s gotten better.  we\\'re running it in production now to forecast many thousands of time series, each with weekly or monthly frequency.i’d love to hear any responses/suggestions you get.you don\\'t need to run bootcamp on your pc.after bootcamp boots into windows, it has a mount of special apple drivers (for windows) just for apple hardware. that\\'s squirreled away in some part of the bootcamp image.which recommends:  but ymmv because that assumes you\\'re trying to make windows the main os on your mac hardware; which is not what you really want.it begins: you’re gonna scare them off_types and quickly erases “blood for the blood god\"_ data scientists for the data science godlet\\'s save the sacrificial references for tomorrow\\'s panel, ooookaaayyyy -_-i will bring the microphone and special effects boxwill there be cartoon hammer boings and fart sounds? i really hope so...for those interested in black box inference : has anyone ever tried scraping data from sites that consistently change their css class names (i guess in an attempt to block scraping...). this site i\\'m trying to get data from seems to be generating new random class/id names every hour or so, which means i can\\'t find the info i\\'m looking for that way. any clever strategies to counter that? can you drill down from a static class or id?hi alums+, anyone work or worked with google analytics data and have any tips? particularly about how to access the raw data? thanks!thanks, i’ll check this out!does anyone know something about the deprecated method .inc(word) of nltk\\'s freqdist() function? in particular, i have heard that word_freq.inc(word) has to be replaced with word_freq[word] += 1, but what if there is a second variable (which is an integer), as in: word_degree.inc(word, degree)?your probably going to want to store degree in a separate dictionaryi.e. word_freq[\\'word\\'] += 1, word_degree[\\'word\\'] = degreeor word_degree[\\'word\\'] += degree depending on what your trying to do with degreei suggest you take a look at what freqdist() does to better understand how you should be using it:  i access google analytics data via big query fairly frequently, and it works well. that may be a premium only feature, though. i am not an expert on ga generally.hi all, does anyone have experience working with druid? i\\'m trying to access a db using pydruid, but i\\'m getting stuck almost immediately...thanks a lot!i\\'m trying to create a random subsample of data from a dataset on bigquery as follows:select * from(select rand() as random, * from [fh-bigquery:reddit_comments.2015_01] order by random)limit 1000however, i get an error saying: \"resources exceeded during query execution.\" what am i doing wrong? is there a better way to accomplish this task?has anyone run into a python blockingioerror? doing a lot of image processing and think it’s running out of memory.: if you are an r user, i highly recommend the rga package. for python, i would recommend the python version of the api  you can probably put the limit 1000 right in after order by?bigquery has issues with running queries that return a lot of rows, you can circumvent that by storing the results in your own table temporarilyso many punchlineshillarioushi all, i was asked a question about the size of our corpus needed in order to have statistical significance in our models. i\\'m not entirely sure how to do that... so would you just train models with different sizes of corpus and see at which size the performance starts to level off? this also depends on how well the data represents the domain and the type of algorithms right? does the training corpus normally has a statistically significant number?what type of models?unspecified - it was a general question posted to our sales team. in general we do binary and multicass classifciation, and regression.are you thinking size of corpus needed to train a tfidfvectorizer/word2vec or number of samples needed for classification? regardless, i would plot performance vs size and see what it looks like. if it continues to increase, you could add morei\\'m thinking samples needed for classification (which we need to determine which target to classify). so this doesn\\'t sound like applying the hypothesis test to corpus right? that\\'s the other place i\\'ve heard of \"statistically significant\" being used.an academic paper on this topic that i was reading recently that you might find relevant:this is more about classification performance rather than statistical signficance, so as  said you might just subsample and look at p-values (or whatever significance test) as a function of subsampling size and look for an asymptotei ran into that paper too. thanks.the other key stats concept to read up on and keep in mind is  \"effect size\", for example, in regression, what\\'s the strength of the relationship you\\'re hoping to detect? if the effect you\\'re trying to measure is big, then you don\\'t need as much data to confirm it exists, but the more subtle the effect you\\'re trying to test for, the more data you\\'ll need to collect.always wish my stats were stronger... thanks i will read up on it! is it worth ordering a hardcopy of this if it is based on java 5.0? i was going to make a similar comment :slightly_smiling_face: even in social science a lot of people don\\'t use/don\\'t know about effect sizes.this would help get acquainted with a bunch of the object oriented principles of java, which i found helpful.  i basically complemented this book by google searches when looking up specific syntax, which after java 8 could be a bit different w/ all the fancy functional stuff it introduced in java 8.  can’t say that’s worth getting a hard-copy (but if it’s like $20, not a bad investment for a physical copy)this is so cool. hackernews didn’t really love it, saying “why not just use the api this businesses have?” but that’s the whole point, most business do not have an api except the phone!anyway, i hate calling businesses so i hope this works as well (eventually) as they demoed)can an insight employee let me know who is the best strategic partnerships point of contact (email) for desv and whatever ya\\'ll are calling devops sv?^^ yes! david@ is working a lot on devops too, for msp it\\'s ericgreat, thanks kathy!agreed, plus businesses robocall me all day every day, time for me to return the favor!hi all,hi all, i\\'m wondering if anyone has used apis for peer2peer lending sites such as lendingclub or prosper before?: ^alsowelcome to all the new fellows!ozan, i\\'ve used the lending club api before. shoot me an email at  for more infothanks  !i’m thinking about doing a project with the angellist data, maybe we can discuss nice to meet you all today - feel free to email me if you want feedback on project ideas or if you have any questions about silicon valley data science. cheers!  if anyone interested to do project on a bitcoin blockchain please let me know. there is a trend to use the blockchain as a public ledger for storing timestamps and signatures. i am thinking about analytics on the custom data that stored in the blockchain transactions (so called op_return type of transactions):  has, recommend dropping him a line at if interested, we use azkaban + gradle here at linkedin ()though probably luigi is much faster to setup and get runningi recently found a price of registrar by tld comparison site that i think is handy.  .nyc is there too but has restrictions.happy mid-autumn festival to all!!!!supermoon + eclipse tonight!yeah! it was always cloudy in austin for the eclipses... it is happening!moonrise is in like 20 minutes i think. sounds like it\\'s pretty cool on the east coast right nowit\\'s about 50% covered now!amazing sight from the east coast !it\\'s super cool!what do people here use for serializing and persisting machine learning models? sure pickling is great for a handful of models but what about a large number of  different models that are each being loaded and trained by an end user and then saved again?i have tried yhat, azure ml, and custom deployments written by software engineers i work with. i’m not sure about how many models is large for you, but those methods can work for tens of models without too much trouble.maybe someone at a bigger company will have better advice than me. :slightly_smiling_face:not at a bigger company, but this is something i’ve been tinkering with.  i’ve been having good luck with modeling frameworks that have inbuilt serialization, especially cross-language implementations (xgboost, h2o)alternately, could try passing around pmmls?: have you tried pmml?   just wasn\\'t sure how well it worked in practice and how extendable it was to custom models i.e. models they dont already have the xml defined for. also thanks for the tip about xgboost and h2o. will check them out. i haven’t, just something that came up at the new job.  at least for out of hte box stuff i think it has apis defined for the common frameworks, sklearn, xgboost, etc.xgboost and h20 have java apis so i can dump out to something that works in our engineering backend with like no overhead, it’s nicei\\'m mostly using python 2.x for no real reason. jupyterhub looks like it might be a good enough reason to switch to 3.xhaven’t used jupyterhub yet, thanks for the link. i’m in the midst of scoping out collaborative ds tools for a growing team, is anyone in love with any of the platforms out there?i am also interested in thisas a ds team of one, for the time being, i haven’t evaluated any of the collaborative tools yet, i have looked into several bi tools (looker, chartio, zoomdata, mode analytics and tableau)there was that thing that came out recently that does everything, what’s it calledalso, i’m in the python 3.x camp as of last falli imagine there’s fairly broad interest, or that it will be a topic that comes up for many teams at some point. i’m scoping out the ecosystem and talking with reps from many of the providers - i’m glad to post what i learn.i messed around with it a little, it was janky but maybe that was a result of it just having been posted to hnyeah,  looks interesting but i haven’t taken the time for a deeper dive yetdoes this deserve it’s own channel, or would it get lost as channel 28+?stay positive tcmorannuggetbot is trying sentiment analysismy initial impression of  is that it’s a pretty nifty mashup of notebook-style collaboration and github-like version control, but it’s killer-application isn’t totally obvious.experiment: fuck you, stay positive tcmoranit worked!hahahai’m a happy unicorn, no love.do you want nuggetbot to tell you to go negative?fishing for positive reinforcement. a little atta-boy, that’s all.:simple_smile:nuggetbot: image  me atta-boyanyone at this event today/yesterday? : i assume you’ve seen this too we use python 2.x with from future import thingsml in mario!  uploaded a file:  and commented: hey there! if you or anybody you know is looking for a new opportunity in europe, brainpool - a london startup founded by a founder phd colleague - is looking for a head of machine learning. feel free to pm me for an introthis is really great, snappy thoughts from a google engineer who has seen many ml systems go into production: hi to all fellows (past/present)~ i\\'m doing a project on removing blinks from photos and i was wondering if anyone knows of any databases with photos of people with eyes open and closed (with the same people so that i can test my method)? i\\'ve found some with different people but that isn\\'t quite what i\\'m looking for. do you know of anything like this? sorry, i\\'m not up to date on available databases. cool project, though ;-) thanks so much! hopefully they can release the data :smiley:hi all! new health data science fellow. i\\'m working on a project using nlp. i\\'m still waiting on getting the data for the data i\\'m consulting for. i was wondering if anyone in the ether knew of a nice primer on doing nlp in python? thanks in advance! :slightly_smiling_face:check out `spacy`, and in particular this tutorial: `spacy` is nice, but in my experience has a pretty big learning curve. depending on what you want to do, you can also look at `nltk`, `textblob` and `gensim`.definitely a +1 for `gensim`. in fact, the tutorial i linked to uses it heavily, especially for topic modeling (lda) and word2vec. `nltk` is best for general language processing (textblob is basically a wrapper around its capabilities iirc), `gensim` is better for numerical heavy lifting with vectorized texts.  there should be a copy of the nltk book in the insight office libraryi\\'ve also used `fuzzywuzzy` for fuzzy string matchingyeah `textblob` just has a slightly nicer api than `nltk` in my opinionoh yeah, simplifies all the defaults by far.  nltk is if you need to get into the internalshas the vader sentiment analyzer built in too, doesn\\'t it?oh also  could try reviving the  channelguys has anybody experience how one can read or add the *.bacpac file (microsoft azure sql backup) to my local database using unix and linux platform i\\'ll echo what others have said recommending nltk and gensim.  nltk is kind of bloated imo so i generally restrict to tools that are also in the corenlp package.  you may also find somethings in here useful: the algorithms in the datasketch package aren\\'t really optimized for performance but it should give you enough breadth to experiment at least.  for instance, we use a variant of minhash to detect algorithmically generated text...has anyone here used the isic archive? i know staplet did a skin lesion project, but not sure on the dataseti was thinking of more focusing on the melanoma vs. benign nevus anglehi y\\'all, in case anyone is thinking about buying a new mac, i happen to have a pretty big apple giftcard that i am looking to cash in :slightly_smiling_face:whatever you do, don\\'t buy a new macbook pro. those things are trash trash trash. i\\'ve been dealing with them for 4 days since starting a new job. one died on the first day. there are no (useful) ports, the touchbar is a travesty. just..ugh.what types of models do you guys typically use for a/b/n tests for seo data with factorial designs? i was thinking of a zero inflated negative binomial regression but there don\\'t seem to be any packages in python for this. does anyone have any experience in this area, and if so can you point me in the right direction?aws emr question — i am using pyspark with python 3.4 in my config as i created a new cluster.  i am using spark streaming.  i already set my pythonhashseed in all my nodes and restarted my slaves in ec2.  as i run a simple spark-submit (not as a step, but in the console), i still get `randomness of a hash string, should be disabled in pythonhashseed` error.  any thoughts?my next plan of action is this starting a new cluster and adding the pythonhashseed in configurations.can you confirm that the worker process can “see” this variable (value)?i’m not sure how you set it, and most likely there is a description on aws, but it is possible that it was not set as systemwide env varso i think this was asked, and then fell off the edge of the world, so i\\'ll ask again:does anyone know where to find *district* level elections results for the whole us? 2016 necessary, further back a big plus! :smile:thanks  .. i was able to see it as the `hadoop` user in the `hadoop` env (confirmed it by running a simple script that hashes a string). no bueno there with my spark-submit job even if i see it.  but setting it in `hadoop-env` which is apparently different from the user `hadoop` did the trick.  i added the export var upon start of a new cluster. so happy dance for me. for now. :slightly_smiling_face:: i’m curious about beaker. let me know if you think it’s worth trying: i really like it so far - it runs on top of ipython notebook (and you can even open ipython with it), but it allows you to use multiple languages and share variables between the  languages. the other thing i really like is that it is easy to annotate your cells and collapse whole sections of the notebook.: thanks! i’ll check it out: i unfortunately don\\'t have a solution for you, but would be interested in hearing about anything you find!spent half a day on these tutes yesterday:  pretty great documentation for an initial release: did you do anything with it beyond their tutorials? i\\'m wondering how easy it is to make something new with it.as long as it’s easier than theano, i’m in.not yet, just tinkering around the edges. i\\'ll let you know what i find offline if you\\'re interestedyes, i\\'m definitely interested. thanks: can you make me a recurrent neural net real quick? i’ll wait...: i\\'m assuming that they did better than theano. hinton is in this group, right? maybe they learned from past projects. it did look easier from the brief glance, but i won\\'t know until i try something. :simple_smile:it would be fun to do something with people using this!twist bioscience in san francisco is currently looking to hire an experienced data engineer (this would be a great fit for someone looking for their second post-insight job) to help organize the data collected by their custom dna synthesis manufacturing pipeline. their product is incredibly cool and the company is growing rapidly. here\\'s the actual job posting: please contact me here or at  if you or anyone else you know is interested.if you have job opportunities you\\'d like to share with the alumni network, please email or message the insight team directly! we may also be able to connect you to data scientists and data engineers with *senior-level* skills and experience! pinned a message to this channel.&gt;4+ years experience in pharma/biotech, or experience with biology/chemistry datathat\\'s a tall oder:)what about more junior level positions? i frequently get asked about jobs at that level and i’ve posted for a few. i know they are not aimed at insight alumnae/i specifically, but they are useful connections to make.great question! i think it\\'s true that most of those positions wouldn\\'t be a great fit for insight fellows or alums. feel free to post :slightly_smiling_face:and one more question - if we are interested in a senior level position, should we contact insight? is there a specific contact person?if you are interested in a senior level position, for yourself, anybody at insight is happy to talk about career growth with you! you can email any team member from your session, or send a slack message, and they can help you get connected to the best party.looking for ds infra advicehow big of a box (or cluster) should i spin up for jupyterhub? for now, mostly python with some spark. eventually gpus for some dl stuff. a data team of ~10 including a mix of de/dshey bay area ds folks! the data science teams over at fitbit wanted us to extend an invitation to insight alum for a ds happy hour they are having at their office next thursday night. more details and registration here ^sweet! i can finally ask about my interview status since they never said anything to me!playing around with neural networkswould anyone who did the remote program be willing to chat with someone finishing their phd?hey everyone, we’re announcing that insight has officially opened applications for our first data engineering session in new york city, which will begin in september!  please spread the word to any one looking to become a data engineer on the east coast::  don’t drink and derive. cool graphs: pymc?????? help neededdoes anyone have any advice about sites the are difficult to scrape ( using  software): try a headless browser like selenium for pythoninteresting article, but worried about significance of finding and authors jump to causation (\"the more they buy, the less likely the product will succeed\"):for all the physicists in ds: a follow up to our discussion in “women in tech” panel: : rand wilcox (usc psychology) and his students have done a lot of work on this (c.f., _introduction to robust estimation and hypothesis testing_). from what i recall, they generally recommend 20% trimmed means, based on simulations that attempt to jointly minimize type 1 and type 2 error rates.20% kind of makes sense conceptually, too. standard parametric tests (e.g., t-tests) generally use all the data, which is 0% trimmed. standard non-parametric rank-based tests (e.g., wilcoxon) generally use the median, which is 50% trimmed. 20% trimmed comes close to splitting the difference, erring a bit on the side of keeping more data.hope that helps.does anyone know of a decent probability course? aside from khan academy i can\\'t find much out there that has full lectures and is reasonably in depth.hi everyone! i wanted to get feedback on a way to deal with a machine learning problem that involves text classification with multi-labels. so i have text data, and they are labeled according to certain categories; so imagine for example they are labeled as: “delivery”, “complaint”, “account”, “cancellation. ”however, some of my data has multi-labels assigned to them. in other words, we have some text that is assigned more than one label (for example, it’s about both “delivery” and “complaint”). for now, to train my classification models i have been ignoring any data that has multi-labels assigned, so that whatever i fed into the model would be “pure”- aka one label. but now i’m in a situation where a significant portion of my data has multi-labels. i was thinking of trying to treat this as a regression problem- so for each label, try to predict a score… does anyone have any feedback on this, or has anyone tried other approaches? thanks!oh and regarding the above post- i would rather not go for unsupervised or generative methods for the moment...you can do any type of multi-class classification and rather than getting back the class pull out the probability for each class (labels in this case) sorry i should have mentioned i’ve been doing that… but there isn’t a way to input multi-labels for one training sample…how about using the same corpus with both labels in the training? i have to think about the drawbacks of this. what is the final outcome of the model? multilabel systems should be able to handle appropriately formatted multilabels, like with `sklearn.preprocessing.multilabelbinarizer`.  going from that to naive bayes seems naturalassigning only one label to the output or it can be up to 4 that you mentioned? the final output should be a probability score for each label... also to clarify: one data pt can have up to 3 labels assigned to itis the issue that you want the ‘target’ of the training to be all of the labels?  essentially you can train multiple classifiers one on each label in that case, if you don’t have a ton of labels.  or use some type of matrix completion methodand if the labels carry information about each other you can pass in the other labels as features when you train a model on one of themnot sure if youll have that data in your actual test data though (e.g. if some labels may be present sometimes but others missing).so you probably don’t want to do thatthe other thing you can do is to create new labels based on the combinations that take place, like complaint&amp;cancellation can be a separate label. specially, if on the business side, complaint and cancellation alone have different meanings that two of them happening at the same time. look up multi output classifier in sklearn, or train a bunch of binary classifiers in general. i did that in keras for my ingredient recognition model. that would be a great idea, except i have a lot of labels (20ish) and not all of them are in “natural” groupingssorry to throw deep learning at this, but this naturally falls onto a neural network architecture where you want to predict a final \"layer\" of your model consisting of 1\\'s and 0\\'s mapping to the multiple labels. there\\'s a conversation on the keras github about this -  it\\'s probably overkill if your data is small, thoughyeah if you want to go the nn route it’s more suited to train on vector targets where each vector is a series of 0s or 1s for each of your labelsinterestingly, that was the first thing i did with an nn, but the vector target was series of outputs from neurons in monkey brains :smile:nns on neurons - how meta :slightly_smiling_face:thanks everyone for the input!does anyone know where i can find a shapefile for all of nyc not subdivided by boroughs etc. i just want to see if coordinates are within nyc or not and all the little bridges between boroughs do not overlay with the borough shapefile. thanks!osm has a list of ways to get various shapefiles: a few places claim to have premade ones (like ) but unclear what that covers...oooh! thanks!hope it\\'s helpful! i haven\\'t worked with city level shape file in the past. mostly country levelthat was an excellent read  and it\\'s a channel by channel setting!!yes, +1 on attention to detail for slack ui teamanyone around with some experience engineering features from / analyzing cookie data?does anyone have any experience accessing the fitbit api from the aws server?solicitation for advice on distributed modeling:i’m finally getting around to doing my first actually large size distributed model training and could use some advice. i’ve got a local xgboost model that i’d like to extend to a distributed training framework. i know xgboost can be run in a distributed fashion through spark or flink, but i also know there are other algorithm options out there that may be more conveniently built in to a distributed framework (such as spark mllib or h2o, both of which have gbm algorithms).  can anyone speak to the pros/cons between these choices? does spark mllib or h2o have a significant ease of use edge? are their gbm algorithms as good in performance as xgboost? any thought are welcome. thanks!p.s. - let’s keep responses in a “thread” to avoid channel spamdanjump: dask has an xgboost implementation. from what i\\'ve seen it\\'s pretty quick and makes distributed computing a breeze. there\\'s a pycon 2017 talk about it, it seems to have matured pretty well.quick question for insight folks: considering the limit of messages in the free version of slack and the growing number of alumni, should one consider either purchasing a paid version or getting a different program? e.g., at the moment one can\\'t see anything before mid may.joao: i know for my company, it costs like $6/user/month.  not sure of all the payment options.yeah, i was looking into the paid version and considering it\\'s around 1,000 users here, it would be around $100k a year. lots of :moneybag:slack is still great for quick pinging of folks here and asking questions.yeah, but it doesn\\'t scale well.read that gitter could be a good alternative. opinions?interesting, not aware of other options like that.what are some good practices for achieving better than random log-loss for time series data that’s bucketed by months? i can easily build a model that performs well when evaluated on the total year, but i want it to achieve good performance in each month alsonot sure it still works, but to my knowledge you can scroll infinitely far back on mobileat some point, you\\'ll hit the 10,000 messages limit, even on mobile. from this current date, that limit is around may 19th.other options are definitely being actively looked at, no updates to share as of yet though! or  may want to chime in re: h20 implementation@h2o has a really good gbm and its super comparable to xgboost. btw xgboost is implemented thru h2o too. additionally, with h2o the advantage is that you do not need to refactor any code and it runs in java. so super low level and you can directly tie it to any framework you use. you just have to write a wrapper class in java (50 lines of code) and that works wonders. h2o as a platform runs distributed - meaning you can train and test in a distributed fashion. h2o in production is an independent piece of java class not needing any external imports. this class is virtually scalable to really large sizes, we have folks in many different companies, especially ebay who run h2o on cluster sizes of 2000 - 3000 machines, and then run these production code on many machines as micro services.mattermost is a slack-clone which is open-sourced and can just be run on some aws server. its not slack, but i don’t think we actually use most of the ultra-powerful slack stuff for insight chat, especially if message backup is really the most important thing. it also supports bots.in addition to the ease and flexibility that h2o provides for taking your models to production, it has distributed implementations of other algorithms as well (glm, gbm, drf, glrm). i particularly like h2o’s elastic net with coordinate descent for finding full regularization path. i would stay away from mllib unless you need svm. please feel free to pm if you have more specific questionswhat types of data exploration plots did you include in your demos for nlp based projects? shared this great wisdom with me this eveningspeed up your sshtotally unrelated, but i’ve greatly enjoyed adding plugins to my jupyter setup (up to and including vim-mode…which is godlike)- plugin manager:- vim-mode: awlong: word frequency analysis for n-grams, pos tagging, sentence semantic visualization (check out spacy)nuggetbot: what\\'s the best heater?nuggetbot: weather nycweather for new york, ny, usacurrently: snow -3.3°c/26°ftoday: snow (3–8 cm.) throughout the day.coming week: mixed precipitation today through thursday, with temperatures rising to 7°c/45°f on sunday.eek - that sounds coldbest heater = soup:  does anyone have experience running apache airflow for etl?  ^here’s an intro to doing etl w/ spark in case that helps.happy to answer questions you might have  .just found this post from a 2015 insight fellow:  fyi as far as i\\'m aware, the lowest latency for this type of thing utilizing spark streaming coupled to ml is in the neighborhood of 100-150ms in case you are latency bound.  this definitely helpsi\\'ll dm you for some more specific qs, thanks a lot, lei was actually a fellow in my cohort (though left the program early to start at thumbtack), and i reached out to him after seeing that blog. unfortunately, his example isn’t so fully fleshed out, and his experience with this stack is mainly exclusive to thumbtack, so code examples are more limited! looking to have lunch with him soon, so hopefully i can provide any other interested parties an update one way or the other.anyone have recommendations for using influxdb and running analytics on top of it? that might be nuggetbot’s most terrifying “soon” yet not too concerned with latency now, looking to give a poc during a talk i’m giving next week, but the play server is more of a “nice to have”, though it’d be sweet. considering just exporting it as pmml and going that route if it’s easier, though it’d be annoying i couldn’t figure it out.for anyone else interested, i posted on stackoverflow: feel free to comment or upvote :simple_smile:also check out  and pfa.  much better than pmml, but implementations are still slow to eek out.we are also looking into using it. i don’t know much about it yet though…i’ve got to build out an efficient data pipeline at work, but i have no idea what i’m doing. we have a dgx1 which i have access and about 200 gigs of data. what’s the proper way to set up a sql environment and parallelized analysis environment? i’m seeing a bunch of docker stuff floating around, but i kind of need a list of ‘best-practices’ or anything to kind of boot-strap myselfany advice?if ur files on s3, use athena (which is aws version of presto)?we have a local machine that’s pretty powerful, its kind of specific to deep learning, but it also has a ton of cpu power, ram, fast hard drivethe whole data set can fit into ram, and there are 32 processor cores to work withhave u looked into mapd?no, but i will, thanks for the suggestion!how many gpu cores?8 tesla p100 gpusyeah mapd sounds like it\\'d work really wellkeep me updated if u go that route cause i am looking into set it up tooalrightythanks!the ciommunity version is free and open sourcehey  so i m in the baby steps of setting it up firsti m not running spark etl yet, more like simple s3 -&gt; csv -&gt; mysqlbut i am struggling on two fronts: (1) the webserver ui is not working as expected and (2) understanding the tech stack side of the server/workers configurationright now i just installed it on a t2.micro instance on aws and didn\\'t make any changes to the config fileit says it has 4 workers, what are those 4 workers? threads on the cpu since it\\'s a single instance?did you see this link: i am not using docker containers at the momentah, okaydo you plan to use docker containers?i’m considering that route, since the dgx-1 is dockerized out of the boxget ityeah, i’d prefer that, just for support reasons/replicabilitywell good to know that it can be donebut, i also have no idea how to use docker, so i’ve got to learn that as welllolyou got the reason i m not using docker containers yetnot that i don\\'t like them just that i don\\'t know how to properly set them upyeah…growing pains. moved from an ultra-small data regime to a slightly large data regimemine is more like moved from a super big data regime with engineers doing lots of that heavy lifting work for me to a large data regime with no engineer to help me :crying_cat_face: more fun though, lots to learnnot sure if worker is a process or a thread in airflow, though should not refer to number of instances.you can also do sql queries in airflow as sqloperators (instead of python or bash operators)thanks! i will come back to some more general qs, but for now the specific issue i am running into is thisso basically i do:airflow webserver, and then i manually start a dag by unpausing it and then pressing the play buttoni m just starting a basic dag from an example... but instead of actually running i see a few odd patterns:(1) does not create logs - if i do view log in the ui*** log file isn\\'t local.*** fetching here: http://:8793/log/hello_world/dummy_task/2017-08-16t18:19:04.412977*** failed to fetch log file from worker.*** reading remote logs...*** unsupported remote log location.(2) it\\'s in running state in the webui forever though doesn\\'t really seem to actually be running(3) task instance details on the ui shows the following messagetask instance detailsdependencies blocking task from getting scheduleddependency\\treasonunknown\\tall dependencies are met but the task instance is not running. in most cases this just means that the task will probably be scheduled soon unless:- the scheduler is down or under heavy load- this task instance already ran and had it\\'s state changed manually (e.g. cleared in the ui)if this task instance does not start soon please contact your airflow administrator for assistance.i am looking for ways to debug this babushka basicallyhave you gotten any “hello world” example in airflow working with two simple tasks executed via `airflow backfill &lt;dag&gt; -s &lt;start_date&gt; -e &lt;end_date&gt;?  if not, start simple and build your way up.it actually worked only one timethen stopped workinglet me try this backfillcheck out the goai initiative: we have been working on developing gpu enabled set of ml algos for use on dgx1. the good thing is that all mentioned products will have an open source/community version.can someone remind me where to find the electronic ds library? is it on google drive?dropboxnot seeing it. would you mind sharing it out? i tried. but it won’t let me share the link since i’m not hte owner. is there a specific book you want?whoever created aliases within yaml without an explicit option to turn them off should die! :headwall:hey boston fellows and alums! just plugging this one more time. come to our data challenge kickoff tomorrow at the cic!there are prizes for the winners! cfp for this year’s nyc dataengconf is open! please submit your talk proposals here:  looking forward to seeing what you come up with!is anyone going to jupytercon in ny?you can use domino’s discount code when you register: domino20come to  and get some advice from those of us who are there :flag-eu:what kind of problem are you working on, supervised or unsupervised?does anyone have some experience setting up a model versioning system? something for tracking production models and their performance as you role out new and retire old models?we keep all our models in an s3 bucket and have a simple folder (bucket) structure of `models/&lt;model-name&gt;/v1.0/&lt;weights files&gt;`.  you can add readme and other documentations as well.  think how open source projects maintain different releases.  i would suggest that you s=start with a simple structure to fit your needs now and modify it as you need it.s3 buckets also have auto-versioning that can be turned on. i typically version by `model/model-name/date=xxxx-xx-xx/time=xxxxxx/model` and have my code use the latest one.(i find this useful for models that are auto-updated regularly)not sure how stable it is but looks like s3 is promising, thanks alldid anyone join spotify as their first ds job after insight? i would love to connect with you!no one i’m aware of  uploaded a file:  and commented: i was given consent by the boston team to post this. sorry .can anyone suggest some dashboards solutions they’ve seen used? this would be for e.g. product teams to track day by day user behavior trends, not real time streaming or anything. i work for a small startup and am trying to identify a good solution — it would be great to talk to some users of various dashboards if you’d be willing to put me in touch! thanks! i have used plotly before, and it is good for dashboards if you want to put something together quickly (but scaling it to larger and more complex data can be tricky). there are some tutorials here: redashmetabasegoogle wave is apache wave.  wrong link  ? it was an earlier re: ok, still has nothing to do with google/apache wavethat article was written 3 years ago, but so many companies have switched to ipv6 for their networks that it seems odd that nearly everyone\\'s cell phone ip is still stuck in an address range used merely for convenience. that being said, i doubt the government even needs your ip to be in their range, with  stingray systems and programs like prism, xkeyscore, muscular,  mystic, and carnivore running i don\\'t think they need much help.brew updatewhoops, sorry - wrong window!anybody have experience building up an ab testing infrastructure and/or using one of the many services out there (i.e. optimizely?)i’m interested in hearing about details of allocation and analysis pitfalls in the wildme too!i saw daniel tunkelang give a talk at an event hosted by ffl a couple years back on this, you can see his slides here: and more context here: none of this really answers the question of which is better, but it might be a helpful place to start?more here: depends on what data you want to use to make recommendations. i just used collaborative filtering with implicit feedback for my project and relied pretty heavily on working with alum ethan and his blog posts thanks so much! super helpful tips. dan, thanks for the recommendation! i\\'ll definitely take a lookthis is great, thanks for sharing these!jupyterlab entered beta today: fyi for those in the bay area: as someone who only uses jupyter occasionally, is this trying to make t more like pycharm?hmm… i doubt that’s how the developers would see it. i think jupyter is more focused on creating and sharing interactive documents. here’s the overview: thanks very much ! uploaded a file: eszti: i haven\\'t performed mapreduce, but we use gcp quite a bit and have been utilizing the google cloud community slack to ask / answer questions. you can join here: there\\'s a #bigquery channel on the slack, so you might have better luck over there thanks !does anyone know of cool public ai companies? i\\'m thinking about investing, but i\\'m concerned about the multiples on the p/es of google, apple, and amazon.nvidiai don\\'t know of any public ones other (other than those large ones)but i work for a vc firm, so my exposure is biased towards start ups :wink:unless you have some new/contrarian info, the upside of ai might already be priced into the stock?yeah, i\\'m not sure nvidia has a lot of upside. the p/e is already pretty high and it\\'s kind of hot right now.only one way to find out!but yeah, probably too late to jump on the train.go the other way and short the ones where you smell bs the consensus could still be incorrect…  people were wrong on fb, netflix, google, amazon at some point in their history. uploaded a file: at least 2016 was a good year for somebody... has a bunch of these...amd is at 10-11 right now… in january it was at 2. not sure what the ceiling is, but it keeps going up and up  and others - i’d like to collect coverage of nips 2016 all together - has anyone got some good blog posts, etc, summarizing what everyone learned? especially want to key into the parallel sessions.i found a couple of posts: , my co-worker wrote these summaries of nips 2016:- day 0/1: - day 2: - day 3: i think they were essentially him turning his stream-of-consciousness notes into a post, so the amount of detail varies depending on how interested he appears to have been in the talk.thanks! not sure if you already saw them but ross and jeremy blogged about nips on medium:         thanks!does anyone use specialized software for model management/documentation?  something along the lines of  ?hey guys if any of you are going to ibm\\'s world of watson i\\'ll be there tuesday and wednesday at the datascience, inc booth.  would be cool to meet insight fellows if anyone is planning to go.true in physics and probably everything else:i recall converting videos to gifs and that working well in flask! the only downside is that they are autoplay iircfor a quick-and-dirty way, you could also upload the video to youtube and embed the youtube html link on your pagethanks erik and ben! i\\'m going to try a couple more things, and if they don\\'t work out, i\\'ll go with the youtube solutionone related question: does anyone know if there needs to be any changes made to nginx to work with mp4 files or other media? here, it says configurations need to be changed but it\\'s a bit unclear how: p.s. i got the .gifs working by uploading them to giphy... thanks :slightly_smiling_face:: your daughter has life figured out already!amazon kinesis firehose — this is pretty useful.from greenhouse’s ceo this morning: &gt; for those of you looking for any kind of ray of light, some hope and some steel in your spine from my brother, the wisest and kindest person i know --&gt; :trump_hair::scream_cat::crying_cat_face:stay positive joyceschanit’s hard, nuggetbot, it’s hardif anyone feels in need of a sugar pick me up this morning - insight in palo alto has lots of donuts!! come by!hey all! i have no idea what i\\'m doing picking data science meetups in nyc. any suggestions?if any one is interested in attending the dataengconf in sf next week, the organizers have offered fellows a discount with code insightvip.  also, if any one would like to attend free, they’re still looking for volunteers. i believe the deal is volunteer 1/2 the time, and you get the other 1/2 to attend talks. if you’re interested in attending, dm me and i’ll connect you to the organizer.hi all! if anyone was thinking of hiring undergraduate summer interns i work at a university with over 100 students who would love to work in data science roles (and have the skills). how is this channel different from  ?yeah i guess there isn’t a difference - i missed that ch when i was looking through. mostly just called it  to match insight’s channel for the ai sessionshas anyone worked with the amazon ml service? please dm me if you\\'re able to answer a few questions about it.thanks! are some of those internships in your specific team?not sure actually--the early career programs have their own internal recruiting avenues that match candidates to one of the many teams in each cityfor anyone how is interested in interview prep on a holiday: just come to the office, we are now 7 people here ...nuggetbot animate me old news alreadyhaha. oddly appropriatewell, i’m still a fan of the product, and joe would be a great person to work with. that’s my official responseindeed...i liked the team and it would be great to work with them and there is room for things to be done.hey all!  i\\'ve set up a channel for alumni/fellows that are attending kdd in sf this week.join  to connect with people that are attending!any text analytics consultants in the dc area interested in a nasa project to predict scientific advances?  pm me.  “predict scientific advances” for nasa sounds awesome and hilarious all at the same time!sounds like a really cool project... side comment, best way to predict sci advances is to make em...also, based on my astrophysics and data science backgrounds, the results of the text analysis for nasa is going to be “nanotech ai cubesat tardigrades on mars and titan from black holes”something about tesla too at this pointthere will also be fleeting dark matter spikesi heartily agree:  awesome and hilarious.  it has been a challenge educating the customer.  hi jon! super long time! one of the ways we handle this is a bit of a manual process. we look at how the data is spread. if the data seems to be running all around the space then higher probability for an outlier. in that case we prefer to bucket the data, and run models on data buckets than actual data itself. or sometimes we use data buckets with higher weights and less weights on the data itself. now this is no book, or a source, but it is something that i follow in practice, and kinda as folks that work with me to follow too, i understand, but it is something that i do to any cybersec and fraud algo. so mabbe this helps? thanks  !thanks  !   i will probably want to ask some follow up questions.   have you tried to run your data thru h2o? ( apologies, if it sounds like a shameless plug) h2o does have standard ways of imputing. mean, median, and mode, but it also has adjecent value fills - back fills or forward fills. this way data locality is preserved. nothing replaces manual inspection, though. no i have not but sounds intriguing.   i don\\'t think we have h2o on prem and visa is very picky about exporting data.  :slightly_smiling_face: &gt;&gt;&gt; even if you go as far as building a model to impute the values - you are not adding any real information. you only use what was available in the other features. so you fixed a technical problem, but not the fundamental modeling issue....the simplest case is missing nominal/categorical values: just make it a different value and you are done. so literally, if you feature is eye color with current values ‘brown’ and ‘green’, make the value ‘noclue’ or something and you are set. thanks.  yup dealing with categorical features is the easy part...there\\'s also advice on continuous features :slightly_smiling_face::  rubicon project is hiring for a data engineer position at the toronto office, and for a data science position at the playa vista office . shoot me an email () if you\\'re interested.re-post from  on the upcoming bold/kafka meetup at intuit - all are welcome! how ai should be used to augment people rather than replace them:anyone going to this panel about hiring data teams at ga tomorrow? inquiring minds in sv-2016b “wanna be in the room where it happens\"  :thinking_face: soon/giphy soon: it’s a bit of a trip from nyc, but if you go i’d love to take a look at your notes! :slightly_smiling_face: i second  - would love to have a peak and some knowledge!: and   will do my best to \"give us a verse, drop some knowledge” as well as include hamilton lyrics in every slack response :microphone:if anyone is familiar with bayesian inference using pymc3 or multi-level modeling in general, i have a question posted on stackoverflow that i’d love to get some insight into ()thanks! ^haha, already moved as off topic, don\\'t ever change se! :wink: (at least they didn\\'t close it!)why are they so terrible about this kind of thing? i left my comment on the question when it was still under se.hey all you people working at various companies, a request:is your company (or one you know of) *\"friendly\"* towards non-engineering majors for internships? my old department (physics) is putting together a resource for their undergrads and reached out to me for a list of such companies. i of course have no idea outside my own company! :confused: data science probably preferred, but anything you can tell me would be helpful! pm if you\\'d like, or respond here, whichever!thanks! :+1:square seems pretty open, and tends to have lots of internswait  and  ... are you saying there\\'s more to data science than being really, really ridiculously good-looking?!?: i\\'m at capital one and we have a large intern program here - i\\'m sure there is a lot of public info on this, but if you need anything else, let me know and i\\'ll try to help...: whats your exact issue? i would imagine the operation is doable with any of the lib players, opencv and pillowanyone update pip on a redhat aws instance and then get distributionnotfound errors calling for pip 6.1.1?downgrading pip seems hard (i have 8.0.2)  carlos is right. opencv had an imwrite command. make sure you use an mxnx4 nd array of uint8s. oh and open cv is bgra, not rgba. and , thanks! i was under the impression that the transparency only applies to color images (i imagine a transparent greyscale image would be a mxnx2 array), so it is necessary to convert from greyscale to color first then?human strikes back:boo humans boo! :wink:agreed, `kill all humans`stay positive, i find nuggetbot’s silence there… disturbing.you see, this is why the singularity is a bad thingstay positive dstoneproofhumans are still a valuable asset to…humans?victory song of the future nuggetbot kingdomsee now, this is what i’m talking about, nuggetboti use the free versionooops restricting access may be a tricky one... i literally just share the link but we are a small company so everyone see everything.. i found this but haven\\'t used it...gotcha. i’d have to check it out.  with no access control we likely couldn’t use it does anyone have experience using praw? for some reason i can\\'t figure out searching posts.i’ve got a little experience with it, but off the bat i should warn you that reddit’s api is rate-limited to like 2 queries/sec and can only return 1000 upstream hits per query, so using it to build an extensive dataset is nontrivialfortunately, the scraping has already been done by a helpful redditor so that might be more useful: he maintains the scraper so the dump updates monthly, but building the initial dataset in like mid-2015 apparently took 10 months of the scraper runningcan someone explain how much capability and longevity the free, open source side of plotly has? interactive plotting in it seems like a game changer, even just for eda, but i don\\'t want to get locked in somehowthe paid version is no different than the free version except for 24/7 customer support and  portal (so called plotly cloud)... what do  u mean by interactive plotting?paid version also allows you to have shared online plots that are also private to your org etc.  in terms of regular plotting, online plots can be useful since you can embed them in places.  with free version you either have to set up your own plotly server, or embed the plots in things that will automatically run the javascript using plotly’s offline mode (e.g. if you save a jupyter notebook with an interactive plot, in offline you may have to re-run the code that generated it if you open the notebook at a later time - there’s a bunch of options on how to deal with that depending on yoru use case).\"paid version also allows you to have shared online plots that are also private to your org etc.\" - if i have my own private server/portal running jupyter/dash those plotly plots are also private (until one clicks on the save and edit in cloud button)... but one can also hide that...oh cooli was just scraping the subset of a not too big subreddit so it wasn\\'t taking too long for me. that\\'s pretty awesome.hi all, i am a new fellow, and about to start my project. i have a few questions: 1- can you scrape the web if the website is loading more info as you scroll down?2- i am trying to figured out a way to remove text from an image, analyze its font (y-label) and train the model with those. later, when an image is loaded, i will recommend a font type. is there anyone who is experienced or familiar with these, and can mentor me through the project?1 is definitely possible, but probably depends on the website and what you\\'re using to scrape it. i don\\'t know if there are libraries with built-in support, but what i would do is go to the network tab of my browser\\'s developer tools, see what urls are being requested when i scroll down, try to reverse engineer a pattern, and then programmatically pull those urls (which will often be json).for 1, typically that’s done using ajax requests via some sort of javascript.  if you can figure out how the page is making the ajax request, you can make it yourself or send the right arguments to it to get more resultsanother approach is to use selenium, which lets you automate web browsersif you get really good, i\\'m going to start calling you selinium i used beatifulsoup in the past on ebay. i did not use ajax.i am not sure how to download it as a json.back in the day i used  to simulate a browsercan make page clicks, enter passwords, etcthis blog post seems to be offering a few more details on the first approach i outlined (but i didn\\'t read it that carefully):  did this project when we were fellowscool. i will look into this. i got another recommendation for selenium, but selenium tutorials are apparently requiring java knowledge which i don\\'t.i don\\'t think you should need java for selenium. i\\'ve only played with it once, but i used python.this is correct. selenium and what not are fun, but you shouldn’t need them to solve this problem. just figure out the ajax and then you can procedurally generate the urls you need and scan the results with beautifulsoup.python is what i know too can you give me more information?to clarify, you might need java installed on your machine, but you shouldn\\'t need to write any java yourselfoh, ok. thank you. :slightly_smiling_face:ddto think i’ve been avoiding cobol all this time...what is cobol? =pno punchcard are best :ppunch cards are great because then you can keep your code in a filing cabinet, without risk of code loss due to hard drive failure!stay positive alex_gudestatements about things!!!awww, i was hoping nuggetbot would respond to exclamation marks. :(fail!!! :wink:damn... too positive for nuggetbot.nuggetbot what do you have against punchcards? :(risk code loss failure !!!stay positive jeremykarnowskihaha!risk code !!!loss failure !!!stay positive jeremykarnowskifailure !!!loss !!!stay positive jeremykarnowskilossstay positive jeremykarnowskihahahanegativenope...calamity ok i\\'m done imagine dropping a stack of a few hundred punch cards on the way to the compiler...yes but no risk of code loss due to hard drive failure!!! :troll: stay positive scstarktrue that: also that\\'s why you draw a diagonal line on the side of themdiagonal line? ah yes, old school hacks! learned about that one from an aged professor i worked for. : yeah! didn\\'t prof st. john tell you about how her mom used to program with punch cards?the line is to be able to preserve order; if it was vertical or zig zagged you couldn\\'t tell! no, i don\\'t remember audrey ever mentioned anything like that...how many different diagonal lines could there be? would they be still helpful in reordering a stack of a few hundred cards scattered on the floor?i don\\'t know how many there could be... i guess enough to keep them legible. plus there are four whole sides of a punch chard. you reassemble them by ordering them in the way the diagonal line mark appears on the card. since it\\'d be at a different height on each cardi will notify the appropriate persons about this.i chatted with someone that said they used to number the cards. is that right? there was also this really great chapter in a book by feynman about making computation with punch cards more robust to errors. it was pretty neat if you\\'ve never read it.cool! i heard that the problem with numbering them was that there would be issues if you wanted to insert new cards in between great advice, thanks austin! i am coordinating dataengconf nyc happening in october. this year, dataengconf will have 4 tracks: data engineering, data science, data products, and ai. please be sure to checkout our videos on youtube () if you haven\\'t heard of us! of course there will be discounts for insight students and alum :wink:i am curious in learning more about what you want to learn in your career. if you’d weigh in on our super short survey re: data eng/sci tools and topics it will help us plan our next event!gentle reminder: rule number 1 of : do not ping @ channel except in case of imminent nuclear holocaust that we could otherwise not escapeor, of course, a reminder to clean up after yourselves in the officeahh  beating me to it!!!yes.  you pinged 1000 people across four timezones. the probability of making someone cranky is high., your are forever immortalizedthere\\'s also the option to mute notifications in using the channel setting cog in the upper right, if the alerts are fucking up your zen too muchsorry for another redshift question, tis for all who have wlm tuning experience.  so, i am digging into the types of queries that are running on redshift, and noticed that a lot of `volt_tt` type queries are run. the concern is that they are running in the `default` queue, which right now i have as concurrency 1, with 1% memory.  do you think this is the reason why querying can be slow? :ty:those should finish really fast so probably noti\\'ve worked with wlm management stuff i\\'ve found the concurency of the queue has almost no effect on performanceassuming your bottleneck is cpuno cpu on the metrics chart is only like 15%, which is way lowi have a person asking about the early deadline (august 14th) for jan 2018 class -- asking if there is a regular deadline? or how should they think of the deadline -- apply as soon as possible after that? or the class is usually full by nov? or interviews start in sept?i’ve changed our cluster type from a bunch of ds2.xlarges to fewer but same amt of storage ds2.8xlarge, and how there are some weird slow drops &amp; create table problem.wel if you main queue is 1%then you might be ram constrainedhmmm… that could be, but why do you think concurrency makes no real difference?hi all, handshake is hiring for a pretty special role in sf, our first (senior, full-time) machine learning engineer (). we’re looking for someone with experience shipping (production-level) machine learning code. i really love working here and would like to get another insight alum on our data team. if you or anyone you know might be interested, feel free to dm me!well we were cpu boundso having more or less concurrency didn\\'t change how much cpu we hadand the % you give to queues only affect ramso we had a dashboard that had a lots of queries that took a few minutes, and changing that queue concurrency to 1 or 5 or 10 didn\\'t change how long it took the entire dashboard to load:nerd_face: man, redshift is not easy to tune:ty: markoh, in the end what did you implement to make your dashboard load faster?  is it giving it more ram?nah nothingit was cpu bound, nothing we could door at least nothing i tried workedif anyone is in the dc area check this out. trevor hastie and rob tibshirani are doing a 2 day course on ml. in case anyone is wondering why we bailed on academia...does anyone know whether it\\'s possible to add a time slider to a map created in folium?anyone use the sephora mobile app and have strong opinions on new desired features? so sorry for using the \"here\". was just informed that\\'s a no no. my bad.yeah but you redeem yourself a bit for doing it to advertise women in tech. :slightly_smiling_face:  also, anyone else notice that there are only 8 women shown from netflix. does this mean there are only 8 there in these related fields?  also, none is a senior data scientist (some senior, just not with that job title).  good on them for hosting this, but also feels a little sparse given the size of the company.yeah, that\\'s bad advertising. we have lots of women in big data here. i think those are just the organizers.suit, no tie is hard to go wrong with.  maybe nice jeans with buttonup and suit jacket for the startup.thanks for the responses! i feel like menswear for interviews can be even more fraught than womenswear…title should probably read *deep learning* alchemyhaha, “here’s my lead, now where’s my gold!?”lecun was mentioned in this article but i still think his response the original talk is worth reading: damn, great response!poll: how much paid paternity leave does your company offer?someone collected information on this recently you have all the answers.#disagree.  suit without tie is always appropriate for men.also: can anyone suggest good resources for learning about supply chain management / analysis?here is the sheet we crowd sourced recently: curious what happens if someone has a second baby during maternity leave at some of these 52 week placesi was introduced to an eth-zurich phd recently who’s looking for a start in data science in europe — would any europe-based alumni be up for talking with him?my team at ableto, a behavioral healthcare provider is looking to add a data scientist at our nyc office! let me know if you\\'re interested in learning more or know someone who might be. this was hilarious: has anyone created visualizations of workflow status? i\\'m trying to make a diagram of the status of the team\\'s automated jobs and their dependencies, where e.g. any jobs that fail would show up as red, and all downstream jobs would also be red. i\\'m guessing d3 is a good way to go, but could use suggestions are there existing packages that do this ,but in a way that doesn’t quite fit your need? i think d3 would work, but didn\\'t know if there were other solutions people had used, since javascript is not my strong suiti’m imagining there have to be industry solutions for this already, in some format. what is the data source for the job tracking?it\\'s mostly managed in azkabangotcha. no experience there, but in general i’d try not-d3 first (something python/r-based), and then higher-level d3 libraries that don’t require as much js if possible. there are a few libraries around that also let you create d3 visualizations via python, which could be a good route if your team is more comfortable therethanks!best page from wikileaks cia dump{◕ ◡ ◕}( •_•)&gt;⌐■-■hi all,i need to create a report that shows customers their individual results. i\\'ve written python scripts for the analytics and the text to go along with them, but i need a way to put them together so that they look nice and are fairly automated. can anyone suggest page design software that i can embed the scripts into? thanks!hey everyone, are any women going to grace hopper this fall and interested in being on a panel about career options? my co-worker had her data scientist panel member drop out at the last minute and the deadline is tomorrow at the end of the day. pm me if you are interested! i used their api to pull flight schedule information for my app. it was super useful!i also used it to disambiguate codeshare flightsthoughts from a phd who became a se:  uploaded a file:  and commented: i set up my phone to automatically record my door-to-door commute times. this plot shows how my morning commute has decreased after i moved from brooklyn to manhattan. uploaded a file:  and commented: my morning commutes usually happen before rush hour while my evening commutes are during rush hour. in addition, i have to pick up my kids from school/summer camp. this makes everything in the evening take longer, but there is still a reduction in commute time after i moved.tasker to record data, r and ggplot2 to make the plotsnice analysis josh! on sight it looks like there\\'s more variance for the manhattan points. why do you think that is? or is it just noise?looking back, it mostly has to do with whether or not i had to drop off or get my kids. the first three days after moving to manhattan,  was out of town and i had to take my kids to summer camp. so, the commute was longer.google is now a subsidiary of alphabet, inc.also, if you watch silicon valley, click on the period after \"drone delivery effort” :smiley:good easter egg !for anyone interested in multiple pairwise comparisons ()teait\\'s old but i only got the internet yesterday: aws in plain english : try nature machine intelligence cometh... i was about to comment \"so they can lock all the content behind a pay-wall, goes completely against how the fields progressed so far\", until i clicked in the link. i\\'m glad there\\'s a signature campaign but i also don\\'t know how effective they are. i worry that the lure of a nature publication will be irresistible for academicians (like in the rest of stem). combine that with all the research that happens behind the walls of proprietary technology companies, i think the field may be worse off in the end.thanks all! greatly appreciate the help :)to decrease the weight that these closed access journals carry government agencies that are funded by tax paper money should mandate that any and all research results supported by them should be published in open access journals. what five words would you use to describe your job? i got a kick out of this: yes, you need to enable x11forwarding in your ssh connectionhey , i did x11 trusted forwarding (ssh -y), i’ve also confirmed my vim has +keyboard and +xterm_keyboard, but still no dice. did you have to change any other settings?me too!does anyone have experience using  for event data tracking : so many crappy living spaces :disappointed: tbh i had always wondered why the bulidings looked so similar hello again smart data scientists! does anyone want to explain parameter sharing with me?in terms of? neural networks?yeahconvolution nn to be specificalright, sorry for the delay, had to reread something to make sure i remembered it correctlyso cnns have \"layers\", but they\\'re really volumes. a conv layer might have say 55x55 \"pixels\" and then something like 25 filtersso it\\'s a 55x55x25 layerso that\\'s a ton of neurons, and worse, each other is connected to a bunch of neurons in the previous layerstay positive alex_gudeso you\\'re going to have something like 55x55x25x(3x3x3 neurons in the previous layer + 1 bias) weightsnow that\\'s a lotparameter sharing reduces this by remove the 55x55 partthe assumption you\\'re making their is that the location on the imagine doesn\\'t determine how important a filter isa filter is either important, or notso we can reduce the parameters from the 55x55 \"pixels\" down to a single oneso, that\\'s the basicswas there anything more specific? :slightly_smiling_face: there isn\\'t too much more to know, but we\\'re also near the edge of my knowledgethat’s right, alex, weight sharing is important because it constrains all nodes in a convolutional layer to detect the _same_ feature across the entire image. building out many conv layers to detect different “building block” shapes is often called feature mapping.  this article explains the concept pretty well: how exactly does it \"reduce it\"?to compute your convolution over the original data set you still have to scan the filters over the original dataset right?right, it doesn\\'t reduce the computation, just the number of parameters you are using to define your model.ohbasically, if you wanted you could have every single neuron have it\\'s own value, but instead you say \"all the ones in this filter are going to share\"so why is the 55x55 ever considered a parameter?if it\\'s part of the original datasetisn\\'t it really something you can\\'t \"tune\"?55x55 is the number of \"pixels\" you have at that point (although they aren\\'t really pixels, they\\'re ... i don\\'t know what we call them, probably just neurons)i think block is the world i\\'d use in my head, hahright, they\\'re 3x3x3 blocks in your examplewell, not exactly :slightly_smiling_face:let me see if i can find a pictureso you have a layer that feeds into your current onbut you don\\'t link a neuron to every part of the previous layerjust a tiny partright, the 3x3x3 blockwell, all the possible 3x3x3 blocksof which there are 55x55 of in your example, right?let\\'s take this to pm so as not to spam general :wink:eric, that link is fantastic!yeah that\\'s a sweet book. also has a good explanation of my question, hah.i think block is appropriate. in video/image processing there\\'s a concept of a macroblockawesome discussion! so the dude who wrote that book is an aussie #justsayinhe was a mate\\'s phd supervisor #coolbyassociation?hi all, i”m a current insight fellow trying to use scrapy to get some text and image info from a website. i’m running into a problem with scrapy not being able to deal with a “#” in the url. specifically, .any suggests are greatly welcome! see if get an opportunity to talk to  or  they both worked on something similar. or mabbe even the same library.for the incoming de fellows:\"we’re in the middle of a data engineering talent shortage\" i’d consider pinging  about data needs of any kind, particularly financialthank you! very helpful. :blush:has anyone ever seen a spreadsheet integrated into a trello card? i know there is a way to attach, for example, a google spreadsheet, but this is just a link to the google spreadsheet. i was wondering if there was a way to have an embedded spreadsheet that you can alter within trello.are there any blockchain experts out there? knows a thing or two igor barinov ( ) works with blockchaindoes anyone here know of any python packages that can be used to do type ii regression? (a.k.a. model ii regression, or reduced major axis regression, and other such names)i’d rather not switch to r just for this but i can’t find a package that does it for python.blogpost about salary negotiations for engineers written by an engineer. kudos to  for bringing it up. those numbers aren’t salary, they’re median pay package (salary + bonus + equity)…wsj had an article about this a while back if you want more info.what happens when you make a neural network hallucinate? : just in case you hadn’t seen this paper:: good find, thanks for sharingi am trying to look for the panama papers data. this data was on github for a brief moment and then removed.anyone knows where else we might find it?that isn’t the whole data set. just some summaries i believe.: thanks!certainly not all of it, ashrith, it\\'s like 2tb: i thought so too. i would like to think that github data had atleast a part of the data.thank you all for your suggestions for web scraping!i really liked bayesian methods for hackers when i needed a crash course for an interview: : save that for june 1st. we\\'ll have about 80-90 new fellows joining this slack channel!nuggetbot has plenty more where that came from:simple_smile:that bot can really dig deep for those nuggets.nuggetbot: animate me welcome unicornhahahaha wtfstay positive laurenbagreed: you heard the bot. stay positive! :smile:lolol hahahwtf now tells nuggetbot to tell me to stay positive?or apparently just wtf nuggetbot!stay positive laurenbhahahahahahilarious:smile:i just ordered the book.thanks ! i like that it has a lot of information and examplesanybody has experience using highcharts on  nyc map?what do you mean by \"highcharts\"?  i\\'ve been doing some stuff with cartodb:  highcharts is a thingcurses, slack, you ruin itfor anyone interested in the upcoming dataengconf in sf on april 7-8, the organizers have given us a 20% off promo code: hakka20xhey everyone! we’re out of storage space. everyone should please go and review the files they have uploaded and delete the ones that are not needed.i think we should also think about making this premium though.amassari: cleaned out mine! how do i check what i ahve uploaded?nm, found it: it was from teh 3 dots menu on the top right for anyone trying to find iti went ahead and deleted all the game of thrones episodes i\\'ve been hosting hereare new files blocked? or are the older files simply falling off?it’s not obvious to me (i got an ambiguous message) but it seems like the new ones are threatenedanyone have any favorite resources for principles of good data visualization? looking for both books and good articles.i\\'ve run into \"there is no space, delete old files\" when trying to upload new ones.exactly but then it let me upload it anyway…confusinglast time i ran into it (months and months ago) it refused to let me upload  one of the best that i still use.  and also there\\'s an upper limit on the number of messages that is stored, around 10,000 i think, which translates into roughly 3 weeks of data (ever decreasing due to later cohorts).premium is not feasible, considering the costs of ~$8 per user per month.unless you can score a discount ;)the solution is to either set up a tool to archive past messages and then delete them, or an alternative client more appropriate for the number of users we currently have like hipchatoh no, i’ve linked the above article, not to showcase snapshot, but just how people would do kind of test their datayou can also try to query the data vol right on mysql &amp; s3, and compare to what’s loaded to redshifta simple count on all the table could work (and be done programatically)hey folks any of y\\'all at the office?  i\\'m downstairs without a key cardhey guys. what is your favorite methods for clustering word vectors (ala word2vec)?and why?clustering for what purpose? with out getting into to much detail, i have short phrase representing expenses from our customers (eg. ‘iphone subscription’, ‘magazine subscriptions’, ‘car maintenance’). using this, information  i want to identify the key categories of expenses. so given a largish list of expenses phrases, cluster them into a smaller more representative list. i’m using affinity propagation, but i wanted advice on algorithmically assessing cluster performance. it seems like the word vectors wouldn’t cluster particularly well since the vectorization would be so sparse — ie, the point similarity would either be very close (for nearly identical phrases) or basically zero for everything else for having no common tokens two quick thoughts - you could try using word movers distance to get phrase similarity and then cluster the resulting similarity matrix - gensim has a wmd implementationyou could also use tsne if your word vectors are not too much more than 100dims - though tsne is more for visualization.if the phrases are that short, i would think a simple bag of words approach might work pretty well. how many times do you see car in your corpus? how many times do you see subscription? use the top counts to set your categories. is there a particular reason you need a more complicated approach? interesting, i wasn’t familiar with wmd… that may well solve the issue with word-vector orthogonalityquestion from a current fellow here: i’m working on my web app. i have a text file of web addresses that i would like to read in and turn into clickable urls. however, my knowledge of flask/bootstrap/jquery is rudimentry and the other fellows here don’t know how to do it either. could anyone here help me read a text document using jquery?   thanks for the recommendations. for some our problem, we’ve found that a word2vec model works reasonably well for word vector generation and have decent performance (qualitatively) with clustering using affinity propagation. i’ve thought about using tsne, however i was under the impression, as well that it’s good for visualizing the clusters in 2d. (maybe 3d). i’ll look into wmd. thanks. i did not even consider using simple bag of words… i’ll think about that. thanks! is there any reason you can\\'t dump it back into python and parse it on that side then return a list of strings to the web side?: have you tried this  ?i\\'m not sure if i understand the question, but missing not at random is a common problem and i\\'m sure there is at least some info out there on how it\\'s typically dealt with. the last chapter of paul allison\\'s book \"missing data\" deals with data not missing at random.  it might be a place to start.  someone on insight slack recommended it to me a few weeks back.   that’s pretty awesome can you give a bit more details on the process you ran through nips ?kevinmercurio: i’ve not looked in to anything myself, but i wonder if multi-user jupyter notebooks exist or you could gerry-rig one?i’ve heard of jupyter hub:  . not sure if that acutally allows 2 users viewing the same notebook at the same time though. i could imagine hacking together some scripting along the lines of:1) candidate logs in to a notebook on your server2) the notebook is configured to repetitively save3) a script running on the server watches for file updates and refreshes a backup copy of the file4) you view the backup notebook and figure out how to automatically reload any changes to disknot sure if that would actually work or how to do all the steps. but surely something hacky is possible if a real product along those lines doesn’t exist alreadydid some quick googing. the following thread describes that jupyterlab is working on such a feature: and it’s possible a first version exists build on top of google drive.  i found this website:which links to this depreciated repo:which says they are now working with jupyter in this repo:which hasn’t been updated in a year. but says in a issue comment that they are working with jupyterlab in this repo:so the take home is:a) one of the above links might have a working shared jupyter notebook on top of drive solutionb) follow jupyterlab as it develops because it looks like they’re building this functionality some possible options here? let me know if you find anything that works for you. i’d be interested in using similar for pair-programming type work as welli wasn\\'t really involved in that, but as far as i know you just submit a proposal for a competition, and they review it ...sorry, that is not very helpful :stuck_out_tongue:hey - suddenly upon re-reading your question i see that you aren’t actually looking for collaborative. just a different content test. sorry…i guess you can disregard my comments. i was thinking you wanted an environment for remote pair programming tests with pandas/plotting you could always make a jupyter notebook with a set of tests/questions. i think they offer authentication well these days, so you could open a server and just give them an access account. but i imagine part of your motivation might be to save yourself the trouble of putting together content.for  me mathematician / statistician statisticiananyone in sf want to come drink wine and listen to a qa with founder of angellist today?when? definitely interested.does any one have some good helpful resources for making recommendations systems?starts at 5. at 405 howardnbc did a data dump of 200,000 tweets from russian bots. it\\'s a fun nlp project to play around with. : for the january session(s) fellows out there interviewing and considering offers. if you want to talk to someone that is the sole data scientist at their company, my slack-door is always open!hey all,  a friend of mine, who is a data scientist at jp morgan and a columbia physics phd,  is participating in  . their front-end guy dropped out and they need someone. if you\\'re interested, let me know. the ticket would be free too! this would be a great networking opportunity, and more importantly, a lot of fun.anyone else attending aws summit today in sf?so amazon is launching a “amazon machine learning” as a service ...interesting... backed by spark+mllib or another stack?\"amazon machine learning is currently available in the us-east-1 region.\"no details on the engines behind it — pretty normal for awsha ha ha — they charge by the data point and predictionindependent of which model you use?it doesn’t specify price schedules by model … they price based on the amount of time consumed and other related compute resourceshere are details from the developers guide on their “types of models” availablewow, this stuff is expensiveanyone else at the bay area deep learning school today?i tried to get in, bit didn\\'t make the cutoffit is available for live streaming as well, fyialso  channel existsdefinitely seeing more companies looking for that machine learning engineer or hybrid ds/dewhere can we add our data? though sadly n=1 for the midwest may not be terribly useful :(every company looking for the #unicornanyone else at the women in analytics conference at facebook today?? (also, plug for the  channel):plus_one: midwest! (chicago)half of my team is there!nice!!hi all, one of our favorite insight mentors derek steer (co-founder/ceo of mode analytics) is doing a quora session. he has a ton of experience and great perspective on data science and analytics throughout the tech industry. the link to ask questions is: , and he will be answering questions/chatting on 4/17 at 2pm. considering the amount of experience on this board, it would be nice if we could get more interesting questions on there than “how does mode analytics work?” :face_with_rolling_eyes:i was here and i saw you! missed you but it was an awesome conference. i hope they\\'ll do it again next year.&gt;women make up 26% of all data science positions. assume _p_ is iid generated from a prior beta distribution. what is the probability your company hires four female data scientists in a row?&gt; feel free to use `pymc`. you have 30 mins.perhaps more importantly, what fraction of them will be promoted within two years... :thinking_face:did you ever found a workaround to this? i\\'m starting a webscraping project this week and don\\'t know yet if my target sites use distil. this looks potentially helpful though: does anyone have experience with making webapps (backed by flask) with an output of a dowloadable file such as a csv?random question: who is responsible for nuggetbot? i want to make a similar one for my team so looking for pointers :slightly_smiling_face: i would start here, i believe a fellow long ago forked the repo and is running it somewhere on insight’s aws resources:awesome, thanks! since the flask logic is in python, you should be able to use basic python commands to write to a file on the file server (like pandas\\' .to_csv()), and then provide a download link in the web template. does that answer your question?yes thanks! i\\'m very close! that was my approach i\\'m pretty html illiterate so just wasn\\'t sure if i could just pass the csv as {{variable}} to my html. is that basically the same as a downloadable link? we have a fellow working on a cool slackbot this session,  . _slack police: you have the right to stay on topic._ yep! there can be weird stuff with permissions and the app designated to open something, but it should work for a csv - just put it where your html and image files are, and make an http:// link to it like you would to one of them. aren\\'t you responsible for nuggetbot?yep! it is just an instance of  running on herokufriederike: thanks!thanks!can anyone recommend a good interactive python tutorial?  i\\'m looking for something like leetcode where they give you exercises to complete on an  editor/interpreter that gives immediate feedback on syntax and correctness, but is structured as an overview of the language.  and i don\\'t want something for non-programmers that teaches what variables and for loops are.i\\'ve found the codecademy course  but it\\'s too slow paced and watered down.  i\\'m hoping for something i can knock out in an afternoon.:  its not interactive but a pretty good place to start python. : i haven’t taken it, but everyone speaks highly of google’s python tutorial.  it might start out basic but i think it ramps up quicker than the codecademy one.for anyone interested in new projects, flink is the newest general processing framework competing with spark and hadoop, which is growing in popularity (especially in europe where it’s from).  unlike spark, it uses true stream processing (similar to storm) rather than micro-batches, but also handles batch, graph, and ml processing.  it’s still new, but growing in popularity quickly!  check out  to learn more: thanks for the python book suggestion: the google python tutorial looks just about right.  i\\'ll try it out.  thanks!i had also found this other tutorial, but the google one looks better organized with fuller explanations:  :  not sure if this is what your looking for but it might help : thanks for the link.  i saw that earlier, but i\\'m looking for something more structured.i want to write blog posts on some interesting data science problems that i get to work on. i was wondering what avenues would you recommend? i was thinking of github, but unless someone actually comes to my github page, its unlikely i would get a lot of audience. any suggestions besides that?doesn\\'t insight like alumni blog posts?: &lt;&gt; insight :blue_heart: alumni blog posts!i assume that since the heart is blue, that means insight loves alumni blog posts in a chilly, passive-aggressive way.stay positive vmgehman(that was a joke, nuggetbot.):laughing:somebody fix we could always delete him… he is not self-aware (yet)yeah, kill nuggetbot!surprised that didn\\'t get a response...i bet this will work:nuggetbot is passive-aggressive.nuggetbot is chilly?huh...blue:thinking_face:assume?heart!man…  where are you, nuggetbot?maybe he auto-destructed and he was self aware…wouldn’t that violate the third law?nuggetbot animate self terminateit took me a really long time to process what i was watching there...yeah. for a moment, i was like, “where are you taking me, nuggetbot?\"also wasn’t the point of that scene that he couldn’t self-terminate so sarah connor had to do it for him?  get your shit together nuggetbotstay positive johnrwalkshitstay positive vmgehmanthat did it...for a moment there i though nuggetbot is say ‘talk positive\\':  use medium! they cross promote blogs throughout their platform. also insight is moving to it :smiley:: do you run the supervisor as root? if so, the root user will not see any local installations for the ec2-user (ubuntu). first try to “sudo su” (to switch the user to root, you will see # at the prompt instead of $ telling you are now the superuser) and try to run the needed command to start your server, make a note of the error. most likely you will need to install your needed items (python, etc.) as root...i’ve also seen arguments against universal income.  universal healthcare though, is pretty awesome afaict that article misses a big point against universal income, which has nothing to do with the cost or the hand-wringing concern about people getting lazy if they\\'re not forced to work (a bad thing??); the idea that the problems that communities of poor people have can be solved by throwing money at them seems like a capitalist fantasy -- imo the reality is that the existence of social safety nets in a broader sense is far more importantbut that\\'s just me`seize the means of production` or smththere have been a lot of studies lately showing that the most effective forms of charity are generally to just give people money and trust them. but of course all studies have biasespretty strong evidence that it works very well (this is one of the most efficient charities on this planet, in terms of lives saved or improved per dollar)(btw, these guys:  are doing good work on rigorously evaluating charities for effectiveness)cool, thanks for sharing the giving linkdoes anyone have any advice for choosing analyses or machine learning methods for open ended questions? it seems like the best method is rarely known a priori. so far the only advice i\\'ve received is to stick to any method i suggest and provide support, but occasionally it seems like they want a specific method or approach to some of these questions and i\\'m curious if there is some standard approach that offers more optimal choices.do you mean like in an interview? i would say always consider precisely what you want out of a model.  some models are more interpretable than others.  also always start simple -- a 1-day answer anywhere in the ballpark is better than a 1-week answer that\\'s correct.  maybe the best approach is to lay out all the options you can think of, then systematically evaluate their strengths and weaknesses (out loud in the case of an interview).yes, for interview questions. i\\'m pretty sure i had the same awful problem!  i\\'ve erased it from my memory but i can try to look up my inelegant work-aroundstay positive jaredg```import osfilename = os.environ.get(\\'pythonstartup\\')if filename and os.path.isfile(filename):    with open(filename) as fobj:       startup_file = fobj.read()    exec(startup_file)```i ended up doing something like this at the top of both my __init__.py and views.py filesit executes a startup file that then finds the right directory like so:```for root, dirs, files in os.walk(other_pythondir1):    sys.path.append(root)```i actually put those last 2 lines in __init__.py file as well, just for good measure.  good luck!hahaa colleague of mine has an open position for data sciences manager at proteus.  let me know if you\\'re interested and i\\'ll put you in touch!  finally. i remember someone telling me  2 years ago they were about to ipo. :stuck_out_tongue:amazon aws selected insight as one of their hot startups of july  probably because insight forces fellows to open aws accounts and then the fellows don\\'t cancel them and so aws gets that 51 cents a month for years.hey all, if anyone in the sf bay area is looking for an experienced data scientist (&amp; phd astrophysicist) who was recently chief data scientist at a cybersecurity startup, let me know cuz i might have just who you’re lookin for! thanks!!anyone have experience with retrieving historical weather data? i want simple information such as temp and precipitation but a lot of sites limit the number of api calls and don\\'t let you batch retrieve. thanks!!!!! by batch i meant enter a range of dates for a single call. merci!!: i do. how far back? what areas?hi josh! thanks so much for responding!! my general hope is to predict aspects of the coming growing season in locations people enter where they are starting or interested in starting a farm. so i\\'m not sure how far back i would have to go to make a good prediction, but for now i\\'m thinking of just focusing on hudson, ny and 10-20 years back.noaa weather historyi wrote a short script to scrape it if you’d like that.: feel free to dm me as well — i used weather data in my project if  suggestion didn’t get at what you need.you’ll need to select the weather stations you want.do any of the current fellows own or have owned bitcoins?i’m fairly certain fcc complaints can be accessed electronically, but all i’m finding at the moment is a weekly csv summary file.  any leads on where to look?(looking for full text of complaints + date/time, etc, etc.): ask :  yes that sounds a bit like  \\'s project. you might ask her tooi used noaa weather histories as  recommends.  from what i could tell it is the richest free source of data.  kinda a pain to transform the flat fixed width files, sometimes sparse with precipitation, but not bad.i\\'ve heard noaa also provides spline interpolated data down to 5mi x 5mi at 1 hour intervals if you want to avoid figuring out attribution to a particular weather station.  never used it though.there is also a commercial airport weather data set that will give you basically global coverage but since it\\'s based on airports (which move around and don\\'t conform to international standards) it can be messy to work with.here is 30yr averaged data from noaa if that is useful: noaa density of weather stations mapped on pretty well to population density from what i remember.deep learning + stealing everyones pinshi everyone! who is the expert on working with pdf files in python? i would love to talk to you for about 10 minutes.: has anyone out there successfully gained access to instagram\\'s api? it\\'s a complicated process and i keep getting errors trying to get an access token. thanks!: how did you access your instagram data?i think i actually just got one. but i haven\\'t tried using it yet...does anybody know of a place where i can get political party affiliation data? the 2016ny channel doesn\\'t seem to have gained momentum yet so i write here: you mean of lawmakers?  and  have gotten such dataalso looking for possible data: does anyone know if there are public data sources of deleted twitter accounts / tweets, blocked twitter accounts, or other similar info?  (besides monitoring twitter for a week to see what gets deleted, of course.): no.  all i need is the percent (registered) democrats and republicans at the county level.hi everyone! does anybody have experience with text extracting from pdf? : try \\'pdftotext\\'     see:  : i\\'ve only tried unsuccessfully so far (a few years ago), but i\\'ll probably need to figure this out myself. there are at least a few python tools out there (pypdf2, slate,...): sunlight foundation is probably what you want.  their api is pretty good and they have lots of python tools., i\\'ve done it, as far as i remember if the pdf is a \\'text\\' pdf, pdftotext (in what used to be called poppler-lib, i think) works. if not, you can use tesseract to do ocr on them, then clean it up. beware that this works ok with text, but not tables.ahh,  had already answered that, and with a link... sorry!it\\'s looking to me like pdfminer3k is the only text extraction module for python 3 (which make it worth sticking with python 2). if you want to talk to someone that worked on this at sunlight foundation (  ) suggestion, lmk, i have a friend that did this work there.   thank you guys :) the pdftotext works well.that looks awesome don\\'t it though? i\\'m thinking about volunteering.i already sent them an email :simple_smile:woah, look at you!nuggetbot: animate me enthusiastic volunteerperfectthat\\'s exactly how i felthair in my face. fighting some storm trooper looking guys.low tech storm trooperhunger games storm troopersweek 3 demo day here at insight right now! anyone have any memories/words of support to share with the current fellows?don\\'t read your slideslife event: first pizza since insightinteresting podcast on terrible interview experiences, with joel grus. would be curious to hear more about terrible interviews from other insight alums. :sweat_smile:hi there! demandbase is hosting the next bay area nlp meetup on wed., june 21st, and we\\'re looking for a speaker. i know the timing is short, but i\\'m hoping you or someone you know already has a presentation from another meetup or conference. the group is very laid back and typically discusses \"real world\" (or at-scale) applications of nlp, rather than overly technical/academic aspects. any data science topic will do, even if it\\'s only tangentially related to nlp. another speaker fell through -- i may give a short \"explainer\" talk on word2vec but we\\'re hoping to pad it out with another presentation. there will be food, drinks, and networking opportunities, please send a dm if you can present :wink:anyone want to see dragon boat festival next weekend (june 10-11th)?   \"if you are thinking only about communication as having the perfect message regardless of how it lands on your audience, then you\\'re likely just to be spraying information at them and not really saying something to them that sticks....that\\'s what i\\'m trying to get scientists to do — to share that excitement and passion that they have with those of us who don\\'t do that for a living.\"i’m guessing that this has come up before, but i am not be able to search in past messages. does insight have any plans on purchasing a slack pro account, so that we can see past messages?: please keep sharing good articles you may come across. it always awesome to read awesome stuff. thanksdoes any one know a good online course that covers time series analysis?don’t know of a fully structured course, but this online textbook is a decent introduction: : thanks.  i have worked through that website but i was wondering if there was more of a coursera type course?: at some point we could even aspire to become a newsletter!: i don’t know of any myself. material like arima and var models are usually covered in econometrics classes. you can find some video-taped lectures on youtube, but i can’t attest to the quality or whether you’d get more out of them than the oxtext uploaded a file: i love this job posting. too bad the salary is so low.: which actually brings up another question.  does anyone have any recommendations for an online course that does finance/economic focused data science?: i can recommend somethings since i\\'m going through them nowit depends on what you\\'re interested in though.  finance as a topic is pretty broad.apologies if this has been covered already, i searched and didn\\'t get any hits: do people have ways of making dashboards that they like? our top contenders right now are wagon () and looker (), but i\\'m happy to check out other things.eventually, data will be piped into an internal ui so at the moment we don\\'t want to build anything (like a webapp), we just want an existing thing that non-technical people can look at without getting confused and technical people can use without having to learn anything new.there are other tools like mode that look great, but that have to access data in our aws and that violates the security agreements we have with our customers so we want a tool that we can run internally. also, this is not for *doing* analytics--the crunching happens in hive--it\\'s just to see some charts, so the data involved is tiny.thanks!mio: shiny is a very good option; it\\'s become very robust and also has a nice shinydashboards package that makes it super easy to make dashboards in particularcf you can definitely plug r into hivethere is a python version of shiny called spyre made by folks at the next big sound, though can’t say i’ve used it:no hadley no crystay positive mpanciai have used spyre in the past and it works pretty well for quick dashboards.sweet, i\\'ll check \\'em out!thanks    awesome, this is eally helpful! :slightly_smiling_face:and going public! :tada:let\\'s talk conferences - any clear winners for data science generalists?  also, i came across this local one, anybody heard of it? i really liked a pydata conference i went to. nice mix of everything - ml, tools, engineering, visualization, deep learning, etc… and all with an extremely applied focus also gave an amazing talk there: aw shucksyou’ve revealed my bias :slightly_smiling_face:hey, i\\'ll take it!great talk ! dia &amp; co visited my session, they seemed really coolthanks!hi friends,my company @point of care, is looking for a data scientist. the position includes exciting opportunities to explore, analyze and model patient and clinician data that is powered by numerous clinician and patient companion apps. this is a chance to work with great people at a well funded, dynamic healthcare technology company. please, message me if you are interested. hey all, this is a really vague debugging question about pyspark but does anyone have any ideas why the exact same code for a spark sql query from a jupyter notebook would run 10 times faster if copy pasted into a pyspark shell?same spark context in both cases? it’s a wild guess but the spark context might be initialized to localhost in the jupyter notebookand/or just not connected to the same clusterhi all, questions around feature importance from random forest.we are building an explanatory model to find out risk drivers of employee safety. we have chosen random forest classification to fit an injury binary factor with features around work hours, training, weather, etc.. roughly speaking, i plan to report that the most important features reported from the random forest model are the drivers. in terms of libraries we have been using *randomforest*, *rfpermute* and *party* in r. i am not a statistician by training. i have read every discussion online and all library manuals, but still cannot get my head around these questions.1. the 2 most commonly used metrics are *gini impurity importance* and *permutation accuracy importance*.\\ta. which one is a better metrics in my use case?\\tb. when the ranking from these 2 metrics disagree, what should i do?2. rfpermute and party also report *p-value* for each feature,\\ta. should p-value play any role in selecting important features? why do we want to know p-value in feature importance?\\tb. what does it mean when a feature is important, but it’s p-value is high?anyone here has experience using apache arrow?pyhive doesn\\'t allow you to use passwords, do these?we\\'ve been experimenting with it mostly from the pyarrow angle.  overall i\\'ve been very happy with it but there are still some features missing that will be nice.  the team that is building it is very good.  wes mckinney is a badass.  i\\'m also excited about the plasma in-memory store but haven\\'t played with it yet.what type of server side configuration do you need to take advantage of it\\'s in mem distributed advantages? are you using it on an emr cluster?i\\'m pretty sure impyla supports kerberos.  it\\'s been a year since i looked at it though. caveat that i\\'m *not* a statistician, so if anyone else has better answers, i\\'ll happily defer to them.1. a quick search suggests that permutation accuracy is probably better for explaining individual feature contribution, but that gini impurity is more common and good for assessing general feature importance. unless the distinction is important for your application, i wouldn\\'t spend time worrying about the difference.2. i might look at how the p-value/z-score *compares* for each feature relative to others, but i wouldn\\'t assign too much meaning to it individually.nothing that complicated yet.  mostly just using it as fast serialization for pandas dataframes.  if you do some work in those directions let me know.  we\\'re mostly waiting for it to be more closely integrated with spark so we can consolidate our codebase to just pyspark.i am not a statistician either- blind leading the blind.  but p value is a measure of the strength of evidence.  so you can have a feature where there is weak evidence of a strong relationship.  or you can have strong evidence of a weak relationship. get it? feature importance measures the strength of the relationship and p-value measures the probability that the feature is significant (reject the null) under the null hypotehis.  i.e. pval = p(reject the null | h0).  i.e., how likely are you to observe a deviation from the null hypothesis under the null being true...in the lr context, one can have a beta coefficient to be very high but a pvalue to be non significantfrom what i understand, the feature importance from tree based models like random forrest is the measure of a feature predictive power. but does that mean the feature with more predictive power is more important? to me, not necessarily. i would go back to the team who is trying to make decisions based on the feature importance and ask them what really importance means. in order words, what are the actions going to be taken. it is sometimes cost/revenue related and some other times, correcting/optimizing some operational steps. once you narrow it down to the relevant features, i would suggest running a linear model with scaled features and look at the magnitude of coefficients. higher magnitude, higher impact.also with regards to this last suggestion by , the signs of those coeffs provides the direction of the effect. hi everyone, i\\'m a former fellow looking for a roommate, preferably female, interested in living in the south san francisco area. if interested, please message me. thanks!can anyone on this channel provide pointers for interviewing with the facebook infrastructure team?practice writing lots of sql.  case statements, rank, lag, lead, pivoting columns.  etc.  not that they would ask anything particularly, but you don’t want to be underprepared if they do.: thanks! i played around a lot with graph theory in my ph.d, what\\'s up?chiming in to say the touch bar is not a problem. i never use it and don\\'t even notice it\\'s theresorry i missed this message, as  already mentioned fuzzywuzzy is your friend. it is fantastic for exactly this purpose but you would have to make sure you also install the python-levenshtein package as well as it can get very time expensive quickly. feel free  to pm me if you need more detail or questions.hi! has anyone here worked with census data? or do you know anyone who has?i have, a little bit yup! polisci background, so i\\'m familiar. what q\\'s do you have? hi. i’m interested in finding population density and income level in sf neighborhoods. but cant find a geographical identifier or zipcode to find the stats for a neighborhoodall i have is pumas and i cant find the dictionary for them to know which is whichif you go to american fact finder, you can specify the dataset you want (decennial census, acs, etc), the specific variables, and the unit of analysis (state, county, zip code, etc. i think block-level is also available), which you can export to csv. there\\'s also a census api if you prefer. hi -- does anyone have experience converting lots of messy addresses to postal service standard addresses? for example, converting \"905 east 6th street\" to \"905 e 6th st\"? that is a trivial example but i have addresses that are messy in a variety of ways. i believe there is a plug-in for excel that can validate the address fields and by doing so it will populate the validated addresses (in a standard format) in additional rows - i believe it\\'s called yadress or something along those linesyou can probably also geocode them in qgis but i\\'m not sure if it will provide a set of standardized addresses as output apart from the long and lat resultsthis table has maps of the puma codes by county: thanks -- this looks like it will do what i need. i got a vba error though (compile error:can\\'t find project or library(yaddressform 41:8)census tracts and census blocks are smaller geographical units that i and others ( and ) have used in insight projects, if those buzzwords might help…carola: hmmm perhaps try googling the error to see if anyone has had the same issue? i\\'ve actually never used that plug-in (just remembered it existed). sorry!no worries, of course i googled it already. thanks for the suggestion!thanks so much! i think i’ll focus on tracts and blocks. i’ll just need to find the lat and long for the center of each tracti believe usps has a web api for this: i don\\'t know how much mess it can clean up thoughthe usps api is decent, there\\'s also the `usaddress` package on github that does essentially pos tagging and parsing on addresses.for small dataset, you can also look into using google’s geo api oh, and if you\\'re wanting the centroid of each tract/block, the census also provides shapefiles has anyone tried to evaluate cluster (e.g., k-means) density at the individual cluster level? i’m trying to find the clumpiest clumps--will probably use average inertia or silhouette coefficient.  any suggestions?seconded for google geocoder: have you looked into using em instead? k-means makes a lot of assumptions about your data set’s geometry that may not be often true for real-world stuff.this is a first pass at the problem, so i haven’t looked into alternatives in any detail.  i’m putting short text documents into a vector space and clustering them. the goal is to find some subset of the clusters that show strong internal similarity.as i understand it, other non-em clustering algorithms allow for more reasonable / different geometric assumptions (e.g., dbscan).  as the timeline stretches out, i expect to do  more serious topic modeling (e.g., lda), but this is my first stab.regarding finding some subset of the clusters that show strong internal similarity: i did this by looking at the similarity matrix and only selecting  words/vectors with strong similarity for clustering.as a preprocessing step were you selecting features or observations w/ strong similarity?i see: you picked only observations that were close to at least one other observation?i was selecting features, i.e words with strong similarity. most clustering algorithms directly output a similarity matrix so i didn\\'t have to calculate from scratch. i think in your case it would be the text document.if you are using acs, there is a great wrapper in python to access the data: just to clarify - there is no spherical restriction on em - in fact it is k-means which is a special case for em, under spherical assumptionsi think maybe earlier was a typoas for cluster estimation in an embedded space - there is really no good reason to assume that the geometry for these high dimensional spaces is spherical for clusters - so you might get wonky results if you start with k-means. one has to be careful with high-dimensional clustering too, because in high dimensional spaces, there is a tendency for ‘everything’ to be close together.the census bureau also has a geocoder api: ..on geocoding, see also  and  .hi  in case you still need a solution, have you tried passing the addresses through the google geo api? the output in a search has a field that is the \"standardized\" (interpreted) input address. just a thought.hello fellows ! i have just moved to la for work and im very excited about the new opportunity ! i live in venice if you are close by lets connect and meet !google api is good but if i recall there\\'s a limit on how many you can call per day ... cheap to purchase more. openstreemtap is another one, but not sure if it has an api... you can use it through free gis software like qgis*openstreetmap thanks for the suggestion...i\\'ve been poring over their docs and it seems they want the query in standard usps format. i haven\\'t tried it though. have you used it?agreed  - my accuracy was shockingly bad in that quizhappy :turkey:anyone knows the easiest way to setup mongodb on ec2?can you install it via apt-get?looks like that works and it starts the process as well```sudo apt-get install mongodb```i\\'m running ec2 amazon linux, it\\'s redhat... yeah i should probably start a new instance with ubuntu, ubuntu seems more friendly to me....ah, okay. before jumping to a new instance you could try using `yum` but i\\'m not familiar with its usagethanks!another questions: does anyone know how to use ssl on ec2?my chrome extension project needs to send https request to my server on ec2or how do i get a valid ssl? glad you figured it out -- as an alternate approach, i ran into a similar issue trying to pass dicts into d3, solved it with the method here:  how does the {{var|safe}} approach work?in case anyone ever wants to share interactive plots in jupyternbs for presentation purposes without too much effort, chris said wrote a nice post recently describing a few minute pieces of javascript that’ll make your life much easier:  i wanted to pass in a list of  dictionaries with strings for keys as my variable. it gives the #39 error but if you pass in the variable in jinja2 safe mode apparently it saves the strings?!  yep that\\'s the exact issue mine had.  got around it using flask.jsonify to avoid ever explicitly handing the strings off to jinja/htmli was wondering: what sort of ds related podcasts does everyone likes to listen to? i really enjoy talking machines and o\\'riley data show, but haven\\'t heard any others that i\\'ve found to be consistently interesting / useful. hi fellas, this may help for interview prep: bit twiddling hacks : : another couple i enjoy are talk python to me (strictly about the python ecosystem but covers ds-related topics frequently) and partially derivative (more fun than useful)a friend\\'s company just released this, it a linux desktop in the cloud built for machine learning! basically allows you to access a supercomputer with a bunch of gpus from your browser with tons of ml packages pre-installed  what\\'s up? how can i help? just curious if you had any tips for getting started with itastrophysicists, i saw data showing that % of astro papers mentioning p-values increased  15 times in the last 10 years. does anybody know why such an explosion? anyone going to the rev conference in sf?has anyone gone through the pains of setting up an llc / s-corp and can provide some insight into going through the process?  i’m looking into it primarily for tax writeoffs.i’ve only had mine for ~1 year, but will be happy to help if i cani’ll be there!llc is your state level entity registration.  this is primarily done to limit liability instead of individual or partnership/s-corp - is a irs classification.  this  makes sense if you are running consulting business with over 200k income where you don’t want take all the money as salary and take some or most of the money as distributions that are taxed at 20% (i think).  very popular among doctor and law groups.  one caveat - you can’t raise money with s-corp.  for that you will need to become c-corp.  if you want an entity that’s going to make 50k on side business, then s-corp is too much work for it imo.:+1:anyone have any conferences or workshops that they would strongly recommend for ml and/or ai? uploaded a file:  and i met over the weekend. we were both wearing our insight shirts and for once, we didn\\'t coordinate our outfits.haha, it’s true!you heard it at insight firstmonica rogati is leaving jawbone and the rumor is she\\'s headed to twitter @mrogati\\'s tweet: : haven’t seen the talk (link?) so i’m likely missing some context, but bias is of course a major issue in data science and any quantitative endeavor involving making decisions from real-world datastay positive zkermishdepending on the industry and role, i’d think it’s often the data scientists responsibility to emphasize the effects of bias, mitigate them using best practices, and honestly report how they influence results.(in the context of ab tests...with stakeholders having preferences for results and laking the statistical knowledge to interpret them appropriately…this can be one of the biggest challenges): you clearly don’t know me at allsolution: make decisions randomlytrump 2016here it is, sorry it was from scipy: hi all! we are looking to hire an experienced data engineer. it’s a position in boston, hello. i\\'m a health fellow in boston this session. i\\'m having trouble getting gunicorn to properly access my static folder on my server. using ip-adress:5000 works perfectly, but using my url results in a site without css.the path to my static folder is: ```/home/ubuntu/webapp/my_dir/static```my index.html starts with:      ```&lt;!-- bootstrap core css --&gt;    &lt;link href=\"../static/css/bootstrap.min.css\" rel=\"stylesheet\"&gt;    &lt;!-- custom css --&gt;    &lt;link href=\"../static/css/business-casual.css\" rel=\"stylesheet\"&gt;```my nginx:```server {    location / {        proxy_pass         proxy_set_header host $host;        proxy_set_header x-real-ip $remote_addr;    }    location /static {        alias /home/ubuntu/webapp/my_dir/static/;    }}```there must be something wrong with my path, but i can\\'t seem to find it.  please let me know if you have any troubleshooting ideas. thanks in advance.just thought my folder structure may help too:simplified folder structure:```webapp    &gt; run.py    &gt; my_dir        &gt; veiws.py        &gt; __init__.py        &gt; static            &gt; css                &gt; bootstrap.css                &gt; bootstrap.min.css        &gt; templates            &gt; index.html```  sqlworkbenchj and datagrip remove the `..` in the paths. they should all start with `/static` thanks for the suggestion, but the problem still persists even with that change i’d recommend using a different method for specifying the paths for the css into your htmlyou can use the `url_for` method combined with templating, as in this answer: i don’t know if it’ll solve your problem, but might be worth giving it a shot no luck with url_for. thanks for the sugestionkeziah: can you share the url and ip?thanks for the offer richwinslow just helped me solve the problem!thanks for all the help  helped me resolve the problem. while my \"/etc/nginx/sites-enabled/application\"  file was in good shape my \"/etc/nginx/sites-enabled/application.save\" file had some typos in it.:+1:volunteer opportunity: chicktech () is seeking tas for a workshop on electronics for high school girls in sf. tas need to be able to read basic schematics and translate to parts on a breadboard. time commitment is 1 day (approx 8 hours). dm me if interested.hi all, need some insights from clustering experts here !i am trying to find clusters in a dataset that has latitude-longitude as two of the features - i found that dbscan works well for lat-lon data - but does it work for lat_lon + other features (mostly categorical)?  or should i try something else?thanks in advance for any pointers! pinned a message to this channel.want to work at tesla? what are people\\'s preferred approaches/libraries/companies for user tables with 100 or so columns (possibly linkage to other tables, at scale)? graph? relational? redshift?live from nips:  if you don’t have time to read through hundreds of papers, or watch hours of videos, we’ve summarized key highlights of day 2, check it out here! hi all, our marketing team at uber is looking for a senior data scientist with 5 years+ industry experience. please ping me if you or anyone you know is interested. :+1: thank you!!  hey @ channel (sike!), i just made  for general discussion on nns, staying up-to-date on the field, and seeking help. come one, come allnever again will i dare to @ channel :woman-gesturing-no:my team is looking to hire an experienced data scientist/manager in our san jose office.  please let me know if you are interested:  all the apis; maybe we should start a new channel for project resources?if anyone\\'s interested, this meetup is next week on cellular automata and deep learning: is anyone else doing the coursera scala course? i\\'ve created a little  channel for chats. 2016 internet trends report is out.parama: i have a few things i can come up with from my experience. 1) clustering with categorical features is tricky because your dataset is already \"clustered\" with whatever the categories it has. 2) scaling greatly affects your clustering results when you mix different units (lat-long + category), so you have to scale features in a way it \"makes sense\". 3) if two or more of your categorical features are correlated, they have significant effect on your clustering. 4) in general, unsupervised learning doesn\\'t have any measure of successfulness, so you have to come up with some type of heuristics for your use case. 5) one degree latitude and longitude don\\'t correspond to real distance. one degree longitude at a pole is 0 km long.: i have been told that dbscan in 2 d works very well on géo data, but as soon as you add a categorical feature the algo will not work. i am not sure if there are any alternatives. i wonder if you could perform the géo clustering, then add a column with the cluster identifier, and perform a second clustering minus the geo data but using the new cluster column?that\\'s how i would approach it as well.what’s a clean pythonic way of providing two constructors for a class? i know i can do some logic with kwargs, but i feel that doesn’t provide the same clarity of use that having a defined signature does?have you tried using `@classmethod`?like  said, e.g.: cool, i’ll have a look, thanks. decorators to the rescue!ah yeah, just what i was looking for. thanks.i saw it on stackoverflow but i was a bit uncertain since it was the second most popular response…sounds like a potential insight project.... is working on a chrome extension that highlights the best responses for python questions on stack overflow!sounds similar to we have a new blog post out! this one is especially for all you physicists out there!   rocks :clap:hi everyone! i am looking for some tool to plot an interactive time series, hopefully more high level than plain d3, any leads on a good package? have you seen nvd3?  it’s built on d3 and reduces the initial setup time for many common chart types. there’s also c3thanks! i will play with those. so far i was trying to use rickshaw, which looks really nice but difficult to customize.i’m probably a little late to the party, but the nginx config file example contained in the widely circulated “the internet, servers, and flask” google slides deck may have a typo. the line `location /static {` should read `location /static/ {` (trailing forward slash added).bmfarley: hey brian! thanks for the info. by chance, in the path contained within `location /static {`, does that path have a trailing slash or not?my guess is that if it’s `location /static {` then the path contained within shouldn’t have a trailing slash, but if it’s `location /static/ {` then the path needs the trailing slashyeah, there’s a mismatch between the two in the example config file — no trailing slash on the location, but a trailing slash on the aliaslet me try it with both missing, too!both missing also works, so it looks like the path mapping is quite literal!good catch! so either both need the trailing slash or both need to omit the trailing slash, correct?that seems to be the case, yes.great. updated the slides. i’ll blast the webdev channelawesome, thanks!guilherme: thanks - i am probably gonna try this out then!hey yusuke - thanks so much for the tips! you are right i am already facing trouble with categorical data as the clusters tend to pick up the feature with most categories and just use that!could you please elaborate on what you mean by scale features to make sense?i mean i understand normalizing for number databut how to scale for say gender and age categories?hey all, i\\'m helping organize an event on the state of data science education. our very own jake klamka will be on a panel and we have dj patil keynoting. if you\\'re in the dc area, you should check it out and please send to anyone you think might be interested. thanks!  commented on ’s file : : now that you have a lot more data how do these plots look? uploaded a file:  uploaded a file: : i was on vacation for a bit in august, have worked from home a few days, and gone on business trips, so the data is actually a bit more sparse than previous weeks. the biggest change is in the evening commute. i no longer have to get my kids from their summer camp in the evenings, so i think the shorter commute trend is here to stay.roughly 24 hours after the previous post, you knew it was coming, day 3 at nips! if you like rl and gans, you will like this one! hi all, particularly those with experience working with/at spotify. i’m looking to get some general information about their hiring process for a friend. if you’d have some time for a quick, high-level chat, i’d really appreciate it. please message me or email at my new blog post is out!this article is awesome ! thanks for sharing. very cool to see gans used for realistic use cases. would you be able to share the related code?just need to clean it up a bit firstthen can share the notebookhey all, i am working with a civic-minded early stage startup that is looking to hire a junior data analyst.  if you have worked with any promising interns that may be interested please encourage them to check out the listing and apply ()oh, lots of new people! is it that time!? :smile:fresh blood!welcome party this friday....welcome party, aka the last time you have fun in 3 months!(i’m trying to scare them)really till? that\\'s how you want to start out? by lying to them?it\\'s more like 4! :wink:@jessica there is enough information here. i haven’t perused through, but it looks like the tutorials and examples are good for beginners as well as advanced. i can help too. i’ve had a sole proprietor llc and now a multi-member llc with s-corp tax designation. llc with s-corp tax election seems to best of both worlds.patrick phelps will be there, i think he\\'s mc  lumiata, the company i work for, is hiring a data engineer. pm me if interested: anyone read judea pearl’s latest on causal inference, _the book of why_? worth it or too watered down?  i’m about a third of the way through it. so far it’s definitely not at all technical compared to e.g. causality, but provides a nice historical (and opinionated) perspective on how we ended up making ‘causality’ a taboo word in statistics/data analysis.oh, and how causal reasoning can help solve some of the statistical ‘paradoxes’ (like simpson’s)any des from airbnb?i\\'m completely new to this whole field but i enjoyed reading this blog post: anyone familiar with the matrix profile methods out of ucr? one of our team members saw a presentation and is now convinced it\\'s the only way to do time series analysis.not that particular technique, but i have done some dynamic time warping, which came out of the same lab. they have done a lot of good work when it comes to times series analysis. i’ll be thereme too!mini insight meetup at the evening mixer tomorrow?cool article about the data science origins of the warriors: icml, nips, kdd?@nothere we’re planning to hire a large new team of ds and de in london over the next year to work on new premium products. if you’re in europe and interested in spotify, let’s talk!what are those? they sound fancy…just a bunch of letters. almost as good of conferences as jkl and ibmright onbetter answer::+1:so were you asking people’s relative opinions about the three? or seeing if anyone was going?ah -- i was intending to respond to ’s question about recommended conferences for ml and aihaha, yes indeed, it all makes sense nowi underutilize threads, evidentlythanks guys. i was bummed to miss nips this year.a quick question: is there any advanced approach of finding the best parameter values better than the grid search?random search usually works better. bayesian search is the most efficient (you can use packages like spearmint)an randomized adaptive search using some variation of the metropolis algorithm would be ideal.thanks guys!meetup is hiring for senior ml and de roles:  ai in actiondoes anyone know a good place for sensor data? nasa has a few interesting datasets of sensor data:  awesome, i will take a closer look.no problem - kaggle also has a few interesting ones: sigopt works pretty well. it’s a commercial product but has a free tier uc irvine also has a decent collection of data sets (sensor and otherwise).  i don’t want to start an editor war, but check out the new post on using emacs for data science:it’s even on the front page of hacker news!sounds like high variance?  what do you mean by “random state” in this case?  the initial seed for the choice of bagging / features?would definitely recommend doing a k-fold validation! how big is the dataset / how many estimators are you running?thanks ! \"is there a paper?\" literally just came up an hour ago!hello, any canadians moving to us with tn visa? :flag-ca:: i did last summer, as did : good paperyeah, he’s a smart guy:  too moved from canada for work.: yep, made the move this year`eh`: i don’t think anyone plays with the random state other than for deterministic reproducibility. it would be easy to convince yourself that the model performs better by validating on your holdout with better performance given a different random state but that’s just due to model variance so that logic doesn’t hold true in the real world where the model will have to perform on different data than the holdout set. bagging in ensemble methods tries to reduce variance at the cost of increased bias, but (as you’re seeing) it won’t be reduced to zero.take a look as the relevant bias-variance tradeoff section in hastie and tibshirani. or this blog post does a decent job of explaining as well: thanks, it logically seemed like the likelihood of this on a larger holdout set would be small but given the inherent randomness of the models it seemed like it is theoretically possible that one set of trees could fit the underlying generating model better than another. i was mainly curious if anyone had also observed this and if they\\'ve ever toyed with it, but the sample here was rather small and could definitely be an artifact of that.it would be easy enough to decide that with cross-validation.in regards to bias, have you seen the pycon talk suggesting that bias could be a major issue in data science? i thought it brought up some interesting points, though given project lengths it seems unlikely it could be taken into consideration outside of large projects essential to the company that could afford an extended development period.but i’ve never toyed with random state as a parameter beforeturns movies into van gogh-esque moviesif you want to learn something, this may help you find free material: thanks for that, i’ll check it out (though i did figure out the superset issue, was actually just some browser settings funkiness :stuck_out_tongue: )i forgot to mention, thanks for the great advice!waadja: the remote program will be convening in la in a couple of weeks!has anyone worked with survival prediction models before and could suggest metrics that best evaluate their accuracies?csyhuang: concordance is one metric ppl use. i heard actuary folks also use ks testi am intrigued by this openstreamtap, however. :beer: friendly poke: please try not to leave your drinking/eating vessels lying around the office - try making it a habit to tidy up at the end of every day. haha you just pinged all past and present fellowsi am no longer in the office :slightly_smiling_face:omggggggno, she\\'s right. cleaning up my desk right now.the message still stands. wherever you are, clean that mess up!the hero we didn’t know we neededi mean, i could probably use a little tidiness guidancecleaning up the sv office now!i am mortified, but also, do it y\\'all this made my dayinstructions unclear, just shipped 2 cases of empty la croix cans to the officethat\\'s right fellows. clean up all your shit!thanks  :slightly_smiling_face: i also find concordance coming along with some survival analysis packages. however, i find it hard to interpret (0.5: random, but literature also says survival models are also in a range of 0.6-0.7. if my model has c = 0.63, is that good enough?).i’ll try the ks test and see if the p-value may help :slightly_smiling_face::smiley: good enough always depends on the problem you are trying to solve. for example, there is no value for a “good enough” aucas former om, i approve this message! i just cleaned up my dinner table. our office manager is thanking you for saying this to me for the 1001th timetime to take down the tiki bar.  *sigh**you\\'re not my supervisor *yes, here: try misspelled variations of a nyc address in the entry field. the adress reported at the top of the results page is taken from data returned from google. it should be automatically correcting the variations. eg try west street 45 nyc, the next page should return the street number at the \\'right\\' order. is this what you need to do?that became my personal motto after you demanded that from all of us the other day. i left a clean desk! i am not sure what happened to it!  that income on the side has got to go! :smile:: stay positive alvingo eat a pie nuggetbotcomposite indexing on mysql table:does anyone know how to set up gunicorn to repond to port 443?do i need to combine gunicorn with nginx?re: gender github articleseems to be a bit unclear… scientifically.a snippet:this is normally the part at which i would question how a study got through peer review, but luckily this time there is a very simple answer: it didn’t. if you read the study, you may notice the giant red “not peer-reviewed” sign on the top of every page. the paper was uploaded to a pre-peer-review site asking for comments. the authors appear to be undergraduate students.but…“i don’t blame the authors for doing a neat study and uploading it to a website. i do blame the entire world media up to and including the bbc for swallowing it uncritically. note that two of the three news sources above failed to report that it is not peer-reviewed.\"anyone ever done anything with  before?maybe   or  could share some thoughts? has worked with their gym before, maybe he can helpsam altman seems to think it\\'s a big deal, and he seems pretty knowledgeable, hah.this is pretty sweet   check out the video at around 2:20 alsocheck out @insightdatasci\\'s tweet: has anyone played around with fb\\'s prophet forecasting library? i\\'m considering it using in python for time series sales forecasting and i\\'m curious to know how it stacks up against other libraries out there and where it falls short (i guess whatever sklearn and stats_models offer). any feedback would be appreciated! :slightly_smiling_face:  diego: i have used it a little bit, i think it is a great starting point for time series modeling particularly if you are interested in breaking out seasonal trends. you can impact the forecast some but it is fairly black-box as far as i can tell so far.we have used it a little for low-risk stuff. for instance we currently have a big monitor in the office forecasting a key metric and comparing to the quarterly goal. i did essentially no tweaks other than setting up a custom list of holidays. i can’t compare much to other packages because forecasting isn’t a substantial part of my job. but, the one data point is that it does come pretty damn close to a pro forecast using basf (big-ass spreadsheet forecasting) methods.hey diego.  i played around with `prophet` (both the r and python versions) in the first few weeks after it was released.  at that time it wasn\\'t stable enough for my purposes--i wanted to forecast ~30k separate time series, but couldn\\'t find a way to script it without the kernel crashing.  so i never got far enough to characterize prophet\\'s accuracy.  but facebook just published an update that may have fixed the stability.  i\\'ve also been using the `forecast` r package, sometimes in conjunction with `hts`.  `forecast` is great--it\\'s stable, mature, and includes a comprehensive set of forecasting algorithms.  it\\'s more flexible but less automated than `prophet`.  i\\'ve also tried `statsmodels` in python but found `forecast` to be far superior.as i understand it, one of its selling points is that it exposes the knobs in a more user friendly way than a raw arima model such as in r forecast.thanks  ,  , and   ! i\\'m interested in forecasting about 300 different time series so it\\'s good to know that prophet had some trouble with scalability in the past (and good to know that r\\'s forecast package was able to handle it), but that it seemed to do a decent job straight out of the box. i\\'ll give it a shot with my data and if it struggles with it i may end up moving to r forecast . thanks a lot! :smile:my understanding is that under the hood it is running stan, so it is something more akin to bayesian structural time series or gaussian processes rather than arimadoes anyone have experience using airbnb knowledge repo within a docker container? we are exploring this option for a company-wide repo and are unclear right now on:1) if we can push to our git server from within the container and 2) if we can make a single repo for all of of our teams (organized in some way …) have you watched the video tutorials on recommendation sys from stanford uni in you tube?  you will find there a very good one on cosine sim but the way to go is matrix factorization i wrote a tutorial about this, if you wanna check it out - though i agree with  that matrix factorization is the way to go:+1: for ’s tutorial, i’ve used it as welldataengconf nyc is coming up october 30 &amp; 31st. come see  speak along with a number of other leaders within the nyc data community! the last day for early bird tickets is this friday and we also have a special promo code for insight alums: insight100 for an additional $100 off the early bird price. see you there! woodoes any one have experience with using pyhive to access hive? hi thanks for response. which video would you pls point to me ? like a link. thanksi’ve used it works through thrift-server.package actually is not maintained anymore, but for me it’s worked fine.thanks!hi all, does anyone have experience with optimizing the phone pick up rate? try impyla it works with hive alsohi! does anyone have any opinions on  ?i\\'m looking for tools to help scale up a ds orgspecifically streamlining workflows and disseminating knowledgei have a question about python tools for hand labeling high resolution time series data with a few hundred positive events.  i know approximately where each positive event occurs, but i need to know precisely.  my current solution is to use bokeh to pan and zoom around the time series data, then select the example associated with the positive event.  i use a javascript callback to generate a web page that contains the data associated with the positive event, then i save that web page to disk.  the process works, but is fairly cumbersome.  i\\'m not tied to bokeh, but i definitely would prefer a solution that can be implemented in python.  thanks!!: : there is something about the “events” that requires visual validation by a human?: yes, i need to hand label a few hundred events to generate a useful training set.hmmm…and there’s no way to auto detect them via smoothing / deltas? (i don’t know how to semi-automate a viz process, unfortunately.)well… i think i have an algorithm that works, but i need to test it using a set of events that i know to be true, plus some that i know to be false.: have you tried the built-in event picker `mpl_connect`?demo code here: : thanks, this looks like it will be useful!:simple_smile:for all the current fellows out there:&gt;&gt;&gt; it takes a lot of interviews to get used to the process and the format and to understand that the stuff you do in technical interviews isn’t actually the stuff you do at work every day. and it takes people in your social circle all going through the same experience, screwing up interviews here and there, and getting back on the horse to realize that poor performance in one interview isn’t predictive of whether you’ll be a good engineer. &lt;-- a history of the technical interview and how to fix some of what\\'s broken.hello! are there any graph theory experts out there? if so i\\'d love to pick your brain about a problem i am trying to solve.  i made some cards in domo recently, havent tried full fledged dashboards. what are you looking to do.  i\\'ll be at odsc this week as well.: are you presenting for sillicon valley data science?not presenting, but will be attending, we have a number of tutorials and panels that we\\'re presenting atyeah, i got an email from one of your peeps about that...hope to see you there. is julia joining you?anyone else attending the odsc west conference?julija won\\'t be joining unfortunately, she has other plansso i have a friend with a db question — maybe someone could exchange some emails with him?he needs to store daily frequency data for ~100k different time series datasets with 20 years of history. each entry in the time series is not necessarily a scalar, and some might be matrices with potentially varying dimensionscontext is buy-side financebasically he’s looking for someone to suggest what storage technologies they should be considering the pros and cons ofcouchdb perhaps? interesting — what’s been your experience?great for logging, particularly when you don\\'t know the size/shape of you input unot*unitand the whole http read/write methods play nice with web based apps academic papers about marketing data science?hey kevin, i\\'d also love to see those papers! hi all, i\\'m a fellow in the remote program. does anyone have experience implementing content-based or collaborative filtering recommendation systems? it\\'s very new to me, so i have several (probably basic) questions! thanks! i used pdfminer which i think contains pdftotext? this enabled me to extract all the text in bounding boxes, along with location information. this is really cool! i may try it out if i run into trouble with pdfminer. thanks! i\\'m interested in it, too. please let me know if you find something useful. thanks! i have done some basic implementation of it and also used matrix factorisation in python during my academic projects. i can try and share some of the useful materials and code for it if needed. but i would also recommend have a look at factorisation machine which can incorporate various features of movies and users as well - ps: i only have some idea about pythonhey. any of you all planning to sell your bike in silicon valley?.i would love to get one. let me know :slightly_smiling_face: i have one to sell, but it\\'s very big (xl)i can\\'t get an estimate from xl actually.. i guess one with 26\" should be good for mei’ve done a bit of it, nothing too sophisticated but i can help if you have any questions,   sorry for hijacking this thread. at customink we are starting to explore airflow as our main etl tool, i’m wondering if i can bother any of you to have an informal discussion on how your framework looks like with airflow? let me know if it’s ok to setup some meeting thanks!hey kenny - you might be better off meeting with an engineer, but i can tell you what i know from a user perspective  - just dm mehi everyone. i\\'m currently an insight health data science fellow and am seeking webscraping advice. my goal is to begin at the following website,  under \"license search\" -&gt; \"search by license number\"the scrapping involves 3 main steps:(1) search query: input a physician license number from a list (one example is: 41586). a successful match (here it is: akhtar, muhammad saleem) will show up as a list of doctors that match the search query. (2) name selection: if there is more than one doctor matching, i have to deal with choosing the correct one based on the doctor\\'s name. perhaps, i can use a dictionary to match the number to the name of interest?(3) save the data: extract the text at the bottom of the page, underneath the header: \"public record actions.\" if anyone has any advice/suggested tutorials/packages for any step in this scrapping process, it would be greatly appreciated. thanks!i’ve have built some recc systems in python and spark ml..would also be willing to try to answer questions :slightly_smiling_face:johnny-do: for webscraping off of interactive sites like this i have used a combination of selenium and beautifulsoup.  here is a useful tutorial i found a while back :beautifulsoup is a great resource. for the interactive part, rely on selenium via```from selenium import webdriverbrowser = webdriver.chrome() #selenium for chromebrowser.get(url)time.sleep(5) # let the user actually see something!html_source = browser.page_sourcesoup = beautifulsoup(html_source) #fetch html source code after renderingbrowser.quit()```i’ll ping again about that pip error, which is specifically `pkg_resources.distributionnotfound: the \\'pip==6.1.1\\' distribution was not found and is required by the application`i can’t solve it for the life of me, and downgrading pip is ridiculously roundabout. if anyone can point me in the right direction, let me know...of course the second i post it i figure out it is a problem with the pip that sudo calls (/usr/bin/pip), and not the user pip (/usr/local/bin/pip), but sudo pip is required for any package installations.is there a way to sign out of the general channel? no, you can’t leave . you can mute it however under ‘channel notification preferences’thanksblog post on generating fonts with adversarial networks: hella coolanybody using or have used keras for regression?  i can\\'t seem to reconcile what model.evaluate gives me versus what i believe the objective is...hi guys! i have a flask app deployment question. i have a pickle file that my code needs. right now, i have stored it under a folder \\'data\\' under \\'static\\' in my \\'app\\' folder. i reference this folder in \\'static/data/\\' in my python code. but when i run the run.py (which is above app folder), the pickle file is not found by flask. on the other hand, another python function within the app folder can read this file! is there a reason this is happening?sounds like a permissions issue on the file... you can test that theory by making it readable to everyone with: `chmod a+r pickle_file.pkl` if it works then then it was a permissions issue, otherwise it\\'s something else!i just figured it out. so i need to give the global path -- \\'app/static/data\\' instead of \\'static/data\\' for run.py, although the local path worked for the main python function within app folder...does anyone know about settingwhat is setting?anybody have ds-related reading suggestions?  i\\'m looking for some like pop-culture-ish type fun stuff.  is nate silver\\'s \"the signal and the noise\" any good, for example?@jaime only if you want to up your nate groupie-ness... :wink:  the book is essentially good for journalists to understand some basic statistical concepts... you\\'re way above that...haha fair enough!  i only know about that book because my mom was asking if i\\'d read it.  maybe i\\'m not the target audience...whoa, looks super interesting, thanks greg!i love the signal and the noise - it\\'s full of powerful examples of people fooling themselves about how good their predictive models arei\\'ve read some chapters, like the one about earthquakes or poker, half a dozen times. so entertainingnice! sounds like just what my commute needs!hey i’m planning an offsite for my team in sf and was wondering if anyone had any good suggestions? preferably that doesn’t have a lot of walking or drinking :)the stuff i have done (roughly ranked)1 ) go-kart racing2 ) painting class at a bar3 ) hiking in marin (toward muir beach)4 ) distillery tour5 ) escape room6 ) cooking class7 ) scavenger hunt8 ) baseball gameobviously 3 and 6 have lots of walking. almost all but the distillery can easily involve minimal or no alcohol.1, 3, and 4 are outside the city (k1 speed is in south sf, distillery is in alameda)6 is good for small groups. 7 and 8 are good for large groups.my favorite was go karts, but the less competitive members were not very into it.woodworking was funthat is terrifying!i will not find it personally fulfilling unless it\\'s blue. i\\'m glad we all agree on that.purple, or i quit.meaning: important to youimpact: matters to the companywe clearly all disagree. can we take this offline and form a team to standardize a pipeline for choosing bike shed colors?that\\'s an excellent idea. we need a streamlined way to make these decisions in the future. but... should the team also have power over other paint choices? or maybe we need a paint committee and form a bikeshed sub committee?hello insight alumni! i am a current insight fellow in seattle on week 2. i am attempting to make a chrome extension that would preserve informative reddit comments and delete the rest. i am having difficulty having my extension run python/communicate with flask as i don\\'t want to open up a new tab but stay on the current page. i am currently using jsonp with ajax to get around cors. my flask is receiving the request, but is unable to send the right information back(error 200). if anybody has any advice i would greatly appreciate it. i only need to send a few lines back. built a pretty awesome chrome extensioncoin toss!information about our stitch fix friends: i have used it beforefor those of you with a ds position opening, a friend of mine is looking. i have been his mentor in the last 2 months. he has been able to do a great deal of work in such a short time on our demand and forecasting problems. he is very comfortable with python, and sql and picks things up very quickly. this was an area that had not been touched by our data science team and helped us with gaining more domain knowledge, as well. our team would love to hire him right after his internship but sadly due to new leadership, we have hiring freeze. i will be happy to provide more details about his technical skills.someone with access needs to program  to do coin flips.hey all, there\\'s a team at netflix looking for people with at least a few years experience (but special cases would likely be made) who are really strong in applied stats (glms, mixed models, hypothesis testing, time series, etc.), is comfortable working with very large data, and who is interested in using data and modeling to improve the netflix product. if you\\'re interested slack me.that\\'s sweet news. i se them a lot so will def buy some of their sweet stock uploaded a file: amazing.i’m so happy they put that in the appstorea friend of mine at wayup is looking for a data scientist with at least a couple years experience. it\\'s a fun company, they connect college students and recent graduates to jobs. they are looking to build more sophisticated recommendation systems and there is lots of nlp work to be done. data is in good shape, one of the founders is an engineer and build the company with engineering in mind (and he is now serious about making an investment in data science). happy to put in a recommendation if anyone is interested. he found it difficult to hire, go figure! :slightly_smiling_face: here is the job description: also, very glad i won\\'t accidentally eat my shoe anymore when i mistake it for a hotdog. it was getting expensive.friederike: +1 to wayup -- i have a friend there as well and he enjoys the culture toodoes anyone have experience working with matrix partners? if so, please dm menew blog post is up.  describes how she built a deep learning model to remove eyeglasses from faces: :rocket: that\\'s a neat project / post !anyone in the nyc office?i’m downstairs and would love a key card to get me inside for mentoring :slightly_smiling_face:welcome new fellows!welcome! have you guy met  yet?woahhi all. i\\'m interested in using past tweets to train my data. i\\'ve found the following api: , which appears to allow access to tweets from 2006 with one-minute granularity. i was wondering if anyone has experience with using past twitter data, and if so, how did you access the data. thanks.this seems to be an useful list of datasets:quora open data set list: hi all, what\\'s the best python library to collect tweets? i\\'ve come across twython and tweepyhere is another bunch of data sets that are available: if you want ds sources, i dropped  a word document with links to datasets into the brainstorming folder at dropbox.  here is the link. please, feel free to add more sources.   hey 2016a fellows! jump into your individual channels (e.g. ds-sv-2016a) to collaborate virtually and in person! there also exist a wide variety of great channels on specifics topics, like python, sql, etc. enjoy!so there was no admonition to avoid twitter data in this session like we got two years ago (because it is overused)? i guess with jack back, twitter is too.does anybody have a link to the results of our insight salary survey?the one that was sent out last year is here: thanks!daaang, we rich!haha yeah we are! _looks at price per square foot of house near work_ oh… :disappointed:hahathis is super useful!working on a v2, let me know if there are requests/requirementsmy goal is more demographics data, more data about job/career transitionshi all, we are looking for a senior data scientist/engineer with a couple years experience.  we’ve got a great location right near powell st bart! a quickly growing workforce (currently about 23 in this location), diverse professional backgrounds from health to backend engineering. right now i’m the only data person. for the love of god someone help me! lots of stuff to explore and/or build. you should follow through the contact in the job description to apply but i can try answer any preliminary questions. i’d love an inclusion of manager roles as many have transitioned to that :point_up:historical information would be great (eg salaries for people\\'s previous data science positions in addition to current), especially for those of us in the middle of the country where there\\'s not a ton of data on :slightly_smiling_face:has anyone here been involved in hiring a summer intern that could give some guidance on what a reasonable hourly rate would be?is it reasonable to take the salary of an entry-level data scientist at your company and convert it to an hourly rate?sure, i\\'m not super familiar with data science internships so wasn\\'t sure if that would be wildly off the mark, but even that is a helpful starting pointfreeor 1 linkedin testimonialwhat are people’s thoughts on the following line in someone’s resume:“• hired 4 data scientists (all women), of which 3 promoted in less than 2 years.”arbitraryis their company promotion schedule that you should be promoted every 6 months, in which case the person did poorly, or every 5 years, in which case they did great?does their company have 100 levels, or 2?promoted within 2 years is not really something that you can evaluatedid they need to hire 10 and only get 4?i\\'m happy to pass judgement from afar, but likely just talking to the person can clarify :wink:i’m really just curious on people’s gut reactions to that bullet point. i think there’s a lot that could be unpacked there.also, the person was scheduled for a phone screen with me and they never picked up the phone…so that doesn’t bode well!i’d ask why the 4th one didn’t get promoted...but without any additional context, i have to admit that my gut reaction to that bullet point was neutral to negativeand i agree with all of kevin’s points, which usually never happensor hired below level and then leveled appropriately within 2 years :slightly_smiling_face:in which case nice job keeping those salaries low for the first 2 yearsfiscally responsible +1only promote when threaten to leaveyeah, it\\'s a weird line to write in a resumeinteresting that no one has touched the `all women` line`all women` rubbed me the wrong way, but i’m having trouble figuring out what exactly bugs meit\\'s a suspicious humble bragthere’s some sort of assumption embedded in there that women are harder to hire and/or promotethat’s what jumped out at me tooi think it is actually more challenging to hire diverse teamsdo you think that team is diverse?sourcing diverse talent is one thing many companies do not do a great job athard to saythe `all women` bit was absolutely the first thing jumped out at meas sort of a humble brag that rubbed me the wrong wayi *think* i’d rather them say, if they really feel the need to humble brag, something like “hired a diverse group of 4 ds”, but even that doesn’t feel/read quite rightagree, i don\\'t like the `all women` line, it feels both performative and also hyper-aware of genderi mean, what i’d really like is to see other evidence on their resume that they truly care about diversity and equalityyup. if diversity had been a problem, then they addressed that in hiring/promoting efforts, why not phrase it that way instead of that all women linecan you bring this person to us for a proper, thorough judgment? we have a lot of questions.i’m interested in whether or not this is a woman’s resume or a man’s?it\\'s 10000% a man\\'s resume^^^it\\'s tough because hiring women data scientists is obviously a good thing (tm). but the choice to call it out just feels strange.a bit like a mitt romney quotei think we pay ours something in the realm of $25 an hour“binders full of women”and yes, good call, it’s a man’s resumei\\'m sure the person would be flattered that so many phds are evaluating the resume right nowthat\\'s phd-ist, kevinwell i hired a team of dentists (all dentites) and i fired all of themwithin 3 daysi like that the guy felt compelled to add the 3/4 promotion rate. like, he put himself in his resume-reader\\'s shoes, and imagined them thinking \"hire women!? but i\\'m sure they were terrible candidates!\"anti-dentite…and one of the ds’s on my team (a woman), tongue in cheek, just suggested that people should start adding more gender and ethnicity info of their hires to their resumes…one last bit from this resume that i feel will also inspire many questions, under hackathons they list (in the top position):“bathroom hack (“p value”)  - a/b tested series of urinal feature enhancements (e.g. bullseye 2. footprints on ground) to improve cleanliness of men’s bathroom.technologies: raspberry pi, pir sensors”“hired 4 data scientists (all white males)”sighi mean, i’d want to phone screen that guy just to see if he’s that ridiculous in real life. but based on that resume alone, i almost certainly wouldn’t hire himi was actually kinda looking forward to chatting with him on the phone screeni would pay real american dollars to hear him answer the question, \"so, what was it like working with all those women?\"you should just be likebro:bro!crushed so much code on that sick p-value hacklove how you did the wordplaypee-valuedamn i just got ithahahahahhaai\\'d rather hear about what a team accomplished than how many of them got promoted. who gets promoted depends a lot on things that are beyond the control of the manager or who the manager hires.and then just see what he doesok enough trolling from mei think we all want to hear an update on how the interview goesaka any delightful quoteswell they didn’t pick up the phone when i called for our schedule phone screen, so not sure they’ll get another chance with us…$50/hour plus housing, but we’re a bigcorp (tm) in silicon valleymaybe he\\'s not used to answering the phone, as a manyeah, denver pay rates are definitely lower than anything in cahe should hire a secretarymaybe you can mansplain over email why you\\'re not moving forward in the processyou mean over man-mail?let me interrupt the fun to say “as a manager i absolutely want to see you built a team on your resume if that’s what i’m hiring for. but that line is super awkward…”likewise i’d want to get into questions of “how did you support your team? how did you develop their careers…” etc. etc. but on the phone in person.on the plus side, i now know what not to write on my resume :wink:just to add to this: $50 + housing is pretty close to (but slightly lower than) junior data scientist yearly salary -&gt; hourly for us, so that might be a good metric.awesome points of reference, thanks!wow haha, lots of (hilarious) cattiness today :smile:my general rule is to give huge benefit of the doubt to everyone in this sort of situation.  it\\'s always a mistake to try to intuit what\\'s going on in someone\\'s head from a resume line.   would definitely just ask about anything that seems \"interesting\" assuming core skills are there.definitely also would be understanding of a missed call, especially if it comes with a good reason (e.g. not an excuse) as long as it\\'s not a pattern.^totally agree about the missed call. i don’t flat-out reject someone for that, but i do make a note in their hr/candidate profileso you\\'re saying it goes into their permanent record :scream:i once missed a phone screen call while waiting for my phone to ring. it was first thing in the morning, and i had forgotten to take my phone out of do not disturb. :man-facepalming:but the more common reason for missing a phone interview is that stuff always comes up at work. in my post-insight job search, i never understood why so many interviewers would begin by asking, \"is now a good time to talk?\" why would i have scheduled an interview if it were a bad time? but now that i\\'ve conducted a job search while working, i understand that there is no such thing as a good time for a phone interview. there are only degrees of badness, and one cannot know in advance how bad a given time will be. if someone misses a phone interview, it may well mean that they\\'re important at work.i absolutely start every phone interview i give with “is now still a good time to talk?”(after introducing myself…)pro tip: you can also find out if you\\'re accidentally on speakerphone at their current place of work by starting the call with, \"is now still a good time to talk about interviewing for a new job?\" pinned a message to this channel.interesting discussion on where ml engineers fall in the world of data science and engineeringthis may be interesting for those still connected with academia:brains, minds and machines summer course 2018a special topics course at mbl woods hole, madirectors: gabriel kreiman, children’s hospital, harvard medical school; and tomaso poggio, massachusetts institute of technologycourse dates: august 9 - august 30, 2018application deadline: april 9, 2018more information: …this course aims to cross-educate computer engineers and neuroscientists; it is appropriate for graduate students, postdocs, and faculty in computer science and/or neuroscience …hey all — i’m looking for a good applied ml interview question. something that gets at problem solving ability and ml understanding, preferably with a way to quantify the responses, though that’s obviously tough. anyone got a good go to?not quite an answer to the specific question, but i sometimes point people to this article when asked about advice around finding good ml/ds candidates. thanks  — i love that article but had forgotten about it. that will definitely help!hi everyone! we suspect our data science team is understaffed. we wanted to get a sense of how many data scientists other comparable companies employ. if you have 30 seconds, would you mind filling out the following survey? happy to share aggregated results! :pray: you guys should hire some idx fellows :wink:no not yet! i’m still poking around, never really got a response on here. i looked at that answer, but didn’t try it - did you?would love to find a solutionhelp , go here to see problemi\\'ve got a friend from canada looking for bay area based internship/coop positions for data analysis/science projects. this would be for 3 months. dm me if you know of anything!hi friends, i am working on an open source project with the world food program that attempts to apply kalman filters to get better estimates for noisy survey data (checkout the  channel for details).  i\\'ve got a complete and presumably correct implementation now but i\\'m looking for someone who\\'s got some applied knowledge in fitting kalman filters to real data, be it in any discipline.  anyone know anyone who might be willing to talk with me about that?hello! has anyone worked with phone records? i have data with calling number, called number and dialed number (sometimes the last two aren\\'t the same) and am not sure the best way to tackle figuring out if certain people are more commonly dialing others. thanks!: isn’t this simply finding the number of occurrences using the pair of numbers within the dataset?maybe so, that\\'s what i\\'m currently doing.  just didn\\'t know if there were other ways of looking at it :)you could do a network-based approach with in and out going calls being your edges.i was thinking something like that too! i\\'ve never done network analysis. is there a python library your recommend? networkx is pretty easy to use: thanks!!!looking for someone whos good with apache storm + java?! :sleepy:possibly  has an idea?hey , what’s your problem?: a friend of mine who works at our company started this with his boyfriend. you can make donations and t shirt purchases all proceeds going to equality florida actionhi guys, i\\'m a ds fellow in the current ny session, having a strange problem making my web app visible on aws. the app uses a number of python packages, which i have installed via miniconda. the app runs locally on aws, but when i try to run it via supervisor on port 80, it does not find the miniconda python installation. i have tried to specify the miniconda path using \\'environment=pythonpath=\\'/home/ubuntu/miniconda2/bin\\' in the supervisor conf, but it does not find it. have any of you faced this issue before and, if so, what was the solution? i know that many past fellows have put their apps in the public domain using aws, so it must work. thanks a lot for any suggestion!stay positive lkashifshot in the dark: perhaps you need `path=` instead of `pythonpath=`? `path` is what linux uses to find programs (like python), `pythonpath` is what python uses to find libraries.tried both path and pythonpath, with same result. but thankshave you tried specifying that specific .conf using the -c option to make sure it\\'s not pulling it from some other location. see here: day in a life of a data scientist: does anybody have any good references on applying ml for supply chain mgmt and inventory forecasting, for parts in a factory/manufacturing context?i imagine the folks at stitch fix doin case you want to look very productive, go to this website are start typinghit `alt` three times when your boss walks byinsight mentioned in the nyt: big mention   and  big mention yes! i just didn\\'t read the whole thing before posting :wink:hey, does anybody know if there is a good package for fixed effect regression models in python 2.7? i’m trying to run a logistic regression model with fixed effects… thanksyou can just add dummy variables for the dimension you’re sweeping outor manually demean (i’ve found doing fe to be a pain in python)i’m totally new to fe models. can you point me to an example?this is pretty good: thank you! i’ll read it and get back to you if needed :wink:hey insight community! i’m staffing up a new project i’m leading at my company (vevo), and i️ am in talks with my senior leadership about bringing on some contractors to help with some data science workloads in the short term (starting asap), with a minimum ~2 month engagement. if you are interested, or if you know of good data science freelancers, please send a pm!thanks to  &amp;  for speaking to steve lohr at the nyt and agreeing to be featured in today\\'s article  i found this guy’s blog to be quite helpful. he doesn’t always focus on supply chain data, but he does have a lot of material for methods/metrics that are applicable to supply chain/inventory forecasting. thank you  , i will take a look!sure thing, good luck! :slightly_smiling_face:just saw this on the way home... way to go carlos, anasuya, and insight!wow, what a coup,  - close to half of the article is an ode to insight, huge pr win!god that looks boringdoes anyone have any examples of how to do time series histograms w/ time interval buckets?  thank you! :bow:if you’re using pandas you might be able to use resample:  are you using r? python/pandas? what frequency of series are you looking at (subsecond? daily?) more like in sql through self joins and stuff, thanks, but wont be using pandas or rif you have postgres or something similar use window functionsit will save you a lot of headache ok, will take a look at window fnthis is interesting/important. white house report on big data and civil rights.  my experience with similar data has been that synonyms, abbreviations, and shorthands (i’m picturing this something like the feed on a credit card statement, which is half-gibberish) shoots a bow approach in the footworks for things like “car” but imagine how many providers/terms there are for say, cell phone plansit depends on how standardized the text is. freeform text is rough for sure, but i\\'ve had success using it for categorizing the notes of sales people in the past.it just took a day of figuring out the most common abbreviations and cleaning them up  thanks, both of you, i should be able to get it to work with your suggestions!hey all, my team at sovrn in boulder, co, is hiring for a data scientist in the r&amp;d group. we’re looking for someone with a strong math background and good coding chops (python and/or java) to work on some really cool optimization and ml problems. if that sounds interesting to you, please pm me. also, i don’t know who the remote program directors are, but i’d be happy to chat with you and see if any of your folks are interested.lisadh:   data engineers - let me know if you\\'d be interested in a job at tachyus i’ve not seen a giant difference in the bs’s who are hired vs phds who are hired. from my perspective, the bs folks seem to be more energetic and adaptive while the phd folks are more careful and ‘big picture’ oriented. i think there’s room for both, but maybe on a more depressing note for phds, i don’t think the degree is magic sauce, at all, and i think that a talented bs can contribute just as much as a phd and can close any significant gaps specific to ds in a year or so. to me, this is because the valuable parts of the phd that overlap with data science are generally just sort of the ‘tool-belt’ that lets you do good science - if the phd was all about getting that tool belt, they would be significantly shorter.as i contemplate these days (maybe too much) the value of my degree, i have to admit that the phd challenged me to think more deeply, creatively, and thoroughly about a subject matter than i ever would have done had i not gone to grad school. as far as a hard skillset is concerned, i don\\'t think the advantage of the \"tool belt\" that phds possess is very significant, and even the size of the tool belt would depend heavily on the field of training (e.g. biostatistics vs mechanical engineering). the question is, once hired into an industry role, is the trained ability to think scientifically valued, or will the value always be confined to the realm of an academic setting? among all the ds roles that are out there, what fraction of jobs would actually benefit from the experience that a phd scientist brings?does machine learning = deep learning now? almost all chatter and job posts on linkedin would indicate so. did i miss the announcement that non-nn techniques are now only called statistical modeling or advanced analytics?if true, that\\'s kind of sad. i may be mistaken, but isn\\'t deep learning really still statistical learning on a fundamental level, as are all ml techniques? taking a bunch of data and identifying patterns and trends that are statistically generalizable. i think if people consider ml to only be dl, it exposes their lack of understanding of what ml/dl really is.i agree, and one source of the lack of understanding might be the hr/practitioner divide. there’s so much hype about ml in general that it’s difficult to sift though to understand real trends, but i wonder if the industry (or at least hr) is settling on titles like ml engineer = dl pipeline engineer much like data scientist became a somewhat normalized title a couple years ago. also ai = deep learning from everything i seem to read…i suspect a lot of it is marketing . would you apply to a position that says “you’ll spend 80% of your time doing logistic regression and 20% of the time cleaning data that is probably unsuitable to use in your logistic regression but you have to anyway”? :smile:but if you’ll be doing nn you’ll be all over that :smile:i\\'m intrigued. tell me more about this 20%-time cleaning data unsuitable for logistic regression job... :wink:haha i’ve seen some interesting things.  none of the features are particularly predictive (because of lack of investment in collecting the right stuff), they’re all super sparse, the classes are super imbalanced, and, of course, there’s not enough of it for undersampling.i’m going to need that model performing well by mid week please :smile:somewhere i have an image of what’s been called the saddest looking roc curve by some of my ds colleagues.hi all, if anybody wants to be a project reviewer for udacity’s machine learning nanodegree let me know. it’s a part time thing that you can do whenever you want and there are no obligations.there isn’t much public information on it but there is this: if this data science thing doesn\\'t work out... i can always review data science students!good post:it\\'s la crow-ix.  like unix hey everyone! every program at insight (data engineering, health data and data science) is gearing up for sessions this fall. so if you have leads on teams that are growing please feel free to reach out to your former program coordinators, program directors, or you know, email jake! :wink: we’d love to hear from you and work with any teams you think are doing exciting, cutting-edge work. don’t forget we have programs in boston, new york and silicon valley. plus our latest addition: health data science in san francisco! :unicorn_face: :bar_chart: :100: :wow:a good read on hiring data scientists ... can anyone recommend a good print publication to stay on top of tech news? i\\'m rather old fashioned and like getting newspapers/magazines in the mail instead of obsessively checking websites. inc. is good if your focus is tech+startup, and startup. pertinent to silicon valley. if you would like a comprehensive coverage, then i personally recommend, bloomberg businessweek + wired (lots of ads but still good) + inc + the economist ( although there can be nothing about tech in politically hot seasons)cool thanks!mit tech review is good, i also like physical media and subscribe to it :smiley:twitterthere are a bunch of individuals if you follow, who will curate the news for youand it can go very specific - crypto, vc, startup, etc...problem is, it takes effort to cultivate ittoo right mate :slightly_smiling_face:hey all, many of you may know me as helping to lead insight’s new york data engineering program. we have an application deadline coming up for the summer session and we would like to increase the number of applications and fellows in the program. considering that alums are our best source of referral, if you could send our (or my) way anyone who may be interested and encourage them to apply at  , i’d really appreciate it. the deadline for de applications is march 29 (new york and silicon valley), and the session would start may 30. questions about the program can be sent to  or me at  many thanks!!friend of mine actively interviewing for ds jobs is looking for a good resource to learn a/b testing. suggestions?i found combination of udacity a/b testing and inferential statistics course very useful for my interviewsit’s not exactly introductory, but i think slater stich’s set of guides () is very good.also, this article (“how optimizely (almost) got me fired”) talks about some of the practical pitfalls of a/b testing: big fan of this postevan miller has several good articles on a/b testing. hey all, hope your friday’s going well!quick question: has anyone incorporated test suites when building ml pipelines? would love any recommendations on conventions / frameworks / gotchas if people have any.i second evan miller’s site and slater’s blog post. between the two of them, you get a pretty comprehensive overview of common methods and tradeoffs/pitfalls  i still think the chainer version is cleaner than tensorflow, but cool either way :slightly_smiling_face:does anyone use databricks? did you have trouble importing keras?thanks davetensorflow (&amp; scikit-learn) workshop in mountain view this saturday:yeah, the default binding are pretty terriblei’ve found it to be fairly fast and scalable. you can also fit multiple time series in parallel using something like parallel in joblib. pinned ’s pdf  to this channel.for fans of the new yorker cartoons, this will be entertaining: uploaded a file:     how do you think... looking to connect with folks who are running ds teams of 10 or more, insight alum or otherwise - please pm or put suggestions in a thread, thanks!if anyone in nyc is interested in attending the upcoming dataengconf on 10/30, the founder has provided all insight fellows 25% off with the coupon code: ins25x (code expires this friday, 10/20)the speakers will include many of our mentors and advisors (e.g. chris wiggins, claudia perlich), as well as ovo genevieve smithi\\'ll be there!: does the elasticsearch (and/or spark-xml parser) lib include s3 libs? if this is a jar, one can see the content (it can be a “fat” jar) by “jar tf the_jar_file.jar”if, for example, there are 2 of the same library being included and they have different versions, you will get an error. if this is not the issue, post the error/exception and i’ll think about it a bit more...thanks  ! i think i figured it out finally. pegasus added the two aws jars to the spark-defaults.conf file under the driver-classpath — you need to do that to access the buckets. but in order to use his elasticsearch-hadoop connector, he specified that jar\\'s classpath on the pyspark command line. looks like that classpath was wiping out the classpath specified in the conf file so the solution was to specify the aws jars on the pyspark command line (or conversely, specify the elasticsearch-hadoop connector in the spark conf file) but not have two classpaths specified in two different places. thanks for taking interest!!glad it worked out! yes, spark’s config file is simply the default settings, when specifying new value with the spark commands (spark-submit, pyspark, etc.) they take priority and will override the defaults suggested in the config.for those who use h2o: what do you use it for? i\\'ve been asked to determine whether we should use it internally, and i\\'m having a hard time cutting through the hype to what it is actually good at.i drink it, usually. often i use it to bathe myself.: ask  he works there, started last december.  : h2o is a commodity software that you can unpack and start using as a ml platform. if you are advanced you can use the libraries and build it from, r, python and java. or if you are starting off, just download it, just the jar, and connect to your localhost:54321. and see yourself as to how easy it is to upload, and model data.nuggetbot check on jonneffjonneff has a happiness average of 1.1666666666666667i love life!!! everything is awesome when you\\'re part of a team.  happy happy happy.  nuggetbot check on jonneffjonneff has a happiness average of 1.3255813953488371nuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 0.9703947368421053that’s so awesome and wonderful! you are the best you amazing great awesome bot!nuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 1.039344262295082that’s a bunch of bullshit! i hate you nuggetbot. why do you destroy all that i love?stay positive jeremykarnowskinuggetbot check on jeremykarnowskijeremykarnowski has a happiness average of 1.0130718954248366nuggetbot check on kumbhanikumbhani has a happiness average of 0.696969696969697nice!a friend of mine without a traditional data science background (only bachelor\\'s in electrical engineering - currently working in a power utility company) wants to make the \\'jump\\' to ds. he\\'s learning python (numpy and scikit-learn) on his own time and just learning about the field as a whole. any advice or suggestions on what else to do to best position himself to make a career transition?  would those online data science courses be helpful to do (here\\'s a random one i found: ).100% of interviewers i\\'ve talked to have asked about what i\\'m currently working on, and/or my last project, so the advice i usually give people is projects, projects, projects. find some data. do a project. there are a couple of nice datasets/project guides on kaggle and elsewhere (e.g. ml blogs).hello .. i am just curious on what visualization tools are being used for json feed from api\\'s? tableau does not support live feed and i am still able to make the web data connector work. we can make custom visualizations from d3 but securing that needs effort (for enterprise). any obvious solution that i am missing?those of you on twitter have probably already seen this...really good readdimensions of  flavor   : we\\'re looking for a data engineer! please email me if you or someone you know is interested. we are thinking of developing a few different dashboards at work and i was wondering if anyone had suggestions for (non-tableau) tools.paid and easy to use or free and a bit harder to use?relatively cheap - doesn’t have to be free, but tableau seems very pricy.looker for just about anything, kibana(if you use elastic search), powerbi from msftor shiny if you use r and want to set it up yourselfthere\\'s a ton moreit\\'s a crowded field chartio, jaspersoft, bime, zoho, sisense, grow,  ducksboard, looker are all potential options from a survey i did about 6 months ago. i would do my due diligence in evaluating each option. also, r launched a dashboard-specific library this summer.tableau and chartio, the leaders in the commercial space, are very pricey.i’m a python person and i’m hoping to move my boss away from r.does anyone have experience/good things to say about quicksight?aww... nothing wrong with r. use both.: nothing wrong with r per se, but it would be easier if we all spoke the same language;) i tried to participate in a quicksight trial, but the permissions required were basically a showstopper for our security requirementsthanks, . good to know.we handle bank data though, so we\\'re extremely paranoid on the security  also starting to look at dashboarding, and we’re subject to bank data requirements too ;/anyone looked at anaconda mosaic, the dashboard component of their cluster management? the options you have will depend a lot on the end-users desire. will business folk be happy with static charts on a dashboard, or will they want to be able to dive into the data a bit more and filter/pivot/join to other data sources? for the former, you can get away with lots of free python options (i.e. jupyter notebooks, caravel) and not-too expensive managed options (i’ve found mode analytics quite good since they’ve added the option to work with ipython notebooks)if your end users want to play with the data themselves more and not write sql, i second looker as a great solution we want the business people to have some flexibility, so looker “looks” great! (sorry, couldn’t help myself)we are implementing looker here too. getting away from ad hoc requests is one of the big reasons.ad-hocs…blehyeah, the pm team asked us yesterday if there was a way to not bother us for everything.we’re also using looker, for those same reasons. it takes some setting up, but provides lots of flexibility for end-users.you may check the airbnb caravel. python/flask stack. open source, no setup for client. definitely curious about folks’ ramp up time with looker setup. it took us longer than i expected at my last gig, even with an analyst who was super familiar with our data model and good with sql working on writing the lookerml full time *and* a data scientist who had experience getting his previous company moved to it helping out (we found the engineering help they provided to get setup pretty useless unless your data model is quite simple)but business end-users absolutely love it and yeah…fewer ad-hocsat my last company, we used looker and it was fine. but people most likely need to know at least some sql. at my current company, we use domo which to me is more intuitive.tableau is the bane of my existence. has anyone successfully transitioned off it?  we’ve had a data engineer working near full time on it for 2 months building out looker dashboards for clients. def need to have solid sql background to build anything useful.hey everyone, we just launched a new insight ai fellowship, open for people who want to move from machine learning to more advanced applied ai work in industry: is it open to robots as well, or just people?you have to pass the turing test to get in actually i am trying to launch my web app on the ec2 instance t2.micro - my python script gets \"killed\" - out of memory. i used 5 gigs out of 8 gigs available (71%). i increased the storage volume to 12 gigs (47% used), the program is still killed. i am now looking at changing ec2 instance type but i have no  idea which one is the most suited? aws expertise is welcome.stay positive fplisson  would love to hear more thoughts on domo — what are the use cases? i think you are confusing memory and hard drive space. a micro instance only has 1 gig of memory, but 8 gigs of hard drive spacestay positive ryan.vilimyour process is getting killed because it’s running out of _ram_stay positive ryan.vilimrunning top will list all your processes and show you how much memory is being used in real timeif you are running out of memory, either change your code, or just upgrade to a bigger instance, it’s cheapyou could also add a few gigs by making a swapfile, but i recommend against that, it’ll be super slow and you don’t even have much hdd anyways. i upgraded my ec2 instance to m4.large (more memory). my program is running fine. i read about swapfile but i considered it as last resort. problem solved. thanks.i highly recommend against it.(a swap file that is)no problemstay positive ryan.vilimnuggetbot check on ryan.vilimryan.vilim has a happiness average of 1 given how a ml model i’m working on is currently performing, i think a happiness of “1” is too high by several millionlol we basically used it for all teams like finance, cs, growth, digital... data team manage it of course. similar to looker, you can make cards, and dashboards. we pull the data from redshift. you can also use d3 for visualization. does this answer you question? (curious as well since i might be deciding a bi tool to go with in the near future) what were the main differences with looker both from the model management (i.e. your) and the business user perspectives?we use looker and i think it’s a steaming pile of garbagethat\\'s slightly hyperbolic, but i’m not a fan1) it’s flexible enough to write queries to do a lot of different things, but not smart enough to do it efficiently. looker support told my boss that they haven’t bothered thinking about performance because most of their customers are on redshift or something comparably fast. we’re on mariadb and we’ve had to implement some incredibly janky hacks to work around looker’s limitations.2) too much config in ui. lookml (based on yaml) is a nice idea, but they’ve only half-heartedly embraced it. you can define a dashboard in lookml, or you can define a dashboard in the gui. you can even convert a lookml dashboard to a gui dashboard. but you can’t modify a gui dashboard by editing lookml. so  if you ever have to do a schema migration…have fun!does anyone have any experience using vowpal wabbit?data pipeline is equivalent to airflowbut airflow is easier to use and test, and cheaperglue i’m guessing is slightly similar to schema registry uploaded a file:  and commented: an early stage, very successful startup is hiring their first data scientist in nyc! this is a phenomenal opportunity to be able to define your own role under fantastic management with a pending patent defending their business model. please take a look at the attached job description and reply to the email at the bottom if you are interested!pretty cool way to think about it, especially&gt;that’s not because optics is easy. it’s because the mental models in optics are organized well.+1 to plotly dashthe insight alumni group on linkedin could use some more members (kudos  for setting it up). just sayin’hey guys does anyone have experience deploying airflow on a heroku instance with authentication? i have been running into an `bcrypt error` :disappointed:here is my stackoverflow question if anyone has any idea passing along something that might be of interest to alumni: wish they had a list of the questions =so cold i use bokeh on a digital ocean server, works fine, never tried it on ec2 did you have to do anything special compared to running locally? i’m getting some errors when i try to run it on the server. the bokeh user guide describes setting up a reverse proxy server using nginx, but i am a bit clueless on how that should work or if it’s even necessary stay positive along528 give me a break if you simply want a small number of users or just yourself to be able to connect and use the server i would suggest the ssh tunnel instead. the nginx reverse proxy config would be recommended if you want to serve a general user on the internet. the bokeh server is run exactly the same way as locally afair, nothing special needs to be done on the serverwe ( and ) are throwing a data science meetup at svds (mountain view). come listen to two short (30-45 minute) talks and socialize at the silicon valley data science office, just off castro st., mountain view. we will have pizza and drinks and we’re hosting insight alumnus andrew zaldivar (google) and allison gilmore (one medical). the evening kicks off next week at 6pm on wednesday, dec. 7th and will wrap up by 8:30pm. see the eventbrite page () for more details! invite anyone you think would enjoy the event! please rsvp on eventbrite if you are planning on attending (so we can order enough pizza).if anybody is interested in post-meetup drinks on castro st., let us know. it’s a 5 minute walk from our office and the mountain view caltrain station (here’s a link to the caltrain timetable for the lazy: ). hope to see you all there!nice article about building usable research code in a team for fellows currently interviewing: i found this list of questions to ask your interviewer be quite helpful. you can find a lot of similar lists on the internet, but this one was particularly thoughtful. uploaded a file:  and commented: insight at night reunites!have asked this before but... anyone here with experience doing image similarity analysis? currently using color-coherence-vectors as feature vectors (they do ok) but wondering if anyone aware of better descriptors? (have tried local-binary-patterns also). unsupervised problem on this side, thus, not sure how to even go at it from the deep learning perspective (new to dl). gist descriptors do well on landscape similarity thanks ! unfortunately, feels not quite suited for in my case.  trying to rank images of objects from the same class with much finer differences. fourier-based descriptor tend to work best when much info stored in the low frequency spectrum (not my case). additionally fourier descp are not invariant to rotations/translations which matters in my case. its a struggle... :face_with_head_bandage:maybe should add that have also considered the heavyweights sift, orb, et al but not much further. issues with speed and feature extraction  using these bad boys.stay positive ccci have a flask/html question! i want to include a user input in the form of a \\'sliding bar\\'. depending on where the user clicks on this bar, i would record a value between 0 and 1. i don\\'t even know what something like this is called in html/javascript. any help ?: try jqueryui slider widget:  (feel free to ask if you have questions about how to use it): i’m not sure what kind of resources you have to scale, but if you have access to cuda-capable gpus then you could use a pre-trained neural net. there are a number of nns pre-trained on images that act as excellent feature extractors. you’d want to do something like take the output of a penultimate neuron layer for the two images you want to compare and then compute mse or cosine distance. how fast are you looking to do the comparison?, great advice! i was able to find some papers pointing in the direction you mention. it does sound very promising although i would have to ramp up my game to go down such a path. currently i have no idea what a config file looks like for those systems, never mind a clue about computational load.  i have gone as far as compiling caffe  on my ubuntu pc (cpu only). do you have any recommendations as to where to look to find info on alternative tools/fast tutorials to quickly understand the technical challenges?  my image data set won\\'t be the bottle neck (i think). they are small pics (size and resolution) and not many (~5000).if anybody wants twitter data .. there is currently 80gb compressed .. uncompressed to ~360gb.. and more coming… it’s in s3… msg me if you need it in your projectthats a lotta tweets: forgot to say that i aim at comparing an incoming reference image with the whole data set. look  to pre-compute the full feature matrix for the full data set and use knn with a given metric to find nearest neighbors avoiding the brute force search (as i do now with my current feature vec).  but will have to run the nn on the incoming vector request in real time.:  did you see lory\\'s post?: i like using chainer because it is pretty readable and fully in python (as opposed to caffe which merely has a python wrapper).  they also have caffe model support so you can simply import the model file for any of the models in the caffe model zoo and then do forward passes to calculate any layer output. : caffe is also not super hard to use with pre-trained models on its own and if you have it compiled that’s half the battle :simple_smile: . i don’t know what “real-time” means in terms of throughput, but for some perspective some of the models i have been building will run through 5000 96x96 color images in seconds on a gpu.: +1 for chainer. i am using it for my project too.: great. i\\'ll dive into the details then. your stats reinforce that idea that running my model should not be a problem even for a cpu (~20x slower order of mag vs gpu?) as i will only be doing 5000 360x360 one time and then is just 1 360x360 per comparison request. only concern left is my images are very specialized (imagine all shoes for example). think a pre-trained network on \"all sorts of pics\" would have learned enough to resolve features that only apply to shoes?: hi adam, i looked at the link and there are some css and js scripts i need to link to to use the slider. but it\\'s not clear where i can access them from. the downloads section has a few but not all, for example the style file in resources folder. i\\'m sure there is some straightforward way to do this that i\\'m missing. if you are using the existing model and not training i think cpu is fine. there is an option to fine-tune the existing nn .thanks ! next step is playing with model to develop intuition for the approach\\'s resolving power within the specific class of objects i\\'m comparing.anyone here has experience saving greyscale image stored in numpy array to  png files with transparency?: hi lory! i may have use for that tweet data…is it historical data?: you actually don’t need to download any of the css/js to use the jquery ui widgets. you can use the  links for the ones that are there. the /resources/demos/style.css is just an example or something; it refers to this file here which is practically empty: yah… not clean tho.. for cleaned.. next week.. only english tweets, user, tweet, date: now its zipped with json in each line: there’s apparently also a bootstrap slider, too, but it seems more complicated to me: : is there a way to get location information? i’m using tweets restricted to us locations, but i’ve been collecting via the stream using bounding box constraints: yes, can filter by locationjust starting with hbase.  is it correct to state that it has no native support for queries?: take a look at apache phoenixthanks  – that’s what it seemed the solution was going to have to be.another big-table style solution would be cassandra with cqli\\'ve updated the repo for jupyter notebook tips and tricks available here:  the readme has install instructions that might help if you run into trouble.nice! i was scraping reddit-wide for my project so it was way easier to have the set then filter it down... still slow thoughgood luck!hey does anyone have access/could connect me to data relating geography related disease emergence? (i.e., lots of patients coming in with the flu in chicago or lots of patients coming in with asthma in sf)that\\'s for asthma in 2012. and i know there\\'s also for other years, 2009, and 2012-2015it\\'s what i used for my insight projectother states also have itlike ct and ny you can check  for this project on pediatric asthma ed visits in california by counties for several years.the asthma rates were scrapped from the california department of public health and califonia breathingand this: lots of options, but you\\'ll have to circumvent the js query page.no on cares about your insight project *runs away*nobody likes me :sad_parrot:bos-2017b does!the community health status indicators dataset has some county-by-county data for some diseases (as well as a bunch of things that are not diseases): . in case it\\'s useful to you i have a couple of tools for working with it in this repo: question on confidence intervals in predictions -- anyone have a suggestion for how to create a confidence interval around a scikit-learn random forest prediction?there was a good post on this topic a few years agolooks like there\\'s also a package implementing (or something similar to) the above idea too. i have not used it, but looks easy enough to plug into a pipeline:  -- that\\'s basically exactly the intuition we approached it with, but it\\'s a bit expensive computationally for a large # of estimators + datasetestimating the variance of the predictions via jackknifing would be my first idea. here\\'s an implementation of such a method: so the idea is jackknife is less expensive than bootstrap?ok i rtfm.  hastie and efron’s method is jackknife after bootstrap and only needs bootstrap replicates used in bagging.  so yes less expensive. thanks! those tools look super useful.from what i understand it\\'s a variation of a \\'leave n out\\' method, but instead of removing them, they are down-weighted by an infinitesimal amount.hello insight friends! can anyone point me toward fellows who have worked with instagram data for their projects? i’ve got a project idea i really like, and i want to get to acquiring data as soon as possible. any help or advice would be appreciatedyou can also look into mechanicalsoup: cool, feel free to get in touch if you have questions about how to use it, though it\\'s been a while since i looked at it so i may not be any use (especially with the d3 map...i have to learn d3 from scratch every time i use it) albert lee from my session didthanks!i\\'ve done some stuff with instagram data in the past, but to my knowledge they no longer support their public python apiagreed! the only way i would use selenium here is if the urls didn\\'t follow an obvious pattern, e.g. they were generated by some horrendously unreadable minified javascriptyeah, i saw that it’s no longer updated. i was hoping to talk with someone who has used it to see if it would be a good way to go for my projectwhat\\'s the benefits of doing that rather than just leaving it out? from a computational perspective, infinitesimally small is just 0?besides giving more stable estimates of the predictions (less variation), it also accounts for bias is the jackknife estimates.hm, it\\'s not immediately obvious to me why that would be. yes larger sample size would strictly decrease calculated sample variance of a single jackknife iteration since you\\'re just increasing the denominator but not the numerator. but i\\'m not seeing why the actual variance of the jackknife estimate (across all iterations) would be smaller since the value of each estimate remains the same. i generated my own data: for a regular jackknife, the bias has a factor of n-1/n. if instead of leaving 1 out, you leave an infinitesimal part delta, the factor would reduce to n-delta/n = 1.something along these lines.thanks ! i’ll look through your project and might hit you up if i have any questionsi’m giving a talk at pag in san diego on ds careers in industry focused on animal genomics on sunday. if you have any work at your companies that you’d like to advertise to academics that is related to genomics in plant and animal (non-human) genomes, please let me know! happy to include it.my experience is that these infinite scrolling pages make ajax requests that can be easily modified to get *all* the posts/images/columns in a single shot so you don’t even have to scrape the pages most of the time my point was that for a computer, infinitesimal delta is equivalent to 0. but yeah on a purely theoretical level, sure. or if you set your delta to something like 1e-5 i guess.does anyone have good tips or resources for mentoring new data scientists in their first role out of academia?mentoring is a very personalized thing :slightly_smiling_face:  would depend on what areas you’ve identified that you think they should improvei guess tip #1, identify ways in which they can growtip #0 : identify and help translate/build on strengthstip #1: build mvps with caveats to deliver value early rather than a better solution that takes more timethen build more mvpsand more mvps :wink: thoroughly go through the processes each company is using to achieve team coordination. but that would necessitate the team has such processes ... :stuck_out_tongue_winking_eye:haha, yes; our processes are mostly developed, but always being refined. i think the biggest difficulty i’ve seen is in getting into a business-focused mindset where there’s a balance between speed and perfection, so i’m trying to figure out how to support new people in that transition. thanks for the suggestions!i found that taking some of the pressure off of them works.  in teh past i’ve just said “i just need this model to do something by x date, give me the best thing you can, but let me worry about the outcome if it doesn’t perform well”  (also, you have to mean this for it to work, the model might not do well :stuck_out_tongue: )make the on-boarding period a sufficiently \"rich\" experience to assist in the change of mentality during this low risk period....?lol my suggestion is the *exact* opposite of  . i guess it depends on the person who is the new hire :slightly_smiling_face:time to save the internet! hey guys, does anyone have any experience porting a sas logistic regression model to sci-kit learn?if i have a s3 dir full of csv files, how long does a emr cluster take to convert them to parquet.  and secondly, what is the best way to convert csvs to parquet (besides emr)?  i’ve started looking into using aws athena / spectrum, but query speed is super slow if data is not in parquet formatideally, converting mysql extractions to parquet files periodically is great, if you know of any such tool, pls let me know! :bow: you should be able to write a pretty simple spark application that uses the csvreader into a spark dataframe and save to another bucket as parquet. i assume you can just submit this job to emr and it’d take care of it. more hosts the faster it is, unless each of your csv files are like 1kb in super small. one catch is to read all csv files at once otherwise there could be schema inconsistencies in parquet from file to file or folder to folder.any good resources on price testing methodologies for susbcription services?any one else losing their mind over gdpr out there? join morning everyone, does anybody by chance have the flask mini tutorial that insight provides to new fellows? this link may work: thanks mike! access requestedfrom ruiz.eulerthat request was probably routed to the nyc office. let me know if you don\\'t get access, and i can download and send you the files.thanks mike, looks like you\\'re right, i would appreciate if you can send them here has started doing a tutorial for boston. he can probably share something with you.there\\'s also this that we give to the de fellows: : if anyone\\'s looking for housing, the other unit in our duplex is opening up:2br, $3800 in the mission (22nd &amp; folsom).two insight alums live upstairs.: i’m the first and still-only data scientist here at greenhouse. we are starting to think about the “career ladder” for me and other data scientists, analysts and engineers. i’d be very interested in seeing the structures and skills that you all have defined at your companies. dm me if you’d be willing to share your insights!: i have permission to share our “data science levels” deck at glassdoor. it’s kind of rough but just a sec...our director (actually a vp now) was planning on a blog post about it sometime anyway. i’d be interested in seeing that as well!that\\'s really great  -- thanks for sharing!nothing really about data engineering in that. that is a different org, but data science &amp; analytics has 1-2 product managers that are product owners for their scrum.at smaller scale we were all under one director.how did the transition between the single director and multiple product owners go?  (i’m in a similar boat to , first ds here and we’re growing our team)when i started there was five ds/analysts and a few engineers, maybe 4-5 who did a mixture of bi etl and sexier big data stuff all under one director who reported to the ceo.then i believe around the same time we brought in a director of data engineering and a second analytics director who has taken part of our team under sales and marketing. my supervisor is the vp (original director) who reports to the cto.i’d say the two areas in which things are a little more stressful big-company-ish is that 1) the core data scientists are now split into two big teams and sometimes we wish were were more intimately connected. we at least have an all data science meeting every two weeks. on the other hand our director was way over loaded before the split so in net it is good.and 2) now i have to go through a product manager to get some data needs implemented. which is sometimes feels slow.the nice thing though about having a dedicated data product manager is that there is less explaining of your needs to engineers and you get more solid delivery commitments. i had a few cases in the old days when i had something 90% done and then it felt like i spent 2x the time just explaining everything to someone who wasn’t really adding much to the project.haha i know the feeling on that last bit...well really just one guy and he’s leaving so idgaf :laughing:i am in a similar boat that i am the only data scientist here at booster as well, just curious if you could share your experiences in regards to having an imapct at organizational level, and dealing with perhaps people who are not used to data.thanks, alan !now it goes more like: “i want all our job descriptions in a new db that is more analytics friendly with metadata a, b, and c so i can build product x and y” and i get “cool, but no, we don’t have time” three times over three months until another product manager is finally really curious about whether we can do y and i finally convince our director that this shit could seriously be cool, yo; and then magically the data product manager comes to me one day and just says “yes”. later we have a couple easy meetings, i don’t hear much for a quarter, and then bam it is ready.making it sound nice being the only data scientist :stuck_out_tongue:although i couldn’t sell them on my title being “god-emperor of data\"oh, yes, enjoy it while it lasts.we’re picking up more out of the current insight session, but it’ll still be an integral team rather than splitting permanently in organization to different groupsthough likely still reporting to the head of business products like i do nowput that shit on a business cardstay positive johnrwalkyou’re not my supervisor nuggetboti was never in a single ds org, but i was surprised to hear later after we had some successes with some internal data products that the “ceo and other leadership never thought you could do x…” and i’m thinking what? they hired data scientists and they didn’t know we could do x? — it makes you realize how important it is for information to flow up the org selling your capabilities., yeah i thought i was good at talking to people at different levels, but someone just recently told me that i should stop thinking about this logically :stuck_out_tongue:you need to feel the answers.dealing with business people is more about personal relationships…that’s the lesson i’ve been trying to keep in heart and mind whenever  i have to talk with them.and just be as explicit as possible. there’s a saying in spanish, cada cabeza es un mundo. each head is its own world, and that has never been truer for me. i was hoping to hear more of what anyone had to say if they were the only ds working around challenging specimens :stuck_out_tongue:well, i think  is maybe being a bit too negative about business-folk.:stuck_out_tongue:that’s why i need to hear it from you the most experienced peoplei think the challenge is for us to come to their level in understanding the business problems and actionable insights that can be deliveredstay positive zkermish…and to communicate in a constructive way when the question they\\'re asking won’t lead to an actionable insightbut great discussion about teams and structure (and thanks for the slides )!. the flip side of ’s comment about information flowing up the org to sell your capabilities is higher-ups asking for things that they think are possible but aren’t. communicating the limitations of data science (often given data limitations) for a non-tech audience is also challengingrelated to that is the perspective i’ve run into now and again: “this data product you built is wrong in cases a, b, and c, when are you fixing it?”  well, actually, i have no immediate plans to fix it because it is correct in the much more common cases; and in fact a human is going to be wrong in cases d, e, and f. even though everyone has heard and mostly accepted the 80/20 rule, sometimes people fall back to absolutes and needs to get a little reminder that getting to 81% is going to require another 20%.stay positive alanoh, and my colleague mario will be on that general assembly “how to hire…” panel tonight in sf. he’s a cool guy, and supposedly our vp of data has prepped him with good advice. if anyone goes you should ask him about banana hefeweizens. apparently he likes that sort of thing.today is a good reminder of why this slack channel is great.one more resource for interview questions (with answers / hint in python) - hi all,i\\'m looking for a presenter for wwcode-sv data science group\\'s monthly talk (august 24 evening at cisco (north san jose)). we had some scheduling issues that\\'s why the last minute call.if any of you would like to give a presentation on a ds or de topic (about 60 min), please let me know. (the topics of the last three meetups were elasticsearch, careers in data science, scalable machine learning in r and python with h2o)please let me know if you have any questions or if you would be interested in giving a presentation on aug 24 or at a later time.thanks,    juditabout wwc:women who code is a global nonprofit organization dedicated to inspiring women to excel in technology careers by creating a global, connected community of women in technology. the organization tripled in 2013 and has grown to be one of the largest communities of women engineers in the world.andrew ng\\'s coursera machine learning course in python: hey all - does anyone have recommendations/resources for ds wanting to work on failing fast and iterating?sprint planning + standups are good to see where people are at and what they plan on doing and how long they plan on working on a task, and can use those as a way to nudge people to refocus.deadlinesmake tickets with clear acceptance criteria, i.e. instead of saying \"build a model\" either put a deadline on it or some other way where they can iterate quicklyi have had my best results with short, non-intimidating standup meetings. if everybody shares a 1-2 minute progress report, no one feels singled out and no one gets left behind. this can be hard if you have members of your team that get kind of attacky, in which case the scope increases to reining them in too. this also starts to get unwieldy if the meetings are too big. i have found that groups of about six work pretty well for this kind of standup. if everyone has to report their progress every day, then it\\'s hard for team members to keep spinning their wheels on tasks with marginal returns.related to brianna\\'s question above about failing fast, how do your teams balance development code vs production code? example, i was given a data dump with lots of raw historical data which i (the ds) wrote python code to build features and train model. now my team wants to push some of the feature creation (e.g. historical averages) back into sql via sas. this production code will be run on new data not the training data, so i can\\'t plug it directly into my python features code.i’m going to guess the lack of replies is due to imposter syndrome. :wink: i’m certainly not a great data scientist, so what can i contribute? that said:i think “great” data scientist know that data science isn’t building models and wrangling numbers; data science is about using data to inform decisions, build products, and make your company money. they know the 80/20 rule, they talk to customers (internal or external) to make sure their work is having impact. they communicate effectively and often.+1 for standups and deadlines. if you\\'re a team lead you can evaluate progress continually, and always descope, descope, descope+1 for deadlines also - you can help them learn to scale back things as deadlines approach.  they should be thinking of the project in pieces.  what does the whole thing look like?  if i had to deliver it tomorrow, what could be cut?and then order those thingsi’m working on something like that right now for example.  deadline next friday - there’s a plan for the whole thing, and then a multi-stage contingency plan of things to pull back if it doesn’t look like they can be completedi would try and understand what the source is of their \"perfectionism\". sometimes, it is fear of being judged, sometimes it is an environment that is perceived to be one in which one can\\'t fail, sometimes it is habit, and sometimes it is lack of knowing how to work in a different fashion. in my experience, the best solution is an adoption of agile (standups, deadlines, pre- and post mortems) adapted to suit data science and ml tailored to the source of the problem.\"what does the whole thing look like?  if i had to deliver it tomorrow, what could be cut?\" -- that is also a good interview question :wink: if you have enough influence on production code then the dev code could go a long way, and be a part of the production code. if there is a hard line between the scientists and the production engineers then you are limited by what the engineers can put into production. also, writing back features to sql, is akin to the dutch boy with his finger in the dam. there are two things at play here, one the company has to evolve, and for the company to evolve they need to see the real value in models - education about data analysis. the fact that models are fast, features engineering is easy, tunable to variations in the data, capable of online learning, unlike a rules based approach are some of the things they need to see and value.more information: we have a small team with no production engineers. the structure is that the data scientists write python code that sits on top of the sas sql pulls created by data analysts.i might be inclined to see if there\\'s a larger problem thatcan be broken down into smaller pieces. this has the advantage of creating small, esaily achievable goals, and also serves as a foundation for figuring out where this person might be getting \"stuck\" in the process.i have also had one instance (and thankfully _only_ one) of someone using \"perfectionism\" as an excuse for never finishing anything because they didn\\'t actually know what they were doing and were generally bad at solving problems they haven\\'t encountered before. :confused: anyone want to save string theory!? all you need is some good data science!(views are my own: you may not believe string theory needs saving, or deserves it. ;-))“*goals and vision*we are very pleased to be hosting the workshop on data science and string theory. this is the first meeting of its kind, where the goal is to unite big data techniques with string theory, in order to systematically understand the string landscape.”haha jeeze they are still at iti don’t really think of the landscape as being science, so hard to tell what data “science” there is to be done with itlet string theory die and good riddancebrutal gentlemen, brutal!and i agree! :smile:i went to grad school at penn with jim halversoni lived with a string theorist in grad school and me and the other astro grads mocked him mercilessly, he graduated in 4.5 years and became a phys prof a couple years later…theorists (a former one myself) don’t need nearly as much time to write a dissertation, as they/we aren’t waiting as much on datai’d be curious who that was, , as faculty positions in theory are frickin’ rarefractionally speaking (as a fraction of phds in theory produced)it was chris beem. he was at stonybrook and then moved to oxfordhas anyone worked with aws glue or aws data pipeline? i\\'m curious to hear about your use case and when would it be an ideal managed service to use.any scala devs here? i\\'m wondering what this syntax means:abstract class treenode[basetype &lt;: treenode[basetype]] extends productabstract class and extends i get, and treenode[something] means that treenode takes a variable of something type, but i don\\'t understand basetype &lt;: treenode[basetype]similarly, abstract class rule[treetype &lt;: treenode[_]] extends logging`a &lt;: b` means `a` has to be a subtype of `b`see is basetype an actual class, or is it just used as a generic class identifier?usually it\\'s either a class or typethe above example i pulled is from the spark source code, and i\\'m confused because i don\\'t see basetype as an actual class defined in the spark packageit seems to be a self-typeah! ok, i think this is pointing me to the right trackthanks a bunch! i really need to go learn scala i have not looked at data pipeline but my company was planning to use aws glue for etling data into redshift from s3. glue did most of the things we needed it to do. however, snowflake managed using home grown apache airflow did everything glue could do faster and for less money so we went with snowflake instead. i think glue needs to be more transparent about it\\'s \"job bookmarking\" feature (how it tracks which files it has processed and which files it has yet to process) and also provide a better view of job run history in general.worth looking into higher kinded types: personal disclaimer: i highly advise against trying to understand this in detail if you are not familiar with elements of type theory, for now just having it in the back of your mind should enable you to sleep better at night :slightly_smiling_face: when you say \\'snowflake\\' you mean snowflake data warehouse, or something else?yes the data warehouse.how did your airflow dag take into account changing schemas? assuming it did  something like the cataloging of tables via the crawler like in glue.it did not, our data is very simple.however, with json data, snowflake can handle changing schema. but not other data typeshmmm, perhaps glue would have been better if you had an evolving schema that wasn\\'t json.then again i can\\'t imagine what kind of task(s) you\\'d need in airflow to take into account changing schemas.i would not want to implement automatic schema discovery by my selffor the video files, if you install nginx-extras instead of just nginx, it will include the mp4 module. then that configuration info goes into nginx.conf, telling it to serve files from the \"video\" folder with the mp4 command. “make sure you are convincing them” words to live by!speaking of domo, looks like they re creating quite the splash at the tableau conference (involving snoop dog no less)   : for those of you working on data science teams, what metrics do you use to measure your performance? what do your performance reviews look like?my team is weird, so probably not representative, but our performance reviews are a mix of target metrics our managers set with us (blog post count, projects completed, some other fuzzier ones) as well as peer reviews from four peers (two we get to pick, and two our manager picks). note: i haven\\'t gone through one yet, but that\\'s what i\\'ve been told.this is awesome. thanks i used java heavily in my last ds role. i recommend learning the standard collections well (which i mostly did from the oracle docs). nd4j is a decent replacement for numpy. apache commons has a lot of general math and stats. feel free to ping me with more questionsexcited to announce our latest insight fellows program focused on data product management. you can read all about it in this blog post:  if you\\'re a data pm, are hiring for one, or just want to compare notes or get involved in the program be sure to reach out to me or , who\\'s developing the programjeremy here. happy to answer any questions you might have about the program here or in private messages.ah thank uhi everyone! get a chance to win 50$ amazon gift card by entering a few made up sentences, follow this link: affordable mechanical turk! enjoys eating all of the peanut m&amp;mshey guys. i’ve been acting as a data pm for the past 6 months or so. happy to get involved and/or chatya damn rightalso, i added fail2ban on my aws - if you go over authentication logs, you can see thousands of attempts to access your server. another good strategy is to enable ssh-key authentication and disable password authentication. you can also enable 2fa via pem.fyi, with respect to the account takeover on aws, i wanted to put this in general, as securing your account is important, esp for new folks. it is crucial to enable ssh-key authentication and disable password authentication to prevent your account from being brute-force attacked. strong password, disable the default user ‘ubuntu’ if you want to be extra secure. if you want to see how often people are trying to log into your instance, just type:grep -i ‘invalid user’ /var/log/auth.logfood for thought.hi, i’m curious to learn about people-you-may-know models on social networks. that is, what features, models, and other components are involved in predicting who a specific user might like to connect with on a social network like facebook, twitter, or linkedin?for the people who use the latest version of macbook pro, is it possible to find a keyboard protector that will have the touchbar work?i was confused as to the equity questioni thought it meant per year, heh.also not sure how it differs from rsuthought someone was getting 350k in equity a year and thought damn! need to get that job!yeah, realized after the fact that those questions may be poorly wordedany advice for clarifying w/out messing up interpretation is welcomewas planning on just merging the two overlapsi\\'d just go with \"equity value per year\" or somethingthis was a good idea. thanks! okay, updated the questions, lets see how responses come in over the weekend. i’ll post once more on monday morning, and then i’ll stop spamming the channel.the original questions/answers are preserved on the spreadsheet (here: ) but the survey now reflects the latest updated questions: has anyone performed a mapreduce in bigquery following these instructions:i have a project and table correctly identified, can ssh into my instance via command line, but when i run the script, it seems to stall at the error: command failed: wait ${subproc} on line 326.this did not fix it:if anyone has experience with this, i would greatly appreciate any tips. please pm me so we don’t spam this channel :slightly_smiling_face: thanks!hi fellows, my company currently use swift for ios app, ruby on rails for the backend server/web app. i want to build a recommendation engine.  any suggestion on which language/stack/pathway i should go with? thanks! totally from scratch.the rec engine should probably be fully independent/separable from either frontend codebase. assuming they both write to the same backend db, i’d recommend your rec engine be written in whatever language is best for that problem (likely a mix of python and sql, perhaps other stacks if you have a particularly large data set you’re building recs from) and you can serve up results via an api (again, probably a combination of python and sql, but ruby could also work well here)the more independent and flexible your rec engine is the easier it’ll be for you to iterate on the it (leading to greater general happiness :wink: )i think there are a few possibilities:1) you simply create the full data by duplicating the rows. as long as you don\\'t have too many features, 500k rows won\\'t be too much for sklearn to handle.2) r-based packages have support for count proportions. the basic idea is that the likelihood is written as a binomial instead of as a bernoulli. e.g., see here for glm () and here for jags ()3) you might be able to use the sample size to adjust the \"weights\" in sklearn\\'s logistic regression. for instance, see the top answer in this thread (), where apparently you can input a weight for each data point. i\\'d try this out on a small synthetic example to see that it matches the results from #1 or #2, though.you are correct that linear regression is the wrong tool for the job, though. :smile:(my sql tutorial slides)automatic text generation fun.  theoretical physicists especially encouraged to try:can you guess a real arxiv paper from a fake one?stay positive ruthtoneranyone using the most recent version of airflow (1.8.1) willing to answer a question about it?  we\\'re seeing weird behavior surrounding backfilling and can\\'t tell if it\\'s a bug or something specific to our setup.  i can start a thread to not clutter the main group, any advice is appreciated!brianna.schuyler: i assume you mean airbnb\\'s right?yes sirdo you guys use it at square?we use a fork of iti have only dabbled myself, but i know jackie has used it a lothmm, ok.  the thing is, there\\'s this new concept in the newer versions called \"catch up\", which is different than a backfill.  so there are like two use cases: 1. we have a dag that runs daily with a `start_date` of last week. when it turns on we want it to “catch up” and process the last 7 days. 2. we have dag that runs daily with a `start_date` of today. we want to manually backfill using `airflow backfill....` and have it run for the last weeks worth of days. in both cases, each day should run to completion before starting the next day.  our dags are doing the correct thing in the case of a \"catch up\" but with a backfill they appear to be ignoring the state of previous days. we\\'ve tried using `depends_on_past`, `wait_for_downstream`, `max_active_runs=1`, and dedicated pools of size 1 and none of them have worked.  it\\'s weird that there doesn\\'t seem to be a lot of chatter about this on the internets.i would toss that in am email to jackie. or try pining her directly, not sure she checks slack too ofteni know she had to convert most of her etls overk, will do!hey sv / sf! you\\'ll receive an evite early next week, but our new session is starting on may 30 so the alumni / welcome party will be friday night 6/2 @ 8pm! save the date and come say hi!ooooo it\\'s coming so soon!soon...thanks guys, this is fantastic. especially, i love it when there is an actual package to do this :slightly_smiling_face:career advice question: i\\'ve gotten a couple of messages from recruiters at big companies that seem to be actually writing to me. i\\'m not interested, and don\\'t think i\\'m qualified enough yet for these to actually be high quality recruitment attempts - they feel right on the border of spammy, and are probably just casting a wide net to get a lot of interviews - but is there any downside to not responding?there is no down side to not responding if you\\'re not looking or interested. some are in fact just spamming everyone, and others are targeting more narrowly. you can generally tell based on how it\\'s written. :wink:i ignore the spam ones, and i follow up on others if interested. a recruiter message on linkedin got me an offer from ms a few months ago that i almost took, except in the end moving to seattle was a bridge too far for my family.if you reply to good ones and politely decline, saying something “thanks for this great opportunity but i’m currently not looking for a change” they often keep you in mind for future positions they have, at which time you might in fact be interested.but ignoring them doesn’t do anything badi attempt to respond in bulk maybe once per week to as many as i can get to in 5-10 minutes or so -- you can copy/paste something like papadimitriou\\'s message and move on. likely they\\'re just searching \"data scientist\" and spamming everyone, and time is valuable so i wouldn\\'t feel bad at all about not responding if you have better things to do with your time :slightly_smiling_face:anyone know of any good resources on proper data modeling? i\\'ve heard great things about this series  but i\\'m on a time crunch :confused:you can send interesting ones to insight :thinking_face: not interested but you can find more people like me at insight?hi all, can i get easily get a mysqldump into postgres?unsure, but maybe you can go mydsqldump -&gt; csv -&gt; postgres?also: this link looks very interesting apache drill claims to be able to do something like thiswe use tableau for lots of things, but people here know many open source python solutions as well thanks , do you have any thoughts on this ?mapdsuperset!what are your favorite resources for setting up a data science environment using jupyter notebooks/python/terminal on a windows pc? what about favorite blogs for doing data science on windows?i’m coming from a unix/mac background and i haven’t used windows in over 10 years.hi becky. i\\'m also a current fellow, in nyc. i\\'m using pyaudioanalysis. it is for audio data but i think many parallels can be drawn. the code is written in such a way that you can modify the functions to your particular needs. check out the features they extract. i work with python in linux, mac and windows.  python and pip works more or less the same in all three operating systems.  virtual environments work the same.  setting up virtualenvwrapper is a little different for windows ().  one little known feature of windows 10 is the linux subsystem - you can basically run linux within windows:  . it’s super cool and it works pretty well.  however, the filesystems are separate.is anyone familiar with spark\\'s catalyst optimizer (automatically optimizes queries for spark sql and the dataframes/datasets api) and writing custom optimizations? i\\'ve been working with the spark-cassandra-connector and discovered that write throughput (from spark to cassandra) can be increased dramatically if the dataframe is sorted by cassandra partition key prior to writing to database. i\\'m considering modifying the connector to automatically perform this sort prior to writing to database by writing a custom catalyst optimization, but i wasn\\'t sure if this is something that is actually possible have you been able to time how long it would take spark to sort the data for each partition key? this should help you decide if it’s worth building it. e.g sorting a df on spark and complete write to cassandra takes 1hr vs just a unsorted write from a spark df to cassandra 2hr. yeah, with a 26 gb and a 260 gb dataset, i\\'ve discovered that sorting the data first would be faster. results may vary of course depending on the exact distribution of the data, but it\\'s significant enough that there is an open issue that datastax (developers of the connector) has put up for it.  it\\'s something they\\'re still looking into. we think that it\\'s worth building, and creating this custom catalyst optimization is the way we\\'re thinking it should be implemented, but i\\'m just not really sure how to work with spark\\'s catalyst optimizerah that’s pretty awesome. you plan on extending the current connector? yeah - it\\'s quite unintuitive that the write speeds would jump up so much by a simple sort, which is why they\\'re thinking of just making the connector automatically sort by partition key before saving. catalyst is basically the optimizer under the hood that makes dataframes so much faster than rdd\\'s (and the reason why spark\\'s contributors highly recommend using df\\'s over rdd\\'s), as it does a bunch of pre-optimizations that aren\\'t possible with rdd transformationsit seems that the best way to implement this automatic sorting is to just add a custom rule in catalyst, but that kind of functionality is quite new in sparki have dealt with a/b testing quite a lot at glassdoor. we have our own in-house ab testing framework. the allocation part of it is mostly a java port of facebook’s planout (find it via the java link on planout’s github repo). the analysis and reporting is custom python. i can’t do a very fair comparison to services such as optimizely because i wasn’t part of the infrastructure decision process (but i think at least one group here *is* using optimizely).i’d say three of my big tips i can give is that to do good a/b testing you need to1) get really really focused on what is important: i see it all the time that executives and product managers look to the simple top-level metrics because they are easy to understand, measure, and move. sometimes you have to settle for these, but at least keep reminding everyone of the limitations and what might matter more (and push to get the data in place to measure these)2) make sure tracking is rock solid. when you are looking at 5% effects all kinds of quirks of the data can cause problems. when doing a/b testing on a new part of our product i don’t think we have ever had a clean test on our first try.3) watch out for the long-tail of users: as an example consider scrapers/bots, we get a fair amount of that activity and do a pretty good job of identifying them, but just a few slipping through your filters and ending up in your tests may shift the results artificially. the trick is this activity can be really similar to your most active users which you definitely do want to include in your results.thanks for the info and pointers ! yeah, we were looking into planout as well since it seems to deal nicely with related experiments via namespacesand, fyi, if you use something more general like planout then expect that it is going to take a lot more work explaining the data model and test setup to product managers. it is not just one users &lt;-&gt; one treatment and some may take a while to full wrap their heads around it. of course the upside is that at scale it is a lot more powerful.on your analysis, what’s your method for controlling for false positives when running many concurrent experiments? it seems to be a common pitfall if not using something like a hierarchical bayesian method or whatever optimizely does (some variant of sequential testing?)thanks  for planout - working at the parameter instead of treatment level seems so obvious in hindsight. [programming] has anyone encountered generating a powerset with some of the elements having a parent-child relationship? trying to visualize the logic to put it into code.: example? also are there any special constraints for including a parent/child relationshipso like if my set = { a, b, c, d} and a is parent to b, is {a,b} a valid component{a} and {a,b} are valid, but {b} isn\\'tok so a child must be accompanied by a parent? ;0and neither is {b, c}, {b, c, d}... etc*;)yeahprobably the easiest way would be generate the full power set and then remove ones where parent isn\\'t preent?present*well i would assume taking the powerset is out of the questionyeahjust wondering if that is out of the questionsince it\\'s obviously exponentialthe powerset is here to staylolthe use case involves finding every combinationwell just take those unaccompanied children homeor take them outpretty inefficient if its sparse thoughtruedo u know how sparse the dataset you are dealing with is tho?how often is there a parent/childi have 100 elements, 20 of them have one or more childrenif not too often i say go for itok never mindhm well given the validity example i would still consider doing it and back-checkor you could always make sure to add the parent firstbecause order doesn\\'t matter here, correct?nopeorder doesn\\'t matterthing is i don\\'t think it\\'d make a difference for timewould it? it\\'s o(n) just to remove them all(i feel like this is more and more of an algorithm interview question)well there would be 2^n total sets to constructworst casewhere n = the number of elements in the original setbut you would have to generate the 2^n sets first and _then_ scan to removeyep, so that\\'s 2^n again isn\\'t it?or wait no, it\\'s o(m) where m is the size of the power setrightwhich is why i was assuming just doing that was out of the question beforei feel like if you create a power set of all the elements w/o children first you would have a good startdon\\'t get me wrong there\\'d still be a vast majority of valid elements ungeneratedbut yeah computing the power set = no buenothere\\'s a good one for lda that has topic on the x axis and words on the y. i found it useful for examining the output side by side.has anyone worked with airbnb’s superset dashboard?johnrwalk: we looked into it, but i think it had a weird requirement for the backend. i’m interested to hear about it if you end up working with it.i’ve been able to get it working on my own machine easily enough, but i’m running into trouble exporting the dashboardswhich seems like it should be simple…that it does…it has the functionality, but there seems to be some underlying flask issue that causes it to not actually present the dialog to get the export filei made a word cloud! :slightly_smiling_face: i looked at it but went with  instead good advise, the backfill is not working eitherdo you have some advise as of how to debug what may be happeningi’d start w/ an extremely simple dag and do a backfill on that and build up from thereyeah so that\\'s what i didit used to work but for some reason it stopped workingit\\'s probably something trivialit\\'s the hello_world exampleit just get\\'s stuck in the run partlet me try another simple examplethis is what i see in the logsi mean in the backfill outputairflow backfill hello_world -s 2017-08-16 -e 2017-08-17[2017-08-17 13:33:25,860] {__init__.py:57} info - using executor celeryexecutor[2017-08-17 13:33:26,244] {models.py:167} info - filling up the dagbag from /home/ubuntu/airflow/dags/home/ubuntu/anaconda3/lib/python3.6/site-packages/airflow/ti_deps/deps/base_ti_dep.py:94: deprecationwarning: generator \\'_get_dep_statuses\\' raised stopiteration  for dep_status in self._get_dep_statuses(ti, session, dep_context):[2017-08-17 13:33:26,771] {models.py:1126} info - dependencies all met for &lt;taskinstance: hello_world.dummy_task 2017-08-16 12:00:00 [scheduled]&gt;[2017-08-17 13:33:26,775] {base_executor.py:50} info - adding to queue: airflow run hello_world dummy_task 2017-08-16t12:00:00 --pickle 1 --local[2017-08-17 13:33:26,785] {models.py:1120} info - dependencies not met for &lt;taskinstance: hello_world.hello_task 2017-08-16 12:00:00 [scheduled]&gt;, dependency \\'trigger rule\\' failed: task\\'s trigger rule \\'all_success\\' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={\\'successes\\': 0, \\'skipped\\': 0, \\'failed\\': 0, \\'upstream_failed\\': 0, \\'done\\': 0}, upstream_task_ids=[\\'dummy_task\\'][2017-08-17 13:33:31,697] {celery_executor.py:78} info - [celery] queuing (\\'hello_world\\', \\'dummy_task\\', datetime.datetime(2017, 8, 16, 12, 0)) through celery, queue=defaultnot sure what’s up, sorry.my only advice would not be specific to the problem but general debugging techniques not specific to your error (e.g. start from scratch again)yeah i\\'d agreei will do some clean up and try it againfor debugging though do you know if one can actually see which workers are running and then get inside what the process is doing?  i could use celery or localexecutor, or sequentialexecutoruse sequential to start “local” is actually what i used to do parallel submitting of tasksthere might be a verbose output when running airflow, not suregreat thanks dan! local executor is working thanks a lot!does anyone use aws kinesis, i’m wondering if it’s a good idea to set your own parition key so that groupings of data go into a shard rather than have it be randomly generated; any suggestions welcomei’ve played around w/ it before. depends what you’ll want to do with the data after it’s in the queue.  for example if you want a consumer to only read in data, e.g. form nyc, then might be a case to partition by city (possibly at the expense of complicating other consumers’ logic though!)and could see a slight performance gain if all the data is already co-located at the queue level.a definite potential downside though is that you may get “hot spots”, like having some shards with a lot of data, but others with very little data.ah great explanation, kind of what i though, thanks for taking the time! pinned a message to this channel.linux to mac is easy for the most partjust use brew as a package manager to make up for the lack of one in the macthe new hardware\\'s\\u200b direction is another matterhey ethan, one more question — i’m a little confused by how lightfm treats the `user_features` matrix in the `fit` method (which is presumably where user attributes are applied).  it’s described in the documentation as “the user weights over the features”, (as opposed to defining the features themselves?) so i’m not sure i’m interpreting it correctly.  also, in supplying users to `predict` does the `user_features` matrix need to be fully populated?  i may not have the full set of attributes for new users coming inagree on linux -&gt; macosdoesn’t take longlot of unix functionality already providedxbox or play station! if we really want to get a war going we should do vi vs emacs :wink: :vim:vim &gt; vi:+1: :vim:or vim vs neovimneovim is my chosen platform! love it!(after 11 years of vim)never heard of neovim — am i missing out on something? (vim user)umm, not super missing out on anything after vim 8 added async plugins.  neovim has a nice terminal though.and it\\'s aiming for the holy grail: a vimlib.c that you can link against from anything. full vim in whatever text editing program or ide is the dream.:heavy_dollar_sign:theres a vim mode in emacs (evil-mode)yeah, it\\'s definitely a bit confusing. the way to think about it is that the row of the `user_features` matrix should match the row of the user in your `interactions` matrix. in terms of the features themselves, you can think of them like regular scikit-learn features. the one key thing is that you might want to one-hot-encode a unique dimension for each user. if you were to _only_ include a single, unique, one-hot-encoded feature with weight 1 for each user and each item, then this would be equivalent to matrix factorizationi\\'m not sure if that all made sense. at the risk of being narcissistic, i wrote a blog post on this  the \"learning to rank + side information\" section details how to add `item_features`.cool, i’ll check it out — i’ve been in meetings all morning so i haven’t gotten to dissect the sample data / examples for the package.as an alternate approach i’m considering a pure model-based approach with a bayesian network, which may handle the user features more cleanlyoh you can use xming, i\\'ve mostly used it when working from a windows laptop to a linux server:grinning:i kind of love where this mac/windows conversation has gone and that it’s gone on this long :wink:  just to add one more thought: historically, mac os has been much closer to linux systems (being bsd based) and with homebrew you basically get all the bells and whistles that devops folks like. i’ve always viewed macs as linux laptops that properly go to sleep and wake back up (at least that’s why i switched off linux 10-ish years ago). being able to shoot the shit with devops folk and understand how linux systems work in pretty great detail should not be undervalued. having devops friends at your company can really help you get things done.mobaxterm is also a good alternative, but i believe they both rely on having a connection to a linux server, not a local installation, right?that\\'s what makes the bash shell as a native application in windows so exciting.vim :+1:backed up a few posts in your blog, definitely more helpful than some of the package documentation i’ve found.  thanks again :slightly_smiling_face:haha thanks. good luck!random question: we all know about ‘lift’ (i.e., improvement of some value given a model or something like that), but what do you call the opposite of that? if your model does worse, then is it “negative lift”? “depression”? “decrease”? or do you just report “lift” as a negative value?“sink”? how well have you found `lightfm` scales to folding in new users?  presumably it needs to rebuild its matrices to integrate a new row (independent of the issue of that user not having any ratings, so it would be cold-starting just off of their `user_features`)i read in some blog that you can use xming with the windows linux subsystem haven\\'t tried it yetin terms of scaling, i\\'m not sure if this is what you\\'re getting at, but you don\\'t need to retrain anything. you just need to provide a `user_features` matrix with the new user\\'s attributes. as such, the only cost is the time it takes to build the matrix (which shouldn\\'t take long, though there\\'s fast and slow ways to do this with sparse matrices)hm.  my understanding was that predictions could only be made on users that were in the original interaction matrix, given that looking at the api docs for the `predict` method it calls for an index for the user-item pairs, as well as (optionally) the `user_features` matrixi think you can pass in a new `user_features` matrix, and you just have to make sure that your user-item pairs match up with that new `user_features` matrix. the only thing is that your columns (feature indices) must remain the same.lift can be negative imoloss?hello!  i made the  channel, please join if u use these technologies“drop\"?in this context i would it call a decrease in x, where x is whatever metric.in some contexts i would be comfortable presenting a negative lift number. (like a,b test results)thanks, all, i like ‘drop’ or ‘negative lift\\': we\\'re hiring for a data-journalist type position at nerdwallet that should be really fun. pm me if you or someone you know might be interested! i\\'m going on mat leave starting the 3rd week of november...does anyone know of anyone who would be interested in a 4-5 month temp position to cover me?  it\\'s at one kings lane and it\\'s awesome here!   basic requirements are sql, (r, bash would be nice but not necessary), tableau experience a plus.  we have a lot of alumni and fellows commuting from nj to nyc — hope everyone is safe and sound this morning!made it thru hoboken 15 minutes before the crashwas looking for a ‘random’ channel, but we don’t have one, so: blatant self-promotion … /giphy entertain us with whimsy!okay...hi guys, does anyone know of any easy to use python, d3/js libraries for showing event data overtime on a map  (all i would need to plug in are latitude/longitude and time)? is quite good.when i think d3.js, i don\\'t usually think easy. :stuck_out_tongue:i\\'ve use bokeh. it\\'s similar to d3.js and can make interactive maps.nvd3 check out folium + vincent examples:  try looking into the mapbox apithanks all! this helps a tonnvd3 is beautiful, but the last time i used it (feb), the documentation for nvd3 was pretty sparse.anyone here do a shiny app for their demo and manage to embed their google slides into the app?industry leaders establish partnership on ai best practices:  russia?no?nah they’d sooner release trump &amp; putin’s honeymoon photos i used an iframe in the ui.r file. see here for the code:  hi alums, thank for participating in this week\\'s interview blitz. starting next week fellows will be going into real interviews and we still need 3 alums to sign up in order for each fellow to have at least 1 mentor session. if you\\'re available, please sign up here --  please dont use the channel tag !!my apologies i meant to post on the ny channel. i don\\'t mind if you use the channel tag !!people… chill.brick killed a guy.: if you don\\'t want to get notifications you can change your notification settings by going to the gear menu and selecting how you\\'d like to receive them.now is a great time to introduce marie paule to those of you who haven\\'t gotten a chance to meet her! she is our newest team member in nyc and she\\'s amazing! please give her a warm welcome :smiley_cat:can we just ask slack to disable the channel notification? someone here has to work therei love that faye is now an at channel meme, remember to keep your office space clean and tidy :d sending stuff to channel is the modern equivalent of reply-all. :smile:more time is spent arguing about the proper use of the channel tag then the amount of time it takes everyone who dislikes the notifications to dismiss them.now that everyone is awake - invitae is _finally_ ready to start doing some cool machine learning on genomic/healthcare data if anyone is interested.  i keep my office space extra un-tidy in honor of hahahah well played not everyone is on us time zones  ..slack is a tool for asynchronous communication to help us use one of our scarce resources most wisely (attention). @ mentions force synchronicity which is useful in case \"hey guys, the church next to the insight nyc office is on fire\", which happened (arguably, this should go in the nyc channel, or we should create an nyc office one). this channel has many new slack users, it takes a while to get used to slack etiquette. in the meanwhile, we create some of the best memes. also,  welcome!fwiw, now is the time to get all your announcements in since everyone will be checking this channel! welcome! if you guys are considering mock phonescreens you can count on me from svhistoric preservation has been battling this kind of shit for a while - look at what could be, if only developers valued rehab &amp; community engagement as opposed to maximum high priced, high density housing: hi andrea, we\\'d love to have you. please sign up at a time that\\'s convenient for you. thanks again! -- he should probably lay low for a whiledo people have slack actually send interruptive notifications their phones for things like pager duty? uploaded a file: yes. :)per ’s advice, today is the last day to use coupon code (ins25x) for the upcoming nyc dataengconf - i know a few of us from insight will be attending, so would love to meet up with othershello all, i am looking for a cloud storage provider with reasonable price. our data is around 100 tb, we need frequent upload but rarely download. any recommendations?!checkout aws glacier - backblaze? (the b2 cloud storage version, not personal) - my company doesi see -- we have them on slack as well but i figured people would use mobile numbersi have notifications set to hear about things on my phoneone of the most recent ai fellows  just posted a blog post about how to generate over 10s of millions of extra text data if you are lacking data for a classification project. read more here: i like this part: \"they were able to provide me with just a single example question for each of the seven sub-classes!\" nice article :slightly_smiling_face:thanks :blush:stay positive alex_gudelean analytic talks about how arbitron moving to more real time data on radios changed modern music, 538 covered this as well, and whether data (or bad data) killed soft jazz.stay positive alex_gude: buzz of! :wink:nuggetbot: weather palo altoweather for palo alto, ca, usacurrently: partly cloudy 19.9°c/68°ftoday: mostly cloudy throughout the day.coming week: no precipitation throughout the week, with temperatures rising to 26°c/79°f on friday.nuggetbot: weather birmingham, alabamaweather for birmingham, al, usacurrently: mostly cloudy 28.3°c/83°ftoday: light rain until this evening.coming week: light rain throughout the week, with temperatures falling to 24°c/75°f on friday.nuggetbot: weather new york cityweather for new york, ny, usacurrently: clear 24.4°c/76°ftoday: partly cloudy starting later this evening.coming week: light rain on sunday through tuesday, with temperatures falling to 21°c/70°f on tuesday.: i found the following to be useful: mitx: 6.041x introduction to probability - the science of uncertainty  hello all. anasuya\\'s and my team at memorial sloan kettering is looking into a projects using wearables / activity trackers, such as garmin, fitbit, jawbone, etc. does anybody know of good datasets from these kind of devices?  not from one device but... maybe?and thanks! yeah, these are useful although seem a bit geared towards identifying activities such as running, or walking from accelerometer / gyroscope data. i\\'m mostly searching for datasets with more derived data, like step counts, or activity indicators ...you should check out my insight project, activeme at  ! i had a lot of trouble getting data sets, but i have at least one jaw bone data set of ~1 year if you would like thathi lois! yeah, that would be great!hey bay area fellows, a close friend of mine is looking for a product manager role. she has a few years experience in fraud product management/operations and has successfully launched her own consumer hardware startup prior. she has some programming/analytical ability, and is flexible on subject area. she works at a larger tech company right now and is looking to grow. if you know of any positions that she might be good for, please message me. thanks so much! :slightly_smiling_face:also, :pinterest: pinterest is hiring data scientists right now for ads and discovery. let me know if you\\'re interested! dm me :grimacing:airbnb releases trove of new york city home-sharing data &gt; the new data set released on tuesday, which is made available only by making an appointment to visit airbnb’s new york city office [...]can somebody in nyc pick this dataset up for the west coasters?  :simple_smile: thought this article was interesting...: we had a talk by nicholas here at li last month - it\\'s a really clever way of disrupting image classifiers with imperceptible permutations since the parameter space is huge. however, it\\'s less clear how you would apply that to text classifiers and other domainsthe nastiest application of that is modifying a stop sign with just the right jitter to make it not seem like a stop sign to a self-driving caranyone looking for a consulting gig or potential side work in the future with an advertising firm? if so, pm me. a friend of mine is looking to incorporate more data science into their work, so it wouldn’t be immediate work, but it could lead to something. my friend is a smart and reasonable person, so it should be a good conversation and upside to pitch your value and the value of your work. i guess i just never cared about the security aspects, but it makes sense since there is more dependence on ml making decisions, how to protect from the “poisoning of training sets” becomes a critical issue.yeah, there is a lot of adversarial aspects to worry about. nicolas\\' work showed that you don\\'t even have to know the model that is being used - just train your own, find the direction of maximal change in the jacobian of the cost function, and add some noise there. the disruption is likely to affect the unknown model in the desired direction.a gallery of interesting ipython notebooks: in case anyone else faces the problem of needing a strong but interpretable classification model (which is explainable to non-ml/non-stats experts) - i hacked together a scikit-learn estimator producing easily understandable decision rules, based on some existing code and some amazing work on bayesian rule lists. i\\'m going to be in the market for a coworking space in the near future. does anybody have good/bad experiences with any spaces in brooklyn/manhattan? if so, please shoot me a private message.hey if anyone\\'s interested, city of boston is having a neat open data challenge (with prizes!) job alert (for senior candidates): capital one is starting an nyc branch of our research-focused “center for machine learning” group. it’s looks like a very cool opportunity where you’d be pioneering new machine learning techniques for the company. see the extended blurb in the child thread for details. i will disclaim that despite a moderate qualification list, they’re currently looking for experienced people that can hit the ground running. message me if you’re interested.sorry for bad formatting, didn’t have a public link to this:&gt;&gt;&gt;at capital one, data is at the center of everything we do. when we launched as a startup we disrupted the credit card industry by individually personalizing every credit card offer using statistical modeling and the relational database - cutting edge technology in 1988! fast-forward a few years, and this little innovation and our passion for data has skyrocketed us to a fortune 200 company and a leader in the world of data-driven decision-making.as a data scientist at capital one’s center for machine learning, you’ll be part of a team that’s leading the next wave of disruption using the latest in distributed computing technologies and operating across billions and billions of customer events to unlock the opportunities that help everyday people save money, time and agony in their financial lives. on any given day you’ll be:-using big data tools (hadoop, spark, kafka, aws) to conduct the analysis of billions of customer transaction records in concert with distributed neural network frameworks and other machine learning libraries-writing software to clean and investigate large, messy, structured and unstructured datasets-integrating with external data sources and apis to discover interesting trends -building machine learning systems from proof of concept through production on clickstream, customer, transaction, or behavioral data (to only name a few!)-creating full data pipelines and novel data tiers for your projects on the road to production-perform internal and external research for publication-contribute to open source projects as part of your jobthe ideal candidate will be:-curious. you ask why, you explore, you\\'re not afraid to blurt out your disruptive idea. you know python, scala, bash, and at least enough java to be dangerous.  you’re constantly exploring new open source tools.-wrangler. you know how to programmatically extract data from anything and model it into human-readable deployments.-creative. big, undefined problems and petabytes of data don\\'t frighten you. you’re used to working with abstract data, and you love discovering new narratives in unmined territories.-forward thinking.  you are always reading about the newest research on methods and techniques.  you actively try and apply these methods in your own projects, kaggle competitions, or in research projects.  you actively follow or contribute to open source software implementing these techniques.twenty-five years after capital one was started it’s still led by its founder. be ready to join a community of the smartest people you’ve ever met, who see the customer first, and want to use their data skills to make a difference.basic qualifications:-bachelor’s degree or military experience -at least 2 years’ experience in open source programming languages for large scale data analysis or simulation-at least 2 years’ experience with machine learning -at least 2 years’ experience with sql and database structures  preferred qualifications:-master’s degree or phd-at least 1 year experience working with aws-at least 4 years’ experience in python, scala, or java for large scale data analysis or simulation-at least 4 years’ experience with machine learning-at least 4 years’ experience with sql and other database query paradigms  capital one will consider sponsoring a new qualified applicant for employment authorization for this position. : interesting. what kind of “limits” are you reaching? rds is fully managed; so scalability shouldn’t be problem. if partitioning, that amount of data isn;t that lot. haven’t used spectrum; but another approach is using redshift(which is olap based postgres):thumbsup:hey guys,i\\'m a de fellow and i\\'m working on the visualization part of my project.i have data representing people on the  the us. in my speed layer i havedetected clusters of people  and i\\'m planning to show them on the map.does anyone have used maps for their project and has feedback on how to go about this? thanks.just spoke to nathan marz about the book he used to learn clojure, he recommended this one, fwiw:excellent… the 4clojure website got me demotivated pretty quickly. gotta learn it the right way@mpancia: what do you think? (i really just want to turn my coffee cup into a donut).  unclear what’s going on with ayasdi at all, i am happy to talk about it at length in privatequestion for the channel -- anyone using jupyterhub  in their data science teams for collaborative jupyter notebook work?separate but another python-related question -- are people who use python using version 2.x or 3.x?insight ai fellow  shows how to deploy a wavenet model on android using tensorflow hi, guys, anyone has experience leveraging azure luis service for text information extraction? how is it compared with self trained nlp models doing similar tasks? is it really usable or just a toy service? thanks a lot!his nickname is jj yet he has no j in him name :thinking_face:does anyone know which is a good library for generating fake data apart from faker in pythonhi guys, does anyone have any insights/links/experience that may be useful in dealing with extracting main topics from text? i know  would recommend eventsim. david, did you ever put together a write-up on how to use it? thanks!  i found the eventsim write-up:  lda and other topic modeling, tf-idf... you might check out this approach:  check out  data generator. it isn’t for big data, but the data types are diverse and it may sufficient for prototyping.thanks  !sure thing! :slightly_smiling_face:some data activism. there\\'s a chance that public climate datasets might not, shall we say, make it through the next few years (something which also happened with the harper administration).folks are working on both trying to mirror the datasets, and also pay for independent dedicated server space.  if either the mirroring or donating interest you, check out this resource:sent that to my friend lucky enough to have a large server and fiber internet :-)if you make your instance with a gpu it will have access  thank you!! that worked! i\\'m new to sql and spent so much time trying to figure this out yesterday to no avail. thank you!!interesting discussion about startup compensation:  second what till_be said. bq can be a butt about storing too much in memory. also, bq bills by gb processed, so be careful with running big queriesyou should get $300 for a free trial, which is plenty for an insight project!oh, got it. i work with bq a lot at my job now, so i have to watch how much i’m processingit’s still really good advice, you can speed up your queries a lot if you design them more carefully and only extract the data/rows you really needi had that problem for sure when  i did my projectthe guy who generated the reddit data for bigquery also makes the datasets available for direct download if you’d prefer to work with them locallyoh really? do you know where? i couldn’t find itbut please make a donation if you use it, the guy’s an absolute saint for generating and maintaining that set (scraper was running continuously for 8 months to generate the initial dump) and helping people with it, and downloads have to be slamming his resourceson a more general note, have folks compared bigquery to aws redshift? i\\'m not terribly familiar with google cloud platform offerings but would love to hear more. : thanks! and you’re right - i played with the scraper for a bit myself, and i can’t imagine how long it must have taken him to gather all that data.yeah, one call per second x something like 20 million api calls to build out the dataset from reddit founding up through fall 2014, then monthly dumps after thati really don’t love bq. it’s often quite a bit slower than redshift, and for similar storage and processing requirements, it’s substantially more expensive. we’re a bit stuck with it though, since some of our apps are built off the google app engine fundamentally do you want to deal with managing a database system?of course not :predshift is a managed service, but it still requires that you do a number of things to ingest and manage that data warehousegenerally if your use case is comprised of occasional ad-hoc queries on data living in an object store (s3/google cloud storage), bigquery might work really wellthose use cases might be similar to some use cases for hiveif you wanted a persistent resource/system, redshift will do that, but you’re going to take on a number of management considerationshi all, what tools/processes do you use to plan and manage your various projects?management? jira. plan? pen and paperkanban or scrum boards?plus a side `rst` file since my handwriting is terrible.i use onenote (quip is nice for this too) and i organize it as follows every quarter: (1) table stating project/ collaborators / next milestone (specific deliverables to that project) / eta(2) a calendar table for each end of week with the dates and on each cell the deliverable for the end of week (high level summary)if i don\\'t complete i just roll them over in terms of the eta and highlight them in yellow, if i complete on time i highlight them in green, and if scope changed/not_relevant i highlight them in red or just erased them sometime if no need to be there.i also sometimes add a detailed project descriptioni used jira, trello, etc. and all the fancy tools ended up being worse for me than just a document with tables and colors that i share with everyone and that collaborators can editmind that we are a small team, if the team was much larger maybe other tools would come in handyquick plug for the  channel, for those of us who deal with the challenges of working remotely or are looking for remote-friendly gigs. i know there are at least a handful of you out there.our team uses jira with scrum boards (or atleast we\\'re moving to that) thank you so much for the detailed answer. scrum boards in jira are the pain of my existence. (hope it works well for you, michelle!) a kanban-like board or table is the closest to a useful tool i have found so far.andrew ng’s coursera course of course.  tibshirani ‘s introduction to statistical learning is a great beginner book along with the free online course at stanford lagunita.   udacity’s intro to ds course.  and of course projects projects projects.  yeah i’m definitely going to have no problem getting the vp to fund a couple more months of testing after i have gotten something to work.  why didn’t i think of that before?and, of course, there is the obvious play of using this system to answer your phone for a small business…but that’s a bit out of google’s market, maybe a good play for squarei\\'m actually facing a very similar issue like this now. i would approach this two ways (ideally both). the first would be to train the model using different sizes of the corpus (e.g. randomly take a sample of 100, 1000, 10000 etc.) and train the model and see if/when the results converge.the second way would be: train the model x number of times, each time using a different bootstrap of the data, and see if the results are same for an independent testing set. you can do this quickly by using x=2 for different corpus sizes and just use the pearson correlation between the predictions.hi all, i think i need some of your brain here. i am trying to run a regression on 5gb of data (1million rows and 5400 features). this data is very sparse where most of the features are one hot-encoding from categorical features. i want to start with lasso or elastic net. but it seems like that my laptop (16gb memory) can not handle it. do you have any suggestions?look for a different feature selection method that doesn\\'t rely on fitting a regression on 5400 variables with 1 million rows :slightly_smiling_face:  or provision a really really large ec2 instancethen you can go back to the regression model once you\\'ve eliminated some features have you tried dimensionality reduction methods?yeah, that is the one that just come into my head. i’ll start with svd. or i can even just look at the correlation matrix.not sure as i haven\\'t used it before, but have you looked into the `dask` and `dask-ml` packages?now i thought more about dimension reduction. all the sparse features are domain_1, domain_2, domain_3,….., ip_1, ip_2, ip_3, ….. id_1, id_2, id_3, …… the dependent variable is `open_rate`. with dimension reduction,  if i remove domain_2 and ip_2, how can i get a predict value with domain_2 and ip_2? and with svd, it is very hard to implement the model.looks promising! :+1: i’ll take a look.what do you mean by \"how can i get a predict value with domain_2 and ip_2\" ?eventually, we would like to know  for domain_2 and ip_2, what is the predicted open_rate at this time of the day.another feature is time.the features are ```time, emails_delivered,  domain_1, domain_2, domain_3,….., ip_1, ip_2, ip_3, ….. id_1, id_2, id_3, …… ```the dependent variable is `open_rate`you can\\'t do both dimensionality reduction that combines multiple features into one and have the same amount of interpretability in the form of fitted coefficients.  you can do some interesting stuff like put in the ip_2 and domain_2 to your model and then sample different values for other features and see what it predicts on average - but all fo these types of methods will make various assumptions and have various issuesyou can try to use a feature selection method that just drops features it doesn\\'t like without re-combining them and then you\\'ll get coefficients for the non-dropped features, with the assumption that the dropped ones are unimportant so you won\\'t care about the predicted open_rate as a function of those.so if i drop domain_2 and ip_2, when i predict the `open_rate` for them, i just assume it does not dependent on these two features.yes, that\\'s what it means to drop those features.  the model won\\'t see them so how can it predict based on them?would you recommend using a forward/backward feature selection in this case?can\\'t really make a recommendation blind :smile:  use the one that works best for your model:+1: thank you so much! what about the old school stepwise regression? that will go feature by feature and select/remove as it grows... it\\'s not guaranteed to be optimal in anyway but i assume that you can run local in 16gb ram computer... do you mean forward/backward feature selection?yes great idea! i’ll definitely look into that. :blush:i\\'d load the data in chunks, use that data to train the model with batch gradient descent, rinse and repeat. if you\\'re using python, check out `sgdclassifier` in scikit-learn, which also has a parameter called `warm_start` that may be helpful for this.it\\'s  :)and we\\'ll have  for toronto :))haha love it!use domino or aws. there’s always a smarter way to do things, but i’ve found that if you just up the machine size, you can solve a lot of problems without having to spend much time on it, and your time is ususally more expensive than an aws machinieone other cool application they mentioned is that google could call \"all businesses\" and ask them if they are open on a given holiday and posting this info in their apps (instead of business hours might be affected...) - stepwise options exist for a reason, but they can be problematic. what do you know about the 5400 variables that you could use to reduce, outside of a statistical reduction technique (e.g., pca)? for example, are \\'domain_2\\' and \\'ip_2\\' related to each other? or do you expect them to have unique predictive value on your dv? seems there must be collinearity among that many variables so i would recommend doing some visualizations of the data to see what kinds of trends you see prior to putting them into a model. also, make sure you are using the right kind of model for your data, depending on the characteristics of the data (e.g., are the predictors mostly binary, are they normally-distributed, etc.)some reading on stepwise regression yeah, that was a genius use case!if you write your model in an sklearn pipeline (`input -&gt; truncatedsvd -&gt; linearregressor -&gt; output`), it is easy to use the same type of data for training and prediction. the pipeline model will handle performing the truncatedsvd the same way on both the training data and the data to be predicted. cross-validation conveniently works out of the box too.worst comes to worst, you can always email the recruiter/poc and ask what normal attire is and dress at or 1 level above thatdoes anyone use something like a \"model manager\"  that you are happy with (in your experience/at\\xa0your company)?by model manager i mean something that tracks the model and parameters being used in production, so that any results are saved and audit-able.this can be particularly important when the parameters are being updated with continuously with new data/training.might not be exactly what you are asking for but we use luigi around hereit is helpful when data is continuously updated and your model contains many dependenciesi guess nothing could stop you from adding your own lightweight manager tracker on top of that: could you add this link and a few thoughts to  ?thanks  i\\'ll take a look and pass that alongwe currently use luigi as task scheduler as well, but it is mainly for resolving dependencies among various tasks you run. (with very limited monitoring to see which tasks have been started/finished) it seems like you\\'re looking for a good logging framework that logs the model (version), parameter values and results?current silicon valley data science fellows - check out the  channel, and see the link there to the contact info googledoc that colin has startedhello all. i am trying to get my web-app running on aws and i get the following error in gunicorn: connection in use: (\\'0.0.0.0\\', 80)i have check to make sure that port 80 is not in use, and it is not.this is my netstat -tulpn:tcp        0      0 0.0.0.0:22              0.0.0.0:*               listen      925/sshd        tcp        0      0 127.0.0.1:5432          0.0.0.0:*               listen      1019/postgres   tcp6       0      0 :::22                   :::*                    listen      925/sshd        udp        0      0 0.0.0.0:68              0.0.0.0:*                           603/dhclient    udp        0      0 0.0.0.0:55074           0.0.0.0:*                           603/dhclient    udp6       0      0 :::31372                :::*                                603/dhclientand when it is running this is my netstat -tulpn:tcp        0      0 0.0.0.0:80              0.0.0.0:*               listen      -               tcp        0      0 0.0.0.0:22              0.0.0.0:*               listen      -               tcp        0      0 127.0.0.1:5432          0.0.0.0:*               listen      -               tcp        0      0 0.0.0.0:5000            0.0.0.0:*               listen      -               tcp6       0      0 :::22                   :::*                    listen      -               udp        0      0 0.0.0.0:68              0.0.0.0:*                           -               udp        0      0 0.0.0.0:55074           0.0.0.0:*                           -               udp6       0      0 :::31372                :::*                                -any advice would really help.any port &lt; 1024 needs superuser to run. so \"sudo command” may help things alongi am already running as root. thanks though.unless i need to sudo when already root?when i just run the web-app without gunicorn on port 80, everything works. so i guess for now i will forego gunicorn for my less robust solution.nope, missed that you were running it as root.that\\'s fine. i should have mentioned it in my original post. for now i have a workaround, so this is not an emergency.hi all - my wife is preparing to move down to the bay area and is looking for a position, preferably in the energy space but she is open to other areas. she recently got her phd in materials engineering and studied electrolyte blends for application in stretchable and flexible batteries, and essentially prototyped a working stretchable battery by the end of her 4th year. she has been trying to get conversations going with apple, tesla, and quite a few other companies in the area but hasn\\'t gotten too far. i was wondering if anyone has any connection to someone who may be good to talk to? in the big picture she is looking for something in business or product development, but is also open to starting out in r&amp;d and transitioning to a more bd or pd heavy role later on. if you have any possible leads, feel free to pm me and i can get you her resume and link her linkedin for you. thanks! i\\'ll be hosting one of the rooms on wednesday unfortunately, but i\\'ll be checking out the conference thursday, likely hanging out with the people at datascience :slightly_smiling_face:  would be cool to meet with any fellows who are attending!papadimitriou.c: yes, will be fun to meet up! registration opens at 8:30am (pst) for my stanford continuing studies workshop. learn tips and tricks to help your data science workflow!details here: i have the same story! see you guys thursday: is anyone using clojure? clojure with apache spark?: is scala not enough for you? :stuck_out_tongue_winking_eye:my boss is anti-scala but pro jvm languagestorm is written in clojurenathan marz can teach clojure in 2 minutes (whether anyone can learn it in 2 minutes has yet to be seen)between the apache and aws stacks, there are too many tools! :wink:: care to jump in here?why is boss anti-scala?i think the primary complaint is not the language itself but the scala that people actually writehaha yeah the number of tools that a company can use is overwhelming…spark is written scala though so i would recommend looking at sparkling thanks, austin. had come across that repo. have you used it?i have not actually. i could ask nathan marz about it when he comes in for the next session as well.cool, thanks.i haven’t went thru it yet, but i’ve heard good things about aphyr’s blog learning clojure:thanks.there’s also looks like google open-sourced colaboratory, which is something like jupyter on google drive:anyone else going to odsc in boston?yepnice, i\\'ll be there all 4 days, maybe we could set up a time/place to meet up during one of the coffee breaks or lunches and have a little insight group. :slightly_smiling_face:great idea - we insight folks can ping the boston and nyc alum to facilitate a gathering. not sure when/where would be best. any thoughts?i\\'ll look around and get back to you. need coordinate with the other folks from the office to make sure we don\\'t have something planned.i\\'ll try to respond by tomorrow afternoon.@ joao i think this may be true as well...its just something that i was trying to make sense of. the way you put it. that makes sense. thanks everyone for point these things out :)how about just using a lasso penalty with logistic regression?  that tends to drive less important feature coefficients to zero.  maybe i am misunderstanding your question.  i prefer using `conda` environments over `virtualenv` in almost every context; polygon and census folks! i\\'m trying to normalize by population. census data is by tract which are small odd shaped polygon regions and i have a rectangular query which may overlap portions of three of four of the tracts. has anyone dealt with extracting portions of data from overlapping polygons?i\\'m not sure exactly what you mean by that -- do you want, like, a percentage of area for each tract that intersects the rectangles?yes basically that, but i think the package shapely might solve all my problems.sure; in r you could pretty easily use `sp` and `rgeos` to get the areas of the pairwise intersectionsthanks!i just had to do this, so :simple_smile: uploaded a file: i\\'m building a model for predicting heart disease using a webcam, and am sorely in need of good data. i\\'d really appreciate it if you guys could record a minute of your heart activity here:  . it only takes a webcam, a desk lamp or smartphone flashlight, and 1 minute of your time. if you could send this link on to anybody you know who does have heart disease, i\\'d be forever in your debt!!! (for now, i only have 14 positive data points; and am supposed to have a \"finished product\" this friday... i have good evidence that my approach works on clinical interbeat interval data; but need webcam data to back up the claim on the web page)find out who is spying on your cell phone: apparently the uk ministry of defence is interested in me.hashcat can now try 8 million passwords per second. and apparently up to 250 billion per second with 8 nvidia titans depending on encryption type.well that\\'s unsettlinghappy thanksgiving all 539 is asking *you* to help them predict the oscars! okay, ill give it a shotthe oscars: an awards shownuggetbot animate me smart ass:simple_smile:crassthe black masking just covers up cells for when the survey was missing that question, or after that question was removed. for example, i changed how i asked about equity midway through the survey, and i added age/ethnicity late into the survey. granted, this is absolutely horrible practice, but i wanted to ball-park the numbers - at least to provide low-confidence prior in the worst case scenario. i\\'ll eventually take what i learned from this survey and make a new, better one, but i\\'d like to avoid spamming folks about their income.i cleaned up the raw survey data, merging cells from different questions on the \\'cleaned\\' sheet, which also has some masking due to the age/gender/ethnicity questions.good polereminds me of i’m interested!is deep learning going to take all of our jobs? i don’t know, but it is starting with front-end developers! check out this awesome project by ai fellow  tools for fe development have always gotten better thoughyes, but having them automated to the extent where you can go from a very specific data input (sketches), to a specific output (website with precise styling) is very exciting to me!what were the other 2.5, i thought there were basically 3and safer than self-driving cars :+1:how and where companies are applying machine learning:   here is an example i used in the past. it involves retrieving an access token thanks!!!!!!i\\'d like to learn as much as possible about practical database design and maintenance (ideally using postgres) in as little time as possible. i have in mind topics such as: how to choose the number of tables, how to think about keys, dealing with time series, etc. any protips for sources or other suggestions?does anyone here use beaker notebook over ipython notebook?'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_re4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove :: emojis \n",
    "tag_re5 = re.sub(':[^0-9]+:', '', tag_re3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "re_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "gen_text_tokens = re_tokenizer.tokenize(tag_re5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'sure', 'if', 'this', 'is', 'a', 'joke', 'or', 'for', 'real', 'that', 's', 'a', 'funny', 'joke', 'but', 'certainly', 'not', 'a', 'viable', 'business', 'rates', 'are', 'too', 'low', 'the', 'cheapest', 'plan', 'is', '15', 'a', 'month', 'for', '2', 'scoops', 'a', 'day', 'which', 'comes', 'out', 'to', 'less', 'than', 'a', 'quarter', 'per', 'scoop', 'that', 's', 'not', 'going', 'to', 'be', 'worth', 'anyone', 's', 'trip', 'nor', 'is', 'there', 'any', 'feasible', 'mechanism', 'accountability', 'unless', 'the', 'poopers', 'wait', 'around', 'for', 'the', 'scoopers', 'which', 'the', 'video', 'says', 'they', 'don', 't', 'need', 'to', 'do', 'anything', 'they', 'might', 'do', 'to', 'implement', 'accountability', 'e', 'g', 'require', 'scoopers', 'to', 'stream', 'live', 'videos', 'of', 'the', 'scooping']\n"
     ]
    }
   ],
   "source": [
    "print(gen_text_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "\n",
    "cleaned_tokens = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for token in gen_text_tokens:\n",
    "    if token not in stop_words:\n",
    "        cleaned_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before removing stop words: 128656\n",
      "Number of tokens after removing stop words: 68213\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens before removing stop words: %d' % len(gen_text_tokens))\n",
    "print('Number of tokens after removing stop words: %d' % len(cleaned_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'joke', 'real', 'funny', 'joke', 'certainly', 'viable', 'business', 'rates', 'low', 'cheapest', 'plan', '15', 'month', '2', 'scoops', 'day', 'comes', 'less', 'quarter', 'per', 'scoop', 'going', 'worth', 'anyone', 'trip', 'feasible', 'mechanism', 'accountability', 'unless', 'poopers', 'wait', 'around', 'scoopers', 'video', 'says', 'need', 'anything', 'might', 'implement', 'accountability', 'e', 'g', 'require', 'scoopers', 'stream', 'live', 'videos', 'scooping', 'would']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alison/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "#lemmatize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = []\n",
    "\n",
    "for token in cleaned_tokens:\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'joke', 'real', 'funny', 'joke', 'certainly', 'viable', 'business', 'rate', 'low', 'cheapest', 'plan', '15', 'month', '2', 'scoop', 'day', 'come', 'le', 'quarter', 'per', 'scoop', 'going', 'worth', 'anyone', 'trip', 'feasible', 'mechanism', 'accountability', 'unless', 'poopers', 'wait', 'around', 'scoopers', 'video', 'say', 'need', 'anything', 'might', 'implement', 'accountability', 'e', 'g', 'require', 'scoopers', 'stream', 'live', 'video', 'scooping', 'would']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "lemm_text = ' '.join(wd for wd in lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9545c410df28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfidf_article_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemm_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/insight/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/insight/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             raise ValueError(\n\u001b[0;32m--> 860\u001b[0;31m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m                 \"string object received.\")\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "tfidf_article_matrix = tf_vectorizer.fit_transform(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
